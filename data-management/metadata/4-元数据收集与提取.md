# 第4章：元数据收集与提取

## 4.1 元数据收集方法

### 4.1.1 自动收集技术

元数据自动收集是通过技术手段自动获取数据源的元数据信息，减少人工干预并提高效率。

**数据库元数据收集**

```python
import pymysql
import psycopg2
from pymongo import MongoClient
from abc import ABC, abstractmethod

class DatabaseMetadataExtractor(ABC):
    """数据库元数据提取器基类"""
    
    def __init__(self, connection_config):
        self.connection_config = connection_config
        self.connection = None
    
    @abstractmethod
    def connect(self):
        """连接数据库"""
        pass
    
    @abstractmethod
    def disconnect(self):
        """断开数据库连接"""
        pass
    
    @abstractmethod
    def extract_schema_metadata(self, schema_name=None):
        """提取模式元数据"""
        pass
    
    @abstractmethod
    def extract_table_metadata(self, schema_name, table_name):
        """提取表元数据"""
        pass
    
    @abstractmethod
    def extract_column_metadata(self, schema_name, table_name):
        """提取列元数据"""
        pass

class MySQLMetadataExtractor(DatabaseMetadataExtractor):
    """MySQL元数据提取器"""
    
    def connect(self):
        """连接MySQL数据库"""
        try:
            self.connection = pymysql.connect(
                host=self.connection_config["host"],
                port=self.connection_config.get("port", 3306),
                user=self.connection_config["user"],
                password=self.connection_config["password"],
                database=self.connection_config.get("database")
            )
            return True
        except Exception as e:
            print(f"连接MySQL失败: {e}")
            return False
    
    def disconnect(self):
        """断开MySQL连接"""
        if self.connection:
            self.connection.close()
    
    def extract_schema_metadata(self, schema_name=None):
        """提取MySQL模式元数据"""
        if not self.connection:
            self.connect()
        
        cursor = self.connection.cursor()
        
        # 获取数据库列表
        if schema_name:
            schema_query = f"SHOW DATABASES LIKE '{schema_name}'"
        else:
            schema_query = "SHOW DATABASES"
        
        cursor.execute(schema_query)
        schemas = []
        
        for row in cursor.fetchall():
            db_name = row[0]
            if db_name not in ['information_schema', 'mysql', 'performance_schema', 'sys']:
                schemas.append({
                    "name": db_name,
                    "type": "database",
                    "charset": self._get_database_charset(db_name)
                })
        
        return schemas
    
    def extract_table_metadata(self, schema_name):
        """提取MySQL表元数据"""
        if not self.connection:
            self.connect()
        
        cursor = self.connection.cursor()
        cursor.execute(f"USE {schema_name}")
        
        # 获取表列表
        cursor.execute("SHOW TABLE STATUS")
        tables = []
        
        for row in cursor.fetchall():
            table_name = row[0]
            engine = row[1]
            row_count = row[4]
            data_size = row[6]
            index_size = row[8]
            collation = row[14]
            comment = row[17]
            
            tables.append({
                "name": table_name,
                "schema": schema_name,
                "engine": engine,
                "row_count": row_count,
                "data_size": data_size,
                "index_size": index_size,
                "collation": collation,
                "comment": comment
            })
        
        return tables
    
    def extract_column_metadata(self, schema_name, table_name):
        """提取MySQL列元数据"""
        if not self.connection:
            self.connect()
        
        cursor = self.connection.cursor()
        cursor.execute(f"USE {schema_name}")
        
        # 获取列信息
        cursor.execute(f"DESCRIBE {table_name}")
        columns = []
        
        for row in cursor.fetchall():
            field_name = row[0]
            field_type = row[1]
            is_nullable = row[2] == "YES"
            key = row[3]
            default_value = row[4]
            extra = row[5]
            
            columns.append({
                "name": field_name,
                "type": field_type,
                "nullable": is_nullable,
                "key": key,
                "default": default_value,
                "extra": extra
            })
        
        return columns
    
    def extract_constraints(self, schema_name, table_name):
        """提取约束信息"""
        if not self.connection:
            self.connect()
        
        cursor = self.connection.cursor()
        
        # 获取外键约束
        cursor.execute(f"""
            SELECT 
                COLUMN_NAME, 
                REFERENCED_TABLE_SCHEMA, 
                REFERENCED_TABLE_NAME, 
                REFERENCED_COLUMN_NAME
            FROM 
                INFORMATION_SCHEMA.KEY_COLUMN_USAGE 
            WHERE 
                TABLE_SCHEMA = %s AND 
                TABLE_NAME = %s AND 
                REFERENCED_TABLE_NAME IS NOT NULL
        """, (schema_name, table_name))
        
        foreign_keys = []
        for row in cursor.fetchall():
            foreign_keys.append({
                "column": row[0],
                "referenced_schema": row[1],
                "referenced_table": row[2],
                "referenced_column": row[3]
            })
        
        return foreign_keys
    
    def extract_indexes(self, schema_name, table_name):
        """提取索引信息"""
        if not self.connection:
            self.connect()
        
        cursor = self.connection.cursor()
        
        cursor.execute(f"""
            SELECT 
                INDEX_NAME, 
                COLUMN_NAME, 
                NON_UNIQUE,
                SEQ_IN_INDEX
            FROM 
                INFORMATION_SCHEMA.STATISTICS 
            WHERE 
                TABLE_SCHEMA = %s AND 
                TABLE_NAME = %s
            ORDER BY 
                INDEX_NAME, SEQ_IN_INDEX
        """, (schema_name, table_name))
        
        indexes = {}
        for row in cursor.fetchall():
            index_name = row[0]
            column_name = row[1]
            non_unique = row[2]
            seq_in_index = row[3]
            
            if index_name not in indexes:
                indexes[index_name] = {
                    "name": index_name,
                    "unique": not non_unique,
                    "columns": []
                }
            
            indexes[index_name]["columns"].append(column_name)
        
        return list(indexes.values())
    
    def _get_database_charset(self, db_name):
        """获取数据库字符集"""
        if not self.connection:
            self.connect()
        
        cursor = self.connection.cursor()
        cursor.execute(f"""
            SELECT DEFAULT_CHARACTER_SET_NAME 
            FROM INFORMATION_SCHEMA.SCHEMATA 
            WHERE SCHEMA_NAME = %s
        """, (db_name,))
        
        result = cursor.fetchone()
        return result[0] if result else None

# PostgreSQL元数据提取器
class PostgreSQLMetadataExtractor(DatabaseMetadataExtractor):
    """PostgreSQL元数据提取器"""
    
    def connect(self):
        """连接PostgreSQL数据库"""
        try:
            self.connection = psycopg2.connect(
                host=self.connection_config["host"],
                port=self.connection_config.get("port", 5432),
                user=self.connection_config["user"],
                password=self.connection_config["password"],
                database=self.connection_config.get("database")
            )
            return True
        except Exception as e:
            print(f"连接PostgreSQL失败: {e}")
            return False
    
    def disconnect(self):
        """断开PostgreSQL连接"""
        if self.connection:
            self.connection.close()
    
    def extract_schema_metadata(self, schema_name=None):
        """提取PostgreSQL模式元数据"""
        if not self.connection:
            self.connect()
        
        cursor = self.connection.cursor()
        
        # 获取模式列表
        if schema_name:
            schema_query = """
                SELECT schema_name 
                FROM information_schema.schemata 
                WHERE schema_name = %s
            """
            cursor.execute(schema_query, (schema_name,))
        else:
            schema_query = """
                SELECT schema_name 
                FROM information_schema.schemata 
                WHERE schema_name NOT IN ('information_schema', 'pg_catalog', 'pg_toast')
            """
            cursor.execute(schema_query)
        
        schemas = []
        for row in cursor.fetchall():
            schemas.append({
                "name": row[0],
                "type": "schema"
            })
        
        return schemas
    
    def extract_table_metadata(self, schema_name):
        """提取PostgreSQL表元数据"""
        if not self.connection:
            self.connect()
        
        cursor = self.connection.cursor()
        
        # 获取表列表
        cursor.execute("""
            SELECT 
                table_name,
                table_type
            FROM information_schema.tables
            WHERE table_schema = %s
        """, (schema_name,))
        
        tables = []
        for row in cursor.fetchall():
            table_name = row[0]
            table_type = row[1]
            
            # 获取表统计信息
            cursor.execute("""
                SELECT 
                    n_tup_ins,
                    n_tup_upd,
                    n_tup_del,
                    n_live_tup,
                    n_dead_tup
                FROM pg_stat_user_tables
                WHERE schemaname = %s AND relname = %s
            """, (schema_name, table_name))
            
            stats = cursor.fetchone()
            
            tables.append({
                "name": table_name,
                "schema": schema_name,
                "type": table_type,
                "inserts": stats[0] if stats else 0,
                "updates": stats[1] if stats else 0,
                "deletes": stats[2] if stats else 0,
                "live_tuples": stats[3] if stats else 0,
                "dead_tuples": stats[4] if stats else 0
            })
        
        return tables
    
    def extract_column_metadata(self, schema_name, table_name):
        """提取PostgreSQL列元数据"""
        if not self.connection:
            self.connect()
        
        cursor = self.connection.cursor()
        
        cursor.execute("""
            SELECT 
                column_name,
                data_type,
                character_maximum_length,
                is_nullable,
                column_default
            FROM information_schema.columns
            WHERE table_schema = %s AND table_name = %s
            ORDER BY ordinal_position
        """, (schema_name, table_name))
        
        columns = []
        for row in cursor.fetchall():
            column_name = row[0]
            data_type = row[1]
            max_length = row[2]
            is_nullable = row[3] == "YES"
            default_value = row[4]
            
            columns.append({
                "name": column_name,
                "type": data_type,
                "max_length": max_length,
                "nullable": is_nullable,
                "default": default_value
            })
        
        return columns
    
    def extract_constraints(self, schema_name, table_name):
        """提取PostgreSQL约束信息"""
        if not self.connection:
            self.connect()
        
        cursor = self.connection.cursor()
        
        # 获取外键约束
        cursor.execute("""
            SELECT
                tc.constraint_name,
                kcu.column_name,
                ccu.table_schema AS foreign_table_schema,
                ccu.table_name AS foreign_table_name,
                ccu.column_name AS foreign_column_name
            FROM information_schema.table_constraints AS tc
            JOIN information_schema.key_column_usage AS kcu
                ON tc.constraint_name = kcu.constraint_name
                AND tc.table_schema = kcu.table_schema
            JOIN information_schema.constraint_column_usage AS ccu
                ON ccu.constraint_name = tc.constraint_name
                AND ccu.table_schema = tc.table_schema
            WHERE tc.constraint_type = 'FOREIGN KEY'
                AND tc.table_schema = %s
                AND tc.table_name = %s
        """, (schema_name, table_name))
        
        foreign_keys = []
        for row in cursor.fetchall():
            foreign_keys.append({
                "name": row[0],
                "column": row[1],
                "foreign_schema": row[2],
                "foreign_table": row[3],
                "foreign_column": row[4]
            })
        
        return foreign_keys

# MongoDB元数据提取器
class MongoMetadataExtractor(DatabaseMetadataExtractor):
    """MongoDB元数据提取器"""
    
    def connect(self):
        """连接MongoDB"""
        try:
            connection_string = f"mongodb://{self.connection_config.get('host', 'localhost')}:{self.connection_config.get('port', 27017)}"
            self.client = MongoClient(connection_string)
            if "database" in self.connection_config:
                self.connection = self.client[self.connection_config["database"]]
            return True
        except Exception as e:
            print(f"连接MongoDB失败: {e}")
            return False
    
    def disconnect(self):
        """断开MongoDB连接"""
        if hasattr(self, 'client') and self.client:
            self.client.close()
    
    def extract_schema_metadata(self, schema_name=None):
        """提取MongoDB数据库元数据"""
        if not self.connect():
            return []
        
        databases = []
        for db_name in self.client.list_database_names():
            if db_name not in ['admin', 'config', 'local']:
                db = self.client[db_name]
                stats = db.command("dbStats")
                
                databases.append({
                    "name": db_name,
                    "type": "database",
                    "size": stats.get("dataSize", 0),
                    "collections": stats.get("collections", 0)
                })
        
        return databases
    
    def extract_table_metadata(self, schema_name):
        """提取MongoDB集合元数据"""
        if not self.connect():
            return []
        
        db = self.client[schema_name]
        collections = []
        
        for collection_name in db.list_collection_names():
            collection = db[collection_name]
            stats = db.command("collStats", collection_name)
            
            collections.append({
                "name": collection_name,
                "schema": schema_name,
                "type": "collection",
                "count": stats.get("count", 0),
                "size": stats.get("size", 0),
                "avg_size": stats.get("avgObjSize", 0),
                "indexes": stats.get("nindexes", 0)
            })
        
        return collections
    
    def extract_column_metadata(self, schema_name, table_name):
        """提取MongoDB字段元数据"""
        if not self.connect():
            return []
        
        db = self.client[schema_name]
        collection = db[table_name]
        
        # 获取样本文档并分析字段
        sample_docs = list(collection.find().limit(100))
        field_stats = {}
        
        for doc in sample_docs:
            for field_name, value in doc.items():
                if field_name not in field_stats:
                    field_stats[field_name] = {
                        "name": field_name,
                        "types": set(),
                        "count": 0,
                        "null_count": 0
                    }
                
                field_stats[field_name]["count"] += 1
                
                if value is None:
                    field_stats[field_name]["null_count"] += 1
                else:
                    field_stats[field_name]["types"].add(type(value).__name__)
        
        # 转换为结果格式
        columns = []
        for field_name, stats in field_stats.items():
            columns.append({
                "name": field_name,
                "types": list(stats["types"]),
                "count": stats["count"],
                "null_count": stats["null_count"],
                "null_ratio": stats["null_count"] / stats["count"] if stats["count"] > 0 else 0
            })
        
        return columns

# 使用示例
def demo_database_metadata_extraction():
    """演示数据库元数据提取"""
    print("=" * 50)
    print("4.1.1 数据库元数据自动收集示例")
    print("=" * 50)
    
    # MySQL连接配置
    mysql_config = {
        "host": "localhost",
        "port": 3306,
        "user": "root",
        "password": "password",
        "database": "ecommerce"
    }
    
    # 创建MySQL元数据提取器
    mysql_extractor = MySQLMetadataExtractor(mysql_config)
    
    # 连接并提取元数据
    if mysql_extractor.connect():
        print("成功连接到MySQL数据库")
        
        # 提取表元数据
        tables = mysql_extractor.extract_table_metadata("ecommerce")
        print(f"MySQL中的表数量: {len(tables)}")
        
        # 提取第一个表的列元数据
        if tables:
            first_table = tables[0]
            columns = mysql_extractor.extract_column_metadata("ecommerce", first_table["name"])
            print(f"表 {first_table['name']} 的列数量: {len(columns)}")
            
            # 提取约束信息
            constraints = mysql_extractor.extract_constraints("ecommerce", first_table["name"])
            print(f"表 {first_table['name']} 的约束数量: {len(constraints)}")
        
        mysql_extractor.disconnect()
    
    return mysql_extractor
```

**文件系统元数据收集**

```python
import os
import hashlib
from pathlib import Path
import pandas as pd
import json
from datetime import datetime

class FileSystemMetadataExtractor:
    """文件系统元数据提取器"""
    
    def __init__(self, base_path):
        self.base_path = Path(base_path)
        self.supported_formats = ['.csv', '.json', '.parquet', '.xlsx', '.txt', '.log']
    
    def extract_directory_metadata(self, directory_path=None):
        """提取目录元数据"""
        if directory_path:
            target_dir = Path(directory_path)
        else:
            target_dir = self.base_path
        
        if not target_dir.exists() or not target_dir.is_dir():
            return None
        
        # 获取目录信息
        stat = target_dir.stat()
        
        # 计算子目录和文件数量
        subdirs = [d for d in target_dir.iterdir() if d.is_dir()]
        files = [f for f in target_dir.iterdir() if f.is_file()]
        
        return {
            "name": target_dir.name,
            "path": str(target_dir),
            "type": "directory",
            "created_at": datetime.fromtimestamp(stat.st_ctime),
            "modified_at": datetime.fromtimestamp(stat.st_mtime),
            "size": sum(f.stat().st_size for f in files),
            "subdirectories": len(subdirs),
            "files": len(files)
        }
    
    def extract_file_metadata(self, file_path):
        """提取文件元数据"""
        file_path = Path(file_path)
        
        if not file_path.exists() or not file_path.is_file():
            return None
        
        stat = file_path.stat()
        
        # 基本文件信息
        metadata = {
            "name": file_path.name,
            "path": str(file_path),
            "type": "file",
            "extension": file_path.suffix,
            "size": stat.st_size,
            "created_at": datetime.fromtimestamp(stat.st_ctime),
            "modified_at": datetime.fromtimestamp(stat.st_mtime),
            "mime_type": self._get_mime_type(file_path)
        }
        
        # 计算文件哈希
        metadata["md5"] = self._calculate_file_hash(file_path)
        
        # 如果是支持的数据文件，提取内容元数据
        if file_path.suffix in self.supported_formats:
            content_metadata = self._extract_content_metadata(file_path)
            if content_metadata:
                metadata.update(content_metadata)
        
        return metadata
    
    def extract_directory_tree(self, max_depth=3):
        """提取目录树元数据"""
        def _traverse_directory(directory, depth):
            if depth > max_depth:
                return None
            
            dir_metadata = self.extract_directory_metadata(directory)
            if not dir_metadata:
                return None
            
            dir_metadata["children"] = []
            
            # 遍历子目录和文件
            for item in sorted(directory.iterdir()):
                if item.is_dir():
                    child_metadata = _traverse_directory(item, depth + 1)
                    if child_metadata:
                        dir_metadata["children"].append(child_metadata)
                elif item.is_file() and item.suffix in self.supported_formats:
                    file_metadata = self.extract_file_metadata(item)
                    if file_metadata:
                        dir_metadata["children"].append(file_metadata)
            
            return dir_metadata
        
        return _traverse_directory(self.base_path, 0)
    
    def _get_mime_type(self, file_path):
        """获取文件MIME类型"""
        # 简化实现，实际应用中可以使用python-magic库
        extension_mapping = {
            '.csv': 'text/csv',
            '.json': 'application/json',
            '.parquet': 'application/octet-stream',
            '.xlsx': 'application/vnd.openxmlformats-officedocument.spreadsheetml.sheet',
            '.txt': 'text/plain',
            '.log': 'text/plain'
        }
        
        return extension_mapping.get(file_path.suffix, 'application/octet-stream')
    
    def _calculate_file_hash(self, file_path, algorithm='md5', chunk_size=8192):
        """计算文件哈希值"""
        hash_func = hashlib.new(algorithm)
        
        with open(file_path, 'rb') as f:
            while chunk := f.read(chunk_size):
                hash_func.update(chunk)
        
        return hash_func.hexdigest()
    
    def _extract_content_metadata(self, file_path):
        """提取文件内容元数据"""
        try:
            if file_path.suffix == '.csv':
                return self._extract_csv_metadata(file_path)
            elif file_path.suffix == '.json':
                return self._extract_json_metadata(file_path)
            elif file_path.suffix == '.parquet':
                return self._extract_parquet_metadata(file_path)
            elif file_path.suffix in ['.xlsx', '.xls']:
                return self._extract_excel_metadata(file_path)
            elif file_path.suffix in ['.txt', '.log']:
                return self._extract_text_metadata(file_path)
        except Exception as e:
            print(f"提取文件内容元数据失败: {e}")
        
        return None
    
    def _extract_csv_metadata(self, file_path):
        """提取CSV文件元数据"""
        # 读取前几行以获取列信息
        df = pd.read_csv(file_path, nrows=10)
        
        return {
            "content_type": "csv",
            "columns": list(df.columns),
            "column_count": len(df.columns),
            "estimated_rows": self._estimate_csv_rows(file_path),
            "encoding": self._detect_encoding(file_path)
        }
    
    def _extract_json_metadata(self, file_path):
        """提取JSON文件元数据"""
        with open(file_path, 'r', encoding='utf-8') as f:
            try:
                data = json.load(f)
                
                if isinstance(data, list):
                    if data and isinstance(data[0], dict):
                        return {
                            "content_type": "json_array",
                            "estimated_records": len(data),
                            "fields": list(data[0].keys()) if data else []
                        }
                elif isinstance(data, dict):
                    return {
                        "content_type": "json_object",
                        "fields": list(data.keys())
                    }
                
                return {
                    "content_type": "json",
                    "structure": type(data).__name__
                }
            except json.JSONDecodeError:
                return {
                    "content_type": "json_invalid"
                }
    
    def _extract_parquet_metadata(self, file_path):
        """提取Parquet文件元数据"""
        try:
            import pyarrow.parquet as pq
            table = pq.read_table(file_path)
            
            return {
                "content_type": "parquet",
                "columns": [field.name for field in table.schema],
                "column_count": table.num_columns,
                "estimated_rows": table.num_rows,
                "schema": str(table.schema)
            }
        except ImportError:
            # 如果没有安装pyarrow，尝试使用pandas
            try:
                df = pd.read_parquet(file_path)
                return {
                    "content_type": "parquet",
                    "columns": list(df.columns),
                    "column_count": len(df.columns),
                    "estimated_rows": len(df)
                }
            except Exception:
                return {
                    "content_type": "parquet",
                    "error": "无法读取parquet文件，需要安装pyarrow库"
                }
    
    def _extract_excel_metadata(self, file_path):
        """提取Excel文件元数据"""
        try:
            # 读取第一个工作表的基本信息
            df = pd.read_excel(file_path, nrows=10)
            
            return {
                "content_type": "excel",
                "columns": list(df.columns),
                "column_count": len(df.columns),
                "estimated_rows": self._estimate_excel_rows(file_path)
            }
        except Exception:
            return {
                "content_type": "excel",
                "error": "无法读取Excel文件，可能需要安装openpyxl库"
            }
    
    def _extract_text_metadata(self, file_path):
        """提取文本文件元数据"""
        encoding = self._detect_encoding(file_path)
        
        with open(file_path, 'r', encoding=encoding, errors='ignore') as f:
            # 读取前1000行
            lines = [line.rstrip() for line in f.readlines(1000)]
            line_count = len(lines)
            
            # 估算总行数
            sample_size = os.path.getsize(file_path)
            sample_content = ''.join(lines[:100])  # 前100行
            sample_line_length = len(sample_content) / min(100, line_count)
            estimated_total_lines = int(sample_size / sample_line_length) if sample_line_length > 0 else line_count
            
            return {
                "content_type": "text",
                "encoding": encoding,
                "estimated_lines": estimated_total_lines,
                "line_ending": self._detect_line_ending(lines)
            }
    
    def _estimate_csv_rows(self, file_path):
        """估算CSV文件行数"""
        with open(file_path, 'r', encoding=self._detect_encoding(file_path)) as f:
            # 读取1MB的数据计算平均行长度
            sample_size = min(1024 * 1024, os.path.getsize(file_path))
            sample = f.read(sample_size)
            lines = sample.count('\n')
            
            if lines > 0:
                avg_line_length = sample_size / lines
                total_size = os.path.getsize(file_path)
                return int(total_size / avg_line_length)
        
        return 0
    
    def _estimate_excel_rows(self, file_path):
        """估算Excel文件行数"""
        try:
            from openpyxl import load_workbook
            
            wb = load_workbook(file_path, read_only=True)
            ws = wb.active
            row_count = ws.max_row
            wb.close()
            
            return row_count
        except ImportError:
            return 0
    
    def _detect_encoding(self, file_path):
        """检测文件编码"""
        import chardet
        
        with open(file_path, 'rb') as f:
            raw_data = f.read(10000)  # 读取前10KB用于检测
            result = chardet.detect(raw_data)
            return result.get('encoding', 'utf-8')
    
    def _detect_line_ending(self, sample_lines):
        """检测行结束符"""
        if not sample_lines:
            return "unknown"
        
        sample = '\n'.join(sample_lines[:50])
        
        if '\r\n' in sample:
            return "CRLF"
        elif '\r' in sample:
            return "CR"
        else:
            return "LF"

def demo_file_system_metadata_extraction():
    """演示文件系统元数据提取"""
    print("\n" + "=" * 50)
    print("4.1.1 文件系统元数据自动收集示例")
    print("=" * 50)
    
    # 创建临时目录和文件用于演示
    import tempfile
    
    with tempfile.TemporaryDirectory() as temp_dir:
        # 创建一些示例文件
        csv_path = os.path.join(temp_dir, "sample.csv")
        with open(csv_path, 'w') as f:
            f.write("id,name,age\n")
            f.write("1,Alice,30\n")
            f.write("2,Bob,25\n")
            f.write("3,Charlie,35\n")
        
        json_path = os.path.join(temp_dir, "sample.json")
        with open(json_path, 'w') as f:
            json.dump([
                {"id": 1, "name": "Alice", "age": 30},
                {"id": 2, "name": "Bob", "age": 25}
            ], f)
        
        txt_path = os.path.join(temp_dir, "sample.txt")
        with open(txt_path, 'w') as f:
            f.write("This is a sample text file.\n")
            f.write("It contains multiple lines.\n")
            f.write("For testing metadata extraction.\n")
        
        # 创建文件系统元数据提取器
        extractor = FileSystemMetadataExtractor(temp_dir)
        
        # 提取目录元数据
        dir_metadata = extractor.extract_directory_metadata()
        print(f"目录元数据:")
        print(f"  路径: {dir_metadata['path']}")
        print(f"  文件数: {dir_metadata['files']}")
        print(f"  子目录数: {dir_metadata['subdirectories']}")
        
        # 提取文件元数据
        csv_metadata = extractor.extract_file_metadata(csv_path)
        print(f"\nCSV文件元数据:")
        print(f"  文件名: {csv_metadata['name']}")
        print(f"  大小: {csv_metadata['size']} 字节")
        print(f"  内容类型: {csv_metadata.get('content_type')}")
        if 'columns' in csv_metadata:
            print(f"  列: {csv_metadata['columns']}")
        
        json_metadata = extractor.extract_file_metadata(json_path)
        print(f"\nJSON文件元数据:")
        print(f"  文件名: {json_metadata['name']}")
        print(f"  内容类型: {json_metadata.get('content_type')}")
        if 'estimated_records' in json_metadata:
            print(f"  估计记录数: {json_metadata['estimated_records']}")
        if 'fields' in json_metadata:
            print(f"  字段: {json_metadata['fields']}")
        
        return extractor
```

### 4.1.2 半自动化收集方法

半自动化收集结合了自动化技术和人工干预，适用于需要专业知识或判断的场景。

```python
import json
from datetime import datetime
from typing import Dict, List, Any

class SemiAutomatedMetadataCollector:
    """半自动化元数据收集器"""
    
    def __init__(self):
        self.collected_metadata = {}
        self.manual_reviews = []
        self.collection_rules = {}
    
    def configure_collection_rules(self, rules):
        """配置收集规则"""
        self.collection_rules = rules
    
    def auto_collect_basic_metadata(self, source_info):
        """自动收集基础元数据"""
        metadata_id = f"{source_info['type']}_{source_info['name']}_{int(datetime.now().timestamp())}"
        
        # 基础元数据
        basic_metadata = {
            "name": source_info.get("name"),
            "type": source_info.get("type"),
            "source": source_info.get("source"),
            "location": source_info.get("location"),
            "created_at": datetime.now(),
            "auto_generated": True
        }
        
        # 根据数据源类型收集特定元数据
        if source_info.get("type") == "api":
            api_metadata = self._collect_api_metadata(source_info)
            basic_metadata.update(api_metadata)
        elif source_info.get("type") == "file":
            file_metadata = self._collect_file_metadata(source_info)
            basic_metadata.update(file_metadata)
        elif source_info.get("type") == "database_table":
            db_metadata = self._collect_db_metadata(source_info)
            basic_metadata.update(db_metadata)
        
        # 存储自动收集的元数据
        self.collected_metadata[metadata_id] = {
            "basic": basic_metadata,
            "detailed": {},
            "manual_review_required": self._check_review_needed(basic_metadata)
        }
        
        return metadata_id
    
    def _collect_api_metadata(self, api_info):
        """收集API元数据"""
        metadata = {
            "endpoint": api_info.get("endpoint"),
            "method": api_info.get("method", "GET"),
            "parameters": api_info.get("parameters", []),
            "headers": api_info.get("headers", {}),
            "response_format": api_info.get("response_format", "json")
        }
        
        # 尝试自动推断参数类型
        if "parameters" in metadata:
            for param in metadata["parameters"]:
                if "type" not in param:
                    param["type"] = self._infer_parameter_type(param)
        
        return metadata
    
    def _collect_file_metadata(self, file_info):
        """收集文件元数据"""
        import os
        from pathlib import Path
        
        file_path = Path(file_info.get("path"))
        
        if not file_path.exists():
            return {"error": "文件不存在"}
        
        stat = file_path.stat()
        
        metadata = {
            "size": stat.st_size,
            "extension": file_path.suffix,
            "modified_at": datetime.fromtimestamp(stat.st_mtime),
            "encoding": self._detect_file_encoding(file_path) if file_path.suffix in ['.txt', '.csv', '.json'] else None
        }
        
        # 如果是CSV文件，尝试分析结构
        if file_path.suffix == '.csv':
            csv_structure = self._analyze_csv_structure(file_path)
            metadata.update(csv_structure)
        
        return metadata
    
    def _collect_db_metadata(self, db_info):
        """收集数据库表元数据"""
        # 这里只是示例，实际应用中会连接数据库获取真实元数据
        metadata = {
            "database": db_info.get("database"),
            "schema": db_info.get("schema"),
            "table": db_info.get("table"),
            "estimated_rows": db_info.get("estimated_rows"),
            "refresh_frequency": db_info.get("refresh_frequency")
        }
        
        # 如果提供了列信息
        if "columns" in db_info:
            metadata["columns"] = db_info["columns"]
        
        return metadata
    
    def _infer_parameter_type(self, param):
        """推断参数类型"""
        param_name = param.get("name", "").lower()
        param_value = param.get("default", "")
        
        # 基于参数名推断类型
        if any(keyword in param_name for keyword in ["date", "time", "at", "from", "to"]):
            return "datetime"
        elif any(keyword in param_name for keyword in ["id", "count", "num", "size", "amount"]):
            return "integer"
        elif any(keyword in param_name for keyword in ["price", "rate", "value", "percent"]):
            return "float"
        elif any(keyword in param_name for keyword in ["flag", "is", "has", "active"]):
            return "boolean"
        elif "email" in param_name:
            return "email"
        
        # 基于默认值推断类型
        if isinstance(param_value, bool):
            return "boolean"
        elif isinstance(param_value, int):
            return "integer"
        elif isinstance(param_value, float):
            return "float"
        
        return "string"
    
    def _detect_file_encoding(self, file_path):
        """检测文件编码"""
        try:
            import chardet
            
            with open(file_path, 'rb') as f:
                raw_data = f.read(10000)  # 读取前10KB用于检测
                result = chardet.detect(raw_data)
                return result.get('encoding', 'utf-8')
        except ImportError:
            return "utf-8"
    
    def _analyze_csv_structure(self, file_path):
        """分析CSV文件结构"""
        try:
            import pandas as pd
            
            # 读取前几行以分析结构
            df_sample = pd.read_csv(file_path, nrows=100)
            
            structure = {
                "columns": list(df_sample.columns),
                "estimated_rows": self._estimate_csv_rows(file_path),
                "data_types": {}
            }
            
            # 推断列数据类型
            for col in df_sample.columns:
                non_null_values = df_sample[col].dropna()
                if not non_null_values.empty:
                    structure["data_types"][col] = str(non_null_values.dtype)
            
            return structure
        except ImportError:
            return {"error": "需要安装pandas来分析CSV文件"}
        except Exception as e:
            return {"error": f"分析CSV文件失败: {e}"}
    
    def _estimate_csv_rows(self, file_path):
        """估算CSV文件行数"""
        try:
            with open(file_path, 'r') as f:
                # 读取1MB的数据计算平均行长度
                sample_size = min(1024 * 1024, os.path.getsize(file_path))
                sample = f.read(sample_size)
                lines = sample.count('\n')
                
                if lines > 0:
                    avg_line_length = sample_size / lines
                    total_size = os.path.getsize(file_path)
                    return int(total_size / avg_line_length)
        except:
            pass
        
        return 0
    
    def _check_review_needed(self, metadata):
        """检查是否需要人工审核"""
        # 如果有错误信息，需要人工审核
        if "error" in metadata:
            return True
        
        # 如果缺少关键信息，需要人工审核
        required_fields = self.collection_rules.get("required_fields", [])
        for field in required_fields:
            if field not in metadata or not metadata[field]:
                return True
        
        # 如果是新的数据源类型，可能需要人工审核
        new_source_types = self.collection_rules.get("review_required_types", [])
        if metadata.get("type") in new_source_types:
            return True
        
        return False
    
    def request_manual_review(self, metadata_id, review_info):
        """请求人工审核"""
        if metadata_id not in self.collected_metadata:
            return False
        
        review = {
            "metadata_id": metadata_id,
            "requested_by": review_info.get("user"),
            "requested_at": datetime.now(),
            "reason": review_info.get("reason", "Standard review process"),
            "status": "pending",
            "notes": review_info.get("notes", "")
        }
        
        self.manual_reviews.append(review)
        return True
    
    def submit_manual_review(self, metadata_id, review_data):
        """提交人工审核结果"""
        if metadata_id not in self.collected_metadata:
            return False
        
        # 查找对应的审核请求
        review_request = None
        for review in self.manual_reviews:
            if review["metadata_id"] == metadata_id and review["status"] == "pending":
                review_request = review
                break
        
        if not review_request:
            return False
        
        # 更新审核状态
        review_request["status"] = "completed"
        review_request["completed_at"] = datetime.now()
        review_request["reviewer"] = review_data.get("reviewer")
        review_request["approved"] = review_data.get("approved", True)
        review_request["review_notes"] = review_data.get("notes", "")
        
        # 更新元数据
        if review_data.get("approved", True):
            self.collected_metadata[metadata_id]["detailed"] = review_data.get("metadata_updates", {})
            self.collected_metadata[metadata_id]["manual_review_required"] = False
        
        return True
    
    def get_metadata_for_review(self, status="pending"):
        """获取待审核的元数据"""
        result = []
        
        for review in self.manual_reviews:
            if review["status"] == status:
                metadata_id = review["metadata_id"]
                if metadata_id in self.collected_metadata:
                    result.append({
                        "review": review,
                        "metadata": self.collected_metadata[metadata_id]
                    })
        
        return result
    
    def finalize_metadata(self, metadata_id):
        """最终确定元数据"""
        if metadata_id not in self.collected_metadata:
            return None
        
        metadata_entry = self.collected_metadata[metadata_id]
        
        if metadata_entry["manual_review_required"]:
            return None  # 需要先完成人工审核
        
        # 合并基础元数据和详细元数据
        final_metadata = metadata_entry["basic"].copy()
        final_metadata.update(metadata_entry["detailed"])
        
        # 添加最终确定的标记
        final_metadata["finalized_at"] = datetime.now()
        final_metadata["finalized_by"] = "system"  # 实际应用中可能是审核人
        
        return final_metadata

def demo_semi_automated_collection():
    """演示半自动化元数据收集"""
    print("\n" + "=" * 50)
    print("4.1.2 半自动化元数据收集示例")
    print("=" * 50)
    
    # 创建收集器并配置规则
    collector = SemiAutomatedMetadataCollector()
    collector.configure_collection_rules({
        "required_fields": ["name", "type", "description"],
        "review_required_types": ["api"]
    })
    
    # 自动收集API元数据
    api_source = {
        "name": "用户服务API",
        "type": "api",
        "endpoint": "https://api.example.com/users",
        "method": "GET",
        "parameters": [
            {"name": "user_id", "type": "integer", "description": "用户ID"},
            {"name": "include_profile", "type": "boolean", "default": False, "description": "是否包含用户画像"}
        ],
        "response_format": "json"
    }
    
    api_metadata_id = collector.auto_collect_basic_metadata(api_source)
    print(f"已收集API元数据，ID: {api_metadata_id}")
    
    # 自动收集文件元数据
    import tempfile
    with tempfile.NamedTemporaryFile(mode='w', suffix='.csv', delete=False) as f:
        f.write("id,name,email\n")
        f.write("1,Alice,alice@example.com\n")
        f.write("2,Bob,bob@example.com\n")
        file_path = f.name
    
    file_source = {
        "name": "用户联系人",
        "type": "file",
        "path": file_path,
        "description": "用户联系人信息CSV文件"
    }
    
    file_metadata_id = collector.auto_collect_basic_metadata(file_source)
    print(f"已收集文件元数据，ID: {file_metadata_id}")
    
    # 请求人工审核
    collector.request_manual_review(api_metadata_id, {
        "user": "data_steward",
        "reason": "新API源需要验证参数定义"
    })
    
    # 查看待审核项
    pending_reviews = collector.get_metadata_for_review("pending")
    print(f"待审核项数量: {len(pending_reviews)}")
    
    # 模拟审核过程
    if pending_reviews:
        review = pending_reviews[0]
        metadata_id = review["review"]["metadata_id"]
        
        # 提交审核结果
        collector.submit_manual_review(metadata_id, {
            "reviewer": "data_steward",
            "approved": True,
            "metadata_updates": {
                "description": "用户服务API，提供用户基本信息查询功能",
                "owner": "用户服务团队",
                "api_documentation": "https://docs.example.com/api/users"
            },
            "notes": "API元数据已验证，参数定义正确"
        })
        
        print(f"已完成审核，ID: {metadata_id}")
    
    # 获取最终确定的元数据
    finalized_metadata = collector.finalize_metadata(api_metadata_id)
    if finalized_metadata:
        print(f"API最终元数据:")
        print(f"  名称: {finalized_metadata.get('name')}")
        print(f"  描述: {finalized_metadata.get('description')}")
        print(f"  所有者: {finalized_metadata.get('owner')}")
    
    # 清理临时文件
    import os
    os.unlink(file_path)
    
    return collector
```

## 4.2 元数据提取技术

### 4.2.1 结构化数据提取

结构化数据提取主要针对数据库、表格等具有明确结构的数据源的元数据提取。

```python
import pandas as pd
import sqlalchemy
from sqlalchemy import create_engine, inspect
from typing import Dict, List, Any

class StructuredDataExtractor:
    """结构化数据提取器"""
    
    def __init__(self, connection_string):
        self.connection_string = connection_string
        self.engine = create_engine(connection_string)
    
    def extract_database_schema(self):
        """提取数据库模式"""
        inspector = inspect(self.engine)
        
        # 获取数据库信息
        database_info = {
            "name": inspector.engine.url.database,
            "driver": inspector.engine.driver,
            "dialect": inspector.dialect.name
        }
        
        return database_info
    
    def extract_schemas(self):
        """提取所有模式"""
        inspector = inspect(self.engine)
        schemas = []
        
        try:
            schema_names = inspector.get_schema_names()
            for schema_name in schema_names:
                if not schema_name.startswith('information_schema'):
                    schemas.append({
                        "name": schema_name,
                        "table_count": len(inspector.get_table_names(schema=schema_name))
                    })
        except Exception as e:
            print(f"提取模式列表失败: {e}")
            # 某些数据库不支持多模式，回退到默认模式
            schemas.append({"name": "public", "table_count": len(inspector.get_table_names())})
        
        return schemas
    
    def extract_tables(self, schema_name=None):
        """提取表信息"""
        inspector = inspect(self.engine)
        
        if schema_name:
            table_names = inspector.get_table_names(schema=schema_name)
        else:
            table_names = inspector.get_table_names()
        
        tables = []
        
        for table_name in table_names:
            table_info = {
                "name": table_name,
                "schema": schema_name,
                "columns": [],
                "indexes": [],
                "foreign_keys": [],
                "primary_keys": []
            }
            
            # 获取列信息
            try:
                columns = inspector.get_columns(table_name, schema=schema_name)
                for column in columns:
                    table_info["columns"].append({
                        "name": column["name"],
                        "type": str(column["type"]),
                        "nullable": column.get("nullable", True),
                        "default": column.get("default"),
                        "autoincrement": column.get("autoincrement", False)
                    })
            except Exception as e:
                print(f"获取表 {table_name} 的列信息失败: {e}")
            
            # 获取索引信息
            try:
                indexes = inspector.get_indexes(table_name, schema=schema_name)
                for index in indexes:
                    table_info["indexes"].append({
                        "name": index["name"],
                        "columns": index["column_names"],
                        "unique": index["unique"],
                        "type": index.get("type", "btree")
                    })
            except Exception as e:
                print(f"获取表 {table_name} 的索引信息失败: {e}")
            
            # 获取主键信息
            try:
                pk = inspector.get_pk_constraint(table_name, schema=schema_name)
                if pk and "constrained_columns" in pk:
                    table_info["primary_keys"] = pk["constrained_columns"]
            except Exception as e:
                print(f"获取表 {table_name} 的主键信息失败: {e}")
            
            # 获取外键信息
            try:
                fks = inspector.get_foreign_keys(table_name, schema=schema_name)
                for fk in fks:
                    table_info["foreign_keys"].append({
                        "name": fk.get("name"),
                        "columns": fk.get("constrained_columns", []),
                        "referred_table": fk.get("referred_table"),
                        "referred_columns": fk.get("referred_columns", [])
                    })
            except Exception as e:
                print(f"获取表 {table_name} 的外键信息失败: {e}")
            
            # 获取表统计信息
            try:
                with self.engine.connect() as conn:
                    # 尝试获取行数（不同的数据库语法可能不同）
                    if inspector.dialect.name == "postgresql":
                        result = conn.execute(f"""
                            SELECT reltuples::bigint AS estimate
                            FROM pg_class
                            WHERE relname = '{table_name}'
                        """)
                    elif inspector.dialect.name == "mysql":
                        result = conn.execute(f"""
                            SELECT table_rows
                            FROM information_schema.tables
                            WHERE table_schema = '{schema_name or "public"}'
                              AND table_name = '{table_name}'
                        """)
                    elif inspector.dialect.name == "sqlite":
                        # SQLite需要特殊处理
                        result = None
                    else:
                        # 通用方法（可能不适用于所有数据库）
                        result = conn.execute(f"SELECT COUNT(*) FROM {table_name}")
                    
                    if result:
                        row_count = result.fetchone()[0]
                        table_info["row_count"] = int(row_count)
            except Exception as e:
                print(f"获取表 {table_name} 的行数失败: {e}")
            
            tables.append(table_info)
        
        return tables
    
    def extract_table_statistics(self, table_name, schema_name=None, sample_size=1000):
        """提取表统计信息"""
        if schema_name:
            full_table_name = f"{schema_name}.{table_name}"
        else:
            full_table_name = table_name
        
        try:
            # 读取样本数据
            query = f"SELECT * FROM {full_table_name} LIMIT {sample_size}"
            df = pd.read_sql(query, self.engine)
            
            if df.empty:
                return {"error": "表为空或无法访问"}
            
            # 计算统计信息
            stats = {
                "total_columns": len(df.columns),
                "sample_rows": len(df),
                "column_statistics": {}
            }
            
            for column_name, dtype in df.dtypes.items():
                column_stats = {
                    "data_type": str(dtype),
                    "null_count": df[column_name].isnull().sum(),
                    "null_percentage": (df[column_name].isnull().sum() / len(df)) * 100
                }
                
                # 根据数据类型计算不同的统计信息
                if dtype in ['int64', 'float64']:
                    column_stats.update({
                        "min": float(df[column_name].min()) if not df[column_name].isnull().all() else None,
                        "max": float(df[column_name].max()) if not df[column_name].isnull().all() else None,
                        "mean": float(df[column_name].mean()) if not df[column_name].isnull().all() else None,
                        "std": float(df[column_name].std()) if not df[column_name].isnull().all() else None
                    })
                elif dtype == 'object':
                    # 字符串类型
                    non_null_values = df[column_name].dropna()
                    if not non_null_values.empty:
                        column_stats.update({
                            "unique_count": non_null_values.nunique(),
                            "unique_percentage": (non_null_values.nunique() / len(non_null_values)) * 100,
                            "min_length": non_null_values.str.len().min(),
                            "max_length": non_null_values.str.len().max(),
                            "avg_length": non_null_values.str.len().mean()
                        })
                        
                        # 如果唯一值较少，列出所有值
                        if non_null_values.nunique() <= 10:
                            column_stats["unique_values"] = non_null_values.unique().tolist()
                
                stats["column_statistics"][column_name] = column_stats
            
            return stats
        
        except Exception as e:
            return {"error": f"提取表统计信息失败: {e}"}
    
    def extract_relationship_graph(self, schema_name=None):
        """提取表关系图"""
        inspector = inspect(self.engine)
        
        if schema_name:
            table_names = inspector.get_table_names(schema=schema_name)
        else:
            table_names = inspector.get_table_names()
        
        nodes = []
        edges = []
        
        # 创建节点（表）
        for table_name in table_names:
            nodes.append({
                "id": f"{schema_name}.{table_name}" if schema_name else table_name,
                "name": table_name,
                "type": "table",
                "schema": schema_name
            })
        
        # 创建边（关系）
        for table_name in table_names:
            try:
                fks = inspector.get_foreign_keys(table_name, schema=schema_name)
                for fk in fks:
                    if "referred_table" in fk and "constrained_columns" in fk:
                        from_table = f"{schema_name}.{table_name}" if schema_name else table_name
                        to_table = f"{schema_name}.{fk['referred_table']}" if schema_name else fk['referred_table']
                        
                        edges.append({
                            "from": from_table,
                            "to": to_table,
                            "type": "foreign_key",
                            "from_columns": fk.get("constrained_columns", []),
                            "to_columns": fk.get("referred_columns", [])
                        })
            except Exception as e:
                print(f"获取表 {table_name} 的关系失败: {e}")
        
        return {
            "nodes": nodes,
            "edges": edges
        }

# SQLite内存数据库演示
def demo_structured_data_extraction():
    """演示结构化数据提取"""
    print("\n" + "=" * 50)
    print("4.2.1 结构化数据提取示例")
    print("=" * 50)
    
    # 创建内存SQLite数据库并添加示例数据
    import sqlite3
    
    conn = sqlite3.connect(":memory:")
    cursor = conn.cursor()
    
    # 创建表
    cursor.execute("""
        CREATE TABLE users (
            id INTEGER PRIMARY KEY,
            name VARCHAR(50) NOT NULL,
            email VARCHAR(100) UNIQUE,
            age INTEGER,
            created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
        )
    """)
    
    cursor.execute("""
        CREATE TABLE orders (
            id INTEGER PRIMARY KEY,
            user_id INTEGER,
            product VARCHAR(50),
            amount DECIMAL(10,2),
            order_date DATE,
            FOREIGN KEY (user_id) REFERENCES users(id)
        )
    """)
    
    # 插入示例数据
    cursor.execute("""
        INSERT INTO users (name, email, age) VALUES
        ('Alice', 'alice@example.com', 30),
        ('Bob', 'bob@example.com', 25),
        ('Charlie', 'charlie@example.com', 35)
    """)
    
    cursor.execute("""
        INSERT INTO orders (user_id, product, amount, order_date) VALUES
        (1, 'Laptop', 999.99, '2023-01-15'),
        (2, 'Phone', 599.99, '2023-02-20'),
        (1, 'Tablet', 299.99, '2023-03-10'),
        (3, 'Headphones', 199.99, '2023-04-05')
    """)
    
    conn.commit()
    
    # 使用SQLAlchemy连接SQLite
    engine = create_engine("sqlite:///:memory:")
    
    # 创建结构化数据提取器
    extractor = StructuredDataExtractor("sqlite:///:memory:")
    
    # 由于这是内存数据库，我们直接连接到已创建的数据库
    extractor.engine = create_engine("sqlite:///:memory:")
    
    # 由于SQLite的限制，这里简化演示
    print("已连接到SQLite数据库")
    
    # 创建内存中的表并提取元数据
    import pandas as pd
    users_df = pd.read_sql("SELECT * FROM users", conn)
    orders_df = pd.read_sql("SELECT * FROM orders", conn)
    
    users_df.to_sql("users", extractor.engine, if_exists="replace", index=False)
    orders_df.to_sql("orders", extractor.engine, if_exists="replace", index=False)
    
    # 提取表信息
    tables = extractor.extract_tables()
    print(f"数据库中的表数量: {len(tables)}")
    
    for table in tables:
        print(f"\n表: {table['name']}")
        print(f"  列数: {len(table['columns'])}")
        print(f"  主键: {table['primary_keys']}")
        
        # 显示列信息
        for column in table['columns']:
            print(f"    {column['name']}: {column['type']} ({'NULL' if column['nullable'] else 'NOT NULL'})")
        
        # 显示外键信息
        for fk in table['foreign_keys']:
            print(f"    外键 {fk['columns']} -> {fk['referred_table']}.{fk['referred_columns']}")
    
    # 提取表统计信息
    for table in tables:
        stats = extractor.extract_table_statistics(table['name'])
        if "error" not in stats:
            print(f"\n表 {table['name']} 统计信息:")
            print(f"  采样行数: {stats['sample_rows']}")
            
            for col_name, col_stats in stats['column_statistics'].items():
                print(f"    列 {col_name} ({col_stats['data_type']}): 空值率 {col_stats['null_percentage']:.1f}%")
                
                if 'min' in col_stats and col_stats['min'] is not None:
                    print(f"      范围: {col_stats['min']} - {col_stats['max']}")
                
                if 'unique_count' in col_stats:
                    print(f"      唯一值: {col_stats['unique_count']} ({col_stats['unique_percentage']:.1f}%)")
    
    # 提取关系图
    relationship_graph = extractor.extract_relationship_graph()
    print(f"\n关系图信息:")
    print(f"  节点数: {len(relationship_graph['nodes'])}")
    print(f"  边数: {len(relationship_graph['edges'])}")
    
    for edge in relationship_graph['edges']:
        print(f"  {edge['from']} -> {edge['to']} ({edge['type']})")
    
    conn.close()
    return extractor
```

## 4.3 元数据整合与标准化

### 4.3.1 元数据映射与转换

元数据整合是将来自不同源的元数据进行统一和标准化的过程，使不同系统的元数据能够协同工作。

```python
import json
import re
from datetime import datetime
from typing import Dict, List, Any, Optional

class MetadataIntegrator:
    """元数据整合器"""
    
    def __init__(self):
        self.source_mappings = {}  # 源系统映射规则
        self.target_standards = {}  # 目标标准
        self.transformation_rules = {}  # 转换规则
        self.integrated_metadata = {}  # 已整合的元数据
    
    def register_source_system(self, system_id, system_config):
        """注册源系统"""
        self.source_mappings[system_id] = {
            "name": system_config["name"],
            "metadata_format": system_config.get("metadata_format", "json"),
            "field_mappings": system_config.get("field_mappings", {}),
            "data_types": system_config.get("data_types", {}),
            "timestamp_formats": system_config.get("timestamp_formats", {})
        }
    
    def define_target_standard(self, standard_name, standard_schema):
        """定义目标标准"""
        self.target_standards[standard_name] = standard_schema
    
    def add_transformation_rule(self, rule_id, rule_config):
        """添加转换规则"""
        self.transformation_rules[rule_id] = {
            "source_pattern": rule_config["source_pattern"],
            "target_format": rule_config["target_format"],
            "transformation_function": rule_config.get("function")
        }
    
    def integrate_metadata(self, source_system_id, metadata, target_standard="default"):
        """整合元数据到目标标准"""
        if source_system_id not in self.source_mappings:
            raise ValueError(f"未知源系统: {source_system_id}")
        
        if target_standard not in self.target_standards:
            raise ValueError(f"未知目标标准: {target_standard}")
        
        source_mapping = self.source_mappings[source_system_id]
        target_schema = self.target_standards[target_standard]
        
        # 初始化结果
        integrated = {
            "source_system": source_system_id,
            "source_system_name": source_mapping["name"],
            "integration_timestamp": datetime.now().isoformat(),
            "target_standard": target_standard
        }
        
        # 应用字段映射
        self._apply_field_mappings(metadata, integrated, source_mapping["field_mappings"])
        
        # 应用数据类型转换
        self._apply_type_conversions(integrated, source_mapping["data_types"], target_schema)
        
        # 应用转换规则
        self._apply_transformation_rules(integrated)
        
        # 标准化格式
        self._standardize_format(integrated, target_schema)
        
        # 存储整合结果
        metadata_id = f"{source_system_id}_{int(datetime.now().timestamp())}"
        self.integrated_metadata[metadata_id] = integrated
        
        return metadata_id
    
    def _apply_field_mappings(self, source, target, field_mappings):
        """应用字段映射"""
        for source_field, target_field in field_mappings.items():
            if source_field in source:
                target[target_field] = source[source_field]
    
    def _apply_type_conversions(self, metadata, type_mapping, target_schema):
        """应用数据类型转换"""
        for field, expected_type in target_schema.get("fields", {}).items():
            if field in metadata:
                current_value = metadata[field]
                
                # 根据源系统类型映射进行转换
                source_type = type_mapping.get(field)
                
                if source_type and expected_type:
                    converted_value = self._convert_type(
                        current_value, 
                        source_type, 
                        expected_type
                    )
                    metadata[field] = converted_value
    
    def _convert_type(self, value, from_type, to_type):
        """数据类型转换"""
        # 简化实现，实际应用中会更复杂
        if from_type == "string" and to_type == "integer":
            try:
                return int(value)
            except (ValueError, TypeError):
                return None
        
        elif from_type == "string" and to_type == "float":
            try:
                return float(value)
            except (ValueError, TypeError):
                return None
        
        elif from_type == "string" and to_type == "datetime":
            # 尝试解析为ISO格式日期时间
            try:
                return datetime.fromisoformat(value).isoformat()
            except (ValueError, TypeError):
                return value
        
        elif from_type == "integer" and to_type == "string":
            return str(value)
        
        elif from_type == "timestamp" and to_type == "datetime":
            if isinstance(value, datetime):
                return value.isoformat()
            return value
        
        return value
    
    def _apply_transformation_rules(self, metadata):
        """应用转换规则"""
        for rule_id, rule in self.transformation_rules.items():
            pattern = rule["source_pattern"]
            target_format = rule["target_format"]
            
            # 检查是否有字段匹配模式
            for field, value in metadata.items():
                if isinstance(value, str) and re.match(pattern, value):
                    # 应用转换函数
                    if rule["transformation_function"]:
                        try:
                            # 使用内置转换函数
                            if rule["transformation_function"] == "upper":
                                metadata[field] = value.upper()
                            elif rule["transformation_function"] == "lower":
                                metadata[field] = value.lower()
                            elif rule["transformation_function"] == "snake_to_camel":
                                metadata[field] = self._snake_to_camel_case(value)
                            # 可以添加更多转换函数
                        except Exception as e:
                            print(f"应用转换规则 {rule_id} 失败: {e}")
    
    def _snake_to_camel_case(self, snake_str):
        """将下划线命名转换为驼峰命名"""
        components = snake_str.split('_')
        return components[0] + ''.join(x.capitalize() for x in components[1:])
    
    def _standardize_format(self, metadata, target_schema):
        """标准化格式"""
        # 确保必填字段存在
        required_fields = target_schema.get("required_fields", [])
        for field in required_fields:
            if field not in metadata:
                metadata[field] = None
        
        # 设置默认值
        default_values = target_schema.get("default_values", {})
        for field, default_value in default_values.items():
            if field in metadata and metadata[field] is None:
                metadata[field] = default_value
        
        # 确保字段名格式正确
        if "field_name_format" in target_schema:
            name_format = target_schema["field_name_format"]
            if name_format == "snake_case":
                # 转换为下划线命名
                new_metadata = {}
                for field, value in metadata.items():
                    new_field = self._camel_to_snake_case(field)
                    new_metadata[new_field] = value
                metadata.clear()
                metadata.update(new_metadata)
    
    def _camel_to_snake_case(self, camel_str):
        """将驼峰命名转换为下划线命名"""
        s1 = re.sub('(.)([A-Z][a-z]+)', r'\1_\2', camel_str)
        return re.sub('([a-z0-9])([A-Z])', r'\1_\2', s1).lower()
    
    def get_integrated_metadata(self, metadata_id):
        """获取已整合的元数据"""
        return self.integrated_metadata.get(metadata_id)
    
    def query_integrated_metadata(self, filters=None):
        """查询已整合的元数据"""
        results = []
        
        for metadata_id, metadata in self.integrated_metadata.items():
            match = True
            
            if filters:
                for field, value in filters.items():
                    if field in metadata:
                        if isinstance(value, str) and isinstance(metadata[field], str):
                            if value.lower() not in metadata[field].lower():
                                match = False
                                break
                        elif metadata[field] != value:
                            match = False
                            break
                    else:
                        match = False
                        break
            
            if match:
                results.append({
                    "id": metadata_id,
                    "metadata": metadata
                })
        
        return results
    
    def merge_metadata(self, metadata_ids, merge_strategy="overwrite"):
        """合并多个元数据"""
        merged = {}
        
        for metadata_id in metadata_ids:
            metadata = self.integrated_metadata.get(metadata_id)
            if not metadata:
                continue
            
            if merge_strategy == "overwrite":
                merged.update(metadata)
            elif merge_strategy == "merge":
                for key, value in metadata.items():
                    if key in merged:
                        if isinstance(merged[key], list) and isinstance(value, list):
                            merged[key].extend(value)
                        elif isinstance(merged[key], dict) and isinstance(value, dict):
                            merged[key].update(value)
                        else:
                            # 对于简单类型，保留第一个值
                            pass
                    else:
                        merged[key] = value
        
        return merged

def demo_metadata_integration():
    """演示元数据整合"""
    print("\n" + "=" * 50)
    print("4.3.1 元数据映射与转换示例")
    print("=" * 50)
    
    # 创建元数据整合器
    integrator = MetadataIntegrator()
    
    # 注册源系统
    integrator.register_source_system("mysql_system", {
        "name": "MySQL业务系统",
        "metadata_format": "json",
        "field_mappings": {
            "TableName": "table_name",
            "TableType": "table_type",
            "Columns": "columns",
            "RowCount": "row_count",
            "CreatedDate": "created_at"
        },
        "data_types": {
            "table_name": "string",
            "table_type": "string",
            "row_count": "integer",
            "created_at": "timestamp"
        }
    })
    
    integrator.register_source_system("oracle_system", {
        "name": "Oracle数据仓库",
        "metadata_format": "json",
        "field_mappings": {
            "OBJECT_NAME": "table_name",
            "OBJECT_TYPE": "table_type",
            "COLUMN_INFO": "columns",
            "NUM_ROWS": "row_count",
            "CREATED": "created_at"
        },
        "data_types": {
            "table_name": "string",
            "table_type": "string",
            "row_count": "number",
            "created_at": "date"
        }
    })
    
    # 定义目标标准
    integrator.define_target_standard("unified_table", {
        "fields": {
            "table_name": "string",
            "table_type": "string",
            "columns": "array",
            "row_count": "integer",
            "created_at": "datetime"
        },
        "required_fields": ["table_name", "table_type"],
        "default_values": {
            "table_type": "TABLE"
        },
        "field_name_format": "snake_case"
    })
    
    # 添加转换规则
    integrator.add_transformation_rule("normalize_table_names", {
        "source_pattern": r"^[A-Z_]+$",
        "target_format": "lowercase",
        "transformation_function": "lower"
    })
    
    # 模拟MySQL系统元数据
    mysql_metadata = {
        "TableName": "USER_PROFILE",
        "TableType": "TABLE",
        "Columns": [
            {"name": "USER_ID", "type": "BIGINT"},
            {"name": "USER_NAME", "type": "VARCHAR(50)"},
            {"name": "EMAIL", "type": "VARCHAR(100)"}
        ],
        "RowCount": "1000000",
        "CreatedDate": "2023-01-15T10:30:00Z"
    }
    
    # 模拟Oracle系统元数据
    oracle_metadata = {
        "OBJECT_NAME": "SALES_SUMMARY",
        "OBJECT_TYPE": "TABLE",
        "COLUMN_INFO": [
            {"name": "SALE_ID", "type": "NUMBER"},
            {"name": "PRODUCT_ID", "type": "NUMBER"},
            {"name": "AMOUNT", "type": "NUMBER(10,2)"}
        ],
        "NUM_ROWS": 5000000,
        "CREATED": "15-JAN-23"
    }
    
    # 整合MySQL元数据
    mysql_id = integrator.integrate_metadata("mysql_system", mysql_metadata, "unified_table")
    print(f"已整合MySQL元数据，ID: {mysql_id}")
    
    # 整合Oracle元数据
    oracle_id = integrator.integrate_metadata("oracle_system", oracle_metadata, "unified_table")
    print(f"已整合Oracle元数据，ID: {oracle_id}")
    
    # 查看整合结果
    mysql_result = integrator.get_integrated_metadata(mysql_id)
    print(f"\nMySQL整合结果:")
    print(f"  表名: {mysql_result['table_name']}")
    print(f"  行数: {mysql_result['row_count']}")
    print(f"  创建时间: {mysql_result['created_at']}")
    
    oracle_result = integrator.get_integrated_metadata(oracle_id)
    print(f"\nOracle整合结果:")
    print(f"  表名: {oracle_result['table_name']}")
    print(f"  行数: {oracle_result['row_count']}")
    print(f"  创建时间: {oracle_result['created_at']}")
    
    # 查询整合的元数据
    query_results = integrator.query_integrated_metadata({"source_system": "mysql_system"})
    print(f"\n查询MySQL系统的元数据: {len(query_results)} 条")
    
    # 合并元数据
    merged_metadata = integrator.merge_metadata([mysql_id, oracle_id], "merge")
    print(f"\n合并后的元数据字段数: {len(merged_metadata)}")
    
    return integrator
```

## 4.4 本章小结

本章详细介绍了元数据收集与提取的相关技术，包括：

1. **元数据收集方法**：介绍了自动收集和半自动化收集的方法和技术
2. **元数据提取技术**：详细讲解了结构化数据提取的实现方法
3. **元数据整合与标准化**：提供了元数据映射、转换和整合的解决方案

通过本章的学习，读者应该能够：
- 设计和实现自动化元数据收集系统
- 从各种数据源中提取结构化和半结构化元数据
- 实现元数据的标准化和整合
- 构建端到端的元数据收集和集成流程

在下一章中，我们将探讨元数据治理与管理，了解如何建立有效的元数据治理体系和管理流程。

---

**思考题**：
1. 在您的组织中，哪些数据源的元数据最难收集？为什么？
2. 如何设计一个既能满足自动化收集需求，又能处理特殊情况的元数据收集系统？
3. 元数据整合过程中可能遇到哪些挑战？如何解决这些挑战？