# 第5章 数据生命周期管理

## 1. 数据生命周期概述

### 1.1 什么是数据生命周期

数据生命周期(Data Lifecycle)是指数据从创建或获取开始，经过存储、处理、使用、共享、归档，最终被销毁的全过程。数据生命周期管理(Data Lifecycle Management, DLM)是一种策略驱动的管理方法，旨在优化数据的存储、使用和保护，同时降低成本并满足合规要求。

### 1.2 数据生命周期阶段

传统上，数据生命周期分为以下主要阶段：

1. **创建与获取(Creation/ Acquisition)**
   - 数据的产生或获取
   - 初始验证和质量检查
   - 元数据捕获和分类

2. **存储与维护(Storage/Maintenance)**
   - 数据存储和管理
   - 备份和恢复
   - 数据访问控制

3. **使用与分析(Usage/Analysis)**
   - 数据访问和检索
   - 数据分析和处理
   - 数据共享和分发

4. **归档与保留(Archive/Retention)**
   - 长期存储
   - 合规性保留
   - 访问频率降低

5. **销毁与处置(Destruction/Disposal)**
   - 安全删除
   - 认证销毁
   - 审计跟踪

### 1.3 数据生命周期管理的价值

数据生命周期管理为组织带来多重价值：

- **成本优化**：根据数据价值和使用频率优化存储资源
- **合规性**：满足法规要求的数据保留和隐私保护
- **风险管理**：降低数据泄露和滥用风险
- **效率提升**：提高数据访问和处理效率
- **决策支持**：确保数据质量和可用性，支持业务决策

## 2. 数据生命周期各阶段详解

### 2.1 创建与获取阶段

#### 2.1.1 数据创建

数据创建是指组织中新生成数据的过程，包括：

- **业务数据生成**：业务系统运行过程中自然产生的数据
- **传感器数据**：IoT设备、监控系统等实时生成的数据
- **用户生成内容**：用户行为、社交媒体内容等

数据创建的关键考虑因素：

```python
# 数据创建时需要考虑的关键因素
class DataCreationConsiderations:
    def __init__(self):
        self.quality_standards = "定义数据质量标准和验证规则"
        self.classification = "根据敏感性和价值对数据进行分类"
        self.ownership = "明确数据所有者和责任"
        self.metadata = "捕获完整的技术和业务元数据"
        self.compliance = "确保符合相关法规要求"
        self.security = "实施适当的安全措施"

    def plan_data_creation(self):
        """规划数据创建过程"""
        return {
            "pre_creation_validation": "创建前验证",
            "real_time_quality_checks": "实时质量检查",
            "automated_metadata_capture": "自动化元数据捕获",
            "classification_at_creation": "创建时分类",
            "access_control_setup": "访问控制设置"
        }
```

#### 2.1.2 数据获取

数据获取是指从外部源获取数据的过程，包括：

- **第三方数据采购**：从数据供应商购买数据
- **公开数据获取**：爬取或下载公开可用的数据
- **合作伙伴数据交换**：与合作伙伴共享和交换数据

数据获取的关键考虑因素：

```python
# 数据获取时需要考虑的关键因素
class DataAcquisitionConsiderations:
    def __init__(self):
        self.legal_rights = "确保获取数据的合法权利"
        self.license_agreement = "审查和理解许可协议"
        self.quality_assessment = "评估数据质量"
        self.format_compatibility = "评估格式兼容性"
        self.integration_complexity = "评估集成复杂性"
        self.privacy_compliance = "确保隐私合规性"
    
    def plan_data_acquisition(self):
        """规划数据获取过程"""
        return {
            "due_diligence": "尽职调查",
            "pilot_testing": "试点测试",
            "quality_validation": "质量验证",
            "transformation_plan": "转换计划",
            "legal_review": "法律审查"
        }
```

### 2.2 存储与维护阶段

#### 2.2.1 数据存储策略

数据存储策略应根据数据的特性、使用频率和价值进行差异化设计：

- **热存储(Hot Storage)**：用于频繁访问的关键业务数据
  - 高性能存储系统(SSD、NVMe)
  - 高可用性和快速恢复
  - 高成本，低容量

- **温存储(Warm Storage)**：用于中等访问频率的数据
  - 混合存储系统(SSD+HDD)
  - 平衡性能和成本
  - 中等成本，中等容量

- **冷存储(Cold Storage)**：用于低访问频率的数据
  - 低成本存储系统(HDD、磁带)
  - 慢速访问，长期保存
  - 低成本，大容量

```python
# 数据存储策略示例
class DataStorageStrategy:
    def __init__(self):
        self.tier_config = {
            "hot": {
                "access_frequency": "每天多次",
                "performance_requirement": "毫秒级响应",
                "storage_type": "SSD/NVMe",
                "retention_period": "6-12个月",
                "cost_per_gb": "高"
            },
            "warm": {
                "access_frequency": "每周几次",
                "performance_requirement": "秒级响应",
                "storage_type": "SSD/HDD混合",
                "retention_period": "1-3年",
                "cost_per_gb": "中"
            },
            "cold": {
                "access_frequency": "每月几次或更少",
                "performance_requirement": "分钟级响应",
                "storage_type": "HDD/磁带",
                "retention_period": "3年以上",
                "cost_per_gb": "低"
            }
        }
    
    def recommend_storage_tier(self, data_characteristics):
        """根据数据特征推荐存储层级"""
        age_months = data_characteristics.get("age_months", 0)
        access_frequency = data_characteristics.get("access_frequency", 0)
        business_criticality = data_characteristics.get("business_criticality", "medium")
        
        if age_months < 6 and access_frequency > 30 and business_criticality == "high":
            return "hot"
        elif age_months < 36 and access_frequency > 4:
            return "warm"
        else:
            return "cold"
    
    def optimize_storage_allocation(self, data_inventory):
        """优化存储分配"""
        recommendations = []
        
        for data_item in data_inventory:
            current_tier = data_item.get("current_tier")
            recommended_tier = self.recommend_storage_tier(data_item)
            
            if current_tier != recommended_tier:
                recommendations.append({
                    "data_id": data_item["id"],
                    "current_tier": current_tier,
                    "recommended_tier": recommended_tier,
                    "estimated_savings": data_item.get("size_gb", 0) * self.tier_config[current_tier]["cost_per_gb"] - 
                                      data_item.get("size_gb", 0) * self.tier_config[recommended_tier]["cost_per_gb"],
                    "performance_impact": self._calculate_performance_impact(current_tier, recommended_tier)
                })
        
        return recommendations
    
    def _calculate_performance_impact(self, current_tier, recommended_tier):
        """计算性能影响"""
        tier_performance = {"hot": 1, "warm": 2, "cold": 3}  # 数字越小性能越好
        current = tier_performance[current_tier]
        recommended = tier_performance[recommended_tier]
        
        if recommended > current:
            return "性能下降"
        elif recommended < current:
            return "性能提升"
        else:
            return "无变化"
```

#### 2.2.2 数据维护活动

数据维护包括定期执行的活动，确保数据的完整性、可用性和安全性：

```python
# 数据维护活动示例
class DataMaintenanceActivities:
    def __init__(self):
        self.maintenance_schedule = {
            "daily": [
                "备份验证",
                "访问日志审查",
                "安全扫描",
                "系统健康检查"
            ],
            "weekly": [
                "数据完整性验证",
                "性能优化",
                "访问权限审查",
                "存储容量监控"
            ],
            "monthly": [
                "全面备份测试",
                "数据质量评估",
                "安全审计",
                "灾难恢复演练"
            ],
            "quarterly": [
                "存储优化评估",
                "数据归档决策",
                "成本效益分析",
                "合规性审查"
            ],
            "annually": [
                "数据保留策略审查",
                "安全政策更新",
                "系统架构评估",
                "长期规划调整"
            ]
        }
    
    def create_maintenance_plan(self, system_criticality, data_volume, compliance_requirements):
        """创建维护计划"""
        plan = {
            "daily_tasks": self.maintenance_schedule["daily"],
            "weekly_tasks": self.maintenance_schedule["weekly"],
            "monthly_tasks": self.maintenance_schedule["monthly"],
            "quarterly_tasks": self.maintenance_schedule["quarterly"],
            "annual_tasks": self.maintenance_schedule["annually"]
        }
        
        # 根据系统关键性调整计划
        if system_criticality == "high":
            plan["daily_tasks"].extend(["实时监控检查", "异常检测扫描"])
            plan["weekly_tasks"].append("深度安全扫描")
        
        # 根据数据量调整计划
        if data_volume > "10TB":
            plan["monthly_tasks"].append("存储使用分析")
            plan["quarterly_tasks"].append("数据分层评估")
        
        # 根据合规要求调整计划
        if compliance_requirements == "strict":
            plan["weekly_tasks"].append("合规性检查")
            plan["monthly_tasks"].append("审计日志分析")
        
        return plan
    
    def track_maintenance_compliance(self, maintenance_plan, execution_log):
        """跟踪维护合规性"""
        compliance_report = {
            "overall_compliance": 0,
            "task_completion": {},
            "overdue_tasks": [],
            "issues": []
        }
        
        all_tasks = []
        for period, tasks in maintenance_plan.items():
            all_tasks.extend([(period, task) for task in tasks])
        
        completed_tasks = set()
        overdue_tasks = []
        
        for period, task in all_tasks:
            task_key = f"{period}:{task}"
            if task_key in execution_log and execution_log[task_key]["completed"]:
                completed_tasks.add(task_key)
                
                # 检查是否逾期完成
                if execution_log[task_key]["overdue"]:
                    overdue_tasks.append({
                        "task": task_key,
                        "due_date": execution_log[task_key]["due_date"],
                        "completed_date": execution_log[task_key]["completed_date"]
                    })
            else:
                overdue_tasks.append({
                    "task": task_key,
                    "due_date": "未完成",
                    "completed_date": "未完成"
                })
        
        compliance_report["task_completion"] = list(completed_tasks)
        compliance_report["overdue_tasks"] = overdue_tasks
        compliance_report["overall_compliance"] = len(completed_tasks) / len(all_tasks) * 100
        
        return compliance_report
```

### 2.3 使用与分析阶段

#### 2.3.1 数据使用管理

数据使用管理包括控制数据访问、确保适当使用和防止滥用：

```python
# 数据使用管理示例
class DataUsageManagement:
    def __init__(self):
        self.usage_policies = {
            "confidential": {
                "allowed_access": "授权用户仅限工作相关目的",
                "logging_level": "详细",
                "approval_required": True,
                "retention_limit": "12个月"
            },
            "internal": {
                "allowed_access": "所有员工仅限业务目的",
                "logging_level": "标准",
                "approval_required": False,
                "retention_limit": "24个月"
            },
            "public": {
                "allowed_access": "任何人任何目的",
                "logging_level": "基础",
                "approval_required": False,
                "retention_limit": "无限制"
            }
        }
    
    def track_data_access(self, user_id, data_id, access_type, purpose, data_classification):
        """跟踪数据访问"""
        access_record = {
            "timestamp": datetime.datetime.now().isoformat(),
            "user_id": user_id,
            "data_id": data_id,
            "access_type": access_type,  # read, write, delete, share
            "purpose": purpose,
            "classification": data_classification,
            "ip_address": "获取IP地址",
            "user_agent": "获取用户代理"
        }
        
        # 检查访问是否符合政策
        policy = self.usage_policies.get(data_classification, {})
        
        if policy.get("approval_required", False) and not self._has_approval(user_id, data_id):
            return {
                "allowed": False,
                "reason": "需要审批才能访问此数据",
                "required_approval": True
            }
        
        # 记录访问
        self._log_access(access_record)
        
        return {
            "allowed": True,
            "access_id": self._generate_access_id(),
            "policy_compliance": self._check_policy_compliance(access_record, policy)
        }
    
    def analyze_usage_patterns(self, usage_logs, time_period):
        """分析使用模式"""
        analysis = {
            "most_accessed_data": self._get_most_accessed(usage_logs),
            "top_users": self._get_top_users(usage_logs),
            "access_frequency_by_hour": self._get_access_frequency_by_hour(usage_logs),
            "access_by_classification": self._group_by_classification(usage_logs),
            "suspicious_activities": self._detect_suspicious_activities(usage_logs)
        }
        
        return analysis
    
    def _has_approval(self, user_id, data_id):
        """检查是否有访问批准"""
        # 实现批准检查逻辑
        return False  # 简化示例
    
    def _log_access(self, access_record):
        """记录访问日志"""
        # 实现日志记录逻辑
        pass
    
    def _generate_access_id(self):
        """生成访问ID"""
        return f"access_{int(time.time() * 1000)}"
    
    def _check_policy_compliance(self, access_record, policy):
        """检查政策合规性"""
        # 实现合规性检查逻辑
        return "合规"
    
    def _get_most_accessed(self, logs):
        """获取最常访问的数据"""
        # 实现分析逻辑
        return []
    
    def _get_top_users(self, logs):
        """获取最活跃用户"""
        # 实现分析逻辑
        return []
    
    def _get_access_frequency_by_hour(self, logs):
        """按小时获取访问频率"""
        # 实现分析逻辑
        return {}
    
    def _group_by_classification(self, logs):
        """按分类分组访问"""
        # 实现分析逻辑
        return {}
    
    def _detect_suspicious_activities(self, logs):
        """检测可疑活动"""
        # 实现检测逻辑
        return []
```

#### 2.3.2 数据分析治理

数据分析治理确保数据分析过程符合组织标准和法规要求：

```python
# 数据分析治理示例
class DataAnalyticsGovernance:
    def __init__(self):
        self.analysis_approval_levels = {
            "low_risk": {
                "datasets": ["内部公开数据", "已脱敏数据"],
                "approval_required": "自动批准",
                "documentation": "基础文档"
            },
            "medium_risk": {
                "datasets": ["内部受限数据", "部分个人数据"],
                "approval_required": "数据管理员批准",
                "documentation": "详细文档和目的说明"
            },
            "high_risk": {
                "datasets": ["敏感个人数据", "商业机密数据"],
                "approval_required": "数据保护官批准",
                "documentation": "全面文档、风险评估和合规证明"
            }
        }
    
    def assess_analysis_risk(self, datasets, analysis_type, output_distribution):
        """评估分析风险"""
        risk_score = 0
        
        # 基于数据集风险
        for dataset in datasets:
            if dataset in self.analysis_approval_levels["high_risk"]["datasets"]:
                risk_score += 3
            elif dataset in self.analysis_approval_levels["medium_risk"]["datasets"]:
                risk_score += 2
            else:
                risk_score += 1
        
        # 基于分析类型
        if analysis_type in ["机器学习模型训练", "预测分析"]:
            risk_score += 2
        elif analysis_type in ["聚合统计", "描述性分析"]:
            risk_score += 1
        
        # 基于输出分发
        if output_distribution in ["公开发布", "第三方共享"]:
            risk_score += 2
        elif output_distribution in ["内部使用", "团队共享"]:
            risk_score += 1
        
        # 确定风险等级
        if risk_score >= 7:
            return "high_risk"
        elif risk_score >= 4:
            return "medium_risk"
        else:
            return "low_risk"
    
    def create_analysis_approval_request(self, analyst_id, datasets, analysis_type, purpose, output_distribution):
        """创建分析批准请求"""
        risk_level = self.assess_analysis_risk(datasets, analysis_type, output_distribution)
        approval_level = self.analysis_approval_levels[risk_level]
        
        request = {
            "request_id": f"req_{int(time.time())}",
            "analyst_id": analyst_id,
            "datasets": datasets,
            "analysis_type": analysis_type,
            "purpose": purpose,
            "output_distribution": output_distribution,
            "risk_level": risk_level,
            "required_approval": approval_level["approval_required"],
            "required_documentation": approval_level["documentation"],
            "status": "待审批",
            "created_at": datetime.datetime.now().isoformat()
        }
        
        # 低风险分析自动批准
        if risk_level == "low_risk":
            request["status"] = "已批准"
            request["approved_at"] = datetime.datetime.now().isoformat()
            request["approved_by"] = "系统自动批准"
        
        return request
    
    def monitor_analysis_compliance(self, approved_requests, analysis_logs):
        """监控分析合规性"""
        compliance_issues = []
        
        for log in analysis_logs:
            request_id = log.get("request_id")
            
            if not request_id:
                compliance_issues.append({
                    "issue": "未批准的分析活动",
                    "analyst_id": log.get("analyst_id"),
                    "timestamp": log.get("timestamp"),
                    "severity": "高"
                })
                continue
            
            # 检查是否超出批准范围
            request = next((r for r in approved_requests if r["request_id"] == request_id), None)
            
            if not request:
                compliance_issues.append({
                    "issue": "未找到批准请求",
                    "analyst_id": log.get("analyst_id"),
                    "timestamp": log.get("timestamp"),
                    "severity": "高"
                })
                continue
            
            # 检查数据集是否超出批准范围
            approved_datasets = set(request["datasets"])
            used_datasets = set(log.get("datasets_used", []))
            
            if not used_datasets.issubset(approved_datasets):
                unapproved_datasets = used_datasets - approved_datasets
                compliance_issues.append({
                    "issue": f"使用未批准的数据集: {unapproved_datasets}",
                    "analyst_id": log.get("analyst_id"),
                    "timestamp": log.get("timestamp"),
                    "severity": "中"
                })
            
            # 检查输出分发是否超出批准范围
            if log.get("output_distribution") != request["output_distribution"]:
                compliance_issues.append({
                    "issue": f"输出分发方式超出批准范围",
                    "analyst_id": log.get("analyst_id"),
                    "timestamp": log.get("timestamp"),
                    "severity": "中"
                })
        
        return compliance_issues
```

### 2.4 归档与保留阶段

#### 2.4.1 数据归档策略

数据归档是指将不常使用的数据从主存储系统转移到成本较低的长期存储系统的过程：

```python
# 数据归档策略示例
class DataArchivingStrategy:
    def __init__(self):
        self.archive_criteria = {
            "time_based": {
                "transactional_data": 36,  # 36个月后归档
                "customer_data": 60,       # 60个月后归档
                "financial_data": 84,      # 84个月后归档
                "analytics_data": 24       # 24个月后归档
            },
            "access_frequency": {
                "low": 30,    # 30天内访问少于1次
                "medium": 90,  # 90天内访问少于3次
                "high": 365    # 365天内访问少于10次
            },
            "business_value": {
                "critical": "不归档或特殊归档",
                "important": "长期保留",
                "useful": "标准归档",
                "minimal": "快速归档或删除"
            }
        }
        
        self.archive_storage = {
            "standard": {
                "cost_factor": 0.3,  # 相对于热存储的成本比例
                "access_time": "分钟级",
                "recovery_time": "小时级",
                "suitability": "一般业务数据"
            },
            "deep": {
                "cost_factor": 0.1,
                "access_time": "小时级",
                "recovery_time": "天级",
                "suitability": "历史数据、备份数据"
            },
            "cold": {
                "cost_factor": 0.05,
                "access_time": "天级",
                "recovery_time": "周级",
                "suitability": "极少访问的数据"
            }
        }
    
    def identify_candidates_for_archiving(self, data_inventory):
        """识别归档候选数据"""
        candidates = []
        current_date = datetime.datetime.now()
        
        for data_item in data_inventory:
            # 时间标准检查
            data_type = data_item.get("type", "analytics_data")
            age_months = (current_date - datetime.datetime.strptime(data_item.get("created_date", current_date.strftime("%Y-%m-%d")), "%Y-%m-%d")).days // 30
            
            time_based_eligible = age_months >= self.archive_criteria["time_based"].get(data_type, 24)
            
            # 访问频率检查
            access_count = data_item.get("access_count_last_90_days", 0)
            access_based_eligible = access_count <= self.archive_criteria["access_frequency"]["low"]
            
            # 业务价值检查
            business_value = data_item.get("business_value", "useful")
            business_value_eligible = business_value in ["useful", "minimal"]
            
            # 综合判断
            if (time_based_eligible and access_based_eligible and business_value_eligible) or \
               (age_months >= self.archive_criteria["time_based"].get(data_type, 24) * 1.5):
                
                # 推荐归档存储类型
                archive_type = self._recommend_archive_type(data_item)
                
                candidates.append({
                    "data_id": data_item["id"],
                    "data_name": data_item.get("name", "未知"),
                    "data_type": data_type,
                    "age_months": age_months,
                    "access_count_last_90_days": access_count,
                    "business_value": business_value,
                    "size_gb": data_item.get("size_gb", 0),
                    "recommended_archive_type": archive_type,
                    "estimated_monthly_savings": data_item.get("size_gb", 0) * self.archive_storage[archive_type]["cost_factor"],
                    "last_access_date": data_item.get("last_access_date", "未知")
                })
        
        # 按预期节省金额排序
        candidates.sort(key=lambda x: x["estimated_monthly_savings"], reverse=True)
        
        return candidates
    
    def _recommend_archive_type(self, data_item):
        """推荐归档存储类型"""
        access_count = data_item.get("access_count_last_90_days", 0)
        business_value = data_item.get("business_value", "useful")
        
        if access_count == 0 and business_value == "minimal":
            return "cold"
        elif access_count <= 3:
            return "deep"
        else:
            return "standard"
    
    def create_archive_plan(self, candidates, budget_constraint=None, capacity_constraint=None):
        """创建归档计划"""
        plan = {
            "selected_for_archiving": [],
            "estimated_monthly_savings": 0,
            "total_size_to_archive": 0,
            "implementation_schedule": [],
            "unselected_reasons": {}
        }
        
        total_size = 0
        total_savings = 0
        
        for candidate in candidates:
            size = candidate["size_gb"]
            savings = candidate["estimated_monthly_savings"]
            
            # 检查预算约束
            if budget_constraint and total_size + size > budget_constraint:
                plan["unselected_reasons"][candidate["data_id"]] = "超出预算约束"
                continue
            
            # 检查容量约束
            if capacity_constraint and total_size + size > capacity_constraint:
                plan["unselected_reasons"][candidate["data_id"]] = "超出容量约束"
                continue
            
            plan["selected_for_archiving"].append(candidate)
            total_size += size
            total_savings += savings
        
        plan["estimated_monthly_savings"] = total_savings
        plan["total_size_to_archive"] = total_size
        
        # 创建实施计划
        plan["implementation_schedule"] = self._create_implementation_schedule(
            plan["selected_for_archiving"]
        )
        
        return plan
    
    def _create_implementation_schedule(self, selected_items):
        """创建实施计划"""
        # 按大小和业务影响排序
        sorted_items = sorted(selected_items, 
                            key=lambda x: (x.get("business_impact", "medium"), x["size_gb"]),
                            reverse=True)
        
        schedule = []
        weekly_capacity = 1000  # 每周可归档的数据量(GB)
        current_week = 1
        current_week_size = 0
        current_week_items = []
        
        for item in sorted_items:
            if current_week_size + item["size_gb"] <= weekly_capacity:
                current_week_items.append(item)
                current_week_size += item["size_gb"]
            else:
                if current_week_items:
                    schedule.append({
                        "week": current_week,
                        "items": current_week_items,
                        "total_size": current_week_size
                    })
                    current_week += 1
                    current_week_size = item["size_gb"]
                    current_week_items = [item]
        
        # 添加最后一周的项目
        if current_week_items:
            schedule.append({
                "week": current_week,
                "items": current_week_items,
                "total_size": current_week_size
            })
        
        return schedule
```

#### 2.4.2 数据保留政策

数据保留政策规定了不同类型数据的保留期限和处置方式：

```python
# 数据保留政策示例
class DataRetentionPolicy:
    def __init__(self):
        self.retention_rules = {
            "customer_data": {
                "default_retention": 2555,  # 7年(天)
                "minimum_retention": 1825,  # 5年
                "maximum_retention": 3650,  # 10年
                "legal_requirements": ["GDPR", "CCPA", "数据保护法"],
                "business_reason": "客户关系管理、合规审计",
                "disposal_method": "安全删除",
                "disapproval_required": False
            },
            "transaction_data": {
                "default_retention": 2555,  # 7年
                "minimum_retention": 1825,  # 5年
                "maximum_retention": 3650,  # 10年
                "legal_requirements": ["税法", "会计准则"],
                "business_reason": "财务审计、税务申报",
                "disposal_method": "安全删除+认证",
                "disapproval_required": True
            },
            "employee_data": {
                "default_retention": 2555,  # 7年
                "minimum_retention": 1825,  # 5年
                "maximum_retention": 3650,  # 10年
                "legal_requirements": ["劳动法", "平等就业法"],
                "business_reason": "人力资源、法律合规",
                "disposal_method": "安全删除+认证",
                "disapproval_required": True
            },
            "analytics_data": {
                "default_retention": 1095,  # 3年
                "minimum_retention": 365,   # 1年
                "maximum_retention": 1825,  # 5年
                "legal_requirements": [],
                "business_reason": "业务分析、趋势预测",
                "disposal_method": "删除",
                "disapproval_required": False
            },
            "email_communications": {
                "default_retention": 1825,  # 5年
                "minimum_retention": 730,   # 2年
                "maximum_retention": 3650,  # 10年
                "legal_requirements": ["证据法", "通信法"],
                "business_reason": "业务记录、法律证据",
                "disposal_method": "安全删除",
                "disapproval_required": False
            }
        }
        
        self.hold_reasons = {
            "legal_hold": "法律诉讼或监管调查",
            "audit_hold": "内部或外部审计",
            "regulatory_hold": "监管调查",
            "business_hold": "业务需要延长保留"
        }
    
    def assess_retention_compliance(self, data_inventory):
        """评估保留合规性"""
        compliance_report = {
            "compliant_items": [],
            "non_compliant_items": [],
            "items_nearing_disposal": [],
            "items_on_hold": [],
            "retention_summary": {}
        }
        
        current_date = datetime.datetime.now()
        
        for data_item in data_inventory:
            data_type = data_item.get("type", "unknown")
            creation_date = datetime.datetime.strptime(data_item.get("created_date", current_date.strftime("%Y-%m-%d")), "%Y-%m-%d")
            
            # 获取保留规则
            rule = self.retention_rules.get(data_type)
            if not rule:
                compliance_report["non_compliant_items"].append({
                    "data_id": data_item["id"],
                    "data_name": data_item.get("name", "未知"),
                    "reason": "未定义的数据类型",
                    "severity": "高"
                })
                continue
            
            # 计算数据年龄
            age_days = (current_date - creation_date).days
            
            # 检查是否有保留锁定
            on_hold = data_item.get("on_hold", False)
            hold_reason = data_item.get("hold_reason", "")
            
            if on_hold:
                compliance_report["items_on_hold"].append({
                    "data_id": data_item["id"],
                    "data_name": data_item.get("name", "未知"),
                    "hold_reason": hold_reason,
                    "hold_expiry": data_item.get("hold_expiry", "无期限")
                })
                continue
            
            # 检查保留合规性
            min_retention = rule["minimum_retention"]
            max_retention = rule["maximum_retention"]
            
            if age_days < min_retention:
                compliance_report["compliant_items"].append({
                    "data_id": data_item["id"],
                    "data_name": data_item.get("name", "未知"),
                    "data_type": data_type,
                    "age_days": age_days,
                    "min_retention_days": min_retention,
                    "status": "未到最低保留期"
                })
            elif age_days >= min_retention and age_days < max_retention:
                compliance_report["compliant_items"].append({
                    "data_id": data_item["id"],
                    "data_name": data_item.get("name", "未知"),
                    "data_type": data_type,
                    "age_days": age_days,
                    "min_retention_days": min_retention,
                    "max_retention_days": max_retention,
                    "status": "合规保留中"
                })
            elif age_days >= max_retention:
                # 检查是否需要审批才能处置
                if rule["disapproval_required"]:
                    status = "需要审批处置"
                    severity = "中"
                else:
                    status = "可处置"
                    severity = "低"
                
                compliance_report["non_compliant_items"].append({
                    "data_id": data_item["id"],
                    "data_name": data_item.get("name", "未知"),
                    "data_type": data_type,
                    "age_days": age_days,
                    "max_retention_days": max_retention,
                    "status": status,
                    "severity": severity,
                    "disposal_method": rule["disposal_method"]
                })
            
            # 检查接近处置期的项目
            days_until_disposal = max_retention - age_days
            if 0 < days_until_disposal <= 90:  # 90天内到期
                compliance_report["items_nearing_disposal"].append({
                    "data_id": data_item["id"],
                    "data_name": data_item.get("name", "未知"),
                    "data_type": data_type,
                    "age_days": age_days,
                    "days_until_disposal": days_until_disposal,
                    "disposal_method": rule["disposal_method"],
                    "approval_required": rule["disapproval_required"]
                })
        
        # 生成保留摘要
        compliance_report["retention_summary"] = {
            "total_items": len(data_inventory),
            "compliant_items": len(compliance_report["compliant_items"]),
            "non_compliant_items": len(compliance_report["non_compliant_items"]),
            "items_nearing_disposal": len(compliance_report["items_nearing_disposal"]),
            "items_on_hold": len(compliance_report["items_on_hold"]),
            "compliance_rate": len(compliance_report["compliant_items"]) / len(data_inventory) * 100
        }
        
        return compliance_report
    
    def apply_legal_hold(self, data_ids, hold_reason, hold_duration_days=None, requested_by=""):
        """应用法律保留"""
        hold_requests = []
        
        for data_id in data_ids:
            hold_expiry = None
            if hold_duration_days:
                expiry_date = datetime.datetime.now() + datetime.timedelta(days=hold_duration_days)
                hold_expiry = expiry_date.strftime("%Y-%m-%d")
            
            hold_request = {
                "data_id": data_id,
                "hold_id": f"hold_{int(time.time())}_{data_id}",
                "hold_reason": hold_reason,
                "hold_applied_date": datetime.datetime.now().strftime("%Y-%m-%d"),
                "hold_expiry_date": hold_expiry,
                "requested_by": requested_by,
                "approved": hold_reason in self.hold_reasons.keys(),
                "status": "已应用" if hold_reason in self.hold_reasons.keys() else "待审批"
            }
            
            hold_requests.append(hold_request)
        
        return hold_requests
    
    def plan_disposal_activities(self, items_for_disposal):
        """规划处置活动"""
        disposal_plan = {
            "immediate_disposal": [],
            "approval_required": [],
            "disposal_timeline": {},
            "estimated_disposal_volume": 0,
            "disposal_methods_summary": {}
        }
        
        total_volume = 0
        disposal_methods_count = {}
        
        for item in items_for_disposal:
            data_type = item.get("data_type", "unknown")
            rule = self.retention_rules.get(data_type, {})
            disposal_method = rule.get("disposal_method", "删除")
            approval_required = rule.get("disapproval_required", False)
            
            volume = item.get("size_gb", 0)
            total_volume += volume
            
            # 计数处置方法
            if disposal_method not in disposal_methods_count:
                disposal_methods_count[disposal_method] = {"count": 0, "volume": 0}
            disposal_methods_count[disposal_method]["count"] += 1
            disposal_methods_count[disposal_method]["volume"] += volume
            
            if approval_required:
                disposal_plan["approval_required"].append({
                    "data_id": item["data_id"],
                    "data_name": item.get("data_name", "未知"),
                    "data_type": data_type,
                    "disposal_method": disposal_method,
                    "volume": volume,
                    "reason": item.get("reason", "超过最大保留期")
                })
            else:
                disposal_plan["immediate_disposal"].append({
                    "data_id": item["data_id"],
                    "data_name": item.get("data_name", "未知"),
                    "data_type": data_type,
                    "disposal_method": disposal_method,
                    "volume": volume
                })
        
        # 创建处置时间线
        disposal_plan["disposal_timeline"] = self._create_disposal_timeline(
            disposal_plan["immediate_disposal"],
            disposal_plan["approval_required"]
        )
        
        disposal_plan["estimated_disposal_volume"] = total_volume
        disposal_plan["disposal_methods_summary"] = disposal_methods_count
        
        return disposal_plan
    
    def _create_disposal_timeline(self, immediate_items, approval_items):
        """创建处置时间线"""
        # 创建4周的时间线
        timeline = {
            "week_1": {"items": [], "volume": 0},
            "week_2": {"items": [], "volume": 0},
            "week_3": {"items": [], "volume": 0},
            "week_4": {"items": [], "volume": 0}
        }
        
        # 第一周：处置不需要审批的小型数据项
        week1_items = [item for item in immediate_items if item["volume"] <= 100]
        for item in week1_items[:10]:  # 限制为10项以避免超载
            timeline["week_1"]["items"].append(item)
            timeline["week_1"]["volume"] += item["volume"]
        
        # 第二周：处置需要审批的小型数据项
        week2_items = [item for item in approval_items if item["volume"] <= 50]
        for item in week2_items[:5]:  # 限制为5项
            timeline["week_2"]["items"].append(item)
            timeline["week_2"]["volume"] += item["volume"]
        
        # 第三周：处置不需要审批的中型数据项
        week3_items = [item for item in immediate_items if item["volume"] > 100 and item["volume"] <= 500]
        for item in week3_items[:5]:  # 限制为5项
            timeline["week_3"]["items"].append(item)
            timeline["week_3"]["volume"] += item["volume"]
        
        # 第四周：处置大型数据项和剩余项目
        remaining_immediate = [item for item in immediate_items if item not in timeline["week_1"]["items"] and item not in timeline["week_3"]["items"]]
        remaining_approval = [item for item in approval_items if item not in timeline["week_2"]["items"]]
        
        for item in remaining_immediate[:3]:  # 限制为3项
            timeline["week_4"]["items"].append(item)
            timeline["week_4"]["volume"] += item["volume"]
        
        for item in remaining_approval[:2]:  # 限制为2项
            timeline["week_4"]["items"].append(item)
            timeline["week_4"]["volume"] += item["volume"]
        
        return timeline
```

### 2.5 销毁与处置阶段

#### 2.5.1 数据销毁方法

数据销毁是确保数据无法恢复的最终处理步骤，不同敏感度的数据需要不同的销毁方法：

```python
# 数据销毁方法示例
class DataDestructionMethods:
    def __init__(self):
        self.destruction_methods = {
            "logical_deletion": {
                "description": "逻辑删除(仅标记为已删除)",
                "effectiveness": "低",
                "recovery_possible": "是",
                "suitable_for": ["测试数据", "临时数据"],
                "cost": "最低",
                "implementation": "update标记字段"
            },
            "standard_deletion": {
                "description": "标准删除(文件系统删除)",
                "effectiveness": "中",
                "recovery_possible": "可能",
                "suitable_for": ["非敏感业务数据", "过期日志"],
                "cost": "低",
                "implementation": "删除命令"
            },
            "secure_deletion": {
                "description": "安全删除(多次覆写)",
                "effectiveness": "高",
                "recovery_possible": "极难",
                "suitable_for": ["个人数据", "内部敏感数据"],
                "cost": "中",
                "implementation": "多次覆写+验证"
            },
            "crypto_shredding": {
                "description": "加密销毁(销毁密钥)",
                "effectiveness": "极高",
                "recovery_possible": "几乎不可能",
                "suitable_for": ["高度敏感数据", "加密存储数据"],
                "cost": "中高",
                "implementation": "销毁加密密钥"
            },
            "physical_destruction": {
                "description": "物理销毁(销毁存储介质)",
                "effectiveness": "绝对",
                "recovery_possible": "不可能",
                "suitable_for": ["绝密数据", "关键基础设施"],
                "cost": "高",
                "implementation": "物理销毁设备"
            }
        }
        
        self.certification_levels = {
            "basic": {
                "description": "基础认证",
                "verification_method": "日志记录",
                "documentation": "销毁记录",
                "witness_required": False
            },
            "standard": {
                "description": "标准认证",
                "verification_method": "销毁验证工具",
                "documentation": "详细报告",
                "witness_required": True
            },
            "certified": {
                "description": "认证销毁",
                "verification_method": "第三方验证",
                "documentation": "认证证书",
                "witness_required": True
            },
            "forensic": {
                "description": "取证级销毁",
                "verification_method": "取证级恢复测试",
                "documentation": "取证报告",
                "witness_required": True
            }
        }
    
    def recommend_destruction_method(self, data_classification, storage_type, compliance_requirements):
        """推荐销毁方法"""
        # 基于数据分类的推荐
        if data_classification in ["绝密", "顶级机密"]:
            recommended_method = "physical_destruction"
        elif data_classification in ["机密", "高度敏感"]:
            recommended_method = "crypto_shredding"
        elif data_classification in ["内部", "受限"]:
            recommended_method = "secure_deletion"
        elif data_classification in ["公开", "内部公开"]:
            recommended_method = "standard_deletion"
        else:
            recommended_method = "logical_deletion"
        
        # 考虑存储类型的影响
        if storage_type == "云存储" and data_classification in ["内部", "受限"]:
            recommended_method = "crypto_shredding"
        elif storage_type == "磁带存储" and data_classification in ["机密", "高度敏感"]:
            recommended_method = "physical_destruction"
        
        # 考虑合规要求的影响
        if compliance_requirements == "GDPR" and data_classification in ["个人数据", "高度敏感"]:
            recommended_method = "crypto_shredding"  # GDPR强调被遗忘权
        elif compliance_requirements == "HIPAA" and data_classification in ["受保护健康信息"]:
            recommended_method = "secure_deletion"
        
        method_details = self.destruction_methods.get(recommended_method, {})
        
        # 推荐认证级别
        if data_classification in ["绝密", "顶级机密", "机密"]:
            certification = "forensic"
        elif data_classification in ["高度敏感", "内部"]:
            certification = "certified"
        elif data_classification in ["受限"]:
            certification = "standard"
        else:
            certification = "basic"
        
        return {
            "recommended_method": recommended_method,
            "method_details": method_details,
            "certification_level": certification,
            "certification_details": self.certification_levels[certification],
            "estimated_cost": self._estimate_destruction_cost(
                recommended_method, certification, storage_type
            ),
            "compliance_coverage": self._check_compliance_coverage(
                recommended_method, compliance_requirements
            )
        }
    
    def create_destruction_plan(self, data_items):
        """创建销毁计划"""
        destruction_plan = {
            "items_for_destruction": [],
            "methods_to_use": {},
            "schedule": {},
            "estimated_cost": 0,
            "resource_requirements": {},
            "risk_mitigation": []
        }
        
        total_cost = 0
        methods_summary = {}
        resource_requirements = {}
        schedule_by_week = {"week_1": [], "week_2": [], "week_3": [], "week_4": []}
        
        # 分析每个数据项的销毁要求
        for item in data_items:
            classification = item.get("classification", "内部")
            storage_type = item.get("storage_type", "磁盘存储")
            compliance = item.get("compliance_requirements", [])
            
            # 推荐销毁方法
            recommendation = self.recommend_destruction_method(
                classification, storage_type, compliance
            )
            
            method = recommendation["recommended_method"]
            cost = recommendation["estimated_cost"]
            total_cost += cost
            
            # 汇总方法使用情况
            if method not in methods_summary:
                methods_summary[method] = {
                    "count": 0,
                    "total_volume": 0,
                    "total_cost": 0,
                    "certification": recommendation["certification_level"]
                }
            
            methods_summary[method]["count"] += 1
            methods_summary[method]["total_volume"] += item.get("size_gb", 0)
            methods_summary[method]["total_cost"] += cost
            
            # 汇总资源需求
            if method not in resource_requirements:
                resource_requirements[method] = {
                    "personnel": self._get_personnel_requirements(method),
                    "tools": self._get_tool_requirements(method),
                    "time": self._get_time_requirements(method)
                }
            
            # 添加到销毁计划
            destruction_item = {
                "data_id": item["id"],
                "data_name": item.get("name", "未知"),
                "classification": classification,
                "size_gb": item.get("size_gb", 0),
                "storage_type": storage_type,
                "destruction_method": method,
                "certification_level": recommendation["certification_level"],
                "estimated_cost": cost,
                "special_considerations": self._get_special_considerations(item)
            }
            
            destruction_plan["items_for_destruction"].append(destruction_item)
            
            # 安排销毁时间(基于方法和数据量)
            week = self._schedule_destruction(destruction_item)
            schedule_by_week[week].append(destruction_item)
        
        destruction_plan["methods_to_use"] = methods_summary
        destruction_plan["schedule"] = schedule_by_week
        destruction_plan["estimated_cost"] = total_cost
        destruction_plan["resource_requirements"] = resource_requirements
        destruction_plan["risk_mitigation"] = self._identify_risk_mitigation(data_items)
        
        return destruction_plan
    
    def execute_destruction(self, destruction_plan, execution_params):
        """执行数据销毁"""
        execution_log = {
            "execution_id": f"exec_{int(time.time())}",
            "start_time": datetime.datetime.now().isoformat(),
            "items_destroyed": [],
            "items_failed": [],
            "verification_results": [],
            "certificates_generated": [],
            "issues": []
        }
        
        # 按计划执行销毁
        for week, items in destruction_plan["schedule"].items():
            week_result = {
                "week": week,
                "items_processed": 0,
                "items_successful": 0,
                "items_failed": 0,
                "errors": []
            }
            
            for item in items:
                try:
                    # 执行销毁
                    destruction_result = self._perform_destruction(item, execution_params)
                    
                    if destruction_result["success"]:
                        execution_log["items_destroyed"].append({
                            "data_id": item["data_id"],
                            "method": item["destruction_method"],
                            "timestamp": destruction_result["timestamp"],
                            "verification": destruction_result["verification"],
                            "certificate_id": destruction_result["certificate_id"]
                        })
                        week_result["items_successful"] += 1
                        
                        # 添加认证证书
                        if destruction_result["certificate_id"]:
                            execution_log["certificates_generated"].append({
                                "certificate_id": destruction_result["certificate_id"],
                                "data_id": item["data_id"],
                                "method": item["destruction_method"],
                                "generated_at": destruction_result["timestamp"]
                            })
                    else:
                        execution_log["items_failed"].append({
                            "data_id": item["data_id"],
                            "method": item["destruction_method"],
                            "error": destruction_result["error"],
                            "timestamp": destruction_result["timestamp"]
                        })
                        week_result["items_failed"] += 1
                        week_result["errors"].append(destruction_result["error"])
                    
                    week_result["items_processed"] += 1
                    
                except Exception as e:
                    execution_log["items_failed"].append({
                        "data_id": item["data_id"],
                        "method": item["destruction_method"],
                        "error": str(e),
                        "timestamp": datetime.datetime.now().isoformat()
                    })
                    week_result["items_failed"] += 1
                    week_result["errors"].append(str(e))
                    week_result["items_processed"] += 1
            
            execution_log["verification_results"].append(week_result)
        
        execution_log["end_time"] = datetime.datetime.now().isoformat()
        execution_log["summary"] = {
            "total_items_processed": len(execution_log["items_destroyed"]) + len(execution_log["items_failed"]),
            "items_successful": len(execution_log["items_destroyed"]),
            "items_failed": len(execution_log["items_failed"]),
            "success_rate": len(execution_log["items_destroyed"]) / (len(execution_log["items_destroyed"]) + len(execution_log["items_failed"])) * 100 if (len(execution_log["items_destroyed"]) + len(execution_log["items_failed"])) > 0 else 0
        }
        
        return execution_log
    
    def _estimate_destruction_cost(self, method, certification, storage_type):
        """估算销毁成本"""
        base_costs = {
            "logical_deletion": 0.1,
            "standard_deletion": 0.5,
            "secure_deletion": 5,
            "crypto_shredding": 8,
            "physical_destruction": 20
        }
        
        certification_multipliers = {
            "basic": 1,
            "standard": 1.5,
            "certified": 2,
            "forensic": 3
        }
        
        storage_multipliers = {
            "磁盘存储": 1,
            "SSD存储": 1.2,
            "云存储": 0.8,
            "磁带存储": 1.5,
            "混合存储": 1.1
        }
        
        return base_costs.get(method, 1) * certification_multipliers.get(certification, 1) * storage_multipliers.get(storage_type, 1)
    
    def _check_compliance_coverage(self, method, compliance_requirements):
        """检查合规覆盖范围"""
        coverage = {
            "GDPR": method in ["secure_deletion", "crypto_shredding", "physical_destruction"],
            "CCPA": method in ["secure_deletion", "crypto_shredding", "physical_destruction"],
            "HIPAA": method in ["secure_deletion", "crypto_shredding", "physical_destruction"],
            "SOX": method in ["standard_deletion", "secure_deletion", "crypto_shredding", "physical_destruction"],
            "PCI-DSS": method in ["secure_deletion", "crypto_shredding", "physical_destruction"]
        }
        
        return coverage
    
    def _get_personnel_requirements(self, method):
        """获取人员需求"""
        requirements = {
            "logical_deletion": "数据管理员",
            "standard_deletion": "数据管理员+IT支持",
            "secure_deletion": "数据管理员+安全专家+IT支持",
            "crypto_shredding": "数据管理员+安全专家+密钥管理员+IT支持",
            "physical_destruction": "数据管理员+安全专家+物理销毁专家+IT支持"
        }
        return requirements.get(method, "数据管理员")
    
    def _get_tool_requirements(self, method):
        """获取工具需求"""
        requirements = {
            "logical_deletion": "数据管理系统",
            "standard_deletion": "标准删除工具",
            "secure_deletion": "安全删除软件",
            "crypto_shredding": "密钥管理系统+加密软件",
            "physical_destruction": "物理销毁设备"
        }
        return requirements.get(method, "标准工具")
    
    def _get_time_requirements(self, method):
        """获取时间需求"""
        requirements = {
            "logical_deletion": "几分钟",
            "standard_deletion": "几小时",
            "secure_deletion": "几小时到几天",
            "crypto_shredding": "几小时",
            "physical_destruction": "几天到几周"
        }
        return requirements.get(method, "几天")
    
    def _get_special_considerations(self, item):
        """获取特殊考虑因素"""
        considerations = []
        
        if item.get("has_backups", False):
            considerations.append("需要确保所有备份也被销毁")
        
        if item.get("distributed_storage", False):
            considerations.append("需要处理分布式存储的所有副本")
        
        if item.get("compliance_requirements"):
            considerations.append(f"需要满足{item['compliance_requirements']}合规要求")
        
        if item.get("business_dependencies", False):
            considerations.append("需要确认业务依赖关系已解除")
        
        return considerations
    
    def _schedule_destruction(self, item):
        """安排销毁时间"""
        # 根据数据敏感性和销毁方法安排时间
        if item["classification"] in ["绝密", "顶级机密"] or item["destruction_method"] == "physical_destruction":
            return "week_4"  # 最后处理最敏感的数据
        elif item["classification"] in ["机密", "高度敏感"] or item["destruction_method"] == "crypto_shredding":
            return "week_3"
        elif item["size_gb"] > 1000:  # 大数据集
            return "week_3"  # 提前安排大数据集销毁
        else:
            return "week_1" or "week_2"  # 普通数据尽早处理
    
    def _identify_risk_mitigation(self, data_items):
        """识别风险缓解措施"""
        mitigation = []
        
        # 检查是否有关键数据
        critical_data = [item for item in data_items if item.get("business_criticality") == "high"]
        if critical_data:
            mitigation.append("对关键业务数据进行最终审核确认")
        
        # 检查是否有分布式数据
        distributed_data = [item for item in data_items if item.get("distributed_storage", False)]
        if distributed_data:
            mitigation.append("确保所有分布式副本都被完全销毁")
        
        # 检查是否有备份
        backup_data = [item for item in data_items if item.get("has_backups", False)]
        if backup_data:
            mitigation.append("制定并执行备份销毁计划")
        
        # 检查合规要求
        compliance_items = [item for item in data_items if item.get("compliance_requirements")]
        if compliance_items:
            mitigation.append("确保销毁过程满足所有相关合规要求")
        
        return mitigation
    
    def _perform_destruction(self, item, execution_params):
        """执行销毁操作"""
        # 模拟销毁操作
        method = item["destruction_method"]
        certification = item["certification_level"]
        
        success = True
        error = None
        
        # 模拟不同方法的成功率和潜在问题
        if method == "logical_deletion":
            # 99%成功率
            if random.random() < 0.01:
                success = False
                error = "系统繁忙，无法更新删除标记"
        elif method == "standard_deletion":
            # 95%成功率
            if random.random() < 0.05:
                success = False
                error = "文件锁定，无法删除"
        elif method == "secure_deletion":
            # 90%成功率
            if random.random() < 0.1:
                success = False
                error = "介质错误，无法完成安全删除"
        elif method == "crypto_shredding":
            # 92%成功率
            if random.random() < 0.08:
                success = False
                error = "密钥管理系统故障"
        elif method == "physical_destruction":
            # 95%成功率
            if random.random() < 0.05:
                success = False
                error = "物理销毁设备故障"
        
        # 生成证书ID
        certificate_id = None
        if success and certification in ["certified", "forensic"]:
            certificate_id = f"cert_{int(time.time())}_{item['data_id']}"
        
        # 验证结果
        verification = {
            "method_used": method,
            "verification_successful": success,
            "verification_timestamp": datetime.datetime.now().isoformat(),
            "verification_method": self._get_verification_method(certification)
        }
        
        return {
            "success": success,
            "error": error,
            "timestamp": datetime.datetime.now().isoformat(),
            "verification": verification,
            "certificate_id": certificate_id
        }
    
    def _get_verification_method(self, certification):
        """获取验证方法"""
        methods = {
            "basic": "系统日志验证",
            "standard": "删除工具日志验证",
            "certified": "独立工具验证",
            "forensic": "取证级恢复测试"
        }
        return methods.get(certification, "基础验证")
```

## 3. 数据生命周期自动化工具

### 3.1 统一生命周期管理平台

```python
# 统一数据生命周期管理平台
import pandas as pd
import numpy as np
import json
import datetime
import time
import random
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.cluster import KMeans
from sklearn.preprocessing import StandardScaler
import schedule
import warnings
warnings.filterwarnings('ignore')

class DataLifecyclePlatform:
    """统一数据生命周期管理平台"""
    
    def __init__(self):
        self.storage_strategy = DataStorageStrategy()
        self.retention_policy = DataRetentionPolicy()
        self.destruction_methods = DataDestructionMethods()
        self.usage_manager = DataUsageManagement()
        
        self.data_inventory = []
        self.lifecycle_events = []
        self.automation_rules = []
        self.alert_thresholds = {
            "storage_utilization": 85,  # 存储利用率阈值(%)
            "retention_compliance": 90,  # 保留合规率阈值(%)
            "disposal_backlog": 30,     # 处置积压天数阈值
            "cost_savings_opportunity": 1000  # 成本节约机会阈值(元)
        }
    
    def register_data_asset(self, data_id, name, data_type, owner, size_gb, 
                          classification, storage_location, creation_date, 
                          business_value="medium", access_pattern="unknown"):
        """注册数据资产"""
        data_asset = {
            "id": data_id,
            "name": name,
            "type": data_type,
            "owner": owner,
            "size_gb": size_gb,
            "classification": classification,
            "storage_location": storage_location,
            "creation_date": creation_date,
            "business_value": business_value,
            "access_pattern": access_pattern,
            "current_storage_tier": self.storage_strategy.recommend_storage_tier({
                "age_months": 0,
                "access_frequency": 0,
                "business_criticality": business_value
            }),
            "last_access_date": None,
            "access_count_30_days": 0,
            "access_count_90_days": 0,
            "on_hold": False,
            "hold_reason": None,
            "hold_expiry": None,
            "retention_min_days": self.retention_policy.retention_rules.get(data_type, {}).get("minimum_retention", 365),
            "retention_max_days": self.retention_policy.retention_rules.get(data_type, {}).get("maximum_retention", 1825),
            "retention_actual_days": 0,
            "status": "active",
            "last_modified": datetime.datetime.now().isoformat()
        }
        
        self.data_inventory.append(data_asset)
        
        # 记录生命周期事件
        self._record_lifecycle_event(data_id, "registered", {
            "storage_tier": data_asset["current_storage_tier"],
            "retention_policy_applied": data_asset["retention_min_days"]
        })
        
        return data_asset
    
    def simulate_usage_patterns(self, days=30):
        """模拟使用模式"""
        current_date = datetime.datetime.now()
        
        for asset in self.data_inventory:
            # 根据访问模式生成模拟访问
            if asset["access_pattern"] == "frequent":
                access_frequency = 15  # 平均每天15次
            elif asset["access_pattern"] == "regular":
                access_frequency = 5   # 平均每天5次
            elif asset["access_pattern"] == "occasional":
                access_frequency = 1   # 平均每天1次
            else:  # rare or unknown
                access_frequency = 0.1  # 平均10天1次
            
            # 生成过去30天的访问记录
            for day in range(days):
                date = current_date - datetime.timedelta(days=day)
                day_accesses = np.random.poisson(access_frequency)
                
                # 更新访问计数
                if day < 30:
                    asset["access_count_30_days"] += day_accesses
                if day < 90:
                    asset["access_count_90_days"] += day_accesses
                
                # 记录最后访问日期
                if day_accesses > 0:
                    asset["last_access_date"] = date.strftime("%Y-%m-%d")
                
                # 记录访问事件
                for _ in range(day_accesses):
                    self._record_lifecycle_event(asset["id"], "access", {
                        "date": date.strftime("%Y-%m-%d"),
                        "access_type": "read"
                    })
        
        return f"已模拟{days}天的使用模式"
    
    def run_lifecycle_analysis(self):
        """运行生命周期分析"""
        analysis_results = {
            "storage_optimization": self._analyze_storage_optimization(),
            "retention_compliance": self._analyze_retention_compliance(),
            "archiving_opportunities": self._analyze_archiving_opportunities(),
            "disposal_candidates": self._analyze_disposal_candidates(),
            "cost_optimization": self._analyze_cost_optimization(),
            "recommendations": []
        }
        
        # 生成综合推荐
        recommendations = []
        
        # 存储优化推荐
        storage_rec = self._generate_storage_recommendations(analysis_results["storage_optimization"])
        recommendations.extend(storage_rec)
        
        # 合规性推荐
        compliance_rec = self._generate_compliance_recommendations(analysis_results["retention_compliance"])
        recommendations.extend(compliance_rec)
        
        # 成本优化推荐
        cost_rec = self._generate_cost_recommendations(analysis_results["cost_optimization"])
        recommendations.extend(cost_rec)
        
        analysis_results["recommendations"] = recommendations
        
        return analysis_results
    
    def execute_automated_actions(self, analysis_results, dry_run=True):
        """执行自动化操作"""
        executed_actions = []
        
        for recommendation in analysis_results["recommendations"]:
            if recommendation["automation_ready"] and not recommendation.get("requires_approval", False):
                if not dry_run:
                    result = self._execute_action(recommendation)
                    executed_actions.append({
                        "action": recommendation["action"],
                        "data_id": recommendation["data_id"],
                        "result": result,
                        "timestamp": datetime.datetime.now().isoformat()
                    })
                else:
                    executed_actions.append({
                        "action": recommendation["action"],
                        "data_id": recommendation["data_id"],
                        "result": "模拟执行",
                        "timestamp": datetime.datetime.now().isoformat()
                    })
        
        return executed_actions
    
    def generate_lifecycle_dashboard(self, analysis_results):
        """生成生命周期仪表板"""
        dashboard_data = {}
        
        # 存储分布图
        dashboard_data["storage_distribution"] = self._prepare_storage_distribution_data()
        
        # 合规性状态图
        dashboard_data["compliance_status"] = self._prepare_compliance_status_data(analysis_results["retention_compliance"])
        
        # 成本优化图
        dashboard_data["cost_optimization"] = self._prepare_cost_optimization_data(analysis_results["cost_optimization"])
        
        # 生命周期阶段分布图
        dashboard_data["lifecycle_stages"] = self._prepare_lifecycle_stages_data()
        
        # 预警信息
        dashboard_data["alerts"] = self._generate_alerts(analysis_results)
        
        return dashboard_data
    
    def visualize_lifecycle_dashboard(self, dashboard_data):
        """可视化生命周期仪表板"""
        fig, axes = plt.subplots(2, 2, figsize=(15, 12))
        fig.suptitle('数据生命周期管理仪表板', fontsize=16)
        
        # 1. 存储分布饼图
        storage_data = dashboard_data["storage_distribution"]
        axes[0, 0].pie(storage_data["values"], labels=storage_data["labels"], autopct='%1.1f%%', startangle=90)
        axes[0, 0].set_title('数据存储层级分布')
        
        # 2. 合规性状态条形图
        compliance_data = dashboard_data["compliance_status"]
        axes[0, 1].bar(compliance_data["categories"], compliance_data["values"], color=['green', 'orange', 'red'])
        axes[0, 1].set_title('数据保留合规性状态')
        axes[0, 1].set_ylabel('数据量(GB)')
        axes[0, 1].set_ylim(0, max(compliance_data["values"]) * 1.2)
        
        # 3. 成本优化条形图
        cost_data = dashboard_data["cost_optimization"]
        axes[1, 0].bar(cost_data["categories"], cost_data["values"])
        axes[1, 0].set_title('潜在月度成本节约')
        axes[1, 0].set_ylabel('成本节约(元)')
        axes[1, 0].tick_params(axis='x', rotation=45)
        
        # 4. 生命周期阶段分布图
        lifecycle_data = dashboard_data["lifecycle_stages"]
        axes[1, 1].pie(lifecycle_data["values"], labels=lifecycle_data["labels"], autopct='%1.1f%%', startangle=90)
        axes[1, 1].set_title('数据生命周期阶段分布')
        
        plt.tight_layout(rect=[0, 0.03, 1, 0.95])
        
        # 显示预警信息
        alerts = dashboard_data["alerts"]
        if alerts:
            alert_text = "\n".join([f"• {alert}" for alert in alerts])
            fig.text(0.5, 0.01, f"预警信息:\n{alert_text}", ha='center', fontsize=10, 
                    bbox={"facecolor":"yellow", "alpha":0.3, "pad":5})
        
        plt.savefig('data_lifecycle_dashboard.png', dpi=300, bbox_inches='tight')
        plt.close()
        
        return "仪表板已生成并保存为 data_lifecycle_dashboard.png"
    
    def run_full_lifecycle_cycle(self, simulate_days=30):
        """运行完整生命周期循环"""
        # 1. 模拟使用模式
        self.simulate_usage_patterns(simulate_days)
        
        # 2. 运行生命周期分析
        analysis_results = self.run_lifecycle_analysis()
        
        # 3. 执行自动化操作(模拟)
        executed_actions = self.execute_automated_actions(analysis_results, dry_run=True)
        
        # 4. 生成仪表板数据
        dashboard_data = self.generate_lifecycle_dashboard(analysis_results)
        
        # 5. 可视化仪表板
        dashboard_result = self.visualize_lifecycle_dashboard(dashboard_data)
        
        return {
            "analysis_results": analysis_results,
            "executed_actions": executed_actions,
            "dashboard_data": dashboard_data,
            "dashboard_result": dashboard_result
        }
    
    def _record_lifecycle_event(self, data_id, event_type, details):
        """记录生命周期事件"""
        event = {
            "data_id": data_id,
            "event_type": event_type,
            "timestamp": datetime.datetime.now().isoformat(),
            "details": details
        }
        self.lifecycle_events.append(event)
    
    def _analyze_storage_optimization(self):
        """分析存储优化机会"""
        # 更新数据资产的年龄和存储层级
        current_date = datetime.datetime.now()
        
        for asset in self.data_inventory:
            # 计算年龄
            if asset.get("creation_date"):
                creation_date = datetime.datetime.strptime(asset["creation_date"], "%Y-%m-%d")
                age_days = (current_date - creation_date).days
                asset["age_days"] = age_days
                asset["age_months"] = age_days // 30
            else:
                asset["age_days"] = 0
                asset["age_months"] = 0
            
            # 计算实际保留天数
            asset["retention_actual_days"] = age_days
            
            # 重新评估存储层级
            recommended_tier = self.storage_strategy.recommend_storage_tier({
                "age_months": asset["age_months"],
                "access_frequency": asset.get("access_count_90_days", 0),
                "business_criticality": asset["business_value"]
            })
            
            asset["recommended_storage_tier"] = recommended_tier
        
        # 使用存储策略优化工具
        storage_optimization = self.storage_strategy.optimize_storage_allocation(self.data_inventory)
        
        return storage_optimization
    
    def _analyze_retention_compliance(self):
        """分析保留合规性"""
        # 更新数据资产的保留状态
        current_date = datetime.datetime.now()
        
        for asset in self.data_inventory:
            data_type = asset["type"]
            rule = self.retention_policy.retention_rules.get(data_type, {})
            
            if rule:
                asset["retention_min_days"] = rule.get("minimum_retention", 365)
                asset["retention_max_days"] = rule.get("maximum_retention", 1825)
                asset["retention_status"] = self._determine_retention_status(asset)
        
        # 使用保留政策评估工具
        compliance_analysis = self.retention_policy.assess_retention_compliance(self.data_inventory)
        
        return compliance_analysis
    
    def _analyze_archiving_opportunities(self):
        """分析归档机会"""
        # 使用归档策略识别候选数据
        archiving_candidates = self.storage_strategy.identify_candidates_for_archiving(self.data_inventory)
        
        # 创建归档计划
        archive_plan = self.storage_strategy.create_archive_plan(archiving_candidates)
        
        return {
            "candidates": archiving_candidates,
            "plan": archive_plan
        }
    
    def _analyze_disposal_candidates(self):
        """分析处置候选数据"""
        # 筛选可能需要处置的数据
        disposal_candidates = []
        
        for asset in self.data_inventory:
            if asset["retention_status"] in ["可处置", "需要审批处置"] and not asset.get("on_hold", False):
                disposal_candidates.append({
                    "data_id": asset["id"],
                    "data_name": asset["name"],
                    "data_type": asset["type"],
                    "classification": asset["classification"],
                    "retention_status": asset["retention_status"],
                    "size_gb": asset["size_gb"],
                    "age_days": asset.get("retention_actual_days", 0)
                })
        
        # 创建处置计划
        disposal_plan = self.retention_policy.plan_disposal_activities(disposal_candidates)
        
        return {
            "candidates": disposal_candidates,
            "plan": disposal_plan
        }
    
    def _analyze_cost_optimization(self):
        """分析成本优化机会"""
        cost_analysis = {
            "storage_savings": 0,
            "retention_savings": 0,
            "total_savings": 0,
            "opportunities": []
        }
        
        # 存储成本节约
        storage_optimization = self._analyze_storage_optimization()
        for rec in storage_optimization:
            if rec.get("estimated_savings", 0) > 0:
                cost_analysis["storage_savings"] += rec["estimated_savings"]
                cost_analysis["opportunities"].append({
                    "type": "存储优化",
                    "data_id": rec["data_id"],
                    "action": f"从{rec['current_tier']}迁移到{rec['recommended_tier']}",
                    "estimated_savings": rec["estimated_savings"]
                })
        
        # 数据保留成本节约
        disposal_candidates = self._analyze_disposal_candidates()
        disposal_plan = disposal_candidates["plan"]
        
        immediate_disposal = disposal_plan.get("immediate_disposal", [])
        for item in immediate_disposal:
            disposal_method = item["disposal_method"]
            method_cost = self.destruction_methods.destruction_methods.get(disposal_method, {}).get("cost", "中")
            
            # 估算存储成本节约
            storage_cost_per_gb = 0.1  # 假设每GB每月成本
            monthly_storage_cost = item["volume"] * storage_cost_per_gb
            annual_savings = monthly_storage_cost * 12
            
            cost_analysis["retention_savings"] += annual_savings
            cost_analysis["opportunities"].append({
                "type": "数据处置",
                "data_id": item["data_id"],
                "action": f"通过{disposal_method}处置数据",
                "estimated_savings": annual_savings
            })
        
        cost_analysis["total_savings"] = cost_analysis["storage_savings"] + cost_analysis["retention_savings"]
        
        return cost_analysis
    
    def _generate_storage_recommendations(self, storage_optimization):
        """生成存储优化推荐"""
        recommendations = []
        
        for rec in storage_optimization:
            if rec.get("estimated_savings", 0) > 0:  # 只有能节省成本才推荐
                recommendation = {
                    "data_id": rec["data_id"],
                    "action": "存储层级迁移",
                    "details": f"将数据从{rec['current_tier']}迁移到{rec['recommended_tier']}",
                    "estimated_savings": rec["estimated_savings"],
                    "performance_impact": rec["performance_impact"],
                    "automation_ready": rec["performance_impact"] != "性能下降",
                    "requires_approval": rec["performance_impact"] == "性能下降",
                    "priority": "高" if rec["estimated_savings"] > 100 else "中"
                }
                recommendations.append(recommendation)
        
        return recommendations
    
    def _generate_compliance_recommendations(self, retention_compliance):
        """生成合规性推荐"""
        recommendations = []
        
        # 对非合规项生成推荐
        for item in retention_compliance["non_compliant_items"]:
            if item["status"] == "可处置":
                recommendation = {
                    "data_id": item["data_id"],
                    "action": "数据处置",
                    "details": f"处置已超过最大保留期({item['max_retention_days']}天)的数据",
                    "estimated_savings": item["size_gb"] * 0.1 * 12,  # 估算年度存储成本
                    "automation_ready": True,
                    "requires_approval": False,
                    "priority": "高"
                }
                recommendations.append(recommendation)
            elif item["status"] == "需要审批处置":
                recommendation = {
                    "data_id": item["data_id"],
                    "action": "数据处置审批",
                    "details": f"请求审批处置已超过最大保留期({item['max_retention_days']}天)的数据",
                    "estimated_savings": item["size_gb"] * 0.1 * 12,  # 估算年度存储成本
                    "automation_ready": False,
                    "requires_approval": True,
                    "priority": "高"
                }
                recommendations.append(recommendation)
        
        # 对接近处置期的项生成推荐
        for item in retention_compliance["items_nearing_disposal"]:
            if not item["approval_required"]:
                recommendation = {
                    "data_id": item["data_id"],
                    "action": "计划数据处置",
                    "details": f"计划在{item['days_until_disposal']}天后处置数据",
                    "estimated_savings": 0,  # 未来节约
                    "automation_ready": False,
                    "requires_approval": False,
                    "priority": "中"
                }
                recommendations.append(recommendation)
        
        return recommendations
    
    def _generate_cost_recommendations(self, cost_optimization):
        """生成成本优化推荐"""
        recommendations = []
        
        for opportunity in cost_optimization["opportunities"]:
            # 只推荐高成本节约机会
            if opportunity["estimated_savings"] >= self.alert_thresholds["cost_savings_opportunity"]:
                recommendation = {
                    "data_id": opportunity["data_id"],
                    "action": opportunity["action"],
                    "details": f"执行{opportunity['type']}操作可节约{opportunity['estimated_savings']:.2f}元",
                    "estimated_savings": opportunity["estimated_savings"],
                    "automation_ready": opportunity["type"] == "存储优化",
                    "requires_approval": opportunity["type"] == "数据处置",
                    "priority": "高" if opportunity["estimated_savings"] > 5000 else "中"
                }
                recommendations.append(recommendation)
        
        return recommendations
    
    def _determine_retention_status(self, asset):
        """确定保留状态"""
        age_days = asset.get("retention_actual_days", 0)
        min_days = asset.get("retention_min_days", 365)
        max_days = asset.get("retention_max_days", 1825)
        
        if asset.get("on_hold", False):
            return "保留锁定"
        elif age_days < min_days:
            return "保留期内"
        elif age_days >= max_days:
            rule = self.retention_policy.retention_rules.get(asset["type"], {})
            if rule.get("disapproval_required", False):
                return "需要审批处置"
            else:
                return "可处置"
        else:
            return "保留期内"
    
    def _execute_action(self, recommendation):
        """执行推荐操作"""
        action = recommendation["action"]
        data_id = recommendation["data_id"]
        
        # 找到数据资产
        asset = next((a for a in self.data_inventory if a["id"] == data_id), None)
        if not asset:
            return {"success": False, "error": "数据资产未找到"}
        
        # 执行操作
        if action == "存储层级迁移":
            old_tier = asset["current_storage_tier"]
            asset["current_storage_tier"] = asset["recommended_storage_tier"]
            
            self._record_lifecycle_event(data_id, "storage_tier_changed", {
                "old_tier": old_tier,
                "new_tier": asset["current_storage_tier"]
            })
            
            return {"success": True, "message": f"已迁移到{asset['current_storage_tier']}层"}
        
        elif action == "数据处置":
            # 从数据清单中移除
            self.data_inventory.remove(asset)
            
            self._record_lifecycle_event(data_id, "disposed", {
                "disposal_method": "标准删除",
                "size_gb": asset["size_gb"]
            })
            
            return {"success": True, "message": "数据已处置"}
        
        else:
            return {"success": False, "error": "未知操作类型"}
    
    def _prepare_storage_distribution_data(self):
        """准备存储分布数据"""
        tier_counts = {"hot": 0, "warm": 0, "cold": 0}
        
        for asset in self.data_inventory:
            tier = asset["current_storage_tier"]
            if tier in tier_counts:
                tier_counts[tier] += asset["size_gb"]
        
        return {
            "labels": list(tier_counts.keys()),
            "values": list(tier_counts.values())
        }
    
    def _prepare_compliance_status_data(self, compliance_analysis):
        """准备合规状态数据"""
        compliant_volume = sum(item["size_gb"] for item in compliance_analysis["compliant_items"])
        non_compliant_volume = sum(item["size_gb"] for item in compliance_analysis["non_compliant_items"])
        nearing_disposal_volume = sum(item["size_gb"] for item in compliance_analysis["items_nearing_disposal"])
        on_hold_volume = sum(asset["size_gb"] for asset in self.data_inventory if asset.get("on_hold", False))
        
        return {
            "categories": ["合规", "不合规", "接近处置", "保留锁定"],
            "values": [compliant_volume, non_compliant_volume, nearing_disposal_volume, on_hold_volume]
        }
    
    def _prepare_cost_optimization_data(self, cost_analysis):
        """准备成本优化数据"""
        storage_savings = cost_analysis["storage_savings"]
        retention_savings = cost_analysis["retention_savings"]
        
        return {
            "categories": ["存储优化", "数据处置"],
            "values": [storage_savings, retention_savings]
        }
    
    def _prepare_lifecycle_stages_data(self):
        """准备生命周期阶段数据"""
        stage_counts = {"创建": 0, "使用": 0, "归档": 0, "待处置": 0}
        
        for asset in self.data_inventory:
            tier = asset["current_storage_tier"]
            
            if asset.get("on_hold", False):
                stage = "保留锁定"
            elif asset["retention_status"] in ["可处置", "需要审批处置"]:
                stage = "待处置"
            elif tier == "hot":
                stage = "使用"
            elif tier == "cold":
                stage = "归档"
            else:
                stage = "使用"
            
            if stage in stage_counts:
                stage_counts[stage] += 1
            else:
                stage_counts[stage] = 1
        
        return {
            "labels": list(stage_counts.keys()),
            "values": list(stage_counts.values())
        }
    
    def _generate_alerts(self, analysis_results):
        """生成预警信息"""
        alerts = []
        
        # 存储利用率预警
        storage_distribution = self._prepare_storage_distribution_data()
        total_size = sum(storage_distribution["values"])
        hot_storage_size = storage_distribution["values"][0]  # 假设第一个是hot存储
        hot_utilization = (hot_storage_size / total_size) * 100 if total_size > 0 else 0
        
        if hot_utilization > self.alert_thresholds["storage_utilization"]:
            alerts.append(f"热存储利用率({hot_utilization:.1f}%)超过阈值({self.alert_thresholds['storage_utilization']}%)")
        
        # 合规性预警
        compliance_analysis = analysis_results["retention_compliance"]
        compliance_rate = compliance_analysis["retention_summary"]["compliance_rate"]
        
        if compliance_rate < self.alert_thresholds["retention_compliance"]:
            alerts.append(f"数据保留合规率({compliance_rate:.1f}%)低于阈值({self.alert_thresholds['retention_compliance']}%)")
        
        # 处置积压预警
        disposal_candidates = analysis_results["disposal_candidates"]
        non_compliant_items = compliance_analysis["non_compliant_items"]
        
        if len(non_compliant_items) > 0:
            max_overdue_days = max([
                item.get("age_days", 0) - item.get("max_retention_days", 0)
                for item in non_compliant_items
            ])
            
            if max_overdue_days > self.alert_thresholds["disposal_backlog"]:
                alerts.append(f"数据处置积压超过{max_overdue_days}天")
        
        return alerts

# 创建并运行数据生命周期平台
lifecycle_platform = DataLifecyclePlatform()

# 注册示例数据资产
sample_data = [
    {"id": "CUST001", "name": "客户基本信息表", "type": "customer_data", "owner": "销售部", "size_gb": 50, "classification": "内部", "storage_location": "数据库服务器A", "creation_date": "2020-01-15", "business_value": "高", "access_pattern": "frequent"},
    {"id": "TRANS001", "name": "交易记录表", "type": "transaction_data", "owner": "财务部", "size_gb": 200, "classification": "机密", "storage_location": "数据库服务器B", "creation_date": "2018-03-20", "business_value": "高", "access_pattern": "regular"},
    {"id": "ANAL001", "name": "用户行为分析数据", "type": "analytics_data", "owner": "数据团队", "size_gb": 300, "classification": "内部", "storage_location": "数据仓库", "creation_date": "2019-05-10", "business_value": "中", "access_pattern": "occasional"},
    {"id": "EMPL001", "name": "员工信息表", "type": "employee_data", "owner": "人力资源部", "size_gb": 20, "classification": "机密", "storage_location": "HR系统", "creation_date": "2017-08-25", "business_value": "高", "access_pattern": "rare"},
    {"id": "EMAIL001", "name": "历史邮件归档", "type": "email_communications", "owner": "IT部门", "size_gb": 500, "classification": "内部", "storage_location": "邮件服务器", "creation_date": "2017-01-05", "business_value": "低", "access_pattern": "rare"},
    {"id": "PROD001", "name": "产品销售数据", "type": "analytics_data", "owner": "产品部", "size_gb": 150, "classification": "内部公开", "storage_location": "商业智能平台", "creation_date": "2021-02-14", "business_value": "高", "access_pattern": "regular"},
    {"id": "FINC001", "name": "财务报表数据", "type": "financial_data", "owner": "财务部", "size_gb": 80, "classification": "机密", "storage_location": "财务系统", "creation_date": "2019-11-30", "business_value": "高", "access_pattern": "frequent"},
    {"id": "MKTG001", "name": "营销活动数据", "type": "analytics_data", "owner": "市场部", "size_gb": 120, "classification": "内部", "storage_location": "营销系统", "creation_date": "2020-07-22", "business_value": "中", "access_pattern": "occasional"}
]

for data in sample_data:
    lifecycle_platform.register_data_asset(**data)

# 运行完整生命周期分析
print("运行数据生命周期管理平台...")
results = lifecycle_platform.run_full_lifecycle_cycle(simulate_days=60)

print("\n=== 数据生命周期分析结果 ===")
print(f"数据资产总数: {len(lifecycle_platform.data_inventory)}")
print(f"存储优化建议: {len(results['analysis_results']['storage_optimization'])}")
print(f"合规性问题: {len(results['analysis_results']['retention_compliance']['non_compliant_items'])}")
print(f"归档候选: {len(results['analysis_results']['archiving_opportunities']['candidates'])}")
print(f"处置候选: {len(results['analysis_results']['disposal_candidates']['candidates'])}")
print(f"潜在月度成本节约: {results['analysis_results']['cost_optimization']['total_savings']:.2f}元")

print("\n=== 主要推荐 ===")
for i, rec in enumerate(results['analysis_results']['recommendations'][:5]):  # 只显示前5个推荐
    print(f"{i+1}. [{rec['priority']}] {rec['action']} - {rec['details']}")
    print(f"   预估节约: {rec['estimated_savings']:.2f}元 | 自动化: {'是' if rec['automation_ready'] else '否'} | 需审批: {'是' if rec['requires_approval'] else '否'}")

print("\n=== 执行的操作 ===")
for action in results['executed_actions'][:5]:  # 只显示前5个操作
    print(f"- {action['action']} (数据ID: {action['data_id']}) - 结果: {action['result']}")

print("\n=== 仪表板结果 ===")
print(results['dashboard_result'])
```

## 4. 数据生命周期最佳实践

### 4.1 生命周期管理策略最佳实践

1. **分层存储策略**
   - 根据数据访问频率和业务价值实施分层存储
   - 定期评估和调整数据存储层级
   - 自动化数据分层过程，减少人工干预

2. **保留政策优化**
   - 基于业务需求和法规要求制定明确的数据保留政策
   - 区分不同数据类型的最小和最大保留期限
   - 实施保留政策监控和合规性检查

3. **自动化处置流程**
   - 建立标准化的数据处置工作流
   - 根据数据分类实施差异化的处置方法
   - 确保处置过程的可追溯性和可审计性

4. **成本效益分析**
   - 定期评估数据存储和处理成本
   - 基于数据价值优化资源配置
   - 识别和实施成本节约机会

5. **隐私保护集成**
   - 在整个生命周期中嵌入隐私保护措施
   - 实施数据最小化原则
   - 确保隐私控制和数据生命周期管理的一致性

### 4.2 技术实现最佳实践

1. **元数据驱动管理**
   - 建立全面的数据元数据体系
   - 基于元数据自动化生命周期决策
   - 确保元数据质量和完整性

2. **策略即代码(Policy as Code)**
   - 使用代码形式定义和管理生命周期策略
   - 实施策略版本控制和审查
   - 自动化策略验证和合规检查

3. **事件驱动架构**
   - 基于数据生命周期事件触发自动化流程
   - 实施事件溯源和审计日志
   - 确保事件处理的可靠性和一致性

4. **智能分析决策**
   - 利用机器学习预测数据访问模式
   - 基于数据使用模式优化存储策略
   - 实施异常检测和预警机制

5. **集成治理框架**
   - 将生命周期管理集成到整体数据治理框架
   - 确保与数据质量、安全和主数据管理的协调
   - 建立跨部门协作机制

### 4.3 组织管理最佳实践

1. **明确责任分工**
   - 定义数据生命周期管理角色和职责
   - 建立数据所有者和管理者机制
   - 实施生命周期管理绩效考核

2. **持续培训和能力建设**
   - 提供生命周期管理相关培训
   - 建立生命周期管理专业社区
   - 分享最佳实践和经验教训

3. **跨部门协作**
   - 建立跨部门生命周期管理协调机制
   - 促进业务和IT部门的有效沟通
   - 确保业务需求与技术实现的一致性

4. **持续改进机制**
   - 定期评估和优化生命周期管理流程
   - 收集和分析生命周期管理指标
   - 实施基于反馈的持续改进

5. **供应商管理**
   - 评估和选择合适的数据管理工具和服务
   - 建立供应商生命周期管理标准
   - 监控供应商服务质量和合规性

## 5. 总结与展望

数据生命周期管理是现代数据治理的核心组成部分，它贯穿于数据从创建到销毁的全过程。通过有效的数据生命周期管理，组织可以实现：

1. **成本优化**：根据数据价值和访问频率优化存储资源分配，显著降低数据管理成本。
2. **合规性保障**：确保数据保留和处置符合法规要求，降低合规风险。
3. **效率提升**：通过自动化流程和智能决策，提高数据管理效率。
4. **风险控制**：降低数据泄露、滥用和丢失的风险。
5. **价值最大化**：确保数据在整个生命周期中持续创造价值。

随着数据量的持续增长和法规环境的不断变化，数据生命周期管理将变得更加重要。未来发展趋势包括：

1. **智能化**：利用人工智能和机器学习技术，实现更精准的数据生命周期决策。
2. **自动化**：通过高级自动化技术，减少人工干预，提高管理效率。
3. **集成化**：将生命周期管理更深度地集成到整体数据架构和业务流程中。
4. **实时化**：实现实时的数据生命周期监控和调整。
5. **隐私优先**：更加注重隐私保护，将隐私原则嵌入到生命周期的每个阶段。

有效的数据生命周期管理需要技术、流程和人员的协同努力，是组织数据治理能力成熟度的重要标志。通过实施本章介绍的方法和工具，组织可以建立全面的数据生命周期管理体系，为数据驱动的决策提供坚实基础。
