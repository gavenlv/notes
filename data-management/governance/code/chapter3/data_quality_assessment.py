#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ•°æ®è´¨é‡è¯„ä¼°å·¥å…·
ç”¨äºè¯„ä¼°æ•°æ®é›†çš„å…­å¤§è´¨é‡ç»´åº¦ï¼šå‡†ç¡®æ€§ã€å®Œæ•´æ€§ã€ä¸€è‡´æ€§ã€æ—¶æ•ˆæ€§ã€å”¯ä¸€æ€§ã€æœ‰æ•ˆæ€§
"""

import pandas as pd
import numpy as np
import re
from datetime import datetime, timedelta
import matplotlib.pyplot as plt
import seaborn as sns
from typing import Dict, List, Tuple, Any
import json
import warnings
warnings.filterwarnings('ignore')

class DataQualityAssessment:
    """æ•°æ®è´¨é‡è¯„ä¼°å™¨"""
    
    def __init__(self, df: pd.DataFrame):
        """
        åˆå§‹åŒ–è¯„ä¼°å™¨
        
        Args:
            df (pd.DataFrame): å¾…è¯„ä¼°çš„æ•°æ®é›†
        """
        self.df = df.copy()
        self.results = {}
        self.dimension_scores = {}
        
    def assess_completeness(self, columns: List[str] = None) -> Dict:
        """
        è¯„ä¼°æ•°æ®å®Œæ•´æ€§
        
        Args:
            columns (List[str]): è¦è¯„ä¼°çš„åˆ—ï¼Œå¦‚æœä¸ºNoneåˆ™è¯„ä¼°æ‰€æœ‰åˆ—
            
        Returns:
            Dict: å®Œæ•´æ€§è¯„ä¼°ç»“æœ
        """
        if columns is None:
            columns = self.df.columns.tolist()
        
        results = {
            'dimension': 'completeness',
            'column_scores': {},
            'overall_score': 0,
            'issues': []
        }
        
        total_cells = 0
        missing_cells = 0
        
        for col in columns:
            col_data = self.df[col]
            total_count = len(col_data)
            missing_count = col_data.isnull().sum()
            completeness_score = (1 - missing_count / total_count) * 100
            
            results['column_scores'][col] = {
                'score': completeness_score,
                'missing_count': missing_count,
                'total_count': total_count,
                'missing_percentage': (missing_count / total_count) * 100
            }
            
            total_cells += total_count
            missing_cells += missing_count
            
            # è¯†åˆ«å®Œæ•´æ€§é—®é¢˜
            if completeness_score < 80:
                results['issues'].append({
                    'column': col,
                    'type': 'low_completeness',
                    'severity': 'high' if completeness_score < 60 else 'medium',
                    'description': f"åˆ— {col} çš„å®Œæ•´ç‡ä»…ä¸º {completeness_score:.2f}%"
                })
        
        results['overall_score'] = (1 - missing_cells / total_cells) * 100
        self.dimension_scores['completeness'] = results['overall_score']
        
        return results
    
    def assess_uniqueness(self, columns: List[str] = None) -> Dict:
        """
        è¯„ä¼°æ•°æ®å”¯ä¸€æ€§
        
        Args:
            columns (List[str]): è¦è¯„ä¼°çš„åˆ—ï¼Œå¦‚æœä¸ºNoneåˆ™è¯„ä¼°æ‰€æœ‰åˆ—
            
        Returns:
            Dict: å”¯ä¸€æ€§è¯„ä¼°ç»“æœ
        """
        if columns is None:
            columns = self.df.columns.tolist()
        
        results = {
            'dimension': 'uniqueness',
            'column_scores': {},
            'overall_score': 0,
            'issues': []
        }
        
        total_duplicates = 0
        total_records = len(self.df) * len(columns)
        
        for col in columns:
            col_data = self.df[col]
            duplicate_count = col_data.duplicated().sum()
            uniqueness_score = (1 - duplicate_count / len(col_data)) * 100
            
            results['column_scores'][col] = {
                'score': uniqueness_score,
                'duplicate_count': duplicate_count,
                'unique_count': col_data.nunique(),
                'total_count': len(col_data),
                'duplicate_percentage': (duplicate_count / len(col_data)) * 100
            }
            
            total_duplicates += duplicate_count
            
            # è¯†åˆ«å”¯ä¸€æ€§é—®é¢˜
            if uniqueness_score < 95:
                results['issues'].append({
                    'column': col,
                    'type': 'low_uniqueness',
                    'severity': 'high' if uniqueness_score < 90 else 'medium',
                    'description': f"åˆ— {col} çš„å”¯ä¸€ç‡ä»…ä¸º {uniqueness_score:.2f}%"
                })
        
        results['overall_score'] = (1 - total_duplicates / total_records) * 100
        self.dimension_scores['uniqueness'] = results['overall_score']
        
        return results
    
    def assess_validity(self, column_rules: Dict[str, Dict] = None) -> Dict:
        """
        è¯„ä¼°æ•°æ®æœ‰æ•ˆæ€§
        
        Args:
            column_rules (Dict[str, Dict]): åˆ—éªŒè¯è§„åˆ™
                æ ¼å¼: {'åˆ—å': {'type': 'regex', 'pattern': 'æ­£åˆ™è¡¨è¾¾å¼', 'type': 'range', 'min': æœ€å°å€¼, 'max': æœ€å¤§å€¼}}
                
        Returns:
            Dict: æœ‰æ•ˆæ€§è¯„ä¼°ç»“æœ
        """
        if column_rules is None:
            # é»˜è®¤è§„åˆ™
            column_rules = self._infer_column_rules()
        
        results = {
            'dimension': 'validity',
            'column_scores': {},
            'overall_score': 0,
            'issues': []
        }
        
        total_records = 0
        valid_records = 0
        
        for col, rule in column_rules.items():
            if col not in self.df.columns:
                continue
                
            col_data = self.df[col].dropna()  # æ’é™¤ç©ºå€¼
            total_count = len(col_data)
            valid_count = 0
            invalid_records = []
            
            for idx, value in col_data.items():
                if self._validate_value(value, rule):
                    valid_count += 1
                else:
                    invalid_records.append(idx)
            
            validity_score = (valid_count / total_count) * 100 if total_count > 0 else 100
            
            results['column_scores'][col] = {
                'score': validity_score,
                'valid_count': valid_count,
                'invalid_count': total_count - valid_count,
                'total_count': total_count,
                'invalid_percentage': ((total_count - valid_count) / total_count) * 100 if total_count > 0 else 0,
                'invalid_records': invalid_records[:10]  # åªè®°å½•å‰10ä¸ªæ— æ•ˆè®°å½•
            }
            
            total_records += total_count
            valid_records += valid_count
            
            # è¯†åˆ«æœ‰æ•ˆæ€§é—®é¢˜
            if validity_score < 90:
                results['issues'].append({
                    'column': col,
                    'type': 'low_validity',
                    'severity': 'high' if validity_score < 80 else 'medium',
                    'description': f"åˆ— {col} çš„æœ‰æ•ˆç‡ä»…ä¸º {validity_score:.2f}%"
                })
        
        results['overall_score'] = (valid_records / total_records) * 100 if total_records > 0 else 100
        self.dimension_scores['validity'] = results['overall_score']
        
        return results
    
    def assess_accuracy(self, reference_data: pd.DataFrame = None, key_columns: List[str] = None) -> Dict:
        """
        è¯„ä¼°æ•°æ®å‡†ç¡®æ€§ï¼ˆéœ€è¦å‚è€ƒæ•°æ®ï¼‰
        
        Args:
            reference_data (pd.DataFrame): å‚è€ƒæ•°æ®é›†
            key_columns (List[str]): ç”¨äºåŒ¹é…çš„é”®åˆ—
            
        Returns:
            Dict: å‡†ç¡®æ€§è¯„ä¼°ç»“æœ
        """
        if reference_data is None or key_columns is None:
            # å¦‚æœæ²¡æœ‰å‚è€ƒæ•°æ®ï¼Œè¿”å›åŸºäºè§„åˆ™çš„å‡†ç¡®æ€§è¯„ä¼°
            return self._assess_accuracy_by_rules()
        
        results = {
            'dimension': 'accuracy',
            'column_scores': {},
            'overall_score': 0,
            'issues': []
        }
        
        # åˆå¹¶æ•°æ®
        merged = pd.merge(
            self.df, reference_data, 
            on=key_columns, 
            suffixes=('_current', '_reference')
        )
        
        total_comparisons = 0
        accurate_comparisons = 0
        
        for col in self.df.columns:
            if col in key_columns or f"{col}_reference" not in merged.columns:
                continue
                
            current_col = f"{col}_current"
            reference_col = f"{col}_reference"
            
            # åªæ¯”è¾ƒéç©ºå€¼
            comparison_mask = merged[current_col].notna() & merged[reference_col].notna()
            comparable_data = merged[comparison_mask]
            
            if len(comparable_data) == 0:
                continue
                
            accurate_count = (comparable_data[current_col] == comparable_data[reference_col]).sum()
            accuracy_score = (accurate_count / len(comparable_data)) * 100
            
            results['column_scores'][col] = {
                'score': accuracy_score,
                'accurate_count': accurate_count,
                'inaccurate_count': len(comparable_data) - accurate_count,
                'total_comparable': len(comparable_data),
                'inaccuracy_percentage': ((len(comparable_data) - accurate_count) / len(comparable_data)) * 100
            }
            
            total_comparisons += len(comparable_data)
            accurate_comparisons += accurate_count
            
            # è¯†åˆ«å‡†ç¡®æ€§é—®é¢˜
            if accuracy_score < 95:
                results['issues'].append({
                    'column': col,
                    'type': 'low_accuracy',
                    'severity': 'high' if accuracy_score < 90 else 'medium',
                    'description': f"åˆ— {col} çš„å‡†ç¡®ç‡ä»…ä¸º {accuracy_score:.2f}%"
                })
        
        results['overall_score'] = (accurate_comparisons / total_comparisons) * 100 if total_comparisons > 0 else 100
        self.dimension_scores['accuracy'] = results['overall_score']
        
        return results
    
    def assess_consistency(self, consistency_rules: Dict[str, Dict] = None) -> Dict:
        """
        è¯„ä¼°æ•°æ®ä¸€è‡´æ€§
        
        Args:
            consistency_rules (Dict[str, Dict]): ä¸€è‡´æ€§è§„åˆ™
                æ ¼å¼: {'rule_name': {'type': 'cross_column', 'columns': ['col1', 'col2'], 'logic': 'col1 > col2'}}
                
        Returns:
            Dict: ä¸€è‡´æ€§è¯„ä¼°ç»“æœ
        """
        if consistency_rules is None:
            consistency_rules = self._infer_consistency_rules()
        
        results = {
            'dimension': 'consistency',
            'rule_scores': {},
            'overall_score': 0,
            'issues': []
        }
        
        total_checks = 0
        consistent_checks = 0
        
        for rule_name, rule in consistency_rules.items():
            rule_type = rule['type']
            
            if rule_type == 'cross_column':
                columns = rule['columns']
                logic = rule['logic']
                
                # æ„å»ºä¸€è‡´æ€§æ£€æŸ¥è¡¨è¾¾å¼
                consistent_count = 0
                total_count = len(self.df)
                
                for idx, row in self.df.iterrows():
                    try:
                        # å®‰å…¨åœ°è¯„ä¼°é€»è¾‘è¡¨è¾¾å¼
                        local_vars = {col: row[col] for col in columns}
                        if eval(logic, {"__builtins__": {}}, local_vars):
                            consistent_count += 1
                    except:
                        # å¦‚æœè¯„ä¼°å¤±è´¥ï¼Œè§†ä¸ºä¸ä¸€è‡´
                        pass
                
                consistency_score = (consistent_count / total_count) * 100
                
                results['rule_scores'][rule_name] = {
                    'score': consistency_score,
                    'consistent_count': consistent_count,
                    'inconsistent_count': total_count - consistent_count,
                    'total_count': total_count,
                    'inconsistency_percentage': ((total_count - consistent_count) / total_count) * 100
                }
                
                total_checks += total_count
                consistent_checks += consistent_count
                
                # è¯†åˆ«ä¸€è‡´æ€§é—®é¢˜
                if consistency_score < 95:
                    results['issues'].append({
                        'rule': rule_name,
                        'type': 'low_consistency',
                        'severity': 'high' if consistency_score < 90 else 'medium',
                        'description': f"è§„åˆ™ {rule_name} çš„ä¸€è‡´æ€§ä»…ä¸º {consistency_score:.2f}%"
                    })
            
            elif rule_type == 'date_logic':
                # æ—¥æœŸé€»è¾‘ä¸€è‡´æ€§æ£€æŸ¥
                columns = rule['columns']
                start_col = columns[0]
                end_col = columns[1]
                
                consistent_mask = (self.df[start_col] <= self.df[end_col]) | self.df[end_col].isna()
                consistent_count = consistent_mask.sum()
                total_count = len(self.df)
                
                consistency_score = (consistent_count / total_count) * 100
                
                results['rule_scores'][rule_name] = {
                    'score': consistency_score,
                    'consistent_count': consistent_count,
                    'inconsistent_count': total_count - consistent_count,
                    'total_count': total_count,
                    'inconsistency_percentage': ((total_count - consistent_count) / total_count) * 100
                }
                
                total_checks += total_count
                consistent_checks += consistent_count
                
                # è¯†åˆ«ä¸€è‡´æ€§é—®é¢˜
                if consistency_score < 95:
                    results['issues'].append({
                        'rule': rule_name,
                        'type': 'low_consistency',
                        'severity': 'high' if consistency_score < 90 else 'medium',
                        'description': f"æ—¥æœŸé€»è¾‘è§„åˆ™ {rule_name} çš„ä¸€è‡´æ€§ä»…ä¸º {consistency_score:.2f}%"
                    })
        
        results['overall_score'] = (consistent_checks / total_checks) * 100 if total_checks > 0 else 100
        self.dimension_scores['consistency'] = results['overall_score']
        
        return results
    
    def assess_timeliness(self, date_column: str, threshold_days: int = 30) -> Dict:
        """
        è¯„ä¼°æ•°æ®æ—¶æ•ˆæ€§
        
        Args:
            date_column (str): æ—¥æœŸåˆ—å
            threshold_days (int): æ—¶æ•ˆæ€§é˜ˆå€¼ï¼ˆå¤©æ•°ï¼‰
            
        Returns:
            Dict: æ—¶æ•ˆæ€§è¯„ä¼°ç»“æœ
        """
        if date_column not in self.df.columns:
            return {
                'dimension': 'timeliness',
                'overall_score': 0,
                'error': f'åˆ— {date_column} ä¸å­˜åœ¨'
            }
        
        results = {
            'dimension': 'timeliness',
            'column_scores': {},
            'overall_score': 0,
            'issues': []
        }
        
        # è½¬æ¢æ—¥æœŸåˆ—
        try:
            date_series = pd.to_datetime(self.df[date_column], errors='coerce')
        except:
            return {
                'dimension': 'timeliness',
                'overall_score': 0,
                'error': f'æ— æ³•è½¬æ¢åˆ— {date_column} ä¸ºæ—¥æœŸæ ¼å¼'
            }
        
        current_date = datetime.now()
        threshold_date = current_date - timedelta(days=threshold_days)
        
        # è®¡ç®—æ—¶æ•ˆæ€§å¾—åˆ†
        timely_records = (date_series >= threshold_date).sum()
        total_records = len(date_series.dropna())
        
        timeliness_score = (timely_records / total_records) * 100 if total_records > 0 else 100
        
        # è®¡ç®—å¹³å‡å»¶è¿Ÿå¤©æ•°
        delays = (current_date - date_series).dt.days
        avg_delay = delays.mean()
        max_delay = delays.max()
        
        results['column_scores'][date_column] = {
            'score': timeliness_score,
            'timely_count': timely_records,
            'outdated_count': total_records - timely_records,
            'total_count': total_records,
            'outdated_percentage': ((total_records - timely_records) / total_records) * 100 if total_records > 0 else 0,
            'average_delay_days': avg_delay,
            'max_delay_days': max_delay
        }
        
        results['overall_score'] = timeliness_score
        self.dimension_scores['timeliness'] = results['overall_score']
        
        # è¯†åˆ«æ—¶æ•ˆæ€§é—®é¢˜
        if timeliness_score < 90:
            results['issues'].append({
                'column': date_column,
                'type': 'low_timeliness',
                'severity': 'high' if timeliness_score < 80 else 'medium',
                'description': f"åˆ— {date_column} çš„æ—¶æ•ˆæ€§ä»…ä¸º {timeliness_score:.2f}%"
            })
        
        return results
    
    def run_full_assessment(self, config: Dict = None) -> Dict:
        """
        è¿è¡Œå…¨é¢çš„æ•°æ®è´¨é‡è¯„ä¼°
        
        Args:
            config (Dict): è¯„ä¼°é…ç½®
            
        Returns:
            Dict: å®Œæ•´çš„è¯„ä¼°ç»“æœ
        """
        if config is None:
            config = self._get_default_config()
        
        results = {
            'dataset_info': {
                'shape': self.df.shape,
                'columns': self.df.columns.tolist(),
                'data_types': self.df.dtypes.astype(str).to_dict(),
                'assessment_time': datetime.now().isoformat()
            },
            'dimension_results': {},
            'overall_score': 0,
            'recommendations': []
        }
        
        # æ‰§è¡Œå„ç»´åº¦è¯„ä¼°
        if 'completeness' in config['dimensions']:
            results['dimension_results']['completeness'] = self.assess_completeness(
                config['completeness'].get('columns')
            )
        
        if 'uniqueness' in config['dimensions']:
            results['dimension_results']['uniqueness'] = self.assess_uniqueness(
                config['uniqueness'].get('columns')
            )
        
        if 'validity' in config['dimensions']:
            results['dimension_results']['validity'] = self.assess_validity(
                config['validity'].get('column_rules')
            )
        
        if 'accuracy' in config['dimensions']:
            accuracy_config = config['accuracy']
            results['dimension_results']['accuracy'] = self.assess_accuracy(
                accuracy_config.get('reference_data'),
                accuracy_config.get('key_columns')
            )
        
        if 'consistency' in config['dimensions']:
            results['dimension_results']['consistency'] = self.assess_consistency(
                config['consistency'].get('rules')
            )
        
        if 'timeliness' in config['dimensions']:
            timeliness_config = config['timeliness']
            results['dimension_results']['timeliness'] = self.assess_timeliness(
                timeliness_config.get('date_column'),
                timeliness_config.get('threshold_days', 30)
            )
        
        # è®¡ç®—æ€»ä½“å¾—åˆ†
        dimension_weights = config.get('dimension_weights', {
            'completeness': 0.25,
            'uniqueness': 0.15,
            'validity': 0.20,
            'accuracy': 0.25,
            'consistency': 0.10,
            'timeliness': 0.05
        })
        
        weighted_score = 0
        total_weight = 0
        
        for dimension, result in results['dimension_results'].items():
            if 'overall_score' in result:
                weight = dimension_weights.get(dimension, 1)
                weighted_score += result['overall_score'] * weight
                total_weight += weight
        
        results['overall_score'] = weighted_score / total_weight if total_weight > 0 else 0
        
        # ç”Ÿæˆæ”¹è¿›å»ºè®®
        results['recommendations'] = self._generate_recommendations(results)
        
        # ä¿å­˜ç»“æœ
        self.results = results
        
        return results
    
    def generate_report(self, output_format: str = 'html', output_path: str = None) -> str:
        """
        ç”Ÿæˆæ•°æ®è´¨é‡æŠ¥å‘Š
        
        Args:
            output_format (str): è¾“å‡ºæ ¼å¼ ('html', 'json', 'markdown')
            output_path (str): è¾“å‡ºè·¯å¾„
            
        Returns:
            str: æŠ¥å‘Šå†…å®¹æˆ–æ–‡ä»¶è·¯å¾„
        """
        if not self.results:
            self.run_full_assessment()
        
        if output_format == 'html':
            report = self._generate_html_report()
        elif output_format == 'json':
            report = json.dumps(self.results, indent=2, ensure_ascii=False)
        elif output_format == 'markdown':
            report = self._generate_markdown_report()
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„è¾“å‡ºæ ¼å¼: {output_format}")
        
        if output_path:
            with open(output_path, 'w', encoding='utf-8') as f:
                f.write(report)
            return output_path
        
        return report
    
    def visualize_results(self, figsize: Tuple[int, int] = (15, 10), save_path: str = None):
        """
        å¯è§†åŒ–æ•°æ®è´¨é‡è¯„ä¼°ç»“æœ
        
        Args:
            figsize (Tuple[int, int]): å›¾å½¢å¤§å°
            save_path (str): ä¿å­˜è·¯å¾„
            
        Returns:
            None
        """
        if not self.results:
            self.run_full_assessment()
        
        fig, axes = plt.subplots(2, 3, figsize=figsize)
        fig.suptitle('æ•°æ®è´¨é‡è¯„ä¼°ç»“æœ', fontsize=16)
        
        # ç»´åº¦å¾—åˆ†é›·è¾¾å›¾
        if self.dimension_scores:
            ax = axes[0, 0]
            self._plot_radar_chart(ax, self.dimension_scores)
            ax.set_title('å„ç»´åº¦å¾—åˆ†')
        
        # ç»´åº¦å¾—åˆ†æŸ±çŠ¶å›¾
        if self.dimension_scores:
            ax = axes[0, 1]
            dimensions = list(self.dimension_scores.keys())
            scores = list(self.dimension_scores.values())
            colors = ['green' if score >= 90 else 'orange' if score >= 80 else 'red' for score in scores]
            
            ax.bar(dimensions, scores, color=colors)
            ax.set_ylim(0, 100)
            ax.set_title('å„ç»´åº¦å¾—åˆ†')
            ax.set_ylabel('å¾—åˆ† (%)')
        
        # å„ç»´åº¦é—®é¢˜æ•°é‡
        if 'dimension_results' in self.results:
            ax = axes[0, 2]
            issue_counts = []
            dimensions = []
            
            for dimension, result in self.results['dimension_results'].items():
                if 'issues' in result:
                    issue_counts.append(len(result['issues']))
                    dimensions.append(dimension)
            
            if dimensions:
                ax.bar(dimensions, issue_counts, color='red')
                ax.set_title('å„ç»´åº¦é—®é¢˜æ•°é‡')
                ax.set_ylabel('é—®é¢˜æ•°é‡')
        
        # æ•°æ®åˆ†å¸ƒæ¦‚è§ˆ
        ax = axes[1, 0]
        numeric_cols = self.df.select_dtypes(include=[np.number]).columns
        if len(numeric_cols) > 0:
            self.df[numeric_cols].hist(ax=ax, bins=20, alpha=0.7)
            ax.set_title('æ•°å€¼å‹æ•°æ®åˆ†å¸ƒ')
        else:
            ax.text(0.5, 0.5, 'æ— æ•°å€¼å‹æ•°æ®', horizontalalignment='center', verticalalignment='center')
            ax.set_title('æ•°æ®åˆ†å¸ƒ')
        
        # ç¼ºå¤±å€¼çƒ­å›¾
        ax = axes[1, 1]
        missing_data = self.df.isnull().sum()
        if missing_data.sum() > 0:
            missing_percent = (missing_data / len(self.df)) * 100
            missing_percent = missing_percent[missing_percent > 0].sort_values(ascending=False)
            
            if len(missing_percent) > 0:
                sns.barplot(x=missing_percent.values, y=missing_percent.index, ax=ax)
                ax.set_title('ç¼ºå¤±å€¼æ¯”ä¾‹')
                ax.set_xlabel('ç¼ºå¤±æ¯”ä¾‹ (%)')
            else:
                ax.text(0.5, 0.5, 'æ— ç¼ºå¤±å€¼', horizontalalignment='center', verticalalignment='center')
                ax.set_title('ç¼ºå¤±å€¼')
        else:
            ax.text(0.5, 0.5, 'æ— ç¼ºå¤±å€¼', horizontalalignment='center', verticalalignment='center')
            ax.set_title('ç¼ºå¤±å€¼')
        
        # ç»¼åˆè¯„åˆ†
        ax = axes[1, 2]
        overall_score = self.results.get('overall_score', 0)
        colors = ['green' if overall_score >= 90 else 'orange' if overall_score >= 80 else 'red']
        ax.bar(['ç»¼åˆè¯„åˆ†'], [overall_score], color=colors)
        ax.set_ylim(0, 100)
        ax.set_title(f'ç»¼åˆè¯„åˆ†: {overall_score:.2f}')
        ax.set_ylabel('å¾—åˆ† (%)')
        
        plt.tight_layout()
        
        if save_path:
            plt.savefig(save_path, dpi=300, bbox_inches='tight')
        
        plt.show()
    
    # è¾…åŠ©æ–¹æ³•
    def _infer_column_rules(self) -> Dict[str, Dict]:
        """æ¨æ–­åˆ—çš„éªŒè¯è§„åˆ™"""
        rules = {}
        
        for col in self.df.columns:
            col_data = self.df[col].dropna()
            if len(col_data) == 0:
                continue
                
            # åŸºäºåˆ—åæ¨æ–­è§„åˆ™
            col_lower = col.lower()
            
            if 'email' in col_lower:
                rules[col] = {
                    'type': 'regex',
                    'pattern': '^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}$'
                }
            elif 'phone' in col_lower or 'tel' in col_lower:
                rules[col] = {
                    'type': 'regex',
                    'pattern': '^1[3-9][0-9]{9}$'  # ä¸­å›½å¤§é™†æ‰‹æœºå·æ ¼å¼
                }
            elif 'age' in col_lower:
                rules[col] = {
                    'type': 'range',
                    'min': 0,
                    'max': 120
                }
            elif 'price' in col_lower or 'amount' in col_lower or 'cost' in col_lower:
                if col_data.dtype in ['int64', 'float64']:
                    min_val = col_data.min()
                    if min_val >= 0:
                        rules[col] = {
                            'type': 'range',
                            'min': 0,
                            'max': col_data.max() * 1.5
                        }
        
        return rules
    
    def _infer_consistency_rules(self) -> Dict[str, Dict]:
        """æ¨æ–­ä¸€è‡´æ€§è§„åˆ™"""
        rules = {}
        
        # åŸºäºåˆ—åæ¨æ–­æ—¥æœŸé€»è¾‘è§„åˆ™
        date_columns = []
        for col in self.df.columns:
            col_lower = col.lower()
            if 'date' in col_lower or 'time' in col_lower:
                date_columns.append(col)
        
        # å¦‚æœæœ‰å¼€å§‹å’Œç»“æŸæ—¥æœŸåˆ—
        start_date_cols = [col for col in date_columns if 'start' in col.lower() or 'begin' in col.lower()]
        end_date_cols = [col for col in date_columns if 'end' in col.lower() or 'finish' in col.lower()]
        
        for start_col in start_date_cols:
            for end_col in end_date_cols:
                if start_col in self.df.columns and end_col in self.df.columns:
                    rule_name = f"{start_col}_before_{end_col}"
                    rules[rule_name] = {
                        'type': 'date_logic',
                        'columns': [start_col, end_col]
                    }
        
        return rules
    
    def _validate_value(self, value: Any, rule: Dict) -> bool:
        """éªŒè¯å•ä¸ªå€¼æ˜¯å¦ç¬¦åˆè§„åˆ™"""
        rule_type = rule['type']
        
        if rule_type == 'regex':
            pattern = rule['pattern']
            return re.match(pattern, str(value)) is not None
        elif rule_type == 'range':
            min_val = rule.get('min')
            max_val = rule.get('max')
            
            if min_val is not None and value < min_val:
                return False
            if max_val is not None and value > max_val:
                return False
            return True
        elif rule_type == 'enum':
            valid_values = rule.get('values', [])
            return value in valid_values
        
        return True
    
    def _assess_accuracy_by_rules(self) -> Dict:
        """åŸºäºè§„åˆ™çš„å‡†ç¡®æ€§è¯„ä¼°ï¼ˆå½“æ²¡æœ‰å‚è€ƒæ•°æ®æ—¶ï¼‰"""
        results = {
            'dimension': 'accuracy',
            'column_scores': {},
            'overall_score': 0,
            'issues': []
        }
        
        # åŸºäºæ•°æ®ç±»å‹å’Œå†…å®¹è¯„ä¼°å‡†ç¡®æ€§
        for col in self.df.columns:
            col_data = self.df[col].dropna()
            if len(col_data) == 0:
                continue
                
            accuracy_score = 100  # é»˜è®¤æ»¡åˆ†
            
            # å¯¹æ•°å€¼å‹æ•°æ®ï¼Œæ£€æŸ¥å¼‚å¸¸å€¼
            if col_data.dtype in ['int64', 'float64']:
                q1 = col_data.quantile(0.25)
                q3 = col_data.quantile(0.75)
                iqr = q3 - q1
                
                outliers = ((col_data < (q1 - 1.5 * iqr)) | (col_data > (q3 + 1.5 * iqr))).sum()
                outlier_rate = outliers / len(col_data)
                
                # å¼‚å¸¸å€¼æ¯”ä¾‹è¶…è¿‡5%æ—¶ï¼Œé™ä½å‡†ç¡®æ€§è¯„åˆ†
                if outlier_rate > 0.05:
                    accuracy_score -= outlier_rate * 50
            
            results['column_scores'][col] = {
                'score': max(accuracy_score, 0),  # ç¡®ä¿å¾—åˆ†ä¸ä½äº0
                'assessment_type': 'rule_based'
            }
        
        # è®¡ç®—æ€»ä½“å‡†ç¡®æ€§å¾—åˆ†
        if results['column_scores']:
            scores = [col_score['score'] for col_score in results['column_scores'].values()]
            results['overall_score'] = sum(scores) / len(scores)
        else:
            results['overall_score'] = 100
        
        self.dimension_scores['accuracy'] = results['overall_score']
        
        return results
    
    def _get_default_config(self) -> Dict:
        """è·å–é»˜è®¤é…ç½®"""
        return {
            'dimensions': ['completeness', 'uniqueness', 'validity', 'accuracy', 'consistency'],
            'dimension_weights': {
                'completeness': 0.25,
                'uniqueness': 0.15,
                'validity': 0.20,
                'accuracy': 0.25,
                'consistency': 0.10,
                'timeliness': 0.05
            }
        }
    
    def _generate_recommendations(self, results: Dict) -> List[str]:
        """ç”Ÿæˆæ”¹è¿›å»ºè®®"""
        recommendations = []
        
        if 'dimension_results' not in results:
            return recommendations
        
        # åŸºäºå„ç»´åº¦é—®é¢˜ç”Ÿæˆå»ºè®®
        dimension_results = results['dimension_results']
        
        # å®Œæ•´æ€§é—®é¢˜
        if 'completeness' in dimension_results:
            completeness_issues = dimension_results['completeness'].get('issues', [])
            if completeness_issues:
                recommendations.append(
                    "å»ºè®®å®æ–½æ•°æ®å®Œæ•´æ€§æ£€æŸ¥ï¼Œåœ¨æ•°æ®å½•å…¥æ—¶éªŒè¯å¿…å¡«å­—æ®µï¼Œ"
                    "å¹¶å®šæœŸæ¸…ç†ç©ºå€¼æ•°æ®ï¼Œæé«˜æ•°æ®å®Œæ•´æ€§ã€‚"
                )
        
        # å”¯ä¸€æ€§é—®é¢˜
        if 'uniqueness' in dimension_results:
            uniqueness_issues = dimension_results['uniqueness'].get('issues', [])
            if uniqueness_issues:
                recommendations.append(
                    "å»ºè®®å®æ–½é‡å¤æ•°æ®æ£€æµ‹æœºåˆ¶ï¼Œå»ºç«‹å”¯ä¸€çº¦æŸï¼Œ"
                    "å¹¶å®šæœŸè¿›è¡Œæ•°æ®å»é‡å¤„ç†ã€‚"
                )
        
        # æœ‰æ•ˆæ€§é—®é¢˜
        if 'validity' in dimension_results:
            validity_issues = dimension_results['validity'].get('issues', [])
            if validity_issues:
                recommendations.append(
                    "å»ºè®®å»ºç«‹æ•°æ®æ ¼å¼éªŒè¯è§„åˆ™ï¼Œåœ¨æ•°æ®å½•å…¥æ—¶è¿›è¡Œå®æ—¶éªŒè¯ï¼Œ"
                    "å¹¶å¯¹ç°æœ‰æ•°æ®è¿›è¡Œæ ¼å¼æ ‡å‡†åŒ–å¤„ç†ã€‚"
                )
        
        # å‡†ç¡®æ€§é—®é¢˜
        if 'accuracy' in dimension_results:
            accuracy_issues = dimension_results['accuracy'].get('issues', [])
            if accuracy_issues:
                recommendations.append(
                    "å»ºè®®å»ºç«‹æ•°æ®å‡†ç¡®æ€§æ£€æŸ¥æœºåˆ¶ï¼Œä¸æƒå¨æ•°æ®æºè¿›è¡Œæ¯”å¯¹ï¼Œ"
                    "å¹¶å®æ–½æ•°æ®å®¡æ ¸æµç¨‹ç¡®ä¿æ•°æ®å‡†ç¡®æ€§ã€‚"
                )
        
        # ä¸€è‡´æ€§é—®é¢˜
        if 'consistency' in dimension_results:
            consistency_issues = dimension_results['consistency'].get('issues', [])
            if consistency_issues:
                recommendations.append(
                    "å»ºè®®å®æ–½è·¨ç³»ç»Ÿæ•°æ®åŒæ­¥æœºåˆ¶ï¼Œå»ºç«‹æ•°æ®ä¸€è‡´æ€§æ£€æŸ¥è§„åˆ™ï¼Œ"
                    "å¹¶å®šæœŸæ‰§è¡Œä¸€è‡´æ€§éªŒè¯ã€‚"
                )
        
        # æ—¶æ•ˆæ€§é—®é¢˜
        if 'timeliness' in dimension_results:
            timeliness_issues = dimension_results['timeliness'].get('issues', [])
            if timeliness_issues:
                recommendations.append(
                    "å»ºè®®ä¼˜åŒ–æ•°æ®æ›´æ–°æµç¨‹ï¼Œå®æ–½å¢é‡æ›´æ–°æœºåˆ¶ï¼Œ"
                    "å¹¶å»ºç«‹æ•°æ®æ—¶æ•ˆæ€§ç›‘æ§ç¡®ä¿æ•°æ®åŠæ—¶æ›´æ–°ã€‚"
                )
        
        # ç»¼åˆè¯„åˆ†è¾ƒä½çš„å»ºè®®
        overall_score = results.get('overall_score', 100)
        if overall_score < 80:
            recommendations.append(
                "æ•°æ®è´¨é‡æ•´ä½“æ°´å¹³è¾ƒä½ï¼Œå»ºè®®å»ºç«‹å…¨é¢çš„æ•°æ®è´¨é‡ç®¡ç†ä½“ç³»ï¼Œ"
                "åŒ…æ‹¬æ•°æ®è´¨é‡æ ‡å‡†ã€æµç¨‹ã€å·¥å…·å’Œç»„ç»‡æ¶æ„ã€‚"
            )
        
        return recommendations
    
    def _generate_html_report(self) -> str:
        """ç”ŸæˆHTMLæ ¼å¼çš„æŠ¥å‘Š"""
        if not self.results:
            return "<html><body>æ— è¯„ä¼°ç»“æœ</body></html>"
        
        html = f"""
        <!DOCTYPE html>
        <html>
        <head>
            <title>æ•°æ®è´¨é‡è¯„ä¼°æŠ¥å‘Š</title>
            <meta charset="utf-8">
            <style>
                body {{ font-family: Arial, sans-serif; margin: 20px; }}
                .header {{ background-color: #f5f5f5; padding: 20px; border-radius: 5px; }}
                .section {{ margin: 20px 0; }}
                .metric {{ display: inline-block; margin: 10px; padding: 10px; border: 1px solid #ddd; border-radius: 5px; min-width: 150px; }}
                .score {{ font-size: 24px; font-weight: bold; }}
                .good {{ color: green; }}
                .warning {{ color: orange; }}
                .bad {{ color: red; }}
                table {{ border-collapse: collapse; width: 100%; }}
                th, td {{ border: 1px solid #ddd; padding: 8px; text-align: left; }}
                th {{ background-color: #f2f2f2; }}
                .chart {{ height: 400px; margin: 20px 0; }}
            </style>
            <script src="https://cdn.jsdelivr.net/npm/chart.js"></script>
        </head>
        <body>
            <div class="header">
                <h1>æ•°æ®è´¨é‡è¯„ä¼°æŠ¥å‘Š</h1>
                <p>è¯„ä¼°æ—¶é—´: {self.results['dataset_info']['assessment_time']}</p>
                <p>æ•°æ®é›†å½¢çŠ¶: {self.results['dataset_info']['shape']}</p>
            </div>
            
            <div class="section">
                <h2>æ€»ä½“è¯„ä¼°</h2>
                <div class="metric">
                    <div>ç»¼åˆè¯„åˆ†</div>
                    <div class="score {self._get_score_class(self.results['overall_score'])}">{self.results['overall_score']:.1f}</div>
                </div>
            </div>
            
            <div class="section">
                <h2>ç»´åº¦è¯„ä¼°ç»“æœ</h2>
                <table>
                    <tr>
                        <th>ç»´åº¦</th>
                        <th>å¾—åˆ†</th>
                        <th>é—®é¢˜æ•°é‡</th>
                        <th>çŠ¶æ€</th>
                    </tr>
        """
        
        # æ·»åŠ ç»´åº¦ç»“æœè¡¨æ ¼
        for dimension, result in self.results.get('dimension_results', {}).items():
            if 'overall_score' in result:
                score = result['overall_score']
                issue_count = len(result.get('issues', []))
                status = "è‰¯å¥½" if score >= 90 else "ä¸€èˆ¬" if score >= 80 else "éœ€æ”¹è¿›"
                status_class = "good" if score >= 90 else "warning" if score >= 80 else "bad"
                
                html += f"""
                    <tr>
                        <td>{dimension}</td>
                        <td class="{self._get_score_class(score)}">{score:.1f}</td>
                        <td>{issue_count}</td>
                        <td class="{status_class}">{status}</td>
                    </tr>
                """
        
        html += """
                </table>
            </div>
            
            <div class="section">
                <h2>é—®é¢˜è¯¦æƒ…</h2>
        """
        
        # æ·»åŠ é—®é¢˜è¯¦æƒ…
        has_issues = False
        for dimension, result in self.results.get('dimension_results', {}).items():
            issues = result.get('issues', [])
            if issues:
                has_issues = True
                html += f"<h3>{dimension} é—®é¢˜</h3><ul>"
                for issue in issues:
                    html += f"<li>{issue['description']}</li>"
                html += "</ul>"
        
        if not has_issues:
            html += "<p>æœªå‘ç°æ•°æ®è´¨é‡é—®é¢˜</p>"
        
        html += """
            </div>
            
            <div class="section">
                <h2>æ”¹è¿›å»ºè®®</h2>
                <ol>
        """
        
        # æ·»åŠ æ”¹è¿›å»ºè®®
        for recommendation in self.results.get('recommendations', []):
            html += f"<li>{recommendation}</li>"
        
        html += """
                </ol>
            </div>
        </body>
        </html>
        """
        
        return html
    
    def _generate_markdown_report(self) -> str:
        """ç”ŸæˆMarkdownæ ¼å¼çš„æŠ¥å‘Š"""
        if not self.results:
            return "# æ•°æ®è´¨é‡è¯„ä¼°æŠ¥å‘Š\n\næ— è¯„ä¼°ç»“æœ\n"
        
        md = f"""# æ•°æ®è´¨é‡è¯„ä¼°æŠ¥å‘Š

## åŸºæœ¬ä¿¡æ¯

- **è¯„ä¼°æ—¶é—´**: {self.results['dataset_info']['assessment_time']}
- **æ•°æ®é›†å½¢çŠ¶**: {self.results['dataset_info']['shape']}

## æ€»ä½“è¯„ä¼°

- **ç»¼åˆè¯„åˆ†**: {self.results['overall_score']:.1f} {self._get_score_emoji(self.results['overall_score'])}

## ç»´åº¦è¯„ä¼°ç»“æœ

| ç»´åº¦ | å¾—åˆ† | é—®é¢˜æ•°é‡ | çŠ¶æ€ |
|------|------|----------|------|
"""
        
        # æ·»åŠ ç»´åº¦ç»“æœè¡¨æ ¼
        for dimension, result in self.results.get('dimension_results', {}).items():
            if 'overall_score' in result:
                score = result['overall_score']
                issue_count = len(result.get('issues', []))
                status = "è‰¯å¥½" if score >= 90 else "ä¸€èˆ¬" if score >= 80 else "éœ€æ”¹è¿›"
                
                md += f"| {dimension} | {score:.1f} | {issue_count} | {status} |\n"
        
        md += "\n## é—®é¢˜è¯¦æƒ…\n\n"
        
        # æ·»åŠ é—®é¢˜è¯¦æƒ…
        has_issues = False
        for dimension, result in self.results.get('dimension_results', {}).items():
            issues = result.get('issues', [])
            if issues:
                has_issues = True
                md += f"### {dimension} é—®é¢˜\n\n"
                for issue in issues:
                    md += f"- {issue['description']}\n"
                md += "\n"
        
        if not has_issues:
            md += "æœªå‘ç°æ•°æ®è´¨é‡é—®é¢˜\n\n"
        
        md += "## æ”¹è¿›å»ºè®®\n\n"
        
        # æ·»åŠ æ”¹è¿›å»ºè®®
        for recommendation in self.results.get('recommendations', []):
            md += f"1. {recommendation}\n"
        
        return md
    
    def _get_score_class(self, score: float) -> str:
        """æ ¹æ®å¾—åˆ†è·å–CSSç±»å"""
        return "good" if score >= 90 else "warning" if score >= 80 else "bad"
    
    def _get_score_emoji(self, score: float) -> str:
        """æ ¹æ®å¾—åˆ†è·å–è¡¨æƒ…ç¬¦å·"""
        return "ğŸŸ¢" if score >= 90 else "ğŸŸ¡" if score >= 80 else "ğŸ”´"
    
    def _plot_radar_chart(self, ax, dimensions_scores):
        """ç»˜åˆ¶é›·è¾¾å›¾"""
        if not dimensions_scores:
            ax.text(0.5, 0.5, 'æ— æ•°æ®', horizontalalignment='center', verticalalignment='center')
            return
        
        # å‡†å¤‡æ•°æ®
        categories = list(dimensions_scores.keys())
        values = list(dimensions_scores.values())
        
        # è®¡ç®—è§’åº¦
        N = len(categories)
        angles = [n / float(N) * 2 * np.pi for n in range(N)]
        angles += angles[:1]  # é—­åˆå›¾å½¢
        values += values[:1]  # é—­åˆå›¾å½¢
        
        # ç»˜åˆ¶é›·è¾¾å›¾
        ax.plot(angles, values, 'o-', linewidth=2)
        ax.fill(angles, values, alpha=0.25)
        
        # è®¾ç½®æ ‡ç­¾
        ax.set_xticks(angles[:-1])
        ax.set_xticklabels(categories)
        ax.set_ylim(0, 100)
        ax.set_title('å„ç»´åº¦å¾—åˆ†')


# ç¤ºä¾‹ä½¿ç”¨
if __name__ == "__main__":
    # åˆ›å»ºç¤ºä¾‹æ•°æ®
    data = {
        'customer_id': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10],
        'name': ['å¼ ä¸‰', 'æå››', 'ç‹äº”', 'èµµå…­', 'é’±ä¸ƒ', 'å­™å…«', None, 'å‘¨ä¹', 'å´å', 'éƒ‘åä¸€'],
        'email': [
            'zhangsan@example.com',
            'lisi@example.com',
            'wangwu@example.com',
            'zhaoliu@example.com',
            'qianqi@example.com',
            'sunba@example.com',
            'invalid-email',  # æ— æ•ˆé‚®ç®±
            'zhoujiu@example.com',
            'wushi@example.com',
            'zhengshiyi@example.com'
        ],
        'phone': [
            '13812345678',
            '13912345678',
            '13612345678',
            '13712345678',
            '13512345678',
            '13412345678',
            '13312345678',
            '13212345678',
            '13112345678',
            'invalid-phone'  # æ— æ•ˆæ‰‹æœºå·
        ],
        'age': [25, 30, 35, 40, 45, 50, 55, 60, 65, 200],  # 200æ˜¯å¼‚å¸¸å€¼
        'registration_date': [
            '2023-01-01',
            '2023-01-15',
            '2023-02-01',
            '2023-02-15',
            '2023-03-01',
            '2023-03-15',
            '2023-04-01',
            '2023-04-15',
            '2023-05-01',
            '2023-05-15'
        ]
    }
    
    df = pd.DataFrame(data)
    
    # åˆ›å»ºè¯„ä¼°å™¨
    assessor = DataQualityAssessment(df)
    
    # é…ç½®è¯„ä¼°è§„åˆ™
    config = {
        'dimensions': ['completeness', 'uniqueness', 'validity', 'accuracy', 'consistency'],
        'completeness': {
            'columns': ['name', 'email', 'phone']
        },
        'uniqueness': {
            'columns': ['customer_id', 'email']
        },
        'validity': {
            'column_rules': {
                'email': {
                    'type': 'regex',
                    'pattern': '^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}$'
                },
                'phone': {
                    'type': 'regex',
                    'pattern': '^1[3-9][0-9]{9}$'
                },
                'age': {
                    'type': 'range',
                    'min': 0,
                    'max': 120
                }
            }
        },
        'timeliness': {
            'date_column': 'registration_date',
            'threshold_days': 365
        },
        'consistency': {
            'rules': {
                'age_range': {
                    'type': 'cross_column',
                    'columns': ['age'],
                    'logic': '0 <= age <= 120'
                }
            }
        }
    }
    
    # è¿è¡Œè¯„ä¼°
    results = assessor.run_full_assessment(config)
    
    # ç”ŸæˆæŠ¥å‘Š
    html_report = assessor.generate_report('html')
    markdown_report = assessor.generate_report('markdown')
    
    # ä¿å­˜æŠ¥å‘Š
    with open('data_quality_report.html', 'w', encoding='utf-8') as f:
        f.write(html_report)
    
    with open('data_quality_report.md', 'w', encoding='utf-8') as f:
        f.write(markdown_report)
    
    # å¯è§†åŒ–ç»“æœ
    assessor.visualize_results(save_path='data_quality_visualization.png')
    
    print("æ•°æ®è´¨é‡è¯„ä¼°å®Œæˆï¼")
    print(f"ç»¼åˆè¯„åˆ†: {results['overall_score']:.2f}")
    print(f"HTMLæŠ¥å‘Š: data_quality_report.html")
    print(f"MarkdownæŠ¥å‘Š: data_quality_report.md")
    print(f"å¯è§†åŒ–å›¾è¡¨: data_quality_visualization.png")