# 第4章：数据清洗与预处理技术

## 4.1 数据清洗概述

数据清洗（Data Cleaning）是数据质量管理过程中至关重要的一步，它涉及识别、纠正或删除数据集中的错误、不一致和不完整的数据。高质量的数据是数据分析和决策的基础，而数据清洗正是确保数据质量的关键环节。

### 4.1.1 什么是数据清洗

数据清洗是指检测和纠正（或删除）数据集中不准确、不完整、不相关或重复的记录的过程。其目的是提高数据的质量，使其更适合进行分析和决策。

### 4.1.2 数据清洗的重要性

1. **提升数据质量**：清除错误和噪声数据，确保数据的准确性
2. **增强分析可靠性**：高质量的数据产生更可靠的分析结果
3. **减少决策风险**：避免基于错误数据做出错误决策
4. **提高处理效率**：干净的数据可以加快处理速度，减少资源消耗

### 4.1.3 数据清洗的基本原则

1. **保持原始数据不变**：在清洗过程中保留原始数据副本
2. **记录清洗过程**：详细记录每一步清洗操作，便于审计和复现
3. **逐步清洗**：采用分步骤的方法，每次只处理一类问题
4. **验证清洗结果**：清洗后验证数据质量是否有改善

## 4.2 常见数据质量问题及清洗方法

### 4.2.1 缺失值处理

缺失值是最常见的数据质量问题之一，可能由数据录入错误、传输失败或其他原因导致。

#### 缺失值的类型

1. **完全随机缺失 (MCAR)**：缺失与任何变量都无关
2. **随机缺失 (MAR)**：缺失与观测变量有关，但与未观测变量无关
3. **非随机缺失 (MNAR)**：缺失与未观测变量有关

#### 缺失值处理策略

##### 1. 删除法

适用于缺失比例较小的情况：

```python
# 删除包含缺失值的行
df_cleaned = df.dropna()

# 删除特定列包含缺失值的行
df_cleaned = df.dropna(subset=['column_name'])

# 删除包含缺失值的列
df_cleaned = df.dropna(axis=1)
```

##### 2. 填充法

使用统计值或预测值填充缺失值：

```python
# 使用均值填充
df['column_name'].fillna(df['column_name'].mean(), inplace=True)

# 使用中位数填充
df['column_name'].fillna(df['column_name'].median(), inplace=True)

# 使用众数填充
mode_value = df['column_name'].mode()[0]
df['column_name'].fillna(mode_value, inplace=True)

# 使用前向填充
df['column_name'].fillna(method='ffill', inplace=True)

# 使用后向填充
df['column_name'].fillna(method='bfill', inplace=True)
```

##### 3. 插值法

使用数学方法估算缺失值：

```python
# 线性插值
df['column_name'].interpolate(method='linear', inplace=True)

# 多项式插值
df['column_name'].interpolate(method='polynomial', order=2, inplace=True)
```

##### 4. 预测法

使用机器学习模型预测缺失值：

```python
from sklearn.impute import KNNImputer

# 使用K近邻算法填充缺失值
imputer = KNNImputer(n_neighbors=5)
df_filled = imputer.fit_transform(df[['numeric_columns']])
```

### 4.2.2 重复数据处理

重复数据会扭曲分析结果，导致统计偏差。

#### 识别重复数据

```python
# 查找完全重复的行
duplicates = df[df.duplicated()]

# 查找基于特定列的重复数据
duplicates = df[df.duplicated(subset=['column1', 'column2'])]

# 显示重复数据的数量
print(f"重复数据数量: {df.duplicated().sum()}")
```

#### 处理重复数据

```python
# 删除重复行，保留第一次出现的记录
df_cleaned = df.drop_duplicates()

# 删除基于特定列的重复数据
df_cleaned = df.drop_duplicates(subset=['column1', 'column2'])

# 删除重复行，保留最后一次出现的记录
df_cleaned = df.drop_duplicates(keep='last')
```

### 4.2.3 异常值处理

异常值是明显偏离正常范围的数据点，可能是由于测量错误或特殊事件导致。

#### 识别异常值

##### 1. 统计方法

```python
# 使用Z-score方法识别异常值
from scipy import stats
import numpy as np

z_scores = np.abs(stats.zscore(df['column_name']))
threshold = 3
outliers = df[z_scores > threshold]

# 使用IQR方法识别异常值
Q1 = df['column_name'].quantile(0.25)
Q3 = df['column_name'].quantile(0.75)
IQR = Q3 - Q1
lower_bound = Q1 - 1.5 * IQR
upper_bound = Q3 + 1.5 * IQR
outliers = df[(df['column_name'] < lower_bound) | (df['column_name'] > upper_bound)]
```

##### 2. 可视化方法

```python
import matplotlib.pyplot as plt
import seaborn as sns

# 箱线图识别异常值
plt.figure(figsize=(10, 6))
sns.boxplot(x=df['column_name'])
plt.title('Boxplot for Outlier Detection')
plt.show()

# 散点图识别异常值
plt.figure(figsize=(10, 6))
plt.scatter(range(len(df)), df['column_name'])
plt.title('Scatter Plot for Outlier Detection')
plt.show()
```

#### 处理异常值

##### 1. 删除异常值

```python
# 删除Z-score大于阈值的记录
df_cleaned = df[z_scores <= threshold]

# 删除IQR范围外的记录
df_cleaned = df[(df['column_name'] >= lower_bound) & (df['column_name'] <= upper_bound)]
```

##### 2. 替换异常值

```python
# 使用边界值替换异常值
df['column_name'] = df['column_name'].clip(lower=lower_bound, upper=upper_bound)

# 使用均值替换异常值
mean_value = df['column_name'].mean()
df.loc[z_scores > threshold, 'column_name'] = mean_value
```

##### 3. 分箱处理

```python
# 将连续变量分箱
df['binned_column'] = pd.cut(df['column_name'], bins=5, labels=False)
```

### 4.2.4 格式不一致处理

数据格式不一致会影响数据处理和分析。

#### 文本格式标准化

```python
# 统一大小写
df['column_name'] = df['column_name'].str.lower()

# 去除前后空格
df['column_name'] = df['column_name'].str.strip()

# 替换特定字符
df['column_name'] = df['column_name'].str.replace('-', '_')

# 标准化日期格式
df['date_column'] = pd.to_datetime(df['date_column'], format='%Y-%m-%d')
```

#### 数值格式标准化

```python
# 移除货币符号并转换为数值
df['price'] = df['price'].str.replace('$', '').str.replace(',', '').astype(float)

# 标准化百分比格式
df['percentage'] = df['percentage'].str.replace('%', '').astype(float) / 100
```

### 4.2.5 数据类型转换

正确的数据类型对于数据分析至关重要。

```python
# 转换为数值类型
df['column_name'] = pd.to_numeric(df['column_name'], errors='coerce')

# 转换为日期类型
df['date_column'] = pd.to_datetime(df['date_column'])

# 转换为类别类型
df['category_column'] = df['category_column'].astype('category')

# 转换为布尔类型
df['bool_column'] = df['bool_column'].astype(bool)
```

## 4.3 数据预处理技术

数据预处理是在数据清洗之后进行的进一步处理，目的是使数据更适合后续的分析和建模。

### 4.3.1 数据标准化

标准化是将数据按比例缩放，使之落入一个小的特定区间。

#### 最小-最大标准化

```python
from sklearn.preprocessing import MinMaxScaler

scaler = MinMaxScaler()
df_scaled = scaler.fit_transform(df[['numeric_columns']])
```

#### Z-score标准化

```python
from sklearn.preprocessing import StandardScaler

scaler = StandardScaler()
df_scaled = scaler.fit_transform(df[['numeric_columns']])
```

### 4.3.2 数据编码

对于分类变量，需要将其转换为数值形式以便处理。

#### 标签编码

```python
from sklearn.preprocessing import LabelEncoder

encoder = LabelEncoder()
df['encoded_column'] = encoder.fit_transform(df['category_column'])
```

#### 独热编码

```python
# 使用pandas进行独热编码
df_encoded = pd.get_dummies(df, columns=['category_column'])

# 使用sklearn进行独热编码
from sklearn.preprocessing import OneHotEncoder

encoder = OneHotEncoder(sparse=False)
encoded_data = encoder.fit_transform(df[['category_column']])
```

### 4.3.3 特征工程

特征工程是从原始数据中提取更有意义的特征。

#### 特征创建

```python
# 从日期中提取特征
df['year'] = df['date_column'].dt.year
df['month'] = df['date_column'].dt.month
df['day_of_week'] = df['date_column'].dt.dayofweek

# 创建组合特征
df['feature_combination'] = df['feature1'] * df['feature2']
```

#### 特征选择

```python
from sklearn.feature_selection import SelectKBest, chi2

# 选择最佳k个特征
selector = SelectKBest(score_func=chi2, k=10)
selected_features = selector.fit_transform(X, y)
```

## 4.4 数据清洗自动化

为了提高效率，我们可以构建自动化的数据清洗流程。

### 4.4.1 构建清洗管道

```python
from sklearn.pipeline import Pipeline
from sklearn.compose import ColumnTransformer
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import StandardScaler, OneHotEncoder

# 定义数值特征和分类特征的处理管道
numeric_features = ['age', 'income', 'score']
categorical_features = ['gender', 'city', 'category']

# 数值特征处理管道
numeric_transformer = Pipeline(steps=[
    ('imputer', SimpleImputer(strategy='median')),
    ('scaler', StandardScaler())
])

# 分类特征处理管道
categorical_transformer = Pipeline(steps=[
    ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),
    ('onehot', OneHotEncoder(handle_unknown='ignore'))
])

# 组合预处理管道
preprocessor = ColumnTransformer(
    transformers=[
        ('num', numeric_transformer, numeric_features),
        ('cat', categorical_transformer, categorical_features)
    ])

# 创建完整的处理管道
pipeline = Pipeline(steps=[('preprocessor', preprocessor)])

# 应用管道
processed_data = pipeline.fit_transform(df)
```

### 4.4.2 清洗质量验证

```python
def validate_data_quality(df_original, df_cleaned):
    """验证清洗后的数据质量"""
    
    print("=== 数据清洗质量报告 ===")
    
    # 行数变化
    rows_before = len(df_original)
    rows_after = len(df_cleaned)
    print(f"清洗前行数: {rows_before}")
    print(f"清洗后行数: {rows_after}")
    print(f"删除行数: {rows_before - rows_after}")
    
    # 缺失值检查
    missing_before = df_original.isnull().sum().sum()
    missing_after = df_cleaned.isnull().sum().sum()
    print(f"清洗前缺失值总数: {missing_before}")
    print(f"清洗后缺失值总数: {missing_after}")
    
    # 重复值检查
    duplicates_before = df_original.duplicated().sum()
    duplicates_after = df_cleaned.duplicated().sum()
    print(f"清洗前重复值数量: {duplicates_before}")
    print(f"清洗后重复值数量: {duplicates_after}")
    
    return {
        'rows_removed': rows_before - rows_after,
        'missing_values_fixed': missing_before - missing_after,
        'duplicates_removed': duplicates_before - duplicates_after
    }
```

## 4.5 实践案例：电商订单数据清洗

让我们通过一个实际的电商订单数据清洗案例来巩固所学知识。

### 4.5.1 数据探索

首先加载并查看原始数据：

```python
import pandas as pd
import numpy as np

# 创建模拟的电商订单数据
np.random.seed(42)
data = {
    'order_id': range(1001, 1101),
    'customer_id': np.random.randint(100, 200, 100),
    'product_name': ['Product_' + str(i) for i in np.random.randint(1, 21, 100)],
    'quantity': np.random.randint(1, 11, 100),
    'unit_price': np.round(np.random.uniform(10, 500, 100), 2),
    'order_date': pd.date_range('2023-01-01', periods=100, freq='D'),
    'customer_email': ['customer' + str(i) + '@example.com' for i in np.random.randint(1, 121, 100)],
    'payment_method': np.random.choice(['Credit Card', 'PayPal', 'Bank Transfer', 'Cash'], 100),
    'shipping_address': ['Address_' + str(i) for i in np.random.randint(1, 51, 100)]
}

# 添加一些数据质量问题
# 1. 缺失值
missing_indices = np.random.choice(100, 10, replace=False)
for idx in missing_indices[:5]:
    data['customer_email'][idx] = None
for idx in missing_indices[5:]:
    data['shipping_address'][idx] = None

# 2. 重复数据
duplicate_indices = np.random.choice(100, 5, replace=False)
for idx in duplicate_indices:
    data['order_id'][idx] = data['order_id'][idx-1] if idx > 0 else 1001

# 3. 异常值
data['quantity'][5] = 1000  # 异常高的数量
data['unit_price'][10] = -50  # 负价格

df = pd.DataFrame(data)
print("原始数据形状:", df.shape)
print("\n前10行数据:")
print(df.head(10))
```

### 4.5.2 数据清洗实现

```python
class EcommerceOrderCleaner:
    """电商订单数据清洗器"""
    
    def __init__(self, df):
        self.df = df.copy()
        self.original_shape = df.shape
        self.cleaning_log = []
    
    def remove_duplicates(self):
        """删除重复订单"""
        before_count = len(self.df)
        self.df = self.df.drop_duplicates(subset=['order_id'], keep='first')
        after_count = len(self.df)
        removed_count = before_count - after_count
        self.cleaning_log.append(f"删除重复订单: {removed_count} 条")
        print(f"删除重复订单: {removed_count} 条")
        return self
    
    def handle_missing_values(self):
        """处理缺失值"""
        # 处理客户邮箱缺失
        email_missing = self.df['customer_email'].isnull().sum()
        if email_missing > 0:
            # 用默认邮箱填充
            self.df['customer_email'].fillna('unknown@example.com', inplace=True)
            self.cleaning_log.append(f"填充客户邮箱缺失值: {email_missing} 条")
            print(f"填充客户邮箱缺失值: {email_missing} 条")
        
        # 处理配送地址缺失
        address_missing = self.df['shipping_address'].isnull().sum()
        if address_missing > 0:
            # 用默认地址填充
            self.df['shipping_address'].fillna('Unknown Address', inplace=True)
            self.cleaning_log.append(f"填充配送地址缺失值: {address_missing} 条")
            print(f"填充配送地址缺失值: {address_missing} 条")
        
        return self
    
    def handle_outliers(self):
        """处理异常值"""
        # 处理数量异常值（超过100的认为是异常）
        quantity_outliers = (self.df['quantity'] > 100).sum()
        if quantity_outliers > 0:
            # 将异常值替换为中位数
            median_quantity = self.df['quantity'][self.df['quantity'] <= 100].median()
            self.df.loc[self.df['quantity'] > 100, 'quantity'] = median_quantity
            self.cleaning_log.append(f"修正数量异常值: {quantity_outliers} 条")
            print(f"修正数量异常值: {quantity_outliers} 条")
        
        # 处理负价格
        negative_prices = (self.df['unit_price'] < 0).sum()
        if negative_prices > 0:
            # 将负价格替换为均值
            mean_price = self.df['unit_price'][self.df['unit_price'] >= 0].mean()
            self.df.loc[self.df['unit_price'] < 0, 'unit_price'] = mean_price
            self.cleaning_log.append(f"修正负价格: {negative_prices} 条")
            print(f"修正负价格: {negative_prices} 条")
        
        return self
    
    def standardize_formats(self):
        """标准化数据格式"""
        # 标准化邮箱格式（转为小写）
        self.df['customer_email'] = self.df['customer_email'].str.lower()
        self.cleaning_log.append("标准化邮箱格式")
        print("标准化邮箱格式")
        
        # 标准化支付方式格式
        payment_mapping = {
            'credit card': 'Credit Card',
            'paypal': 'PayPal',
            'bank transfer': 'Bank Transfer',
            'cash': 'Cash'
        }
        self.df['payment_method'] = self.df['payment_method'].replace(payment_mapping)
        self.cleaning_log.append("标准化支付方式格式")
        print("标准化支付方式格式")
        
        return self
    
    def add_calculated_fields(self):
        """添加计算字段"""
        # 计算订单总金额
        self.df['total_amount'] = self.df['quantity'] * self.df['unit_price']
        self.cleaning_log.append("添加订单总金额字段")
        print("添加订单总金额字段")
        
        # 提取订单年月
        self.df['order_year'] = self.df['order_date'].dt.year
        self.df['order_month'] = self.df['order_date'].dt.month
        self.cleaning_log.append("添加订单年月字段")
        print("添加订单年月字段")
        
        return self
    
    def validate_results(self):
        """验证清洗结果"""
        print("\n=== 数据清洗验证报告 ===")
        print(f"原始数据形状: {self.original_shape}")
        print(f"清洗后数据形状: {self.df.shape}")
        print(f"删除记录数: {self.original_shape[0] - self.df.shape[0]}")
        
        # 检查缺失值
        missing_values = self.df.isnull().sum().sum()
        print(f"剩余缺失值: {missing_values}")
        
        # 检查重复值
        duplicate_orders = self.df.duplicated(subset=['order_id']).sum()
        print(f"剩余重复订单: {duplicate_orders}")
        
        # 检查异常值
        quantity_outliers = (self.df['quantity'] > 100).sum()
        negative_prices = (self.df['unit_price'] < 0).sum()
        print(f"剩余数量异常值: {quantity_outliers}")
        print(f"剩余负价格: {negative_prices}")
        
        return self
    
    def get_cleaning_log(self):
        """获取清洗日志"""
        return self.cleaning_log
    
    def get_cleaned_data(self):
        """获取清洗后的数据"""
        return self.df


# 执行数据清洗
cleaner = EcommerceOrderCleaner(df)
cleaned_df = (cleaner
              .remove_duplicates()
              .handle_missing_values()
              .handle_outliers()
              .standardize_formats()
              .add_calculated_fields()
              .validate_results()
              .get_cleaned_data())

print("\n清洗后数据前5行:")
print(cleaned_df.head())
```

### 4.5.3 清洗效果评估

```python
def evaluate_cleaning_effect(original_df, cleaned_df):
    """评估清洗效果"""
    print("\n=== 数据清洗效果评估 ===")
    
    # 1. 数据完整性评估
    print("1. 数据完整性:")
    orig_completeness = 1 - (original_df.isnull().sum().sum() / (original_df.shape[0] * original_df.shape[1]))
    clean_completeness = 1 - (cleaned_df.isnull().sum().sum() / (cleaned_df.shape[0] * cleaned_df.shape[1]))
    print(f"  清洗前完整性: {orig_completeness:.2%}")
    print(f"  清洗后完整性: {clean_completeness:.2%}")
    print(f"  提升幅度: {clean_completeness - orig_completeness:.2%}")
    
    # 2. 数据一致性评估
    print("\n2. 数据一致性:")
    orig_duplicates = original_df.duplicated(subset=['order_id']).sum()
    clean_duplicates = cleaned_df.duplicated(subset=['order_id']).sum()
    print(f"  清洗前重复订单: {orig_duplicates}")
    print(f"  清洗后重复订单: {clean_duplicates}")
    print(f"  减少重复订单: {orig_duplicates - clean_duplicates}")
    
    # 3. 数据有效性评估
    print("\n3. 数据有效性:")
    orig_invalid_qty = (original_df['quantity'] > 100).sum()
    clean_invalid_qty = (cleaned_df['quantity'] > 100).sum()
    print(f"  清洗前异常数量: {orig_invalid_qty}")
    print(f"  清洗后异常数量: {clean_invalid_qty}")
    
    orig_negative_price = (original_df['unit_price'] < 0).sum()
    clean_negative_price = (cleaned_df['unit_price'] < 0).sum()
    print(f"  清洗前负价格: {orig_negative_price}")
    print(f"  清洗后负价格: {clean_negative_price}")

# 执行评估
evaluate_cleaning_effect(df, cleaned_df)
```

## 4.6 数据清洗最佳实践

### 4.6.1 制定清洗策略

1. **先全局后局部**：先处理影响整个数据集的问题，再处理特定字段的问题
2. **先粗后细**：先做大幅度的清洗，再进行精细化调整
3. **记录每步操作**：详细记录清洗过程，便于回溯和优化

### 4.6.2 保持数据血缘

```python
import hashlib
import json

class DataLineageTracker:
    """数据血缘追踪器"""
    
    def __init__(self):
        self.operations = []
    
    def record_operation(self, operation_name, description, input_hash=None):
        """记录操作"""
        operation = {
            'timestamp': pd.Timestamp.now().isoformat(),
            'operation': operation_name,
            'description': description,
            'input_hash': input_hash
        }
        self.operations.append(operation)
    
    def generate_data_hash(self, df):
        """生成数据哈希值"""
        data_string = df.to_json()
        return hashlib.md5(data_string.encode()).hexdigest()
    
    def export_lineage(self, filename):
        """导出数据血缘信息"""
        with open(filename, 'w') as f:
            json.dump(self.operations, f, indent=2)

# 使用示例
lineage_tracker = DataLineageTracker()
# 在每步清洗操作后记录
# lineage_tracker.record_operation("remove_duplicates", "删除重复订单", input_hash)
```

### 4.6.3 建立质量门禁

```python
def quality_gate(df, thresholds):
    """质量门禁检查"""
    issues = []
    
    # 检查缺失值比例
    missing_ratio = df.isnull().sum().sum() / (df.shape[0] * df.shape[1])
    if missing_ratio > thresholds.get('max_missing_ratio', 0.05):
        issues.append(f"缺失值比例过高: {missing_ratio:.2%}")
    
    # 检查重复值比例
    duplicate_ratio = df.duplicated().sum() / len(df)
    if duplicate_ratio > thresholds.get('max_duplicate_ratio', 0.01):
        issues.append(f"重复值比例过高: {duplicate_ratio:.2%}")
    
    # 检查异常值
    if 'quantity' in df.columns:
        outlier_ratio = (df['quantity'] > 100).sum() / len(df)
        if outlier_ratio > thresholds.get('max_outlier_ratio', 0.02):
            issues.append(f"数量异常值比例过高: {outlier_ratio:.2%}")
    
    if issues:
        raise ValueError("数据质量未达到标准:\n" + "\n".join(issues))
    
    return True

# 使用示例
# thresholds = {
#     'max_missing_ratio': 0.05,
#     'max_duplicate_ratio': 0.01,
#     'max_outlier_ratio': 0.02
# }
# quality_gate(cleaned_df, thresholds)
```

## 4.7 小结

本章详细介绍了数据清洗与预处理技术，涵盖了常见的数据质量问题及其处理方法，包括缺失值处理、重复数据处理、异常值处理、格式标准化等。我们还学习了数据预处理技术如标准化、编码和特征工程，并通过电商订单数据的实际案例演示了完整的数据清洗流程。

下一章我们将深入探讨数据验证与质量规则，学习如何建立系统的数据验证机制。

---

**思考题：**
1. 在您的业务场景中，哪种数据质量问题最为常见？应该采用什么样的清洗策略？
2. 如何平衡数据清洗的彻底性和处理效率？

**实践练习：**
选择您工作中遇到的一个实际数据集，应用本章学到的数据清洗技术进行全面清洗，并评估清洗效果。

**延伸阅读：**
- "Data Cleaning: Problems and Current Approaches" by Rahm and Do
- "Practical Data Science with R" by Zumel and Mount
- Scikit-learn数据预处理文档