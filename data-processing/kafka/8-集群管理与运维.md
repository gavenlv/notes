# 第8章：集群管理与运维

## 目录
1. [Kafka集群架构设计](#Kafka集群架构设计)
2. [集群部署与配置](#集群部署与配置)
3. [集群监控与告警](#集群监控与告警)
4. [故障检测与自动恢复](#故障检测与自动恢复)
5. [性能调优与容量规划](#性能调优与容量规划)
6. [安全配置与访问控制](#安全配置与访问控制)
7. [运维自动化工具](#运维自动化工具)
8. [集群扩缩容策略](#集群扩缩容策略)
9. [灾难恢复与备份](#灾难恢复与备份)
10. [运维最佳实践](#运维最佳实践)

---

## Kafka集群架构设计

### 集群拓扑设计原则

```java
package com.kafka.tutorial.cluster;

import java.io.*;
import java.util.*;
import java.util.concurrent.*;
import java.util.concurrent.atomic.*;
import java.util.stream.Collectors;

/**
 * Kafka集群架构设计与拓扑管理
 */
public class KafkaClusterArchitecture {
    
    /**
     * 集群节点配置
     */
    public static class BrokerConfig {
        private final String brokerId;
        private final String host;
        private final int port;
        private final int internalPort;
        private final List<String> rackIds;
        private final Map<String, String> customProperties;
        private final ResourceConfig resources;
        private final SecurityConfig security;
        
        public BrokerConfig(String brokerId, String host, int port, int internalPort) {
            this.brokerId = brokerId;
            this.host = host;
            this.port = port;
            this.internalPort = internalPort;
            this.rackIds = new ArrayList<>();
            this.customProperties = new HashMap<>();
            this.resources = new ResourceConfig();
            this.security = new SecurityConfig();
        }
        
        public void addRackId(String rackId) {
            this.rackIds.add(rackId);
        }
        
        public void setProperty(String key, String value) {
            this.customProperties.put(key, value);
        }
        
        // Getters
        public String getBrokerId() { return brokerId; }
        public String getHost() { return host; }
        public int getPort() { return port; }
        public int getInternalPort() { return internalPort; }
        public List<String> getRackIds() { return rackIds; }
        public Map<String, String> getCustomProperties() { return customProperties; }
        public ResourceConfig getResources() { return resources; }
        public SecurityConfig getSecurity() { return security; }
    }
    
    /**
     * 资源配置
     */
    public static class ResourceConfig {
        private int maxHeapSizeMB = 1024;
        private int maxOffHeapSizeMB = 0;
        private int numNetworkThreads = 3;
        private int numIoThreads = 8;
        private int socketSendBufferBytes = 100 * 1024;
        private int socketReceiveBufferBytes = 100 * 1024;
        private int socketRequestMaxBytes = 100 * 1024 * 1024;
        private int numPartitions = 1;
        private int defaultReplicationFactor = 1;
        
        // Network performance tuning
        private int numReplicaFetchers = 4;
        private int replicaFetchMaxBytes = 1024 * 1024;
        private int replicaFetchWaitMaxMs = 500;
        private int replicaHighWatermarkCheckpointIntervalMs = 5000;
        private int replicaFetchBackoffMs = 1000;
        
        // Log configuration
        private int numRecoveryThreadsPerDataDir = 1;
        private int logFlushIntervalMessages = Long.MAX_VALUE;
        private long logFlushIntervalMs = Long.MAX_VALUE;
        private int logFlushOffsetCheckpointIntervalMs = 60000;
        
        // Getters and Setters
        public int getMaxHeapSizeMB() { return maxHeapSizeMB; }
        public void setMaxHeapSizeMB(int maxHeapSizeMB) { this.maxHeapSizeMB = maxHeapSizeMB; }
        public int getMaxOffHeapSizeMB() { return maxOffHeapSizeMB; }
        public void setMaxOffHeapSizeMB(int maxOffHeapSizeMB) { this.maxOffHeapSizeMB = maxOffHeapSizeMB; }
        public int getNumNetworkThreads() { return numNetworkThreads; }
        public void setNumNetworkThreads(int numNetworkThreads) { this.numNetworkThreads = numNetworkThreads; }
        public int getNumIoThreads() { return numIoThreads; }
        public void setNumIoThreads(int numIoThreads) { this.numIoThreads = numIoThreads; }
        public int getSocketSendBufferBytes() { return socketSendBufferBytes; }
        public void setSocketSendBufferBytes(int socketSendBufferBytes) { this.socketSendBufferBytes = socketSendBufferBytes; }
        public int getSocketReceiveBufferBytes() { return socketReceiveBufferBytes; }
        public void setSocketReceiveBufferBytes(int socketReceiveBufferBytes) { this.socketReceiveBufferBytes = socketReceiveBufferBytes; }
        public int getSocketRequestMaxBytes() { return socketRequestMaxBytes; }
        public void setSocketRequestMaxBytes(int socketRequestMaxBytes) { this.socketRequestMaxBytes = socketRequestMaxBytes; }
        public int getNumPartitions() { return numPartitions; }
        public void setNumPartitions(int numPartitions) { this.numPartitions = numPartitions; }
        public int getDefaultReplicationFactor() { return defaultReplicationFactor; }
        public void setDefaultReplicationFactor(int defaultReplicationFactor) { this.defaultReplicationFactor = defaultReplicationFactor; }
        public int getNumReplicaFetchers() { return numReplicaFetchers; }
        public void setNumReplicaFetchers(int numReplicaFetchers) { this.numReplicaFetchers = numReplicaFetchers; }
        public int getReplicaFetchMaxBytes() { return replicaFetchMaxBytes; }
        public void setReplicaFetchMaxBytes(int replicaFetchMaxBytes) { this.replicaFetchMaxBytes = replicaFetchMaxBytes; }
        public int getReplicaFetchWaitMaxMs() { return replicaFetchWaitMaxMs; }
        public void setReplicaFetchWaitMaxMs(int replicaFetchWaitMaxMs) { this.replicaFetchWaitMaxMs = replicaFetchWaitMaxMs; }
        public int getReplicaHighWatermarkCheckpointIntervalMs() { return replicaHighWatermarkCheckpointIntervalMs; }
        public void setReplicaHighWatermarkCheckpointIntervalMs(int replicaHighWatermarkCheckpointIntervalMs) { this.replicaHighWatermarkCheckpointIntervalMs = replicaHighWatermarkCheckpointIntervalMs; }
        public int getReplicaFetchBackoffMs() { return replicaFetchBackoffMs; }
        public void setReplicaFetchBackoffMs(int replicaFetchBackoffMs) { this.replicaFetchBackoffMs = replicaFetchBackoffMs; }
        public int getNumRecoveryThreadsPerDataDir() { return numRecoveryThreadsPerDataDir; }
        public void setNumRecoveryThreadsPerDataDir(int numRecoveryThreadsPerDataDir) { this.numRecoveryThreadsPerDataDir = numRecoveryThreadsPerDataDir; }
        public int getLogFlushIntervalMessages() { return logFlushIntervalMessages; }
        public void setLogFlushIntervalMessages(int logFlushIntervalMessages) { this.logFlushIntervalMessages = logFlushIntervalMessages; }
        public long getLogFlushIntervalMs() { return logFlushIntervalMs; }
        public void setLogFlushIntervalMs(long logFlushIntervalMs) { this.logFlushIntervalMs = logFlushIntervalMs; }
        public int getLogFlushOffsetCheckpointIntervalMs() { return logFlushOffsetCheckpointIntervalMs; }
        public void setLogFlushOffsetCheckpointIntervalMs(int logFlushOffsetCheckpointIntervalMs) { this.logFlushOffsetCheckpointIntervalMs = logFlushOffsetCheckpointIntervalMs; }
    }
    
    /**
     * 安全配置
     */
    public static class SecurityConfig {
        private boolean sslEnabled = false;
        private boolean saslEnabled = false;
        private String sslKeyStoreLocation = "";
        private String sslKeyStorePassword = "";
        private String sslKeyPassword = "";
        private String sslTrustStoreLocation = "";
        private String sslTrustStorePassword = "";
        private String saslMechanism = "PLAIN";
        private String saslJaasConfig = "";
        
        // Getters and Setters
        public boolean isSslEnabled() { return sslEnabled; }
        public void setSslEnabled(boolean sslEnabled) { this.sslEnabled = sslEnabled; }
        public boolean isSaslEnabled() { return saslEnabled; }
        public void setSaslEnabled(boolean saslEnabled) { this.saslEnabled = saslEnabled; }
        public String getSslKeyStoreLocation() { return sslKeyStoreLocation; }
        public void setSslKeyStoreLocation(String sslKeyStoreLocation) { this.sslKeyStoreLocation = sslKeyStoreLocation; }
        public String getSslKeyStorePassword() { return sslKeyStorePassword; }
        public void setSslKeyStorePassword(String sslKeyStorePassword) { this.sslKeyStorePassword = sslKeyStorePassword; }
        public String getSslKeyPassword() { return sslKeyPassword; }
        public void setSslKeyPassword(String sslKeyPassword) { this.sslKeyPassword = sslKeyPassword; }
        public String getSslTrustStoreLocation() { return sslTrustStoreLocation; }
        public void setSslTrustStoreLocation(String sslTrustStoreLocation) { this.sslTrustStoreLocation = sslTrustStoreLocation; }
        public String getSslTrustStorePassword() { return sslTrustStorePassword; }
        public void setSslTrustStorePassword(String sslTrustStorePassword) { this.sslTrustStorePassword = sslTrustStorePassword; }
        public String getSaslMechanism() { return saslMechanism; }
        public void setSaslMechanism(String saslMechanism) { this.saslMechanism = saslMechanism; }
        public String getSaslJaasConfig() { return saslJaasConfig; }
        public void setSaslJaasConfig(String saslJaasConfig) { this.saslJaasConfig = saslJaasConfig; }
    }
    
    /**
     * 集群拓扑
     */
    public static class ClusterTopology {
        private final String clusterName;
        private final Map<String, BrokerConfig> brokers;
        private final Map<String, List<String>> rackMap;
        private final List<String> bootstrapServers;
        private final ZooKeeperConfig zooKeeperConfig;
        
        public ClusterTopology(String clusterName) {
            this.clusterName = clusterName;
            this.brokers = new HashMap<>();
            this.rackMap = new HashMap<>();
            this.bootstrapServers = new ArrayList<>();
            this.zooKeeperConfig = new ZooKeeperConfig();
        }
        
        public void addBroker(BrokerConfig brokerConfig) {
            brokers.put(brokerConfig.getBrokerId(), brokerConfig);
            bootstrapServers.add(brokerConfig.getHost() + ":" + brokerConfig.getPort());
            
            // 更新机架映射
            for (String rackId : brokerConfig.getRackIds()) {
                rackMap.computeIfAbsent(rackId, k -> new ArrayList<>())
                       .add(brokerConfig.getBrokerId());
            }
        }
        
        public void removeBroker(String brokerId) {
            BrokerConfig broker = brokers.remove(brokerId);
            if (broker != null) {
                bootstrapServers.remove(broker.getHost() + ":" + broker.getPort());
                
                // 从机架映射中移除
                for (String rackId : broker.getRackIds()) {
                    rackMap.getOrDefault(rackId, new ArrayList<>()).remove(brokerId);
                }
            }
        }
        
        public List<BrokerConfig> getBrokersInRack(String rackId) {
            List<String> brokerIds = rackMap.getOrDefault(rackId, new ArrayList<>());
            return brokerIds.stream()
                          .map(brokers::get)
                          .collect(Collectors.toList());
        }
        
        public int getTotalBrokers() {
            return brokers.size();
        }
        
        public int getTotalRacks() {
            return rackMap.size();
        }
        
        // Getters
        public String getClusterName() { return clusterName; }
        public Map<String, BrokerConfig> getBrokers() { return brokers; }
        public Map<String, List<String>> getRackMap() { return rackMap; }
        public List<String> getBootstrapServers() { return bootstrapServers; }
        public ZooKeeperConfig getZooKeeperConfig() { return zooKeeperConfig; }
    }
    
    /**
     * ZooKeeper配置
     */
    public static class ZooKeeperConfig {
        private String connectString = "localhost:2181";
        private int sessionTimeout = 30000;
        private int connectionTimeout = 18000;
        private int maxInFlightRequests = 500;
        private boolean secureACL = false;
        private String securityProtocol = "PLAINTEXT";
        
        // Getters and Setters
        public String getConnectString() { return connectString; }
        public void setConnectString(String connectString) { this.connectString = connectString; }
        public int getSessionTimeout() { return sessionTimeout; }
        public void setSessionTimeout(int sessionTimeout) { this.sessionTimeout = sessionTimeout; }
        public int getConnectionTimeout() { return connectionTimeout; }
        public void setConnectionTimeout(int connectionTimeout) { this.connectionTimeout = connectionTimeout; }
        public int getMaxInFlightRequests() { return maxInFlightRequests; }
        public void setMaxInFlightRequests(int maxInFlightRequests) { this.maxInFlightRequests = maxInFlightRequests; }
        public boolean isSecureACL() { return secureACL; }
        public void setSecureACL(boolean secureACL) { this.secureACL = secureACL; }
        public String getSecurityProtocol() { return securityProtocol; }
        public void setSecurityProtocol(String securityProtocol) { this.securityProtocol = securityProtocol; }
    }
    
    /**
     * 集群容量规划器
     */
    public static class CapacityPlanner {
        
        public static class CapacityRequirement {
            private final long messagesPerSecond;
            private final long averageMessageSize;
            private final int replicationFactor;
            private final long retentionHours;
            private final int availabilityRequirement; // 1-99.99%
            
            public CapacityRequirement(long messagesPerSecond, long averageMessageSize, 
                                     int replicationFactor, long retentionHours, int availabilityRequirement) {
                this.messagesPerSecond = messagesPerSecond;
                this.averageMessageSize = averageMessageSize;
                this.replicationFactor = replicationFactor;
                this.retentionHours = retentionHours;
                this.availabilityRequirement = availabilityRequirement;
            }
            
            public long getRawDataRate() {
                return messagesPerSecond * averageMessageSize;
            }
            
            public long getReplicaDataRate() {
                return getRawDataRate() * replicationFactor;
            }
            
            public long getEstimatedStorageRequirement() {
                return getReplicaDataRate() * retentionHours * 3600; // bytes
            }
            
            // Getters
            public long getMessagesPerSecond() { return messagesPerSecond; }
            public long getAverageMessageSize() { return averageMessageSize; }
            public int getReplicationFactor() { return replicationFactor; }
            public long getRetentionHours() { return retentionHours; }
            public int getAvailabilityRequirement() { return availabilityRequirement; }
        }
        
        public static class ClusterRecommendation {
            private final int recommendedBrokers;
            private final int recommendedPartitionsPerTopic;
            private final List<BrokerHardwareSpec> hardwareSpecs;
            private final Map<String, Object> configurationRecommendations;
            
            public ClusterRecommendation(int recommendedBrokers, int recommendedPartitionsPerTopic,
                                       List<BrokerHardwareSpec> hardwareSpecs,
                                       Map<String, Object> configurationRecommendations) {
                this.recommendedBrokers = recommendedBrokers;
                this.recommendedPartitionsPerTopic = recommendedPartitionsPerTopic;
                this.hardwareSpecs = hardwareSpecs;
                this.configurationRecommendations = configurationRecommendations;
            }
            
            // Getters
            public int getRecommendedBrokers() { return recommendedBrokers; }
            public int getRecommendedPartitionsPerTopic() { return recommendedPartitionsPerTopic; }
            public List<BrokerHardwareSpec> getHardwareSpecs() { return hardwareSpecs; }
            public Map<String, Object> getConfigurationRecommendations() { return configurationRecommendations; }
        }
        
        public static class BrokerHardwareSpec {
            private final int cpuCores;
            private final long memoryGB;
            private final long diskGB;
            private final int networkGbps;
            private final String diskType; // SSD, HDD
            
            public BrokerHardwareSpec(int cpuCores, long memoryGB, long diskGB, 
                                    int networkGbps, String diskType) {
                this.cpuCores = cpuCores;
                this.memoryGB = memoryGB;
                this.diskGB = diskGB;
                this.networkGbps = networkGbps;
                this.diskType = diskType;
            }
            
            // Getters
            public int getCpuCores() { return cpuCores; }
            public long getMemoryGB() { return memoryGB; }
            public long getDiskGB() { return diskGB; }
            public int getNetworkGbps() { return networkGbps; }
            public String getDiskType() { return diskType; }
        }
        
        public static ClusterRecommendation planCluster(CapacityRequirement requirement) {
            // 计算基础指标
            long rawDataRate = requirement.getRawDataRate(); // bytes/second
            long replicaDataRate = requirement.getReplicaDataRate();
            long storageRequired = requirement.getEstimatedStorageRequirement();
            
            // 计算建议的broker数量
            int brokers = calculateOptimalBrokers(rawDataRate, replicaDataRate, storageRequired);
            
            // 计算分区建议
            int partitionsPerTopic = calculateOptimalPartitions(rawDataRate, brokers);
            
            // 硬件规格建议
            List<BrokerHardwareSpec> hardwareSpecs = suggestHardwareSpecs(storageRequired, rawDataRate);
            
            // 配置建议
            Map<String, Object> configRecommendations = generateConfigRecommendations(requirement);
            
            return new ClusterRecommendation(brokers, partitionsPerTopic, hardwareSpecs, configRecommendations);
        }
        
        private static int calculateOptimalBrokers(long dataRate, long replicaDataRate, long storageRequired) {
            // 基于数据速率和存储要求的broker数量
            int brokerByStorage = (int) Math.ceil(storageRequired / (1024.0 * 1024 * 1024 * 1000)); // 假设每个broker 1TB
            int brokerByThroughput = (int) Math.ceil(replicaDataRate / (1024.0 * 1024 * 50)); // 假设每个broker 50MB/s
            
            return Math.max(brokerByStorage, brokerByThroughput, 3); // 最小3个broker
        }
        
        private static int calculateOptimalPartitions(long dataRate, int brokers) {
            // 基于目标吞吐量计算分区数
            int minPartitions = (int) Math.ceil(dataRate / (1024.0 * 1024 * 10)); // 假设每个分区10MB/s
            
            // 确保分区数是broker数的倍数或约数
            int brokerFactor = brokers;
            int partitions = Math.max(minPartitions, brokerFactor);
            
            // 限制在合理范围内
            return Math.min(Math.max(partitions, 1), 1000);
        }
        
        private static List<BrokerHardwareSpec> suggestHardwareSpecs(long storageRequired, long dataRate) {
            List<BrokerHardwareSpec> specs = new ArrayList<>();
            
            // 大数据量场景
            if (storageRequired > 1024L * 1024 * 1024 * 1024) { // > 1TB
                specs.add(new BrokerHardwareSpec(16, 64, 4000, 10, "SSD"));
            } 
            // 中等数据量场景
            else if (storageRequired > 1024L * 1024 * 1024 * 100) { // > 100GB
                specs.add(new BrokerHardwareSpec(8, 32, 1000, 10, "SSD"));
            } 
            // 小数据量场景
            else {
                specs.add(new BrokerHardwareSpec(4, 16, 500, 1, "HDD"));
            }
            
            return specs;
        }
        
        private static Map<String, Object> generateConfigRecommendations(CapacityRequirement requirement) {
            Map<String, Object> recommendations = new HashMap<>();
            
            // JVM设置
            recommendations.put("Xmx", "4g");
            recommendations.put("Xms", "4g");
            
            // 性能配置
            recommendations.put("num.network.threads", 8);
            recommendations.put("num.io.threads", 16);
            recommendations.put("socket.send.buffer.bytes", 102400);
            recommendations.put("socket.receive.buffer.bytes", 102400);
            
            // 副本配置
            recommendations.put("offsets.topic.replication.factor", requirement.getReplicationFactor());
            recommendations.put("transaction.state.log.replication.factor", requirement.getReplicationFactor());
            recommendations.put("transaction.state.log.min.isr", Math.max(1, requirement.getReplicationFactor() - 1));
            
            // 日志配置
            recommendations.put("log.retention.hours", requirement.getRetentionHours());
            recommendations.put("log.segment.bytes", 1073741824L); // 1GB
            recommendations.put("log.retention.check.interval.ms", 300000); // 5分钟
            
            return recommendations;
        }
    }
    
    /**
     * 演示集群架构设计
     */
    public static void demonstrateClusterDesign() {
        System.out.println("=== Kafka集群架构设计演示 ===\n");
        
        // 1. 创建集群拓扑
        ClusterTopology cluster = new ClusterTopology("production-cluster");
        
        // 2. 配置broker
        // 数据中心1 - 机架A
        BrokerConfig broker1 = new BrokerConfig("1", "kafka-01.datacenter1.com", 9092, 9093);
        broker1.addRackId("dc1-rack-a");
        broker1.getResources().setMaxHeapSizeMB(8192);
        broker1.getResources().setNumIoThreads(16);
        cluster.addBroker(broker1);
        
        // 数据中心1 - 机架B
        BrokerConfig broker2 = new BrokerConfig("2", "kafka-02.datacenter1.com", 9092, 9093);
        broker2.addRackId("dc1-rack-b");
        broker2.getResources().setMaxHeapSizeMB(8192);
        broker2.getResources().setNumIoThreads(16);
        cluster.addBroker(broker2);
        
        // 数据中心2 - 机架A
        BrokerConfig broker3 = new BrokerConfig("3", "kafka-03.datacenter2.com", 9092, 9093);
        broker3.addRackId("dc2-rack-a");
        broker3.getResources().setMaxHeapSizeMB(8192);
        broker3.getResources().setNumIoThreads(16);
        cluster.addBroker(broker3);
        
        // 数据中心2 - 机架B
        BrokerConfig broker4 = new BrokerConfig("4", "kafka-04.datacenter2.com", 9092, 9093);
        broker4.addRackId("dc2-rack-b");
        broker4.getResources().setMaxHeapSizeMB(8192);
        broker4.getResources().setNumIoThreads(16);
        cluster.addBroker(broker4);
        
        // 3. 配置ZooKeeper
        cluster.getZooKeeperConfig().setConnectString("zk1.datacenter1.com:2181,zk2.datacenter1.com:2181,zk3.datacenter2.com:2181");
        cluster.getZooKeeperConfig().setSessionTimeout(30000);
        
        // 4. 输出集群信息
        System.out.println("集群配置完成:");
        System.out.println("  集群名称: " + cluster.getClusterName());
        System.out.println("  Broker总数: " + cluster.getTotalBrokers());
        System.out.println("  机架总数: " + cluster.getTotalRacks());
        System.out.println("  引导服务器: " + String.join(", ", cluster.getBootstrapServers()));
        
        System.out.println("\n机架分布:");
        for (Map.Entry<String, List<String>> entry : cluster.getRackMap().entrySet()) {
            System.out.println("  " + entry.getKey() + ": " + String.join(", ", entry.getValue()));
        }
        
        // 5. 容量规划演示
        System.out.println("\n=== 容量规划演示 ===");
        
        CapacityPlanner.CapacityRequirement requirement = new CapacityPlanner.CapacityRequirement(
            100000,        // 10万条消息/秒
            1024,          // 1KB平均消息大小
            3,             // 3副本
            168,           // 7天保留期
            99.99          // 99.99%可用性要求
        );
        
        CapacityPlanner.ClusterRecommendation recommendation = 
            CapacityPlanner.planCluster(requirement);
        
        System.out.println("容量规划结果:");
        System.out.println("  建议Broker数量: " + recommendation.getRecommendedBrokers());
        System.out.println("  建议分区数: " + recommendation.getRecommendedPartitionsPerTopic());
        System.out.println("  原始数据速率: " + (requirement.getRawDataRate() / 1024.0 / 1024) + " MB/s");
        System.out.println("  副本数据速率: " + (requirement.getReplicaDataRate() / 1024.0 / 1024) + " MB/s");
        System.out.println("  预计存储需求: " + (requirement.getEstimatedStorageRequirement() / 1024.0 / 1024 / 1024) + " GB");
        
        System.out.println("\n硬件规格建议:");
        for (CapacityPlanner.BrokerHardwareSpec spec : recommendation.getHardwareSpecs()) {
            System.out.println("  CPU: " + spec.getCpuCores() + " 核");
            System.out.println("  内存: " + spec.getMemoryGB() + " GB");
            System.out.println("  磁盘: " + spec.getDiskGB() + " GB (" + spec.getDiskType() + ")");
            System.out.println("  网络: " + spec.getNetworkGbps() + " Gbps");
        }
        
        System.out.println("\n配置建议:");
        for (Map.Entry<String, Object> entry : recommendation.getConfigurationRecommendations().entrySet()) {
            System.out.println("  " + entry.getKey() + ": " + entry.getValue());
        }
    }
}
```

---

## 集群部署与配置

### 自动化部署工具

```java
package com.kafka.tutorial.cluster;

import java.io.*;
import java.nio.file.*;
import java.util.*;
import java.util.concurrent.*;
import java.util.stream.Collectors;

/**
 * Kafka集群自动化部署与配置管理
 */
public class KafkaClusterDeployment {
    
    /**
     * 部署计划
     */
    public static class DeploymentPlan {
        private final String environment; // dev, staging, prod
        private final String version;
        private final List<String> brokerIds;
        private final Map<String, BrokerDeploymentConfig> brokerConfigs;
        private final Map<String, String> globalProperties;
        private final DeploymentStrategy strategy;
        
        public DeploymentPlan(String environment, String version) {
            this.environment = environment;
            this.version = version;
            this.brokerIds = new ArrayList<>();
            this.brokerConfigs = new HashMap<>();
            this.globalProperties = new HashMap<>();
            this.strategy = new DeploymentStrategy();
        }
        
        public void addBroker(String brokerId, BrokerDeploymentConfig config) {
            brokerIds.add(brokerId);
            brokerConfigs.put(brokerId, config);
        }
        
        public void setGlobalProperty(String key, String value) {
            globalProperties.put(key, value);
        }
        
        // Getters
        public String getEnvironment() { return environment; }
        public String getVersion() { return version; }
        public List<String> getBrokerIds() { return brokerIds; }
        public Map<String, BrokerDeploymentConfig> getBrokerConfigs() { return brokerConfigs; }
        public Map<String, String> getGlobalProperties() { return globalProperties; }
        public DeploymentStrategy getStrategy() { return strategy; }
    }
    
    /**
     * Broker部署配置
     */
    public static class BrokerDeploymentConfig {
        private final String host;
        private final String rackId;
        private final Map<String, String> properties;
        private final Map<String, String> environmentVariables;
        private final String jvmSettings;
        private final List<String> dependencies;
        
        public BrokerDeploymentConfig(String host, String rackId) {
            this.host = host;
            this.rackId = rackId;
            this.properties = new HashMap<>();
            this.environmentVariables = new HashMap<>();
            this.jvmSettings = "-Xmx4g -Xms4g -XX:+UseG1GC";
            this.dependencies = new ArrayList<>();
        }
        
        public void setProperty(String key, String value) {
            properties.put(key, value);
        }
        
        public void setEnvironmentVariable(String key, String value) {
            environmentVariables.put(key, value);
        }
        
        public void setJvmSettings(String jvmSettings) {
            this.jvmSettings = jvmSettings;
        }
        
        public void addDependency(String dependency) {
            dependencies.add(dependency);
        }
        
        // Getters
        public String getHost() { return host; }
        public String getRackId() { return rackId; }
        public Map<String, String> getProperties() { return properties; }
        public Map<String, String> getEnvironmentVariables() { return environmentVariables; }
        public String getJvmSettings() { return jvmSettings; }
        public List<String> getDependencies() { return dependencies; }
    }
    
    /**
     * 部署策略
     */
    public static class DeploymentStrategy {
        private boolean rollingRestart = true;
        private int maxUnavailableBrokers = 1;
        private int healthCheckInterval = 10; // seconds
        private int healthCheckTimeout = 30; // seconds
        private boolean useController = true;
        private String preDeploymentScript = "";
        private String postDeploymentScript = "";
        
        // Getters and Setters
        public boolean isRollingRestart() { return rollingRestart; }
        public void setRollingRestart(boolean rollingRestart) { this.rollingRestart = rollingRestart; }
        public int getMaxUnavailableBrokers() { return maxUnavailableBrokers; }
        public void setMaxUnavailableBrokers(int maxUnavailableBrokers) { this.maxUnavailableBrokers = maxUnavailableBrokers; }
        public int getHealthCheckInterval() { return healthCheckInterval; }
        public void setHealthCheckInterval(int healthCheckInterval) { this.healthCheckInterval = healthCheckInterval; }
        public int getHealthCheckTimeout() { return healthCheckTimeout; }
        public void setHealthCheckTimeout(int healthCheckTimeout) { this.healthCheckTimeout = healthCheckTimeout; }
        public boolean isUseController() { return useController; }
        public void setUseController(boolean useController) { this.useController = useController; }
        public String getPreDeploymentScript() { return preDeploymentScript; }
        public void setPreDeploymentScript(String preDeploymentScript) { this.preDeploymentScript = preDeploymentScript; }
        public String getPostDeploymentScript() { return postDeploymentScript; }
        public void setPostDeploymentScript(String postDeploymentScript) { this.postDeploymentScript = postDeploymentScript; }
    }
    
    /**
     * 配置生成器
     */
    public static class ConfigGenerator {
        
        /**
         * 生成server.properties文件
         */
        public static String generateServerProperties(BrokerDeploymentConfig config, 
                                                     Map<String, String> globalProps) {
            StringBuilder sb = new StringBuilder();
            
            // 基础配置
            sb.append("# Kafka Server Configuration\n");
            sb.append("# Generated automatically for ").append(config.getHost()).append("\n\n");
            
            // 网络配置
            sb.append("# Network Configuration\n");
            sb.append("listeners=PLAINTEXT://").append(config.getHost()).append(":9092\n");
            sb.append("advertised.listeners=PLAINTEXT://").append(config.getHost()).append(":9092\n");
            sb.append("num.network.threads=8\n");
            sb.append("num.io.threads=16\n");
            sb.append("socket.send.buffer.bytes=102400\n");
            sb.append("socket.receive.buffer.bytes=102400\n");
            sb.append("socket.request.max.bytes=104857600\n\n");
            
            // 日志配置
            sb.append("# Log Configuration\n");
            sb.append("log.dirs=/opt/kafka/data\n");
            sb.append("num.partitions=1\n");
            sb.append("num.recovery.threads.per.data.dir=1\n");
            sb.append("log.retention.hours=168\n");
            sb.append("log.segment.bytes=1073741824\n");
            sb.append("log.retention.check.interval.ms=300000\n");
            sb.append("log.cleanup.policy=delete\n\n");
            
            // 副本配置
            sb.append("# Replication Configuration\n");
            sb.append("offsets.topic.replication.factor=3\n");
            sb.append("transaction.state.log.replication.factor=3\n");
            sb.append("transaction.state.log.min.isr=2\n");
            sb.append("default.replication.factor=3\n");
            sb.append("min.insync.replicas=2\n\n");
            
            // Zookeeper配置
            sb.append("# Zookeeper Configuration\n");
            sb.append("zookeeper.connect=zk1:2181,zk2:2181,zk3:2181/kafka\n");
            sb.append("zookeeper.connection.timeout.ms=18000\n");
            sb.append("zookeeper.session.timeout.ms=30000\n\n");
            
            // Broker配置
            sb.append("# Broker Configuration\n");
            sb.append("broker.id=-1\n"); // 动态分配
            sb.append("delete.topic.enable=true\n");
            sb.append("auto.create.topics.enable=false\n");
            sb.append("num.network.threads=8\n");
            sb.append("num.io.threads=16\n");
            
            // 机架配置
            if (config.getRackId() != null && !config.getRackId().isEmpty()) {
                sb.append("broker.rack=").append(config.getRackId()).append("\n");
            }
            
            sb.append("\n");
            
            // 应用全局配置
            sb.append("# Global Properties\n");
            for (Map.Entry<String, String> entry : globalProps.entrySet()) {
                sb.append(entry.getKey()).append("=").append(entry.getValue()).append("\n");
            }
            sb.append("\n");
            
            // 应用broker特定配置
            if (!config.getProperties().isEmpty()) {
                sb.append("# Broker-specific Properties\n");
                for (Map.Entry<String, String> entry : config.getProperties().entrySet()) {
                    sb.append(entry.getKey()).append("=").append(entry.getValue()).append("\n");
                }
            }
            
            return sb.toString();
        }
        
        /**
         * 生成systemd服务文件
         */
        public static String generateSystemdService(BrokerDeploymentConfig config, String kafkaHome) {
            StringBuilder sb = new StringBuilder();
            
            sb.append("[Unit]\n");
            sb.append("Description=Apache Kafka Server\n");
            sb.append("Documentation=http://kafka.apache.org/documentation.html\n");
            sb.append("Requires=zookeeper.service\n");
            sb.append("After=zookeeper.service\n\n");
            
            sb.append("[Service]\n");
            sb.append("Type=simple\n");
            sb.append("User=kafka\n");
            sb.append("Group=kafka\n");
            
            // 环境变量
            if (!config.getEnvironmentVariables().isEmpty()) {
                sb.append("Environment=");
                boolean first = true;
                for (Map.Entry<String, String> env : config.getEnvironmentVariables().entrySet()) {
                    if (!first) sb.append(" ");
                    sb.append("'").append(env.getKey()).append("=").append(env.getValue()).append("'");
                    first = false;
                }
                sb.append("\n");
            }
            
            sb.append("ExecStart=").append(kafkaHome).append("/bin/kafka-server-start.sh ");
            sb.append(kafkaHome).append("/config/server.properties\n");
            sb.append("ExecStop=").append(kafkaHome).append("/bin/kafka-server-stop.sh\n");
            sb.append("TimeoutStopSec=180\n");
            sb.append("Restart=on-failure\n");
            sb.append("RestartSec=10\n");
            sb.append("LimitNOFILE=65536\n\n");
            
            sb.append("[Install]\n");
            sb.append("WantedBy=multi-user.target\n");
            
            return sb.toString();
        }
        
        /**
         * 生成启动脚本
         */
        public static String generateStartScript(String environment, List<String> brokerIds) {
            StringBuilder sb = new StringBuilder();
            
            sb.append("#!/bin/bash\n");
            sb.append("# Kafka Cluster Startup Script for ").append(environment).append("\n");
            sb.append("set -e\n\n");
            
            sb.append("echo \"Starting Kafka cluster in ").append(environment).append(" environment\"\n");
            sb.append("echo \"Brokers: ").append(String.join(", ", brokerIds)).append("\"\n\n");
            
            sb.append("# Check if Kafka is installed\n");
            sb.append("if [ ! -d \"/opt/kafka\" ]; then\n");
            sb.append("    echo \"Error: Kafka not found at /opt/kafka\"\n");
            sb.append("    exit 1\n");
            sb.append("fi\n\n");
            
            sb.append("# Start ZooKeeper first\n");
            sb.append("echo \"Starting ZooKeeper...\"\n");
            sb.append("/opt/kafka/bin/zookeeper-server-start.sh -daemon /opt/kafka/config/zookeeper.properties\n");
            sb.append("sleep 10\n\n");
            
            sb.append("# Start Kafka brokers\n");
            sb.append("echo \"Starting Kafka brokers...\"\n");
            for (String brokerId : brokerIds) {
                sb.append("systemctl start kafka-").append(brokerId).append(" || {\n");
                sb.append("    echo \"Failed to start broker ").append(brokerId).append("\"\n");
                sb.append("    exit 1\n");
                sb.append("}\n");
                sb.append("sleep 5\n");
            }
            
            sb.append("\necho \"Kafka cluster started successfully\"\n");
            
            return sb.toString();
        }
        
        /**
         * 生成部署脚本
         */
        public static String generateDeployScript(String environment, String version) {
            StringBuilder sb = new StringBuilder();
            
            sb.append("#!/bin/bash\n");
            sb.append("# Kafka Cluster Deployment Script for ").append(environment).append("\n");
            sb.append("# Version: ").append(version).append("\n");
            sb.append("set -e\n\n");
            
            sb.append("KAFKA_VERSION=\"").append(version).append("\"\n");
            sb.append("DOWNLOAD_URL=\"https://archive.apache.org/dist/kafka/\").append(KAFKA_VERSION).append(\"/kafka_\").append(KAFKA_VERSION).append(\".tgz\"\n");
            sb.append("KAFKA_HOME=\"/opt/kafka\"\n");
            sb.append("BACKUP_DIR=\"/opt/kafka-backup/$(date +%Y%m%d_%H%M%S)\"\n\n");
            
            sb.append("# Backup existing installation\n");
            sb.append("if [ -d \"$KAFKA_HOME\" ]; then\n");
            sb.append("    echo \"Backing up existing Kafka installation...\"\n");
            sb.append("    mkdir -p $BACKUP_DIR\n");
            sb.append("    mv $KAFKA_HOME $BACKUP_DIR/\n");
            sb.append("fi\n\n");
            
            sb.append("# Download and extract Kafka\n");
            sb.append("echo \"Downloading Kafka $KAFKA_VERSION...\"\n");
            sb.append("wget $DOWNLOAD_URL -O kafka.tgz\n");
            sb.append("tar -xzf kafka.tgz\n\n");
            
            sb.append("# Move to final location\n");
            sb.append("mv kafka_").append(version).append(" $KAFKA_HOME\n");
            sb.append("rm kafka.tgz\n\n");
            
            sb.append("# Set permissions\n");
            sb.append("chown -R kafka:kafka $KAFKA_HOME\n");
            sb.append("chmod +x $KAFKA_HOME/bin/*.sh\n\n");
            
            sb.append("# Start services\n");
            sb.append("echo \"Starting services...\"\n");
            sb.append("/opt/kafka/bin/kafka-server-start.sh -daemon /opt/kafka/config/server.properties\n\n");
            
            sb.append("echo \"Deployment completed successfully\"\n");
            
            return sb.toString();
        }
    }
    
    /**
     * 部署执行器
     */
    public static class DeploymentExecutor {
        private final String workspacePath;
        private final DeploymentPlan plan;
        
        public DeploymentExecutor(String workspacePath, DeploymentPlan plan) {
            this.workspacePath = workspacePath;
            this.plan = plan;
        }
        
        /**
         * 执行部署
         */
        public DeploymentResult execute() throws IOException {
            DeploymentResult result = new DeploymentResult();
            long startTime = System.currentTimeMillis();
            
            try {
                // 1. 生成配置文件
                System.out.println("Generating configuration files...");
                generateConfigurationFiles();
                
                // 2. 验证部署计划
                System.out.println("Validating deployment plan...");
                validateDeployment();
                
                // 3. 执行部署
                if (plan.getStrategy().isRollingRestart()) {
                    executeRollingDeployment();
                } else {
                    executeFullRestartDeployment();
                }
                
                // 4. 验证部署结果
                System.out.println("Validating deployment...");
                validateDeploymentResult();
                
                result.setSuccess(true);
                result.setMessage("Deployment completed successfully");
                
            } catch (Exception e) {
                result.setSuccess(false);
                result.setMessage("Deployment failed: " + e.getMessage());
                System.err.println("Deployment failed: " + e.getMessage());
                e.printStackTrace();
            } finally {
                result.setDuration(System.currentTimeMillis() - startTime);
            }
            
            return result;
        }
        
        /**
         * 生成配置文件
         */
        private void generateConfigurationFiles() throws IOException {
            Path configDir = Paths.get(workspacePath, "configs");
            Files.createDirectories(configDir);
            
            // 为每个broker生成配置文件
            for (Map.Entry<String, BrokerDeploymentConfig> entry : plan.getBrokerConfigs().entrySet()) {
                String brokerId = entry.getKey();
                BrokerDeploymentConfig config = entry.getValue();
                
                // 生成server.properties
                String serverProps = ConfigGenerator.generateServerProperties(config, plan.getGlobalProperties());
                Files.write(configDir.resolve("server-" + brokerId + ".properties"), 
                           serverProps.getBytes());
                
                // 生成systemd服务文件
                String systemdService = ConfigGenerator.generateSystemdService(config, "/opt/kafka");
                Files.write(configDir.resolve("kafka-" + brokerId + ".service"), 
                           systemdService.getBytes());
            }
            
            // 生成全局配置
            generateGlobalConfigs(configDir);
            
            // 生成部署脚本
            generateDeploymentScripts(configDir);
        }
        
        /**
         * 生成全局配置
         */
        private void generateGlobalConfigs(Path configDir) throws IOException {
            // 生成环境变量文件
            StringBuilder envVars = new StringBuilder();
            envVars.append("# Kafka Environment Variables\n");
            envVars.append("KAFKA_OPTS=\"-Xmx4g -Xms4g -XX:+UseG1GC\"\n");
            envVars.append("KAFKA_LOG_DIR=\"/var/log/kafka\"\n");
            envVars.append("KAFKA_PID_DIR=\"/var/run/kafka\"\n");
            
            Files.write(configDir.resolve("kafka-env.sh"), envVars.toString().getBytes());
            
            // 生成启动脚本
            String startScript = ConfigGenerator.generateStartScript(plan.getEnvironment(), plan.getBrokerIds());
            Files.write(configDir.resolve("start-cluster.sh"), startScript.getBytes());
            
            // 生成部署脚本
            String deployScript = ConfigGenerator.generateDeployScript(plan.getEnvironment(), plan.getVersion());
            Files.write(configDir.resolve("deploy.sh"), deployScript.getBytes());
        }
        
        /**
         * 生成部署脚本
         */
        private void generateDeploymentScripts(Path configDir) throws IOException {
            StringBuilder deployScript = new StringBuilder();
            deployScript.append("#!/bin/bash\n");
            deployScript.append("# Automated Kafka Cluster Deployment\n");
            deployScript.append("set -e\n\n");
            deployScript.append("echo \"Starting deployment for ").append(plan.getEnvironment()).append(" environment\"\n\n");
            
            // 预部署检查
            deployScript.append("# Pre-deployment checks\n");
            deployScript.append("echo \"Running pre-deployment checks...\"\n");
            deployScript.append("# Check disk space\n");
            deployScript.append("df -h /opt\n");
            deployScript.append("# Check memory\n");
            deployScript.append("free -h\n");
            deployScript.append("# Check Java installation\n");
            deployScript.append("java -version\n\n");
            
            // 复制配置文件
            deployScript.append("# Copy configuration files\n");
            deployScript.append("cp configs/*.properties /opt/kafka/config/\n");
            deployScript.append("cp configs/*.service /etc/systemd/system/\n");
            deployScript.append("systemctl daemon-reload\n\n");
            
            // 启动服务
            deployScript.append("# Start services\n");
            deployScript.append("systemctl enable zookeeper\n");
            deployScript.append("systemctl start zookeeper\n");
            deployScript.append("sleep 10\n");
            for (String brokerId : plan.getBrokerIds()) {
                deployScript.append("systemctl enable kafka-").append(brokerId).append("\n");
                deployScript.append("systemctl start kafka-").append(brokerId).append("\n");
                deployScript.append("sleep 5\n");
            }
            
            deployScript.append("\necho \"Deployment completed successfully\"\n");
            
            Files.write(configDir.resolve("deploy-cluster.sh"), deployScript.toString().getBytes());
        }
        
        /**
         * 验证部署计划
         */
        private void validateDeployment() throws DeploymentException {
            // 检查broker配置
            if (plan.getBrokerConfigs().isEmpty()) {
                throw new DeploymentException("No broker configurations found");
            }
            
            // 检查机架分布
            Map<String, Long> rackCounts = plan.getBrokerConfigs().values().stream()
                .map(BrokerDeploymentConfig::getRackId)
                .filter(Objects::nonNull)
                .collect(Collectors.groupingBy(r -> r, Collectors.counting()));
            
            // 确保每个机架有足够的broker
            for (Map.Entry<String, Long> entry : rackCounts.entrySet()) {
                if (entry.getValue() < 2) {
                    System.out.println("Warning: Rack " + entry.getKey() + " has only " + entry.getValue() + " broker(s)");
                }
            }
            
            // 检查配置冲突
            Set<String> hosts = new HashSet<>();
            for (BrokerDeploymentConfig config : plan.getBrokerConfigs().values()) {
                if (!hosts.add(config.getHost())) {
                    throw new DeploymentException("Duplicate host found: " + config.getHost());
                }
            }
        }
        
        /**
         * 执行滚动部署
         */
        private void executeRollingDeployment() throws IOException {
            System.out.println("Executing rolling deployment...");
            
            List<String> brokerIds = plan.getBrokerIds();
            int maxUnavailable = plan.getStrategy().getMaxUnavailableBrokers();
            
            for (int i = 0; i < brokerIds.size(); i += maxUnavailable) {
                List<String> batch = brokerIds.subList(i, Math.min(i + maxUnavailable, brokerIds.size()));
                
                System.out.println("Processing batch: " + String.join(", ", batch));
                
                // 停止当前批次的broker
                for (String brokerId : batch) {
                    System.out.println("Stopping broker " + brokerId);
                    // executeCommand("systemctl stop kafka-" + brokerId);
                }
                
                // 等待集群稳定
                System.out.println("Waiting for cluster stabilization...");
                try {
                    Thread.sleep(30000); // 30秒
                } catch (InterruptedException e) {
                    Thread.currentThread().interrupt();
                    throw new IOException("Deployment interrupted", e);
                }
                
                // 启动broker
                for (String brokerId : batch) {
                    System.out.println("Starting broker " + brokerId);
                    // executeCommand("systemctl start kafka-" + brokerId);
                }
                
                // 等待broker启动
                System.out.println("Waiting for brokers to start...");
                try {
                    Thread.sleep(60000); // 60秒
                } catch (InterruptedException e) {
                    Thread.currentThread().interrupt();
                    throw new IOException("Deployment interrupted", e);
                }
            }
        }
        
        /**
         * 执行全量重启部署
         */
        private void executeFullRestartDeployment() throws IOException {
            System.out.println("Executing full restart deployment...");
            
            // 停止所有broker
            for (String brokerId : plan.getBrokerIds()) {
                System.out.println("Stopping broker " + brokerId);
                // executeCommand("systemctl stop kafka-" + brokerId);
            }
            
            // 等待关闭
            System.out.println("Waiting for graceful shutdown...");
            try {
                Thread.sleep(30000);
            } catch (InterruptedException e) {
                Thread.currentThread().interrupt();
                throw new IOException("Deployment interrupted", e);
            }
            
            // 启动所有broker
            for (String brokerId : plan.getBrokerIds()) {
                System.out.println("Starting broker " + brokerId);
                // executeCommand("systemctl start kafka-" + brokerId);
            }
            
            // 等待集群启动
            System.out.println("Waiting for cluster to be ready...");
            try {
                Thread.sleep(60000);
            } catch (InterruptedException e) {
                Thread.currentThread().interrupt();
                throw new IOException("Deployment interrupted", e);
            }
        }
        
        /**
         * 验证部署结果
         */
        private void validateDeploymentResult() {
            System.out.println("Validating cluster health...");
            
            // 这里应该实现实际的健康检查
            // 例如：检查broker状态、测试生产者/消费者等
            try {
                Thread.sleep(5000);
                System.out.println("Health check passed");
            } catch (InterruptedException e) {
                Thread.currentThread().interrupt();
                throw new RuntimeException("Health check interrupted", e);
            }
        }
    }
    
    /**
     * 部署结果
     */
    public static class DeploymentResult {
        private boolean success;
        private String message;
        private long duration;
        private List<String> warnings = new ArrayList<>();
        private List<String> errors = new ArrayList<>();
        
        // Getters and Setters
        public boolean isSuccess() { return success; }
        public void setSuccess(boolean success) { this.success = success; }
        public String getMessage() { return message; }
        public void setMessage(String message) { this.message = message; }
        public long getDuration() { return duration; }
        public void setDuration(long duration) { this.duration = duration; }
        public List<String> getWarnings() { return warnings; }
        public List<String> getErrors() { return errors; }
        
        @Override
        public String toString() {
            StringBuilder sb = new StringBuilder();
            sb.append("Deployment Result:\n");
            sb.append("  Success: ").append(success).append("\n");
            sb.append("  Message: ").append(message).append("\n");
            sb.append("  Duration: ").append(duration / 1000).append(" seconds\n");
            
            if (!warnings.isEmpty()) {
                sb.append("  Warnings:\n");
                for (String warning : warnings) {
                    sb.append("    - ").append(warning).append("\n");
                }
            }
            
            if (!errors.isEmpty()) {
                sb.append("  Errors:\n");
                for (String error : errors) {
                    sb.append("    - ").append(error).append("\n");
                }
            }
            
            return sb.toString();
        }
    }
    
    /**
     * 部署异常
     */
    public static class DeploymentException extends Exception {
        public DeploymentException(String message) {
            super(message);
        }
        
        public DeploymentException(String message, Throwable cause) {
            super(message, cause);
        }
    }
    
    /**
     * 演示集群部署
     */
    public static void demonstrateClusterDeployment() throws IOException {
        System.out.println("=== Kafka集群部署演示 ===\n");
        
        // 创建部署计划
        DeploymentPlan plan = new DeploymentPlan("production", "3.5.0");
        
        // 设置全局属性
        plan.setGlobalProperty("log.retention.hours", "168");
        plan.setGlobalProperty("log.segment.bytes", "1073741824");
        plan.setGlobalProperty("num.network.threads", "8");
        plan.setGlobalProperty("num.io.threads", "16");
        
        // 配置broker
        BrokerDeploymentConfig broker1 = new BrokerDeploymentConfig("kafka-01.datacenter1.com", "dc1-rack-a");
        broker1.setProperty("log.dirs", "/opt/kafka/data1,/opt/kafka/data2");
        broker1.setProperty("socket.send.buffer.bytes", "102400");
        broker1.setProperty("socket.receive.buffer.bytes", "102400");
        broker1.setEnvironmentVariable("KAFKA_HEAP_OPTS", "-Xmx4g -Xms4g");
        plan.addBroker("1", broker1);
        
        BrokerDeploymentConfig broker2 = new BrokerDeploymentConfig("kafka-02.datacenter1.com", "dc1-rack-b");
        broker2.setProperty("log.dirs", "/opt/kafka/data1,/opt/kafka/data2");
        broker2.setEnvironmentVariable("KAFKA_HEAP_OPTS", "-Xmx4g -Xms4g");
        plan.addBroker("2", broker2);
        
        BrokerDeploymentConfig broker3 = new BrokerDeploymentConfig("kafka-03.datacenter2.com", "dc2-rack-a");
        broker3.setProperty("log.dirs", "/opt/kafka/data1,/opt/kafka/data2");
        broker3.setEnvironmentVariable("KAFKA_HEAP_OPTS", "-Xmx4g -Xms4g");
        plan.addBroker("3", broker3);
        
        // 配置部署策略
        plan.getStrategy().setRollingRestart(true);
        plan.getStrategy().setMaxUnavailableBrokers(1);
        plan.getStrategy().setHealthCheckInterval(10);
        
        // 执行部署
        String workspace = "deployment_workspace";
        DeploymentExecutor executor = new DeploymentExecutor(workspace, plan);
        DeploymentResult result = executor.execute();
        
        // 输出结果
        System.out.println("\n" + result.toString());
        
        // 显示生成的文件
        System.out.println("Generated configuration files:");
        Path configDir = Paths.get(workspace, "configs");
        if (Files.exists(configDir)) {
            Files.walk(configDir)
                 .filter(Files::isRegularFile)
                 .forEach(file -> System.out.println("  " + file.getFileName()));
        }
        
        System.out.println("\n=== 部署演示完成 ===");
    }
}
```

---

## 集群监控与告警

### 监控指标收集器

```java
package com.kafka.tutorial.cluster;

import java.io.*;
import java.net.*;
import java.util.*;
import java.util.concurrent.*;
import java.util.concurrent.atomic.*;
import java.time.*;

/**
 * Kafka集群监控与告警系统
 */
public class KafkaClusterMonitoring {
    
    /**
     * 监控指标定义
     */
    public static class Metrics {
        
        public enum MetricType {
            COUNTER,    // 递增计数
            GAUGE,      // 当前值
            HISTOGRAM,  // 直方图
            TIMER       // 时间计时
        }
        
        public static class Metric {
            private final String name;
            private final MetricType type;
            private final String description;
            private final Map<String, String> tags;
            private volatile double value;
            private final long timestamp;
            
            public Metric(String name, MetricType type, String description, Map<String, String> tags) {
                this.name = name;
                this.type = type;
                this.description = description;
                this.tags = tags != null ? new HashMap<>(tags) : new HashMap<>();
                this.value = 0;
                this.timestamp = System.currentTimeMillis();
            }
            
            public void setValue(double value) {
                this.value = value;
            }
            
            public void increment(double amount) {
                this.value += amount;
            }
            
            public double getValue() { return value; }
            public String getName() { return name; }
            public MetricType getType() { return type; }
            public String getDescription() { return description; }
            public Map<String, String> getTags() { return tags; }
            public long getTimestamp() { return timestamp; }
        }
        
        public static class BrokerMetrics {
            private final String brokerId;
            private final Map<String, Metric> metrics;
            
            public BrokerMetrics(String brokerId) {
                this.brokerId = brokerId;
                this.metrics = new ConcurrentHashMap<>();
                initializeMetrics();
            }
            
            private void initializeMetrics() {
                // 消息指标
                metrics.put("messages_per_second", new Metric("kafka.messages_per_second", 
                    MetricType.GAUGE, "Messages per second", tagMap("broker", brokerId)));
                metrics.put("bytes_in_per_second", new Metric("kafka.bytes_in_per_second", 
                    MetricType.GAUGE, "Bytes in per second", tagMap("broker", brokerId)));
                metrics.put("bytes_out_per_second", new Metric("kafka.bytes_out_per_second", 
                    MetricType.GAUGE, "Bytes out per second", tagMap("broker", brokerId)));
                
                // 请求指标
                metrics.put("produce_requests_per_second", new Metric("kafka.produce_requests_per_second", 
                    MetricType.GAUGE, "Produce requests per second", tagMap("broker", brokerId)));
                metrics.put("fetch_requests_per_second", new Metric("kafka.fetch_requests_per_second", 
                    MetricType.GAUGE, "Fetch requests per second", tagMap("broker", brokerId)));
                metrics.put("request_time_99p", new Metric("kafka.request_time_99p", 
                    MetricType.GAUGE, "Request time 99th percentile", tagMap("broker", brokerId)));
                
                // 副本指标
                metrics.put("under_replicated_partitions", new Metric("kafka.under_replicated_partitions", 
                    MetricType.GAUGE, "Under replicated partitions", tagMap("broker", brokerId)));
                metrics.put("offline_partitions", new Metric("kafka.offline_partitions", 
                    MetricType.GAUGE, "Offline partitions", tagMap("broker", brokerId)));
                metrics.put("isr_shrinks_per_second", new Metric("kafka.isr_shrinks_per_second", 
                    MetricType.GAUGE, "ISR shrinks per second", tagMap("broker", brokerId)));
                
                // 消费者滞后
                metrics.put("consumer_lag_sum", new Metric("kafka.consumer_lag_sum", 
                    MetricType.GAUGE, "Total consumer lag", tagMap("broker", brokerId)));
                
                // 控制器指标
                metrics.put("controller_active", new Metric("kafka.controller_active", 
                    MetricType.GAUGE, "Controller active", tagMap("broker", brokerId)));
                
                // 健康指标
                metrics.put("jvm_gc_count", new Metric("kafka.jvm_gc_count", 
                    MetricType.COUNTER, "JVM GC count", tagMap("broker", brokerId)));
                metrics.put("jvm_gc_time", new Metric("kafka.jvm_gc_time", 
                    MetricType.COUNTER, "JVM GC time", tagMap("broker", brokerId)));
                metrics.put("heap_used_mb", new Metric("kafka.heap_used_mb", 
                    MetricType.GAUGE, "Heap used MB", tagMap("broker", brokerId)));
                metrics.put("disk_used_percent", new Metric("kafka.disk_used_percent", 
                    MetricType.GAUGE, "Disk used percent", tagMap("broker", brokerId)));
            }
            
            public Metric getMetric(String name) {
                return metrics.get(name);
            }
            
            public Collection<Metric> getAllMetrics() {
                return metrics.values();
            }
            
            // Getters
            public String getBrokerId() { return brokerId; }
            public Map<String, Metric> getMetrics() { return metrics; }
        }
        
        private static Map<String, String> tagMap(String... keyValues) {
            Map<String, String> tags = new HashMap<>();
            for (int i = 0; i < keyValues.length; i += 2) {
                tags.put(keyValues[i], keyValues[i + 1]);
            }
            return tags;
        }
    }
    
    /**
     * JMX监控客户端
     */
    public static class JMXMonitor {
        private final String host;
        private final int port;
        private final String jmxServiceUrl;
        private MBeanServerConnection connection;
        
        public JMXMonitor(String host, int port) {
            this.host = host;
            this.port = port;
            this.jmxServiceUrl = String.format("service:jmx:rmi:///jndi/rmi://%s:%s/jmxrmi", host, port);
        }
        
        public void connect() throws IOException {
            try {
                JMXServiceURL url = new JMXServiceURL(jmxServiceUrl);
                JMXConnector connector = JMXConnectorFactory.newJMXConnector(url, null);
                connector.connect();
                this.connection = connector.getMBeanServerConnection();
                System.out.println("Connected to JMX: " + jmxServiceUrl);
            } catch (Exception e) {
                throw new IOException("Failed to connect to JMX", e);
            }
        }
        
        public double getMetric(String mBeanName, String attribute) throws Exception {
            if (connection == null) {
                throw new IllegalStateException("Not connected to JMX");
            }
            
            ObjectName objectName = new ObjectName(mBeanName);
            return ((Number) connection.getAttribute(objectName, attribute)).doubleValue();
        }
        
        public Map<String, Double> getMultipleMetrics(String mBeanName, List<String> attributes) throws Exception {
            Map<String, Double> result = new HashMap<>();
            ObjectName objectName = new ObjectName(mBeanName);
            
            for (String attribute : attributes) {
                try {
                    Double value = ((Number) connection.getAttribute(objectName, attribute)).doubleValue();
                    result.put(attribute, value);
                } catch (Exception e) {
                    System.err.println("Failed to get attribute " + attribute + " from " + mBeanName + ": " + e.getMessage());
                }
            }
            
            return result;
        }
        
        public void disconnect() {
            // JMX连接会在Connector关闭时自动清理
        }
    }
    
    /**
     * 集群状态收集器
     */
    public static class ClusterStatusCollector {
        private final List<String> brokerHosts;
        private final Map<String, JMXMonitor> jmxClients;
        private final Map<String, Metrics.BrokerMetrics> brokerMetrics;
        
        public ClusterStatusCollector(List<String> brokerHosts) {
            this.brokerHosts = brokerHosts;
            this.jmxClients = new HashMap<>();
            this.brokerMetrics = new HashMap<>();
            initializeCollectors();
        }
        
        private void initializeCollectors() {
            for (String host : brokerHosts) {
                String brokerId = extractBrokerId(host);
                jmxClients.put(brokerId, new JMXMonitor(host, 9999));
                brokerMetrics.put(brokerId, new Metrics.BrokerMetrics(brokerId));
            }
        }
        
        private String extractBrokerId(String host) {
            // 从主机名中提取broker ID (假设命名约定为kafka-XX)
            String[] parts = host.split("-");
            return parts.length > 1 ? parts[parts.length - 1] : host;
        }
        
        /**
         * 收集集群状态
         */
        public ClusterStatus collectStatus() {
            ClusterStatus status = new ClusterStatus();
            
            for (Map.Entry<String, JMXMonitor> entry : jmxClients.entrySet()) {
                String brokerId = entry.getKey();
                JMXMonitor jmxClient = entry.getValue();
                Metrics.BrokerMetrics metrics = brokerMetrics.get(brokerId);
                
                try {
                    // 连接JMX
                    jmxClient.connect();
                    
                    // 收集消息指标
                    collectMessageMetrics(jmxClient, metrics);
                    
                    // 收集请求指标
                    collectRequestMetrics(jmxClient, metrics);
                    
                    // 收集副本指标
                    collectReplicaMetrics(jmxClient, metrics);
                    
                    // 收集健康指标
                    collectHealthMetrics(jmxClient, metrics);
                    
                    // 收集控制器指标
                    collectControllerMetrics(jmxClient, metrics);
                    
                    status.addBrokerStatus(brokerId, BrokerStatus.ONLINE);
                    
                } catch (Exception e) {
                    System.err.println("Failed to collect metrics for broker " + brokerId + ": " + e.getMessage());
                    status.addBrokerStatus(brokerId, BrokerStatus.OFFLINE);
                } finally {
                    jmxClient.disconnect();
                }
            }
            
            return status;
        }
        
        private void collectMessageMetrics(JMXMonitor jmxClient, Metrics.BrokerMetrics metrics) throws Exception {
            // 收集消息生产指标
            Map<String, Double> produceMetrics = jmxClient.getMultipleMetrics(
                "kafka.server:type=BrokerTopicMetrics,name=MessagesPerSec",
                Arrays.asList("Count", "Rate")
            );
            
            if (produceMetrics.containsKey("Rate")) {
                metrics.getMetric("messages_per_second").setValue(produceMetrics.get("Rate"));
            }
            
            // 收集字节输入指标
            Map<String, Double> byteInMetrics = jmxClient.getMultipleMetrics(
                "kafka.server:type=BrokerTopicMetrics,name=BytesInPerSec",
                Arrays.asList("Count", "Rate")
            );
            
            if (byteInMetrics.containsKey("Rate")) {
                metrics.getMetric("bytes_in_per_second").setValue(byteInMetrics.get("Rate"));
            }
            
            // 收集字节输出指标
            Map<String, Double> byteOutMetrics = jmxClient.getMultipleMetrics(
                "kafka.server:type=BrokerTopicMetrics,name=BytesOutPerSec",
                Arrays.asList("Count", "Rate")
            );
            
            if (byteOutMetrics.containsKey("Rate")) {
                metrics.getMetric("bytes_out_per_second").setValue(byteOutMetrics.get("Rate"));
            }
        }
        
        private void collectRequestMetrics(JMXMonitor jmxClient, Metrics.BrokerMetrics metrics) throws Exception {
            // 收集produce请求指标
            Map<String, Double> produceRequestMetrics = jmxClient.getMultipleMetrics(
                "kafka.server:type=RequestMetrics,name=ProduceRequestsPerSec,request=Produce",
                Arrays.asList("Count", "Rate")
            );
            
            if (produceRequestMetrics.containsKey("Rate")) {
                metrics.getMetric("produce_requests_per_second").setValue(produceRequestMetrics.get("Rate"));
            }
            
            // 收集fetch请求指标
            Map<String, Double> fetchRequestMetrics = jmxClient.getMultipleMetrics(
                "kafka.server:type=RequestMetrics,name=FetchRequestsPerSec,request=Fetch",
                Arrays.asList("Count", "Rate")
            );
            
            if (fetchRequestMetrics.containsKey("Rate")) {
                metrics.getMetric("fetch_requests_per_second").setValue(fetchRequestMetrics.get("Rate"));
            }
            
            // 收集请求时间指标
            Map<String, Double> requestTimeMetrics = jmxClient.getMultipleMetrics(
                "kafka.server:type=RequestMetrics,name=TotalTimeMs,request=Produce",
                Arrays.asList("Mean", "99thPercentile")
            );
            
            if (requestTimeMetrics.containsKey("99thPercentile")) {
                metrics.getMetric("request_time_99p").setValue(requestTimeMetrics.get("99thPercentile"));
            }
        }
        
        private void collectReplicaMetrics(JMXMonitor jmxClient, Metrics.BrokerMetrics metrics) throws Exception {
            // 收集under replicated partitions
            double underReplicated = jmxClient.getMetric(
                "kafka.server:type=ReplicaManager,name=UnderReplicatedPartitions",
                "Value"
            );
            metrics.getMetric("under_replicated_partitions").setValue(underReplicated);
            
            // 收集offline partitions
            double offlinePartitions = jmxClient.getMetric(
                "kafka.server:type=Partition,name=OfflinePartitionsCount",
                "Value"
            );
            metrics.getMetric("offline_partitions").setValue(offlinePartitions);
            
            // 收集ISR收缩指标
            Map<String, Double> isrMetrics = jmxClient.getMultipleMetrics(
                "kafka.server:type=ReplicaManager,name=IsrShrinksPerSec",
                Arrays.asList("Count", "Rate")
            );
            
            if (isrMetrics.containsKey("Rate")) {
                metrics.getMetric("isr_shrinks_per_second").setValue(isrMetrics.get("Rate"));
            }
        }
        
        private void collectHealthMetrics(JMXMonitor jmxClient, Metrics.BrokerMetrics metrics) throws Exception {
            // 收集JVM指标
            Map<String, Double> gcMetrics = jmxClient.getMultipleMetrics(
                "java.lang:type=GarbageCollector,name=*",
                Arrays.asList("CollectionCount", "CollectionTime")
            );
            
            if (gcMetrics.containsKey("CollectionCount")) {
                metrics.getMetric("jvm_gc_count").setValue(gcMetrics.get("CollectionCount"));
            }
            
            if (gcMetrics.containsKey("CollectionTime")) {
                metrics.getMetric("jvm_gc_time").setValue(gcMetrics.get("CollectionTime"));
            }
            
            // 收集堆内存使用
            Map<String, Double> memoryMetrics = jmxClient.getMultipleMetrics(
                "java.lang:type=Memory",
                Arrays.asList("HeapMemoryUsage")
            );
            
            if (memoryMetrics.containsKey("HeapMemoryUsage")) {
                double usedMB = memoryMetrics.get("HeapMemoryUsage") / (1024 * 1024);
                metrics.getMetric("heap_used_mb").setValue(usedMB);
            }
        }
        
        private void collectControllerMetrics(JMXMonitor jmxClient, Metrics.BrokerMetrics metrics) throws Exception {
            // 检查是否为控制器
            try {
                double activeController = jmxClient.getMetric(
                    "kafka.controller:type=KafkaController,name=ActiveControllerCount",
                    "Value"
                );
                metrics.getMetric("controller_active").setValue(activeController > 0 ? 1.0 : 0.0);
            } catch (Exception e) {
                // 可能不是控制器broker
                metrics.getMetric("controller_active").setValue(0.0);
            }
        }
        
        // Getters
        public Map<String, Metrics.BrokerMetrics> getBrokerMetrics() { return brokerMetrics; }
    }
    
    /**
     * 集群状态
     */
    public static class ClusterStatus {
        private final Map<String, BrokerStatus> brokerStatuses;
        private final long timestamp;
        private final List<String> issues;
        
        public ClusterStatus() {
            this.brokerStatuses = new HashMap<>();
            this.timestamp = System.currentTimeMillis();
            this.issues = new ArrayList<>();
        }
        
        public void addBrokerStatus(String brokerId, BrokerStatus status) {
            brokerStatuses.put(brokerId, status);
            if (status == BrokerStatus.OFFLINE) {
                issues.add("Broker " + brokerId + " is offline");
            }
        }
        
        public BrokerStatus getBrokerStatus(String brokerId) {
            return brokerStatuses.getOrDefault(brokerId, BrokerStatus.UNKNOWN);
        }
        
        public boolean isHealthy() {
            return issues.isEmpty() && brokerStatuses.values().stream()
                .allMatch(status -> status == BrokerStatus.ONLINE);
        }
        
        public long getOnlineBrokerCount() {
            return brokerStatuses.values().stream()
                .mapToLong(status -> status == BrokerStatus.ONLINE ? 1 : 0)
                .sum();
        }
        
        // Getters
        public Map<String, BrokerStatus> getBrokerStatuses() { return brokerStatuses; }
        public long getTimestamp() { return timestamp; }
        public List<String> getIssues() { return issues; }
    }
    
    public enum BrokerStatus {
        ONLINE, OFFLINE, DEGRADED, UNKNOWN
    }
    
    /**
     * 告警规则管理器
     */
    public static class AlertRuleManager {
        private final List<AlertRule> rules;
        private final AlertCallback alertCallback;
        
        public AlertRuleManager(AlertCallback alertCallback) {
            this.rules = new ArrayList<>();
            this.alertCallback = alertCallback;
            initializeRules();
        }
        
        private void initializeRules() {
            // 离线broker告警
            rules.add(new AlertRule(
                "broker_offline",
                "Broker {brokerId} is offline",
                AlertSeverity.CRITICAL,
                rule -> rule.getConditions().get("status") == BrokerStatus.OFFLINE
            ));
            
            // 高延迟告警
           