package com.kafka.tutorial.cluster;

import java.io.*;
import java.nio.file.*;
import java.util.*;
import java.util.concurrent.*;
import java.util.stream.Collectors;

/**
 * Kafka集群自动化部署与配置管理
 */
public class KafkaClusterDeployment {
    
    /**
     * 部署计划
     */
    public static class DeploymentPlan {
        private final String environment; // dev, staging, prod
        private final String version;
        private final List<String> brokerIds;
        private final Map<String, BrokerDeploymentConfig> brokerConfigs;
        private final Map<String, String> globalProperties;
        private final DeploymentStrategy strategy;
        
        public DeploymentPlan(String environment, String version) {
            this.environment = environment;
            this.version = version;
            this.brokerIds = new ArrayList<>();
            this.brokerConfigs = new HashMap<>();
            this.globalProperties = new HashMap<>();
            this.strategy = new DeploymentStrategy();
        }
        
        public void addBroker(String brokerId, BrokerDeploymentConfig config) {
            brokerIds.add(brokerId);
            brokerConfigs.put(brokerId, config);
        }
        
        public void setGlobalProperty(String key, String value) {
            globalProperties.put(key, value);
        }
        
        // Getters
        public String getEnvironment() { return environment; }
        public String getVersion() { return version; }
        public List<String> getBrokerIds() { return brokerIds; }
        public Map<String, BrokerDeploymentConfig> getBrokerConfigs() { return brokerConfigs; }
        public Map<String, String> getGlobalProperties() { return globalProperties; }
        public DeploymentStrategy getStrategy() { return strategy; }
    }
    
    /**
     * Broker部署配置
     */
    public static class BrokerDeploymentConfig {
        private final String host;
        private final String rackId;
        private final Map<String, String> properties;
        private final Map<String, String> environmentVariables;
        private final String jvmSettings;
        private final List<String> dependencies;
        
        public BrokerDeploymentConfig(String host, String rackId) {
            this.host = host;
            this.rackId = rackId;
            this.properties = new HashMap<>();
            this.environmentVariables = new HashMap<>();
            this.jvmSettings = "-Xmx4g -Xms4g -XX:+UseG1GC";
            this.dependencies = new ArrayList<>();
        }
        
        public void setProperty(String key, String value) {
            properties.put(key, value);
        }
        
        public void setEnvironmentVariable(String key, String value) {
            environmentVariables.put(key, value);
        }
        
        public void setJvmSettings(String jvmSettings) {
            this.jvmSettings = jvmSettings;
        }
        
        public void addDependency(String dependency) {
            dependencies.add(dependency);
        }
        
        // Getters
        public String getHost() { return host; }
        public String getRackId() { return rackId; }
        public Map<String, String> getProperties() { return properties; }
        public Map<String, String> getEnvironmentVariables() { return environmentVariables; }
        public String getJvmSettings() { return jvmSettings; }
        public List<String> getDependencies() { return dependencies; }
    }
    
    /**
     * 部署策略
     */
    public static class DeploymentStrategy {
        private boolean rollingRestart = true;
        private int maxUnavailableBrokers = 1;
        private int healthCheckInterval = 10; // seconds
        private int healthCheckTimeout = 30; // seconds
        private boolean useController = true;
        private String preDeploymentScript = "";
        private String postDeploymentScript = "";
        
        // Getters and Setters
        public boolean isRollingRestart() { return rollingRestart; }
        public void setRollingRestart(boolean rollingRestart) { this.rollingRestart = rollingRestart; }
        public int getMaxUnavailableBrokers() { return maxUnavailableBrokers; }
        public void setMaxUnavailableBrokers(int maxUnavailableBrokers) { this.maxUnavailableBrokers = maxUnavailableBrokers; }
        public int getHealthCheckInterval() { return healthCheckInterval; }
        public void setHealthCheckInterval(int healthCheckInterval) { this.healthCheckInterval = healthCheckInterval; }
        public int getHealthCheckTimeout() { return healthCheckTimeout; }
        public void setHealthCheckTimeout(int healthCheckTimeout) { this.healthCheckTimeout = healthCheckTimeout; }
        public boolean isUseController() { return useController; }
        public void setUseController(boolean useController) { this.useController = useController; }
        public String getPreDeploymentScript() { return preDeploymentScript; }
        public void setPreDeploymentScript(String preDeploymentScript) { this.preDeploymentScript = preDeploymentScript; }
        public String getPostDeploymentScript() { return postDeploymentScript; }
        public void setPostDeploymentScript(String postDeploymentScript) { this.postDeploymentScript = postDeploymentScript; }
    }
    
    /**
     * 配置生成器
     */
    public static class ConfigGenerator {
        
        /**
         * 生成server.properties文件
         */
        public static String generateServerProperties(BrokerDeploymentConfig config, 
                                                     Map<String, String> globalProps) {
            StringBuilder sb = new StringBuilder();
            
            // 基础配置
            sb.append("# Kafka Server Configuration\n");
            sb.append("# Generated automatically for ").append(config.getHost()).append("\n\n");
            
            // 网络配置
            sb.append("# Network Configuration\n");
            sb.append("listeners=PLAINTEXT://").append(config.getHost()).append(":9092\n");
            sb.append("advertised.listeners=PLAINTEXT://").append(config.getHost()).append(":9092\n");
            sb.append("num.network.threads=8\n");
            sb.append("num.io.threads=16\n");
            sb.append("socket.send.buffer.bytes=102400\n");
            sb.append("socket.receive.buffer.bytes=102400\n");
            sb.append("socket.request.max.bytes=104857600\n\n");
            
            // 日志配置
            sb.append("# Log Configuration\n");
            sb.append("log.dirs=/opt/kafka/data\n");
            sb.append("num.partitions=1\n");
            sb.append("num.recovery.threads.per.data.dir=1\n");
            sb.append("log.retention.hours=168\n");
            sb.append("log.segment.bytes=1073741824\n");
            sb.append("log.retention.check.interval.ms=300000\n");
            sb.append("log.cleanup.policy=delete\n\n");
            
            // 副本配置
            sb.append("# Replication Configuration\n");
            sb.append("offsets.topic.replication.factor=3\n");
            sb.append("transaction.state.log.replication.factor=3\n");
            sb.append("transaction.state.log.min.isr=2\n");
            sb.append("default.replication.factor=3\n");
            sb.append("min.insync.replicas=2\n\n");
            
            // Zookeeper配置
            sb.append("# Zookeeper Configuration\n");
            sb.append("zookeeper.connect=zk1:2181,zk2:2181,zk3:2181/kafka\n");
            sb.append("zookeeper.connection.timeout.ms=18000\n");
            sb.append("zookeeper.session.timeout.ms=30000\n\n");
            
            // Broker配置
            sb.append("# Broker Configuration\n");
            sb.append("broker.id=-1\n"); // 动态分配
            sb.append("delete.topic.enable=true\n");
            sb.append("auto.create.topics.enable=false\n");
            sb.append("num.network.threads=8\n");
            sb.append("num.io.threads=16\n");
            
            // 机架配置
            if (config.getRackId() != null && !config.getRackId().isEmpty()) {
                sb.append("broker.rack=").append(config.getRackId()).append("\n");
            }
            
            sb.append("\n");
            
            // 应用全局配置
            sb.append("# Global Properties\n");
            for (Map.Entry<String, String> entry : globalProps.entrySet()) {
                sb.append(entry.getKey()).append("=").append(entry.getValue()).append("\n");
            }
            sb.append("\n");
            
            // 应用broker特定配置
            if (!config.getProperties().isEmpty()) {
                sb.append("# Broker-specific Properties\n");
                for (Map.Entry<String, String> entry : config.getProperties().entrySet()) {
                    sb.append(entry.getKey()).append("=").append(entry.getValue()).append("\n");
                }
            }
            
            return sb.toString();
        }
        
        /**
         * 生成systemd服务文件
         */
        public static String generateSystemdService(BrokerDeploymentConfig config, String kafkaHome) {
            StringBuilder sb = new StringBuilder();
            
            sb.append("[Unit]\n");
            sb.append("Description=Apache Kafka Server\n");
            sb.append("Documentation=http://kafka.apache.org/documentation.html\n");
            sb.append("Requires=zookeeper.service\n");
            sb.append("After=zookeeper.service\n\n");
            
            sb.append("[Service]\n");
            sb.append("Type=simple\n");
            sb.append("User=kafka\n");
            sb.append("Group=kafka\n");
            
            // 环境变量
            if (!config.getEnvironmentVariables().isEmpty()) {
                sb.append("Environment=");
                boolean first = true;
                for (Map.Entry<String, String> env : config.getEnvironmentVariables().entrySet()) {
                    if (!first) sb.append(" ");
                    sb.append("'").append(env.getKey()).append("=").append(env.getValue()).append("'");
                    first = false;
                }
                sb.append("\n");
            }
            
            sb.append("ExecStart=").append(kafkaHome).append("/bin/kafka-server-start.sh ");
            sb.append(kafkaHome).append("/config/server.properties\n");
            sb.append("ExecStop=").append(kafkaHome).append("/bin/kafka-server-stop.sh\n");
            sb.append("TimeoutStopSec=180\n");
            sb.append("Restart=on-failure\n");
            sb.append("RestartSec=10\n");
            sb.append("LimitNOFILE=65536\n\n");
            
            sb.append("[Install]\n");
            sb.append("WantedBy=multi-user.target\n");
            
            return sb.toString();
        }
        
        /**
         * 生成启动脚本
         */
        public static String generateStartScript(String environment, List<String> brokerIds) {
            StringBuilder sb = new StringBuilder();
            
            sb.append("#!/bin/bash\n");
            sb.append("# Kafka Cluster Startup Script for ").append(environment).append("\n");
            sb.append("set -e\n\n");
            
            sb.append("echo \"Starting Kafka cluster in ").append(environment).append(" environment\"\n");
            sb.append("echo \"Brokers: ").append(String.join(", ", brokerIds)).append("\"\n\n");
            
            sb.append("# Check if Kafka is installed\n");
            sb.append("if [ ! -d \"/opt/kafka\" ]; then\n");
            sb.append("    echo \"Error: Kafka not found at /opt/kafka\"\n");
            sb.append("    exit 1\n");
            sb.append("fi\n\n");
            
            sb.append("# Start ZooKeeper first\n");
            sb.append("echo \"Starting ZooKeeper...\"\n");
            sb.append("/opt/kafka/bin/zookeeper-server-start.sh -daemon /opt/kafka/config/zookeeper.properties\n");
            sb.append("sleep 10\n\n");
            
            sb.append("# Start Kafka brokers\n");
            sb.append("echo \"Starting Kafka brokers...\"\n");
            for (String brokerId : brokerIds) {
                sb.append("systemctl start kafka-").append(brokerId).append(" || {\n");
                sb.append("    echo \"Failed to start broker ").append(brokerId).append("\"\n");
                sb.append("    exit 1\n");
                sb.append("}\n");
                sb.append("sleep 5\n");
            }
            
            sb.append("\necho \"Kafka cluster started successfully\"\n");
            
            return sb.toString();
        }
        
        /**
         * 生成部署脚本
         */
        public static String generateDeployScript(String environment, String version) {
            StringBuilder sb = new StringBuilder();
            
            sb.append("#!/bin/bash\n");
            sb.append("# Kafka Cluster Deployment Script for ").append(environment).append("\n");
            sb.append("# Version: ").append(version).append("\n");
            sb.append("set -e\n\n");
            
            sb.append("KAFKA_VERSION=\"").append(version).append("\"\n");
            sb.append("DOWNLOAD_URL=\"https://archive.apache.org/dist/kafka/\").append(KAFKA_VERSION).append(\"/kafka_\").append(KAFKA_VERSION).append(\".tgz\"\n");
            sb.append("KAFKA_HOME=\"/opt/kafka\"\n");
            sb.append("BACKUP_DIR=\"/opt/kafka-backup/$(date +%Y%m%d_%H%M%S)\"\n\n");
            
            sb.append("# Backup existing installation\n");
            sb.append("if [ -d \"$KAFKA_HOME\" ]; then\n");
            sb.append("    echo \"Backing up existing Kafka installation...\"\n");
            sb.append("    mkdir -p $BACKUP_DIR\n");
            sb.append("    mv $KAFKA_HOME $BACKUP_DIR/\n");
            sb.append("fi\n\n");
            
            sb.append("# Download and extract Kafka\n");
            sb.append("echo \"Downloading Kafka $KAFKA_VERSION...\"\n");
            sb.append("wget $DOWNLOAD_URL -O kafka.tgz\n");
            sb.append("tar -xzf kafka.tgz\n\n");
            
            sb.append("# Move to final location\n");
            sb.append("mv kafka_").append(version).append(" $KAFKA_HOME\n");
            sb.append("rm kafka.tgz\n\n");
            
            sb.append("# Set permissions\n");
            sb.append("chown -R kafka:kafka $KAFKA_HOME\n");
            sb.append("chmod +x $KAFKA_HOME/bin/*.sh\n\n");
            
            sb.append("# Start services\n");
            sb.append("echo \"Starting services...\"\n");
            sb.append("/opt/kafka/bin/kafka-server-start.sh -daemon /opt/kafka/config/server.properties\n\n");
            
            sb.append("echo \"Deployment completed successfully\"\n");
            
            return sb.toString();
        }
    }
    
    /**
     * 部署执行器
     */
    public static class DeploymentExecutor {
        private final String workspacePath;
        private final DeploymentPlan plan;
        
        public DeploymentExecutor(String workspacePath, DeploymentPlan plan) {
            this.workspacePath = workspacePath;
            this.plan = plan;
        }
        
        /**
         * 执行部署
         */
        public DeploymentResult execute() throws IOException {
            DeploymentResult result = new DeploymentResult();
            long startTime = System.currentTimeMillis();
            
            try {
                // 1. 生成配置文件
                System.out.println("Generating configuration files...");
                generateConfigurationFiles();
                
                // 2. 验证部署计划
                System.out.println("Validating deployment plan...");
                validateDeployment();
                
                // 3. 执行部署
                if (plan.getStrategy().isRollingRestart()) {
                    executeRollingDeployment();
                } else {
                    executeFullRestartDeployment();
                }
                
                // 4. 验证部署结果
                System.out.println("Validating deployment...");
                validateDeploymentResult();
                
                result.setSuccess(true);
                result.setMessage("Deployment completed successfully");
                
            } catch (Exception e) {
                result.setSuccess(false);
                result.setMessage("Deployment failed: " + e.getMessage());
                System.err.println("Deployment failed: " + e.getMessage());
                e.printStackTrace();
            } finally {
                result.setDuration(System.currentTimeMillis() - startTime);
            }
            
            return result;
        }
        
        /**
         * 生成配置文件
         */
        private void generateConfigurationFiles() throws IOException {
            Path configDir = Paths.get(workspacePath, "configs");
            Files.createDirectories(configDir);
            
            // 为每个broker生成配置文件
            for (Map.Entry<String, BrokerDeploymentConfig> entry : plan.getBrokerConfigs().entrySet()) {
                String brokerId = entry.getKey();
                BrokerDeploymentConfig config = entry.getValue();
                
                // 生成server.properties
                String serverProps = ConfigGenerator.generateServerProperties(config, plan.getGlobalProperties());
                Files.write(configDir.resolve("server-" + brokerId + ".properties"), 
                           serverProps.getBytes());
                
                // 生成systemd服务文件
                String systemdService = ConfigGenerator.generateSystemdService(config, "/opt/kafka");
                Files.write(configDir.resolve("kafka-" + brokerId + ".service"), 
                           systemdService.getBytes());
            }
            
            // 生成全局配置
            generateGlobalConfigs(configDir);
            
            // 生成部署脚本
            generateDeploymentScripts(configDir);
        }
        
        /**
         * 生成全局配置
         */
        private void generateGlobalConfigs(Path configDir) throws IOException {
            // 生成环境变量文件
            StringBuilder envVars = new StringBuilder();
            envVars.append("# Kafka Environment Variables\n");
            envVars.append("KAFKA_OPTS=\"-Xmx4g -Xms4g -XX:+UseG1GC\"\n");
            envVars.append("KAFKA_LOG_DIR=\"/var/log/kafka\"\n");
            envVars.append("KAFKA_PID_DIR=\"/var/run/kafka\"\n");
            
            Files.write(configDir.resolve("kafka-env.sh"), envVars.toString().getBytes());
            
            // 生成启动脚本
            String startScript = ConfigGenerator.generateStartScript(plan.getEnvironment(), plan.getBrokerIds());
            Files.write(configDir.resolve("start-cluster.sh"), startScript.getBytes());
            
            // 生成部署脚本
            String deployScript = ConfigGenerator.generateDeployScript(plan.getEnvironment(), plan.getVersion());
            Files.write(configDir.resolve("deploy.sh"), deployScript.getBytes());
        }
        
        /**
         * 生成部署脚本
         */
        private void generateDeploymentScripts(Path configDir) throws IOException {
            StringBuilder deployScript = new StringBuilder();
            deployScript.append("#!/bin/bash\n");
            deployScript.append("# Automated Kafka Cluster Deployment\n");
            deployScript.append("set -e\n\n");
            deployScript.append("echo \"Starting deployment for ").append(plan.getEnvironment()).append(" environment\"\n\n");
            
            // 预部署检查
            deployScript.append("# Pre-deployment checks\n");
            deployScript.append("echo \"Running pre-deployment checks...\"\n");
            deployScript.append("# Check disk space\n");
            deployScript.append("df -h /opt\n");
            deployScript.append("# Check memory\n");
            deployScript.append("free -h\n");
            deployScript.append("# Check Java installation\n");
            deployScript.append("java -version\n\n");
            
            // 复制配置文件
            deployScript.append("# Copy configuration files\n");
            deployScript.append("cp configs/*.properties /opt/kafka/config/\n");
            deployScript.append("cp configs/*.service /etc/systemd/system/\n");
            deployScript.append("systemctl daemon-reload\n\n");
            
            // 启动服务
            deployScript.append("# Start services\n");
            deployScript.append("systemctl enable zookeeper\n");
            deployScript.append("systemctl start zookeeper\n");
            deployScript.append("sleep 10\n");
            for (String brokerId : plan.getBrokerIds()) {
                deployScript.append("systemctl enable kafka-").append(brokerId).append("\n");
                deployScript.append("systemctl start kafka-").append(brokerId).append("\n");
                deployScript.append("sleep 5\n");
            }
            
            deployScript.append("\necho \"Deployment completed successfully\"\n");
            
            Files.write(configDir.resolve("deploy-cluster.sh"), deployScript.toString().getBytes());
        }
        
        /**
         * 验证部署计划
         */
        private void validateDeployment() throws DeploymentException {
            // 检查broker配置
            if (plan.getBrokerConfigs().isEmpty()) {
                throw new DeploymentException("No broker configurations found");
            }
            
            // 检查机架分布
            Map<String, Long> rackCounts = plan.getBrokerConfigs().values().stream()
                .map(BrokerDeploymentConfig::getRackId)
                .filter(Objects::nonNull)
                .collect(Collectors.groupingBy(r -> r, Collectors.counting()));
            
            // 确保每个机架有足够的broker
            for (Map.Entry<String, Long> entry : rackCounts.entrySet()) {
                if (entry.getValue() < 2) {
                    System.out.println("Warning: Rack " + entry.getKey() + " has only " + entry.getValue() + " broker(s)");
                }
            }
            
            // 检查配置冲突
            Set<String> hosts = new HashSet<>();
            for (BrokerDeploymentConfig config : plan.getBrokerConfigs().values()) {
                if (!hosts.add(config.getHost())) {
                    throw new DeploymentException("Duplicate host found: " + config.getHost());
                }
            }
        }
        
        /**
         * 执行滚动部署
         */
        private void executeRollingDeployment() throws IOException {
            System.out.println("Executing rolling deployment...");
            
            List<String> brokerIds = plan.getBrokerIds();
            int maxUnavailable = plan.getStrategy().getMaxUnavailableBrokers();
            
            for (int i = 0; i < brokerIds.size(); i += maxUnavailable) {
                List<String> batch = brokerIds.subList(i, Math.min(i + maxUnavailable, brokerIds.size()));
                
                System.out.println("Processing batch: " + String.join(", ", batch));
                
                // 停止当前批次的broker
                for (String brokerId : batch) {
                    System.out.println("Stopping broker " + brokerId);
                    // executeCommand("systemctl stop kafka-" + brokerId);
                }
                
                // 等待集群稳定
                System.out.println("Waiting for cluster stabilization...");
                try {
                    Thread.sleep(30000); // 30秒
                } catch (InterruptedException e) {
                    Thread.currentThread().interrupt();
                    throw new IOException("Deployment interrupted", e);
                }
                
                // 启动broker
                for (String brokerId : batch) {
                    System.out.println("Starting broker " + brokerId);
                    // executeCommand("systemctl start kafka-" + brokerId);
                }
                
                // 等待broker启动
                System.out.println("Waiting for brokers to start...");
                try {
                    Thread.sleep(60000); // 60秒
                } catch (InterruptedException e) {
                    Thread.currentThread().interrupt();
                    throw new IOException("Deployment interrupted", e);
                }
            }
        }
        
        /**
         * 执行全量重启部署
         */
        private void executeFullRestartDeployment() throws IOException {
            System.out.println("Executing full restart deployment...");
            
            // 停止所有broker
            for (String brokerId : plan.getBrokerIds()) {
                System.out.println("Stopping broker " + brokerId);
                // executeCommand("systemctl stop kafka-" + brokerId);
            }
            
            // 等待关闭
            System.out.println("Waiting for graceful shutdown...");
            try {
                Thread.sleep(30000);
            } catch (InterruptedException e) {
                Thread.currentThread().interrupt();
                throw new IOException("Deployment interrupted", e);
            }
            
            // 启动所有broker
            for (String brokerId : plan.getBrokerIds()) {
                System.out.println("Starting broker " + brokerId);
                // executeCommand("systemctl start kafka-" + brokerId);
            }
            
            // 等待集群启动
            System.out.println("Waiting for cluster to be ready...");
            try {
                Thread.sleep(60000);
            } catch (InterruptedException e) {
                Thread.currentThread().interrupt();
                throw new IOException("Deployment interrupted", e);
            }
        }
        
        /**
         * 验证部署结果
         */
        private void validateDeploymentResult() {
            System.out.println("Validating cluster health...");
            
            // 这里应该实现实际的健康检查
            // 例如：检查broker状态、测试生产者/消费者等
            try {
                Thread.sleep(5000);
                System.out.println("Health check passed");
            } catch (InterruptedException e) {
                Thread.currentThread().interrupt();
                throw new RuntimeException("Health check interrupted", e);
            }
        }
    }
    
    /**
     * 部署结果
     */
    public static class DeploymentResult {
        private boolean success;
        private String message;
        private long duration;
        private List<String> warnings = new ArrayList<>();
        private List<String> errors = new ArrayList<>();
        
        // Getters and Setters
        public boolean isSuccess() { return success; }
        public void setSuccess(boolean success) { this.success = success; }
        public String getMessage() { return message; }
        public void setMessage(String message) { this.message = message; }
        public long getDuration() { return duration; }
        public void setDuration(long duration) { this.duration = duration; }
        public List<String> getWarnings() { return warnings; }
        public List<String> getErrors() { return errors; }
        
        @Override
        public String toString() {
            StringBuilder sb = new StringBuilder();
            sb.append("Deployment Result:\n");
            sb.append("  Success: ").append(success).append("\n");
            sb.append("  Message: ").append(message).append("\n");
            sb.append("  Duration: ").append(duration / 1000).append(" seconds\n");
            
            if (!warnings.isEmpty()) {
                sb.append("  Warnings:\n");
                for (String warning : warnings) {
                    sb.append("    - ").append(warning).append("\n");
                }
            }
            
            if (!errors.isEmpty()) {
                sb.append("  Errors:\n");
                for (String error : errors) {
                    sb.append("    - ").append(error).append("\n");
                }
            }
            
            return sb.toString();
        }
    }
    
    /**
     * 部署异常
     */
    public static class DeploymentException extends Exception {
        public DeploymentException(String message) {
            super(message);
        }
        
        public DeploymentException(String message, Throwable cause) {
            super(message, cause);
        }
    }
    
    /**
     * 演示集群部署
     */
    public static void demonstrateClusterDeployment() throws IOException {
        System.out.println("=== Kafka集群部署演示 ===\n");
        
        // 创建部署计划
        DeploymentPlan plan = new DeploymentPlan("production", "3.5.0");
        
        // 设置全局属性
        plan.setGlobalProperty("log.retention.hours", "168");
        plan.setGlobalProperty("log.segment.bytes", "1073741824");
        plan.setGlobalProperty("num.network.threads", "8");
        plan.setGlobalProperty("num.io.threads", "16");
        
        // 配置broker
        BrokerDeploymentConfig broker1 = new BrokerDeploymentConfig("kafka-01.datacenter1.com", "dc1-rack-a");
        broker1.setProperty("log.dirs", "/opt/kafka/data1,/opt/kafka/data2");
        broker1.setProperty("socket.send.buffer.bytes", "102400");
        broker1.setProperty("socket.receive.buffer.bytes", "102400");
        broker1.setEnvironmentVariable("KAFKA_HEAP_OPTS", "-Xmx4g -Xms4g");
        plan.addBroker("1", broker1);
        
        BrokerDeploymentConfig broker2 = new BrokerDeploymentConfig("kafka-02.datacenter1.com", "dc1-rack-b");
        broker2.setProperty("log.dirs", "/opt/kafka/data1,/opt/kafka/data2");
        broker2.setEnvironmentVariable("KAFKA_HEAP_OPTS", "-Xmx4g -Xms4g");
        plan.addBroker("2", broker2);
        
        BrokerDeploymentConfig broker3 = new BrokerDeploymentConfig("kafka-03.datacenter2.com", "dc2-rack-a");
        broker3.setProperty("log.dirs", "/opt/kafka/data1,/opt/kafka/data2");
        broker3.setEnvironmentVariable("KAFKA_HEAP_OPTS", "-Xmx4g -Xms4g");
        plan.addBroker("3", broker3);
        
        // 配置部署策略
        plan.getStrategy().setRollingRestart(true);
        plan.getStrategy().setMaxUnavailableBrokers(1);
        plan.getStrategy().setHealthCheckInterval(10);
        
        // 执行部署
        String workspace = "deployment_workspace";
        DeploymentExecutor executor = new DeploymentExecutor(workspace, plan);
        DeploymentResult result = executor.execute();
        
        // 输出结果
        System.out.println("\n" + result.toString());
        
        // 显示生成的文件
        System.out.println("Generated configuration files:");
        Path configDir = Paths.get(workspace, "configs");
        if (Files.exists(configDir)) {
            Files.walk(configDir)
                 .filter(Files::isRegularFile)
                 .forEach(file -> System.out.println("  " + file.getFileName()));
        }
        
        System.out.println("\n=== 部署演示完成 ===");
    }
}
```

---

## 集群监控与告警

### 监控指标收集器

```java
package com.kafka.tutorial.cluster;

import java.io.*;
import java.net.*;
import java.util.*;
import java.util.concurrent.*;
import java.util.concurrent.atomic.*;
import java.time.*;

/**
 * Kafka集群监控与告警系统
 */
public class KafkaClusterMonitoring {
    
    /**
     * 监控指标定义
     */
    public static class Metrics {
        
        public enum MetricType {
            COUNTER,    // 递增计数
            GAUGE,      // 当前值
            HISTOGRAM,  // 直方图
            TIMER       // 时间计时
        }
        
        public static class Metric {
            private final String name;
            private final MetricType type;
            private final String description;
            private final Map<String, String> tags;
            private volatile double value;
            private final long timestamp;
            
            public Metric(String name, MetricType type, String description, Map<String, String> tags) {
                this.name = name;
                this.type = type;
                this.description = description;
                this.tags = tags != null ? new HashMap<>(tags) : new HashMap<>();
                this.value = 0;
                this.timestamp = System.currentTimeMillis();
            }
            
            public void setValue(double value) {
                this.value = value;
            }
            
            public void increment(double amount) {
                this.value += amount;
            }
            
            public double getValue() { return value; }
            public String getName() { return name; }
            public MetricType getType() { return type; }
            public String getDescription() { return description; }
            public Map<String, String> getTags() { return tags; }
            public long getTimestamp() { return timestamp; }
        }
        
        public static class BrokerMetrics {
            private final String brokerId;
            private final Map<String, Metric> metrics;
            
            public BrokerMetrics(String brokerId) {
                this.brokerId = brokerId;
                this.metrics = new ConcurrentHashMap<>();
                initializeMetrics();
            }
            
            private void initializeMetrics() {
                // 消息指标
                metrics.put("messages_per_second", new Metric("kafka.messages_per_second", 
                    MetricType.GAUGE, "Messages per second", tagMap("broker", brokerId)));
                metrics.put("bytes_in_per_second", new Metric("kafka.bytes_in_per_second", 
                    MetricType.GAUGE, "Bytes in per second", tagMap("broker", brokerId)));
                metrics.put("bytes_out_per_second", new Metric("kafka.bytes_out_per_second", 
                    MetricType.GAUGE, "Bytes out per second", tagMap("broker", brokerId)));
                
                // 请求指标
                metrics.put("produce_requests_per_second", new Metric("kafka.produce_requests_per_second", 
                    MetricType.GAUGE, "Produce requests per second", tagMap("broker", brokerId)));
                metrics.put("fetch_requests_per_second", new Metric("kafka.fetch_requests_per_second", 
                    MetricType.GAUGE, "Fetch requests per second", tagMap("broker", brokerId)));
                metrics.put("request_time_99p", new Metric("kafka.request_time_99p", 
                    MetricType.GAUGE, "Request time 99th percentile", tagMap("broker", brokerId)));
                
                // 副本指标
                metrics.put("under_replicated_partitions", new Metric("kafka.under_replicated_partitions", 
                    MetricType.GAUGE, "Under replicated partitions", tagMap("broker", brokerId)));
                metrics.put("offline_partitions", new Metric("kafka.offline_partitions", 
                    MetricType.GAUGE, "Offline partitions", tagMap("broker", brokerId)));
                metrics.put("isr_shrinks_per_second", new Metric("kafka.isr_shrinks_per_second", 
                    MetricType.GAUGE, "ISR shrinks per second", tagMap("broker", brokerId)));
                
                // 消费者滞后
                metrics.put("consumer_lag_sum", new Metric("kafka.consumer_lag_sum", 
                    MetricType.GAUGE, "Total consumer lag", tagMap("broker", brokerId)));
                
                // 控制器指标
                metrics.put("controller_active", new Metric("kafka.controller_active", 
                    MetricType.GAUGE, "Controller active", tagMap("broker", brokerId)));
                
                // 健康指标
                metrics.put("jvm_gc_count", new Metric("kafka.jvm_gc_count", 
                    MetricType.COUNTER, "JVM GC count", tagMap("broker", brokerId)));
                metrics.put("jvm_gc_time", new Metric("kafka.jvm_gc_time", 
                    MetricType.COUNTER, "JVM GC time", tagMap("broker", brokerId)));
                metrics.put("heap_used_mb", new Metric("kafka.heap_used_mb", 
                    MetricType.GAUGE, "Heap used MB", tagMap("broker", brokerId)));
                metrics.put("disk_used_percent", new Metric("kafka.disk_used_percent", 
                    MetricType.GAUGE, "Disk used percent", tagMap("broker", brokerId)));
            }
            
            public Metric getMetric(String name) {
                return metrics.get(name);
            }
            
            public Collection<Metric> getAllMetrics() {
                return metrics.values();
            }
            
            // Getters
            public String getBrokerId() { return brokerId; }
            public Map<String, Metric> getMetrics() { return metrics; }
        }
        
        private static Map<String, String> tagMap(String... keyValues) {
            Map<String, String> tags = new HashMap<>();
            for (int i = 0; i < keyValues.length; i += 2) {
                tags.put(keyValues[i], keyValues[i + 1]);
            }
            return tags;
        }
    }
    
    /**
     * JMX监控客户端
     */
    public static class JMXMonitor {
        private final String host;
        private final int port;
        private final String jmxServiceUrl;
        private MBeanServerConnection connection;
        
        public JMXMonitor(String host, int port) {
            this.host = host;
            this.port = port;
            this.jmxServiceUrl = String.format("service:jmx:rmi:///jndi/rmi://%s:%s/jmxrmi", host, port);
        }
        
        public void connect() throws IOException {
            try {
                JMXServiceURL url = new JMXServiceURL(jmxServiceUrl);
                JMXConnector connector = JMXConnectorFactory.newJMXConnector(url, null);
                connector.connect();
                this.connection = connector.getMBeanServerConnection();
                System.out.println("Connected to JMX: " + jmxServiceUrl);
            } catch (Exception e) {
                throw new IOException("Failed to connect to JMX", e);
            }
        }
        
        public double getMetric(String mBeanName, String attribute) throws Exception {
            if (connection == null) {
                throw new IllegalStateException("Not connected to JMX");
            }
            
            ObjectName objectName = new ObjectName(mBeanName);
            return ((Number) connection.getAttribute(objectName, attribute)).doubleValue();
        }
        
        public Map<String, Double> getMultipleMetrics(String mBeanName, List<String> attributes) throws Exception {
            Map<String, Double> result = new HashMap<>();
            ObjectName objectName = new ObjectName(mBeanName);
            
            for (String attribute : attributes) {
                try {
                    Double value = ((Number) connection.getAttribute(objectName, attribute)).doubleValue();
                    result.put(attribute, value);
                } catch (Exception e) {
                    System.err.println("Failed to get attribute " + attribute + " from " + mBeanName + ": " + e.getMessage());
                }
            }
            
            return result;
        }
        
        public void disconnect() {
            // JMX连接会在Connector关闭时自动清理
        }
    }
    
    /**
     * 集群状态收集器
     */
    public static class ClusterStatusCollector {
        private final List<String> brokerHosts;
        private final Map<String, JMXMonitor> jmxClients;
        private final Map<String, Metrics.BrokerMetrics> brokerMetrics;
        
        public ClusterStatusCollector(List<String> brokerHosts) {
            this.brokerHosts = brokerHosts;
            this.jmxClients = new HashMap<>();
            this.brokerMetrics = new HashMap<>();
            initializeCollectors();
        }
        
        private void initializeCollectors() {
            for (String host : brokerHosts) {
                String brokerId = extractBrokerId(host);
                jmxClients.put(brokerId, new JMXMonitor(host, 9999));
                brokerMetrics.put(brokerId, new Metrics.BrokerMetrics(brokerId));
            }
        }
        
        private String extractBrokerId(String host) {
            // 从主机名中提取broker ID (假设命名约定为kafka-XX)
            String[] parts = host.split("-");
            return parts.length > 1 ? parts[parts.length - 1] : host;
        }
        
        /**
         * 收集集群状态
         */
        public ClusterStatus collectStatus() {
            ClusterStatus status = new ClusterStatus();
            
            for (Map.Entry<String, JMXMonitor> entry : jmxClients.entrySet()) {
                String brokerId = entry.getKey();
                JMXMonitor jmxClient = entry.getValue();
                Metrics.BrokerMetrics metrics = brokerMetrics.get(brokerId);
                
                try {
                    // 连接JMX
                    jmxClient.connect();
                    
                    // 收集消息指标
                    collectMessageMetrics(jmxClient, metrics);
                    
                    // 收集请求指标
                    collectRequestMetrics(jmxClient, metrics);
                    
                    // 收集副本指标
                    collectReplicaMetrics(jmxClient, metrics);
                    
                    // 收集健康指标
                    collectHealthMetrics(jmxClient, metrics);
                    
                    // 收集控制器指标
                    collectControllerMetrics(jmxClient, metrics);
                    
                    status.addBrokerStatus(brokerId, BrokerStatus.ONLINE);
                    
                } catch (Exception e) {
                    System.err.println("Failed to collect metrics for broker " + brokerId + ": " + e.getMessage());
                    status.addBrokerStatus(brokerId, BrokerStatus.OFFLINE);
                } finally {
                    jmxClient.disconnect();
                }
            }
            
            return status;
        }
        
        private void collectMessageMetrics(JMXMonitor jmxClient, Metrics.BrokerMetrics metrics) throws Exception {
            // 收集消息生产指标
            Map<String, Double> produceMetrics = jmxClient.getMultipleMetrics(
                "kafka.server:type=BrokerTopicMetrics,name=MessagesPerSec",
                Arrays.asList("Count", "Rate")
            );
            
            if (produceMetrics.containsKey("Rate")) {
                metrics.getMetric("messages_per_second").setValue(produceMetrics.get("Rate"));
            }
            
            // 收集字节输入指标
            Map<String, Double> byteInMetrics = jmxClient.getMultipleMetrics(
                "kafka.server:type=BrokerTopicMetrics,name=BytesInPerSec",
                Arrays.asList("Count", "Rate")
            );
            
            if (byteInMetrics.containsKey("Rate")) {
                metrics.getMetric("bytes_in_per_second").setValue(byteInMetrics.get("Rate"));
            }
            
            // 收集字节输出指标
            Map<String, Double> byteOutMetrics = jmxClient.getMultipleMetrics(
                "kafka.server:type=BrokerTopicMetrics,name=BytesOutPerSec",
                Arrays.asList("Count", "Rate")
            );
            
            if (byteOutMetrics.containsKey("Rate")) {
                metrics.getMetric("bytes_out_per_second").setValue(byteOutMetrics.get("Rate"));
            }
        }
        
        private void collectRequestMetrics(JMXMonitor jmxClient, Metrics.BrokerMetrics metrics) throws Exception {
            // 收集produce请求指标
            Map<String, Double> produceRequestMetrics = jmxClient.getMultipleMetrics(
                "kafka.server:type=RequestMetrics,name=ProduceRequestsPerSec,request=Produce",
                Arrays.asList("Count", "Rate")
            );
            
            if (produceRequestMetrics.containsKey("Rate")) {
                metrics.getMetric("produce_requests_per_second").setValue(produceRequestMetrics.get("Rate"));
            }
            
            // 收集fetch请求指标
            Map<String, Double> fetchRequestMetrics = jmxClient.getMultipleMetrics(
                "kafka.server:type=RequestMetrics,name=FetchRequestsPerSec,request=Fetch",
                Arrays.asList("Count", "Rate")
            );
            
            if (fetchRequestMetrics.containsKey("Rate")) {
                metrics.getMetric("fetch_requests_per_second").setValue(fetchRequestMetrics.get("Rate"));
            }
            
            // 收集请求时间指标
            Map<String, Double> requestTimeMetrics = jmxClient.getMultipleMetrics(
                "kafka.server:type=RequestMetrics,name=TotalTimeMs,request=Produce",
                Arrays.asList("Mean", "99thPercentile")
            );
            
            if (requestTimeMetrics.containsKey("99thPercentile")) {
                metrics.getMetric("request_time_99p").setValue(requestTimeMetrics.get("99thPercentile"));
            }
        }
        
        private void collectReplicaMetrics(JMXMonitor jmxClient, Metrics.BrokerMetrics metrics) throws Exception {
            // 收集under replicated partitions
            double underReplicated = jmxClient.getMetric(
                "kafka.server:type=ReplicaManager,name=UnderReplicatedPartitions",
                "Value"
            );
            metrics.getMetric("under_replicated_partitions").setValue(underReplicated);
            
            // 收集offline partitions
            double offlinePartitions = jmxClient.getMetric(
                "kafka.server:type=Partition,name=OfflinePartitionsCount",
                "Value"
            );
            metrics.getMetric("offline_partitions").setValue(offlinePartitions);
            
            // 收集ISR收缩指标
            Map<String, Double> isrMetrics = jmxClient.getMultipleMetrics(
                "kafka.server:type=ReplicaManager,name=IsrShrinksPerSec",
                Arrays.asList("Count", "Rate")
            );
            
            if (isrMetrics.containsKey("Rate")) {
                metrics.getMetric("isr_shrinks_per_second").setValue(isrMetrics.get("Rate"));
            }
        }
        
        private void collectHealthMetrics(JMXMonitor jmxClient, Metrics.BrokerMetrics metrics) throws Exception {
            // 收集JVM指标
            Map<String, Double> gcMetrics = jmxClient.getMultipleMetrics(
                "java.lang:type=GarbageCollector,name=*",
                Arrays.asList("CollectionCount", "CollectionTime")
            );
            
            if (gcMetrics.containsKey("CollectionCount")) {
                metrics.getMetric("jvm_gc_count").setValue(gcMetrics.get("CollectionCount"));
            }
            
            if (gcMetrics.containsKey("CollectionTime")) {
                metrics.getMetric("jvm_gc_time").setValue(gcMetrics.get("CollectionTime"));
            }
            
            // 收集堆内存使用
            Map<String, Double> memoryMetrics = jmxClient.getMultipleMetrics(
                "java.lang:type=Memory",
                Arrays.asList("HeapMemoryUsage")
            );
            
            if (memoryMetrics.containsKey("HeapMemoryUsage")) {
                double usedMB = memoryMetrics.get("HeapMemoryUsage") / (1024 * 1024);
                metrics.getMetric("heap_used_mb").setValue(usedMB);
            }
        }
        
        private void collectControllerMetrics(JMXMonitor jmxClient, Metrics.BrokerMetrics metrics) throws Exception {
            // 检查是否为控制器
            try {
                double activeController = jmxClient.getMetric(
                    "kafka.controller:type=KafkaController,name=ActiveControllerCount",
                    "Value"
                );
                metrics.getMetric("controller_active").setValue(activeController > 0 ? 1.0 : 0.0);
            } catch (Exception e) {
                // 可能不是控制器broker
                metrics.getMetric("controller_active").setValue(0.0);
            }
        }
        
        // Getters
        public Map<String, Metrics.BrokerMetrics> getBrokerMetrics() { return brokerMetrics; }
    }
    
    /**
     * 集群状态
     */
    public static class ClusterStatus {
        private final Map<String, BrokerStatus> brokerStatuses;
        private final long timestamp;
        private final List<String> issues;
        
        public ClusterStatus() {
            this.brokerStatuses = new HashMap<>();
            this.timestamp = System.currentTimeMillis();
            this.issues = new ArrayList<>();
        }
        
        public void addBrokerStatus(String brokerId, BrokerStatus status) {
            brokerStatuses.put(brokerId, status);
            if (status == BrokerStatus.OFFLINE) {
                issues.add("Broker " + brokerId + " is offline");
            }
        }
        
        public BrokerStatus getBrokerStatus(String brokerId) {
            return brokerStatuses.getOrDefault(brokerId, BrokerStatus.UNKNOWN);
        }
        
        public boolean isHealthy() {
            return issues.isEmpty() && brokerStatuses.values().stream()
                .allMatch(status -> status == BrokerStatus.ONLINE);
        }
        
        public long getOnlineBrokerCount() {
            return brokerStatuses.values().stream()
                .mapToLong(status -> status == BrokerStatus.ONLINE ? 1 : 0)
                .sum();
        }
        
        // Getters
        public Map<String, BrokerStatus> getBrokerStatuses() { return brokerStatuses; }
        public long getTimestamp() { return timestamp; }
        public List<String> getIssues() { return issues; }
    }
    
    public enum BrokerStatus {
        ONLINE, OFFLINE, DEGRADED, UNKNOWN
    }
    
    /**
     * 告警规则管理器
     */
    public static class AlertRuleManager {
        private final List<AlertRule> rules;
        private final AlertCallback alertCallback;
        
        public AlertRuleManager(AlertCallback alertCallback) {
            this.rules = new ArrayList<>();
            this.alertCallback = alertCallback;
            initializeRules();
        }
        
        private void initializeRules() {
            // 离线broker告警
            rules.add(new AlertRule(
                "broker_offline",
                "Broker {brokerId} is offline",
                AlertSeverity.CRITICAL,
                rule -> rule.getConditions().get("status") == BrokerStatus.OFFLINE
            ));
            
            // 高延迟告警
           
