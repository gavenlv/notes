# 第6章：消费者开发实战

## 目录
1. [消费者基础概念与API](#消费者基础概念与API)
2. [消费者组与负载均衡](#消费者组与负载均衡)
3. [偏移量管理与存储机制](#偏移量管理与存储机制)
4. [多线程消费者实现](#多线程消费者实现)
5. [消费模式与反序列化](#消费模式与反序列化)
6. [批量消费与效率优化](#批量消费与效率优化)
7. [消费者的可靠性保证](#消费者的可靠性保证)
8. [流处理与实时计算](#流处理与实时计算)
9. [实战项目：实时数据分析系统](#实战项目实时数据分析系统)
10. [最佳实践与性能调优](#最佳实践与性能调优)

---

## 消费者基础概念与API

### Kafka消费者核心概念

Kafka消费者是负责从Kafka集群读取数据的客户端应用程序，是实现数据消费和处理的关键组件。

#### 消费者的核心职责：
1. **订阅管理**：管理对主题和分区的订阅
2. **消费进度**：维护消费位置（偏移量）
3. **负载均衡**：在消费者组内实现分区分配
4. **心跳管理**：与消费者组协调器保持连接
5. **消息处理**：获取、解码和处理消息
6. **偏移量提交**：管理消费进度的持久化

### KafkaConsumer API架构

```java
package com.kafka.tutorial.consumer;

import org.apache.kafka.clients.consumer.*;
import org.apache.kafka.common.serialization.StringDeserializer;
import org.apache.kafka.common.TopicPartition;
import org.apache.kafka.common.header.Headers;
import org.apache.kafka.common.header.Header;

import java.util.*;
import java.util.concurrent.*;
import java.util.concurrent.atomic.AtomicBoolean;
import java.util.concurrent.atomic.AtomicLong;
import java.time.Duration;

/**
 * 基础Kafka消费者实现
 */
public class BasicKafkaConsumer {
    private final KafkaConsumer<String, String> consumer;
    private final String clientId;
    private final AtomicBoolean closed = new AtomicBoolean(false);
    private final AtomicLong messagesConsumed = new AtomicLong(0);
    private final AtomicLong processingErrors = new AtomicLong(0);
    private final ConsumerMetrics consumerMetrics;
    
    public BasicKafkaConsumer(Properties configs) {
        this.clientId = configs.getProperty(ConsumerConfig.CLIENT_ID_CONFIG, "basic-consumer");
        this.consumer = new KafkaConsumer<>(configs);
        this.consumerMetrics = new ConsumerMetrics();
    }
    
    /**
     * 创建基础消费者配置
     */
    public static Properties createBasicConfigs() {
        Properties configs = new Properties();
        
        // 基础连接配置
        configs.put(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG, "localhost:9092");
        configs.put(ConsumerConfig.CLIENT_ID_CONFIG, "basic-consumer");
        configs.put(ConsumerConfig.GROUP_ID_CONFIG, "basic-consumer-group");
        
        // 反序列化配置
        configs.put(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class.getName());
        configs.put(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class.getName());
        
        // 消费行为配置
        configs.put(ConsumerConfig.ENABLE_AUTO_COMMIT_CONFIG, false); // 手动提交偏移量
        configs.put(ConsumerConfig.AUTO_OFFSET_RESET_CONFIG, "earliest");
        configs.put(ConsumerConfig.MAX_POLL_RECORDS_CONFIG, 100);
        configs.put(ConsumerConfig.MAX_POLL_INTERVAL_MS_CONFIG, 300000); // 5分钟
        configs.put(ConsumerConfig.SESSION_TIMEOUT_MS_CONFIG, 30000); // 30秒
        configs.put(ConsumerConfig.HEARTBEAT_INTERVAL_MS_CONFIG, 3000); // 3秒
        
        return configs;
    }
    
    /**
     * 订阅主题
     */
    public void subscribe(String... topics) {
        consumer.subscribe(Arrays.asList(topics));
        System.out.println("已订阅主题: " + Arrays.toString(topics));
    }
    
    /**
     * 订阅主题并设置消费参数
     */
    public void subscribeWithOptions(String topic, ConsumerRebalanceListener listener) {
        Properties configs = createBasicConfigs();
        KafkaConsumer<String, String> customConsumer = new KafkaConsumer<>(configs);
        
        // 设置重平衡监听器
        customConsumer.subscribe(Collections.singletonList(topic), listener);
        this.consumer = customConsumer;
        System.out.println("已订阅主题: " + topic + "，并设置重平衡监听器");
    }
    
    /**
     * 手动分配分区
     */
    public void assignPartitions(String topic, int... partitions) {
        List<TopicPartition> topicPartitions = new ArrayList<>();
        
        for (int partition : partitions) {
            topicPartitions.add(new TopicPartition(topic, partition));
        }
        
        consumer.assign(topicPartitions);
        System.out.println("已手动分配分区: " + topicPartitions);
    }
    
    /**
     * 基础消费循环
     */
    public void consumeLoop(MessageProcessor messageProcessor) {
        System.out.println("开始消费循环...");
        
        try {
            while (!closed.get()) {
                // 拉取消息
                ConsumerRecords<String, String> records = consumer.poll(Duration.ofSeconds(1));
                
                if (!records.isEmpty()) {
                    processRecords(records, messageProcessor);
                    commitOffsets(records);
                }
            }
        } catch (Exception e) {
            System.err.println("消费循环异常: " + e.getMessage());
            throw new RuntimeException("消费循环失败", e);
        } finally {
            close();
        }
    }
    
    /**
     * 异步消费
     */
    public void consumeAsync(MessageAsyncProcessor asyncProcessor) {
        System.out.println("开始异步消费...");
        
        CompletableFuture<Void> consumeFuture = CompletableFuture.runAsync(() -> {
            try {
                while (!closed.get()) {
                    ConsumerRecords<String, String> records = consumer.poll(Duration.ofSeconds(1));
                    
                    if (!records.isEmpty()) {
                        processRecordsAsync(records, asyncProcessor);
                    }
                }
            } catch (Exception e) {
                System.err.println("异步消费异常: " + e.getMessage());
                asyncProcessor.onError(e);
            }
        });
        
        return consumeFuture;
    }
    
    /**
     * 批量消费
     */
    public void consumeBatch(BatchMessageProcessor batchProcessor) {
        System.out.println("开始批量消费...");
        
        try {
            while (!closed.get()) {
                ConsumerRecords<String, String> records = consumer.poll(Duration.ofSeconds(1));
                
                if (!records.isEmpty()) {
                    batchProcessor.processBatch(records);
                    commitOffsets(records);
                    messagesConsumed.addAndGet(records.count());
                }
            }
        } catch (Exception e) {
            System.err.println("批量消费异常: " + e.getMessage());
            throw new RuntimeException("批量消费失败", e);
        } finally {
            close();
        }
    }
    
    /**
     * 处理单条消息
     */
    private void processRecords(ConsumerRecords<String, String> records, MessageProcessor processor) {
        for (ConsumerRecord<String, String> record : records) {
            try {
                processor.processMessage(record);
                messagesConsumed.incrementAndGet();
                
                // 更新统计信息
                consumerMetrics.recordMessageProcessed(record);
                
            } catch (Exception e) {
                processingErrors.incrementAndGet();
                processor.onError(record, e);
                System.err.println("处理消息失败: " + e.getMessage());
            }
        }
    }
    
    /**
     * 异步处理消息
     */
    private void processRecordsAsync(ConsumerRecords<String, String> records, MessageAsyncProcessor asyncProcessor) {
        List<CompletableFuture<Void>> futures = new ArrayList<>();
        
        for (ConsumerRecord<String, String> record : records) {
            CompletableFuture<Void> future = CompletableFuture.runAsync(() -> {
                try {
                    asyncProcessor.processMessageAsync(record);
                    messagesConsumed.incrementAndGet();
                    consumerMetrics.recordMessageProcessed(record);
                } catch (Exception e) {
                    processingErrors.incrementAndGet();
                    asyncProcessor.onError(e);
                }
            });
            futures.add(future);
        }
        
        // 等待所有消息处理完成
        CompletableFuture.allOf(futures.toArray(new CompletableFuture[0]))
            .thenRun(() -> commitOffsets(records))
            .exceptionally(ex -> {
                System.err.println("异步处理失败: " + ex.getMessage());
                return null;
            });
    }
    
    /**
     * 提交偏移量
     */
    private void commitOffsets(ConsumerRecords<String, String> records) {
        try {
            Map<TopicPartition, OffsetAndMetadata> offsetsToCommit = new HashMap<>();
            
            for (TopicPartition partition : records.partitions()) {
                List<ConsumerRecord<String, String>> partitionRecords = records.records(partition);
                long lastOffset = partitionRecords.get(partitionRecords.size() - 1).offset();
                offsetsToCommit.put(partition, new OffsetAndMetadata(lastOffset + 1));
            }
            
            // 异步提交
            consumer.commitAsync(offsetsToCommit, (offsets, exception) -> {
                if (exception != null) {
                    System.err.println("提交偏移量失败: " + exception.getMessage());
                } else {
                    System.out.println("偏移量提交成功: " + offsets.size() + " 个分区");
                }
            });
            
        } catch (Exception e) {
            System.err.println("提交偏移量时发生异常: " + e.getMessage());
        }
    }
    
    /**
     * 提交特定偏移量
     */
    public void commitSpecificOffset(String topic, int partition, long offset) {
        TopicPartition topicPartition = new TopicPartition(topic, partition);
        OffsetAndMetadata offsetAndMetadata = new OffsetAndMetadata(offset + 1);
        
        Map<TopicPartition, OffsetAndMetadata> offsetMap = new HashMap<>();
        offsetMap.put(topicPartition, offsetAndMetadata);
        
        try {
            consumer.commitSync(offsetMap);
            System.out.println("已提交偏移量: topic=" + topic + ", partition=" + partition + ", offset=" + offset);
        } catch (Exception e) {
            System.err.println("提交偏移量失败: " + e.getMessage());
        }
    }
    
    /**
     * 获取当前消费者状态
     */
    public ConsumerState getConsumerState() {
        Set<TopicPartition> assignedPartitions = consumer.assignment();
        Map<TopicPartition, Long> committedOffsets = new HashMap<>();
        Map<TopicPartition, Long> endOffsets = new HashMap<>();
        
        for (TopicPartition partition : assignedPartitions) {
            OffsetAndMetadata committed = consumer.committed(partition);
            Long endOffset = consumer.endOffsets(Collections.singletonList(partition)).get(partition);
            
            if (committed != null) {
                committedOffsets.put(partition, committed.offset());
            }
            endOffsets.put(partition, endOffset);
        }
        
        return new ConsumerState(
            clientId,
            assignedPartitions,
            committedOffsets,
            endOffsets,
            messagesConsumed.get(),
            processingErrors.get(),
            consumerMetrics
        );
    }
    
    /**
     * 优雅关闭消费者
     */
    public void close() {
        if (closed.compareAndSet(false, true)) {
            try {
                consumer.commitSync(); // 提交最后的偏移量
                consumer.close(Duration.ofSeconds(30));
                System.out.println("消费者已关闭: " + clientId);
            } catch (Exception e) {
                System.err.println("关闭消费者时发生错误: " + e.getMessage());
            }
        }
    }
    
    /**
     * 消息处理器接口
     */
    public interface MessageProcessor {
        void processMessage(ConsumerRecord<String, String> record) throws Exception;
        default void onError(ConsumerRecord<String, String> record, Exception exception) {
            System.err.println("处理消息时发生错误: " + exception.getMessage());
        }
    }
    
    /**
     * 异步消息处理器接口
     */
    public interface MessageAsyncProcessor {
        void processMessageAsync(ConsumerRecord<String, String> record) throws Exception;
        default void onError(Exception exception) {
            System.err.println("异步处理消息时发生错误: " + exception.getMessage());
        }
    }
    
    /**
     * 批量消息处理器接口
     */
    public interface BatchMessageProcessor {
        void processBatch(ConsumerRecords<String, String> records) throws Exception;
        default void onError(Exception exception) {
            System.err.println("批量处理消息时发生错误: " + exception.getMessage());
        }
    }
    
    /**
     * 消费者状态
     */
    public static class ConsumerState {
        private final String clientId;
        private final Set<TopicPartition> assignedPartitions;
        private final Map<TopicPartition, Long> committedOffsets;
        private final Map<TopicPartition, Long> endOffsets;
        private final long messagesConsumed;
        private final long processingErrors;
        private final ConsumerMetrics metrics;
        
        public ConsumerState(String clientId, Set<TopicPartition> assignedPartitions,
                           Map<TopicPartition, Long> committedOffsets,
                           Map<TopicPartition, Long> endOffsets,
                           long messagesConsumed, long processingErrors,
                           ConsumerMetrics metrics) {
            this.clientId = clientId;
            this.assignedPartitions = assignedPartitions;
            this.committedOffsets = new HashMap<>(committedOffsets);
            this.endOffsets = new HashMap<>(endOffsets);
            this.messagesConsumed = messagesConsumed;
            this.processingErrors = processingErrors;
            this.metrics = metrics;
        }
        
        public Map<TopicPartition, Long> getLaggedOffsets() {
            Map<TopicPartition, Long> laggedOffsets = new HashMap<>();
            
            for (TopicPartition partition : assignedPartitions) {
                Long committed = committedOffsets.get(partition);
                Long end = endOffsets.get(partition);
                
                if (committed != null && end != null) {
                    long lag = end - committed;
                    laggedOffsets.put(partition, lag);
                }
            }
            
            return laggedOffsets;
        }
        
        public double getOverallLag() {
            return getLaggedOffsets().values().stream().mapToLong(Long::longValue).sum();
        }
        
        public boolean isHealthy() {
            return processingErrors == 0 || (double) processingErrors / messagesConsumed < 0.05; // 错误率小于5%
        }
        
        // Getters
        public String getClientId() { return clientId; }
        public Set<TopicPartition> getAssignedPartitions() { return assignedPartitions; }
        public Map<TopicPartition, Long> getCommittedOffsets() { return committedOffsets; }
        public Map<TopicPartition, Long> getEndOffsets() { return endOffsets; }
        public long getMessagesConsumed() { return messagesConsumed; }
        public long getProcessingErrors() { return processingErrors; }
        public ConsumerMetrics getMetrics() { return metrics; }
        
        @Override
        public String toString() {
            return String.format("ConsumerState{clientId='%s', partitions=%d, consumed=%d, errors=%d, lag=%.0f, healthy=%s}", 
                clientId, assignedPartitions.size(), messagesConsumed, processingErrors, getOverallLag(), isHealthy());
        }
    }
    
    /**
     * 消费者指标
     */
    public static class ConsumerMetrics {
        private final AtomicLong totalMessagesProcessed = new AtomicLong(0);
        private final AtomicLong totalBytesProcessed = new AtomicLong(0);
        private final Map<TopicPartition, AtomicLong> partitionMessageCount = new ConcurrentHashMap<>();
        private final Map<TopicPartition, AtomicLong> partitionByteCount = new ConcurrentHashMap<>();
        private final AtomicLong processingTimeTotal = new AtomicLong(0);
        
        public void recordMessageProcessed(ConsumerRecord<String, String> record) {
            totalMessagesProcessed.incrementAndGet();
            totalBytesProcessed.addAndGet(record.serializedValueSize() != -1 ? record.serializedValueSize() : record.value().length());
            
            partitionMessageCount.computeIfAbsent(record.topicPartition(), k -> new AtomicLong(0)).incrementAndGet();
            partitionByteCount.computeIfAbsent(record.topicPartition(), k -> new AtomicLong(0))
                .addAndGet(record.serializedValueSize() != -1 ? record.serializedValueSize() : record.value().length());
        }
        
        public void recordProcessingTime(long processingTime) {
            processingTimeTotal.addAndGet(processingTime);
        }
        
        public double getAverageProcessingTime() {
            long total = totalMessagesProcessed.get();
            return total > 0 ? (double) processingTimeTotal.get() / total : 0.0;
        }
        
        public double getThroughputPerSecond() {
            return getAverageProcessingTime() > 0 ? 1000.0 / getAverageProcessingTime() : 0.0;
        }
        
        public Map<TopicPartition, Double> getPartitionDistribution() {
            Map<TopicPartition, Double> distribution = new HashMap<>();
            long total = totalMessagesProcessed.get();
            
            if (total == 0) return distribution;
            
            partitionMessageCount.forEach((partition, count) -> {
                distribution.put(partition, (double) count.get() / total);
            });
            
            return distribution;
        }
        
        // Getters
        public long getTotalMessagesProcessed() { return totalMessagesProcessed.get(); }
        public long getTotalBytesProcessed() { return totalBytesProcessed.get(); }
        public Map<TopicPartition, AtomicLong> getPartitionMessageCount() { return partitionMessageCount; }
        public Map<TopicPartition, AtomicLong> getPartitionByteCount() { return partitionByteCount; }
        public long getProcessingTimeTotal() { return processingTimeTotal.get(); }
    }
}
```

### 自定义反序列化器

```java
package com.kafka.tutorial.consumer;

import org.apache.kafka.common.serialization.Deserializer;
import org.apache.kafka.common.errors.SerializationException;

import com.fasterxml.jackson.databind.ObjectMapper;
import com.fasterxml.jackson.databind.JsonNode;

import java.util.Map;
import java.util.function.Function;

/**
 * 自定义反序列化器实现
 */
public class CustomDeserializers {
    
    /**
     * JSON反序列化器
     */
    public static class JsonDeserializer<T> implements Deserializer<T> {
        private final Class<T> targetType;
        private final ObjectMapper objectMapper;
        
        public JsonDeserializer(Class<T> targetType) {
            this.targetType = targetType;
            this.objectMapper = new ObjectMapper();
        }
        
        @Override
        public void configure(Map<String, ?> configs, boolean isKey) {
            // 配置反序列化器
        }
        
        @Override
        public T deserialize(String topic, byte[] data) {
            if (data == null) {
                return null;
            }
            
            try {
                return objectMapper.readValue(data, targetType);
            } catch (Exception e) {
                throw new SerializationException("反序列化JSON数据失败", e);
            }
        }
        
        @Override
        public void close() {
            // 清理资源
        }
    }
    
    /**
     * Avro反序列化器
     */
    public static class AvroDeserializer<T> implements Deserializer<T> {
        private final Class<T> targetType;
        private final org.apache.avro.io.DecoderFactory decoderFactory;
        
        public AvroDeserializer(Class<T> targetType) {
            this.targetType = targetType;
            this.decoderFactory = new org.apache.avro.io.DecoderFactory();
        }
        
        @Override
        public void configure(Map<String, ?> configs, boolean isKey) {
            // 配置反序列化器
        }
        
        @Override
        public T deserialize(String topic, byte[] data) {
            if (data == null) {
                return null;
            }
            
            try {
                // 这里应该实现实际的Avro反序列化逻辑
                // 由于涉及复杂的Avro schema，暂时返回null
                return null;
            } catch (Exception e) {
                throw new SerializationException("反序列化Avro数据失败", e);
            }
        }
        
        @Override
        public void close() {
            // 清理资源
        }
    }
    
    /**
     * Protobuf反序列化器
     */
    public static class ProtobufDeserializer<T> implements Deserializer<T> {
        private final com.google.protobuf.Parser<T> parser;
        
        public ProtobufDeserializer(com.google.protobuf.Parser<T> parser) {
            this.parser = parser;
        }
        
        @Override
        public void configure(Map<String, ?> configs, boolean isKey) {
            // 配置反序列化器
        }
        
        @Override
        public T deserialize(String topic, byte[] data) {
            if (data == null) {
                return null;
            }
            
            try {
                return parser.parseFrom(data);
            } catch (Exception e) {
                throw new SerializationException("反序列化Protobuf数据失败", e);
            }
        }
        
        @Override
        public void close() {
            // 清理资源
        }
    }
    
    /**
     * 压缩数据反序列化器
     */
    public static class CompressedDeserializer<T> implements Deserializer<T> {
        private final Deserializer<T> innerDeserializer;
        private final java.util.zip.Inflater inflater;
        
        public CompressedDeserializer(Deserializer<T> innerDeserializer) {
            this.innerDeserializer = innerDeserializer;
            this.inflater = new java.util.zip.Inflater();
        }
        
        @Override
        public void configure(Map<String, ?> configs, boolean isKey) {
            innerDeserializer.configure(configs, isKey);
        }
        
        @Override
        public T deserialize(String topic, byte[] data) {
            if (data == null) {
                return null;
            }
            
            try {
                // 解压缩数据
                byte[] decompressedData = decompress(data);
                return innerDeserializer.deserialize(topic, decompressedData);
            } catch (Exception e) {
                throw new SerializationException("解压缩数据失败", e);
            }
        }
        
        private byte[] decompress(byte[] compressedData) {
            ByteArrayOutputStream outputStream = new ByteArrayOutputStream();
            byte[] buffer = new byte[1024];
            
            try {
                inflater.setInput(compressedData);
                
                while (!inflater.finished()) {
                    int count = inflater.inflate(buffer);
                    outputStream.write(buffer, 0, count);
                }
                
                return outputStream.toByteArray();
            } catch (Exception e) {
                throw new SerializationException("解压数据失败", e);
            }
        }
        
        @Override
        public void close() {
            innerDeserializer.close();
            inflater.end();
        }
    }
    
    /**
     * 容错反序列化器
     */
    public static class FaultTolerantDeserializer<T> implements Deserializer<T> {
        private final Deserializer<T> primaryDeserializer;
        private final Deserializer<T> fallbackDeserializer;
        private final Function<byte[], T> customDeserializer;
        
        public FaultTolerantDeserializer(Deserializer<T> primaryDeserializer, 
                                       Deserializer<T> fallbackDeserializer) {
            this(primaryDeserializer, fallbackDeserializer, null);
        }
        
        public FaultTolerantDeserializer(Deserializer<T> primaryDeserializer,
                                       Function<byte[], T> customDeserializer) {
            this(primaryDeserializer, null, customDeserializer);
        }
        
        private FaultTolerantDeserializer(Deserializer<T> primaryDeserializer,
                                        Deserializer<T> fallbackDeserializer,
                                        Function<byte[], T> customDeserializer) {
            this.primaryDeserializer = primaryDeserializer;
            this.fallbackDeserializer = fallbackDeserializer;
            this.customDeserializer = customDeserializer;
        }
        
        @Override
        public void configure(Map<String, ?> configs, boolean isKey) {
            primaryDeserializer.configure(configs, isKey);
            if (fallbackDeserializer != null) {
                fallbackDeserializer.configure(configs, isKey);
            }
        }
        
        @Override
        public T deserialize(String topic, byte[] data) {
            if (data == null) {
                return null;
            }
            
            // 尝试主反序列化器
            try {
                return primaryDeserializer.deserialize(topic, data);
            } catch (Exception e) {
                System.err.println("主反序列化器失败，尝试备用反序列化器: " + e.getMessage());
                
                // 尝试备用反序列化器
                if (fallbackDeserializer != null) {
                    try {
                        return fallbackDeserializer.deserialize(topic, data);
                    } catch (Exception fallbackError) {
                        System.err.println("备用反序列化器也失败，尝试自定义反序列化器");
                    }
                }
                
                // 尝试自定义反序列化器
                if (customDeserializer != null) {
                    try {
                        return customDeserializer.apply(data);
                    } catch (Exception customError) {
                        System.err.println("所有反序列化器都失败");
                    }
                }
                
                throw new SerializationException("所有反序列化方法都失败了");
            }
        }
        
        @Override
        public void close() {
            primaryDeserializer.close();
            if (fallbackDeserializer != null) {
                fallbackDeserializer.close();
            }
        }
    }
    
    /**
     * 反序列化器工厂
     */
    public static class DeserializerFactory {
        
        public static <T> Deserializer<T> createJsonDeserializer(Class<T> targetType) {
            return new JsonDeserializer<>(targetType);
        }
        
        public static <T> Deserializer<T> createAvroDeserializer(Class<T> targetType) {
            return new AvroDeserializer<>(targetType);
        }
        
        public static <T> Deserializer<T> createProtobufDeserializer(com.google.protobuf.Parser<T> parser) {
            return new ProtobufDeserializer<>(parser);
        }
        
        public static <T> Deserializer<T> createCompressedDeserializer(Deserializer<T> innerDeserializer) {
            return new CompressedDeserializer<>(innerDeserializer);
        }
        
        public static <T> Deserializer<T> createFaultTolerantDeserializer(Deserializer<T> primaryDeserializer,
                                                                         Deserializer<T> fallbackDeserializer) {
            return new FaultTolerantDeserializer<>(primaryDeserializer, fallbackDeserializer);
        }
        
        public static <T> Deserializer<T> createFaultTolerantDeserializer(Deserializer<T> primaryDeserializer,
                                                                         Function<byte[], T> customDeserializer) {
            return new FaultTolerantDeserializer<>(primaryDeserializer, customDeserializer);
        }
    }
    
    /**
     * 复杂对象反序列化器
     */
    public static class ComplexObjectDeserializer<T> implements Deserializer<T> {
        private final Function<Map<String, Object>, T> objectConstructor;
        private final Function<byte[], Map<String, Object>> dataParser;
        
        public ComplexObjectDeserializer(Function<Map<String, Object>, T> objectConstructor,
                                       Function<byte[], Map<String, Object>> dataParser) {
            this.objectConstructor = objectConstructor;
            this.dataParser = dataParser;
        }
        
        @Override
        public void configure(Map<String, ?> configs, boolean isKey) {
            // 配置反序列化器
        }
        
        @Override
        public T deserialize(String topic, byte[] data) {
            if (data == null) {
                return null;
            }
            
            try {
                Map<String, Object> rawData = dataParser.apply(data);
                return objectConstructor.apply(rawData);
            } catch (Exception e) {
                throw new SerializationException("复杂对象反序列化失败", e);
            }
        }
        
        @Override
        public void close() {
            // 清理资源
        }
    }
}
```

---

## 消费者组与负载均衡

### 消费者组管理实现

```java
package com.kafka.tutorial.consumer;

import org.apache.kafka.clients.consumer.*;
import org.apache.kafka.common.TopicPartition;
import org.apache.kafka.common.errors.AuthorizationException;
import org.apache.kafka.common.errors.FencedInstanceIdException;
import org.apache.kafka.common.errors.WakeupException;

import java.util.*;
import java.util.concurrent.*;
import java.util.concurrent.atomic.AtomicBoolean;
import java.util.concurrent.atomic.AtomicInteger;
import java.util.concurrent.atomic.AtomicLong;
import java.util.concurrent.locks.ReentrantReadWriteLock;

/**
 * 消费者组管理实现
 */
public class ConsumerGroupManager {
    private final String groupId;
    private final Map<String, KafkaConsumer<String, String>> consumers;
    private final ExecutorService consumerExecutor;
    private final ConsumerGroupConfig config;
    private final ConsumerGroupMetrics groupMetrics;
    private final AtomicBoolean running = new AtomicBoolean(false);
    private final ReentrantReadWriteLock metricsLock = new ReentrantReadWriteLock();
    
    public ConsumerGroupManager(String groupId, ConsumerGroupConfig config) {
        this.groupId = groupId;
        this.config = config;
        this.consumers = new ConcurrentHashMap<>();
        this.consumerExecutor = Executors.newFixedThreadPool(config.getConsumerThreads());
        this.groupMetrics = new ConsumerGroupMetrics(groupId);
    }
    
    /**
     * 消费者组配置
     */
    public static class ConsumerGroupConfig {
        private final int consumerThreads;
        private final int partitionsPerConsumer;
        private final boolean enableRebalanceListener;
        private final long rebalanceTimeoutMs;
        private final long heartbeatIntervalMs;
        private final long sessionTimeoutMs;
        
        public ConsumerGroupConfig(Builder builder) {
            this.consumerThreads = builder.consumerThreads;
            this.partitionsPerConsumer = builder.partitionsPerConsumer;
            this.enableRebalanceListener = builder.enableRebalanceListener;
            this.rebalanceTimeoutMs = builder.rebalanceTimeoutMs;
            this.heartbeatIntervalMs = builder.heartbeatIntervalMs;
            this.sessionTimeoutMs = builder.sessionTimeoutMs;
        }
        
        public static class Builder {
            private int consumerThreads = 4;
            private int partitionsPerConsumer = 2;
            private boolean enableRebalanceListener = true;
            private long rebalanceTimeoutMs = 60000; // 60秒
            private long heartbeatIntervalMs = 3000; // 3秒
            private long sessionTimeoutMs = 30000; // 30秒
            
            public Builder consumerThreads(int consumerThreads) {
                this.consumerThreads = consumerThreads;
                return this;
            }
            
            public Builder partitionsPerConsumer(int partitionsPerConsumer) {
                this.partitionsPerConsumer = partitionsPerConsumer;
                return this;
            }
            
            public Builder enableRebalanceListener(boolean enableRebalanceListener) {
                this.enableRebalanceListener = enableRebalanceListener;
                return this;
            }
            
            public Builder rebalanceTimeoutMs(long rebalanceTimeoutMs) {
                this.rebalanceTimeoutMs = rebalanceTimeoutMs;
                return this;
            }
            
            public Builder heartbeatIntervalMs(long heartbeatIntervalMs) {
                this.heartbeatIntervalMs = heartbeatIntervalMs;
                return this;
            }
            
            public Builder sessionTimeoutMs(long sessionTimeoutMs) {
                this.sessionTimeoutMs = sessionTimeoutMs;
                return this;
            }
            
            public ConsumerGroupConfig build() {
                return new ConsumerGroupConfig(this);
            }
        }
        
        // Getters
        public int getConsumerThreads() { return consumerThreads; }
        public int getPartitionsPerConsumer() { return partitionsPerConsumer; }
        public boolean isEnableRebalanceListener() { return enableRebalanceListener; }
        public long getRebalanceTimeoutMs() { return rebalanceTimeoutMs; }
        public long getHeartbeatIntervalMs() { return heartbeatIntervalMs; }
        public long getSessionTimeoutMs() { return sessionTimeoutMs; }
    }
    
    /**
     * 启动消费者组
     */
    public void start(String... topics) {
        if (!running.compareAndSet(false, true)) {
            throw new IllegalStateException("消费者组已经在运行中");
        }
        
        System.out.println("启动消费者组: " + groupId + ", 主题: " + Arrays.toString(topics));
        
        // 创建和启动消费者线程
        for (int i = 0; i < config.getConsumerThreads(); i++) {
            String consumerId = groupId + "-consumer-" + i;
            KafkaConsumer<String, String> consumer = createConsumer(consumerId);
            consumers.put(consumerId, consumer);
            
            ConsumerWorker worker = new ConsumerWorker(consumer, consumerId, topics);
            consumerExecutor.submit(worker);
        }
        
        // 启动监控线程
        startMonitoring();
    }
    
    /**
     * 创建Kafka消费者
     */
    private KafkaConsumer<String, String> createConsumer(String clientId) {
        Properties configs = BasicKafkaConsumer.createBasicConfigs();
        configs.put(ConsumerConfig.CLIENT_ID_CONFIG, clientId);
        configs.put(ConsumerConfig.GROUP_ID_CONFIG, groupId);
        configs.put(ConsumerConfig.HEARTBEAT_INTERVAL_MS_CONFIG, config.getHeartbeatIntervalMs());
        configs.put(ConsumerConfig.SESSION_TIMEOUT_MS_CONFIG, config.getSessionTimeoutMs());
        configs.put(ConsumerConfig.MAX_POLL_INTERVAL_MS_CONFIG, config.getRebalanceTimeoutMs());
        configs.put(ConsumerConfig.ENABLE_AUTO_COMMIT_CONFIG, false);
        
        KafkaConsumer<String, String> consumer = new KafkaConsumer<>(configs);
        
        if (config.isEnableRebalanceListener()) {
            ConsumerRebalanceListener rebalanceListener = new ConsumerGroupRebalanceListener();
            consumer.subscribe(Arrays.asList("topic1", "topic2"), rebalanceListener);
        }
        
        return consumer;
    }
    
    /**
     * 消费者组重平衡监听器
     */
    private class ConsumerGroupRebalanceListener implements ConsumerRebalanceListener {
        @Override
        public void onPartitionsRevoked(Collection<TopicPartition> partitions) {
            System.out.println("分区被回收: " + partitions);
            
            // 在分区重新分配前提交偏移量
            for (KafkaConsumer<String, String> consumer : consumers.values()) {
                try {
                    consumer.commitSync();
                    System.out.println("已提交当前偏移量");
                } catch (Exception e) {
                    System.err.println("提交偏移量失败: " + e.getMessage());
                }
            }
            
            groupMetrics.recordRebalance("revoked", partitions.size());
        }
        
        @Override
        public void onPartitionsAssigned(Collection<TopicPartition> partitions) {
            System.out.println("分区被分配: " + partitions);
            
            // 分区分配后的初始化工作
            for (TopicPartition partition : partitions) {
                groupMetrics.recordPartitionAssignment(partition);
            }
            
            groupMetrics.recordRebalance("assigned", partitions.size());
        }
    }
    
    /**
     * 消费者工作线程
     */
    private class ConsumerWorker implements Runnable {
        private final KafkaConsumer<String, String> consumer;
        private final String consumerId;
        private final String[] topics;
        private final AtomicBoolean stopped = new AtomicBoolean(false);
        
        public ConsumerWorker(KafkaConsumer<String, String> consumer, String consumerId, String[] topics) {
            this.consumer = consumer;
            this.consumerId = consumerId;
            this.topics = topics;
        }
        
        @Override
        public void run() {
            System.out.println("启动消费者工作线程: " + consumerId);
            
            try {
                consumer.subscribe(Arrays.asList(topics));
                
                while (!stopped.get() && running.get()) {
                    try {
                        ConsumerRecords<String, String> records = consumer.poll(Duration.ofSeconds(1));
                        
                        if (!records.isEmpty()) {
                            processRecords(records);
                            commitOffsets(records);
                        }
                        
                    } catch (WakeupException e) {
                        // 忽略唤醒异常
                        System.out.println("消费者 " + consumerId + " 接收到唤醒信号");
                    } catch (Exception e) {
                        System.err.println("消费者 " + consumerId + " 处理异常: " + e.getMessage());
                        groupMetrics.recordError(consumerId, e);
                        
                        // 短暂休眠后重试
                        Thread.sleep(1000);
                    }
                }
                
            } catch (Exception e) {
                System.err.println("消费者工作线程 " + consumerId + " 异常退出: " + e.getMessage());
            } finally {
                try {
                    consumer.close();
                    System.out.println("消费者工作线程 " + consumerId + " 已关闭");
                } catch (Exception e) {
                    System.err.println("关闭消费者 " + consumerId + " 时发生错误: " + e.getMessage());
                }
            }
        }
        
        private void processRecords(ConsumerRecords<String, String> records) {
            long startTime = System.currentTimeMillis();
            int messageCount = 0;
            
            try {
                for (ConsumerRecord<String, String> record : records) {
                    // 处理消息的逻辑
                    processMessage(record);
                    messageCount++;
                    groupMetrics.recordMessageProcessed(record);
                }
                
                long processingTime = System.currentTimeMillis() - startTime;
                groupMetrics.recordProcessingTime(consumerId, processingTime);
                System.out.println("消费者 " + consumerId + " 处理了 " + messageCount + " 条消息，耗时 " + processingTime + "ms");
                
            } catch (Exception e) {
                System.err.println("处理消息时发生错误: " + e.getMessage());
                groupMetrics.recordError(consumerId, e);
            }
        }
        
        private void processMessage(ConsumerRecord<String, String> record) {
            // 这里实现具体的消息处理逻辑
            // 例如：数据转换、存储、计算等
            
            // 模拟消息处理
            try {
                Thread.sleep(1); // 模拟处理时间
            } catch (InterruptedException e) {
                Thread.currentThread().interrupt();
            }
            
            // 更新组统计信息
            groupMetrics.recordMessageProcessed(record);
        }
        
        private void commitOffsets(ConsumerRecords<String, String> records) {
            try {
                Map<TopicPartition, OffsetAndMetadata> offsetsToCommit = new HashMap<>();
                
                for (TopicPartition partition : records.partitions()) {
                    List<ConsumerRecord<String, String>> partitionRecords = records.records(partition);
                    long lastOffset = partitionRecords.get(partitionRecords.size() - 1).offset();
                    offsetsToCommit.put(partition, new OffsetAndMetadata(lastOffset + 1));
                }
                
                consumer.commitAsync(offsetsToCommit, (offsets, exception) -> {
                    if (exception != null) {
                        System.err.println("消费者 " + consumerId + " 提交偏移量失败: " + exception.getMessage());
                        groupMetrics.recordCommitError(consumerId, exception);
                    } else {
                        groupMetrics.recordCommitSuccess(consumerId, offsets.size());
                    }
                });
                
            } catch (Exception e) {
                System.err.println("消费者 " + consumerId + " 提交偏移量时发生异常: " + e.getMessage());
                groupMetrics.recordCommitError(consumerId, e);
            }
        }
        
        public void stop() {
            stopped.set(true);
            consumer.wakeup();
        }
    }
    
    /**
     * 启动监控线程
     */
    private void startMonitoring() {
        ScheduledExecutorService monitorExecutor = Executors.newSingleThreadScheduledExecutor();
        
        monitorExecutor.scheduleAtFixedRate(() -> {
            try {
                ConsumerGroupStatus status = getGroupStatus();
                printGroupStatus(status);
            } catch (Exception e) {
                System.err.println("获取组状态时发生错误: " + e.getMessage());
            }
        }, 5, 30, TimeUnit.SECONDS); // 5秒后开始，每30秒检查一次
        
        // 添加关闭钩子
        Runtime.getRuntime().addShutdownHook(new Thread(() -> {
            monitorExecutor.shutdown();
            try {
                if (!monitorExecutor.awaitTermination(5, TimeUnit.SECONDS)) {
                    monitorExecutor.shutdownNow();
                }
            } catch (InterruptedException e) {
                monitorExecutor.shutdownNow();
                Thread.currentThread().interrupt();
            }
        }));
    }
    
    /**
     * 获取消费者组状态
     */
    public ConsumerGroupStatus getGroupStatus() {
        Map<String, ConsumerInstanceStatus> instanceStatuses = new HashMap<>();
        
        for (Map.Entry<String, KafkaConsumer<String, String>> entry : consumers.entrySet()) {
            String consumerId = entry.getKey();
            KafkaConsumer<String, String> consumer = entry.getValue();
            
            try {
                Set<TopicPartition> assignedPartitions = consumer.assignment();
                Map<TopicPartition, Long> committedOffsets = new HashMap<>();
                Map<TopicPartition, Long> endOffsets = new HashMap<>();
                
                for (TopicPartition partition : assignedPartitions) {
                    OffsetAndMetadata committed = consumer.committed(partition);
                    Long endOffset = consumer.endOffsets(Collections.singletonList(partition)).get(partition);
                    
                    if (committed != null) {
                        committedOffsets.put(partition, committed.offset());
                    }
                    endOffsets.put(partition, endOffset);
                }
                
                ConsumerInstanceStatus status = new ConsumerInstanceStatus(
                    consumerId, assignedPartitions, committedOffsets, endOffsets, true);
                instanceStatuses.put(consumerId, status);
                
            } catch (Exception e) {
                ConsumerInstanceStatus errorStatus = new ConsumerInstanceStatus(
                    consumerId, Collections.emptySet(), Collections.emptyMap(), Collections.emptyMap(), false);
                errorStatus.setErrorMessage(e.getMessage());
                instanceStatuses.put(consumerId, errorStatus);
            }
        }
        
        return new ConsumerGroupStatus(groupId, instanceStatuses, groupMetrics);
    }
    
    /**
     * 打印组状态
     */
    private void printGroupStatus(ConsumerGroupStatus status) {
        System.out.println("\n=== 消费者组状态 ===");
        System.out.println("组ID: " + status.getGroupId());
        System.out.println("总实例数: " + status.getInstanceStatuses().size());
        System.out.println("活跃实例数: " + status.getActiveInstanceCount());
        System.out.println("总消息数: " + status.getTotalMessagesProcessed());
        System.out.println("错误数: " + status.getTotalErrors());
        System.out.println("重平衡次数: " + status.getTotalRebalances());
        
        System.out.println("\n实例详情:");
        for (ConsumerInstanceStatus instance : status.getInstanceStatuses().values()) {
            System.out.println("  " + instance);
        }
        
        System.out.println("==================\n");
    }
    
    /**
     * 停止消费者组
     */
    public void stop() {
        if (!running.compareAndSet(true, false)) {
            return;
        }
        
        System.out.println("停止消费者组: " + groupId);
        
        // 停止所有消费者
        for (KafkaConsumer<String, String> consumer : consumers.values()) {
            consumer.wakeup();
        }
        
        consumerExecutor.shutdown();
        try {
            if (!consumerExecutor.awaitTermination(30, TimeUnit.SECONDS)) {
                consumerExecutor.shutdownNow();
            }
        } catch (InterruptedException e) {
            consumerExecutor.shutdownNow();
            Thread.currentThread().interrupt();
        }
        
        // 关闭所有消费者连接
        for (KafkaConsumer<String, String> consumer : consumers.values()) {
            try {
                consumer.close(Duration.ofSeconds(10));
            } catch (Exception e) {
                System.err.println("关闭消费者时发生错误: " + e.getMessage());
            }
        }
        
        System.out.println("消费者组 " + groupId + " 已停止");
    }
    
    /**
     * 消费者实例状态
     */
    public static class ConsumerInstanceStatus {
        private final String consumerId;
        private final Set<TopicPartition> assignedPartitions;
        private final Map<TopicPartition, Long> committedOffsets;
        private final Map<TopicPartition, Long> endOffsets;
        private final boolean isActive;
        private String errorMessage;
        
        public ConsumerInstanceStatus(String consumerId, Set<TopicPartition> assignedPartitions,
                                    Map<TopicPartition, Long> committedOffsets,
                                    Map<TopicPartition, Long> endOffsets, boolean isActive) {
            this.consumerId = consumerId;
            this.assignedPartitions = assignedPartitions;
            this.committedOffsets = new HashMap<>(committedOffsets);
            this.endOffsets = new HashMap<>(endOffsets);
            this.isActive = isActive;
        }
        
        public Map<TopicPartition, Long> getLaggedOffsets() {
            Map<TopicPartition, Long> laggedOffsets = new HashMap<>();
            
            for (TopicPartition partition : assignedPartitions) {
                Long committed = committedOffsets.get(partition);
                Long end = endOffsets.get(partition);
                
                if (committed != null && end != null) {
                    long lag = end - committed;
                    laggedOffsets.put(partition, lag);
                }
            }
            
            return laggedOffsets;
        }
        
        public long getTotalLag() {
            return getLaggedOffsets().values().stream().mapToLong(Long::longValue).sum();
        }
        
        public void setErrorMessage(String errorMessage) {
            this.errorMessage = errorMessage;
        }
        
        // Getters
        public String getConsumerId() { return consumerId; }
        public Set<TopicPartition> getAssignedPartitions() { return assignedPartitions; }
        public Map<TopicPartition, Long> getCommittedOffsets() { return committedOffsets; }
        public Map<TopicPartition, Long> getEndOffsets() { return endOffsets; }
        public boolean isActive() { return isActive; }
        public String getErrorMessage() { return errorMessage; }
        
        @Override
        public String toString() {
            StringBuilder sb = new StringBuilder();
            sb.append(consumerId).append(" (").append(isActive ? "活跃" : "非活跃").append(")");
            sb.append(" - 分区数: ").append(assignedPartitions.size());
            sb.append(" - 延迟: ").append(getTotalLag());
            
            if (errorMessage != null) {
                sb.append(" - 错误: ").append(errorMessage);
            }
            
            return sb.toString();
        }
    }
    
    /**
     * 消费者组状态
     */
    public static class ConsumerGroupStatus {
        private final String groupId;
        private final Map<String, ConsumerInstanceStatus> instanceStatuses;
        private final ConsumerGroupMetrics metrics;
        
        public ConsumerGroupStatus(String groupId, Map<String, ConsumerInstanceStatus> instanceStatuses,
                                 ConsumerGroupMetrics metrics) {
            this.groupId = groupId;
            this.instanceStatuses = new HashMap<>(instanceStatuses);
            this.metrics = metrics;
        }
        
        public int getActiveInstanceCount() {
            return (int) instanceStatuses.values().stream()
                .filter(ConsumerInstanceStatus::isActive)
                .count();
        }
        
        public long getTotalMessagesProcessed() {
            return metrics.getTotalMessagesProcessed();
        }
        
        public long getTotalErrors() {
            return metrics.getTotalErrors();
        }
        
        public int getTotalRebalances() {
            return metrics.getTotalRebalances();
        }
        
        public Map<String, Long> getInstanceMessageCounts() {
            return metrics.getInstanceMessageCounts();
        }
        
        // Getters
        public String getGroupId() { return groupId; }
        public Map<String, ConsumerInstanceStatus> getInstanceStatuses() { return instanceStatuses; }
        public ConsumerGroupMetrics getMetrics() { return metrics; }
    }
    
    /**
     * 消费者组指标
     */
    public static class ConsumerGroupMetrics {
        private final String groupId;
        private final AtomicLong totalMessagesProcessed = new AtomicLong(0);
        private final AtomicLong totalErrors = new AtomicLong(0);
        private final AtomicLong totalRebalances = new AtomicLong(0);
        private final Map<String, AtomicLong> instanceMessageCounts = new ConcurrentHashMap<>();
        private final Map<String, AtomicLong> instanceErrorCounts = new ConcurrentHashMap<>();
        private final Map<String, AtomicLong> instanceRebalanceCounts = new ConcurrentHashMap<>();
        
        public ConsumerGroupMetrics(String groupId) {
            this.groupId = groupId;
        }
        
        public void recordMessageProcessed(ConsumerRecord<String, String> record) {
            totalMessagesProcessed.incrementAndGet();
            
            String consumerId = Thread.currentThread().getName();
            instanceMessageCounts.computeIfAbsent(consumerId, k -> new AtomicLong(0)).incrementAndGet();
        }
        
        public void recordError(String consumerId, Exception error) {
            totalErrors.incrementAndGet();
            instanceErrorCounts.computeIfAbsent(consumerId, k -> new AtomicLong(0)).incrementAndGet();
        }
        
        public void recordRebalance(String type, int partitionCount) {
            totalRebalances.incrementAndGet();
        }
        
        public void recordProcessingTime(String consumerId, long processingTime) {
            // 记录处理时间用于性能分析
        }
        
        public void recordCommitSuccess(String consumerId, int partitionCount) {
            // 记录提交成功的分区数
        }
        
        public void recordCommitError(String consumerId, Exception error) {
            instanceErrorCounts.computeIfAbsent(consumerId, k -> new AtomicLong(0)).incrementAndGet();
        }
        
        public void recordPartitionAssignment(TopicPartition partition) {
            // 记录分区分配信息
        }
        
        // Getters
        public String getGroupId() { return groupId; }
        public long getTotalMessagesProcessed() { return totalMessagesProcessed.get(); }
        public long getTotalErrors() { return totalErrors.get(); }
        public int getTotalRebalances() { return totalRebalances.intValue(); }
        public Map<String, AtomicLong> getInstanceMessageCounts() { return instanceMessageCounts; }
        public Map<String, AtomicLong> getInstanceErrorCounts() { return instanceErrorCounts; }
        public Map<String, AtomicLong> getInstanceRebalanceCounts() { return instanceRebalanceCounts; }
    }
}
```

### 负载均衡策略

```java
package com.kafka.tutorial.consumer;

import org.apache.kafka.clients.consumer.ConsumerRebalanceListener;
import org.apache.kafka.common.TopicPartition;
import org.apache.kafka.common.serialization.StringDeserializer;

import java.util.*;
import java.util.concurrent.*;
import java.util.concurrent.atomic.AtomicLong;

/**
 * 消费者负载均衡策略实现
 */
public class LoadBalancedConsumers {
    
    /**
     * 基于性能的自适应负载均衡
     */
    public static class AdaptiveLoadBalancer {
        private final Map<String, ConsumerPerformance> consumerPerformance;
        private final Map<TopicPartition, String> partitionAssignments;
        private final AtomicLong totalAssignments = new AtomicLong(0);
        
        public AdaptiveLoadBalancer() {
            this.consumerPerformance = new ConcurrentHashMap<>();
            this.partitionAssignments = new ConcurrentHashMap<>();
        }
        
        /**
         * 消费者性能指标
         */
        public static class ConsumerPerformance {
            private final String consumerId;
            private final AtomicLong messagesProcessed = new AtomicLong(0);
            private final AtomicLong totalProcessingTime = new AtomicLong(0);
            private final AtomicLong errorCount = new AtomicLong(0);
            private final AtomicLong lastActivityTime = new AtomicLong(System.currentTimeMillis());
            
            public ConsumerPerformance(String consumerId) {
                this.consumerId = consumerId;
            }
            
            public void recordMessageProcessed(long processingTime) {
                messagesProcessed.incrementAndGet();
                totalProcessingTime.addAndGet(processingTime);
                lastActivityTime.set(System.currentTimeMillis());
            }
            
            public void recordError() {
                errorCount.incrementAndGet();
            }
            
            public double getThroughput() {
                long totalMessages = messagesProcessed.get();
                long totalTime = totalProcessingTime.get();
                return totalTime > 0 ? (double) totalMessages / totalTime : 0.0;
            }
            
            public double getErrorRate() {
                long totalMessages = messagesProcessed.get();
                long errors = errorCount.get();
                return totalMessages > 0 ? (double) errors / totalMessages : 0.0;
            }
            
            public long getLastActivityAge() {
                return System.currentTimeMillis() - lastActivityTime.get();
            }
            
            public boolean isHealthy() {
                return getErrorRate() < 0.1 && getLastActivityAge() < 30000; // 错误率<10%，30秒内活跃
            }
            
            public double getLoadScore() {
                // 综合评分：吞吐量权重70%，健康度权重30%
                double throughput = getThroughput();
                double healthScore = isHealthy() ? 1.0 : 0.0;
                
                return throughput * 0.7 + healthScore * 0.3;
            }
            
            // Getters
            public String getConsumerId() { return consumerId; }
            public long getMessagesProcessed() { return messagesProcessed.get(); }
            public long getTotalProcessingTime() { return totalProcessingTime.get(); }
            public long getErrorCount() { return errorCount.get(); }
            public long getLastActivityTime() { return lastActivityTime.get(); }
        }
        
        /**
         * 注册消费者
         */
        public void registerConsumer(String consumerId) {
            consumerPerformance.putIfAbsent(consumerId, new ConsumerPerformance(consumerId));
            System.out.println("注册消费者: " + consumerId);
        }
        
        /**
         * 记录消费者性能
         */
        public void recordConsumerPerformance(String consumerId, long processingTime) {
            ConsumerPerformance performance = consumerPerformance.get(consumerId);
            if (performance != null) {
                performance.recordMessageProcessed(processingTime);
            }
        }
        
        /**
         * 记录消费者错误
         */
        public void recordConsumerError(String consumerId) {
            ConsumerPerformance performance = consumerPerformance.get(consumerId);
            if (performance != null) {
                performance.recordError();
            }
        }
        
        /**
         * 智能分区重分配
         */
        public List<TopicPartition> rebalancePartitions(String consumerId, Collection<TopicPartition> availablePartitions) {
            List<TopicPartition> newAssignments = new ArrayList<>();
            List<String> healthyConsumers = getHealthyConsumers();
            
            if (healthyConsumers.isEmpty()) {
                System.err.println("没有健康的消费者可用");
                return newAssignments;
            }
            
            // 按负载评分排序消费者
            List<String> sortedConsumers = healthyConsumers.stream()
                .sorted((c1, c2) -> {
                    double score1 = consumerPerformance.get(c1).getLoadScore();
                    double score2 = consumerPerformance.get(c2).getLoadScore();
                    return Double.compare(score2, score1); // 降序排列
                })
                .collect(ArrayList::new, ArrayList::add, ArrayList::addAll);
            
            // 分配分区
            int totalPartitions = availablePartitions.size();
            int consumersCount = sortedConsumers.size();
            int partitionsPerConsumer = totalPartitions / consumersCount;
            
            // 对于当前消费者，增加其分配的分段数
            int targetIndex = sortedConsumers.indexOf(consumerId);
            if (targetIndex >= 0) {
                int extraPartitions = totalPartitions % consumersCount;
                int startIndex = targetIndex * partitionsPerConsumer + Math.min(targetIndex, extraPartitions);
                int endIndex = startIndex + partitionsPerConsumer + (targetIndex < extraPartitions ? 1 : 0);
                
                List<TopicPartition> partitionList = new ArrayList<>(availablePartitions);
                for (int i = startIndex; i < Math.min(endIndex, partitionList.size()); i++) {
                    if (i < partitionList.size()) {
                        newAssignments.add(partitionList.get(i));
                    }
                }
            }
            
            // 更新分区分配记录
            for (TopicPartition partition : newAssignments) {
                partitionAssignments.put(partition, consumerId);
            }
            
            totalAssignments.incrementAndGet();
            
            System.out.println("为消费者 " + consumerId + " 分配了 " + newAssignments.size() + " 个分区");
            return newAssignments;
        }
        
        /**
         * 获取健康消费者列表
         */
        private List<String> getHealthyConsumers() {
            return consumerPerformance.entrySet().stream()
                .filter(entry -> entry.getValue().isHealthy())
                .map(Map.Entry::getKey)
                .collect(ArrayList::new, ArrayList::add, ArrayList::addAll);
        }
        
        /**
         * 获取消费者状态报告
         */
        public LoadBalanceReport getLoadBalanceReport() {
            Map<String, ConsumerPerformance> performanceMap = new HashMap<>(consumerPerformance);
            Map<TopicPartition, String> currentAssignments = new HashMap<>(partitionAssignments);
            
            return new LoadBalanceReport(performanceMap, currentAssignments);
        }
        
        /**
         * 负载均衡报告
         */
        public static class LoadBalanceReport {
            private final Map<String, ConsumerPerformance> performanceMap;
            private final Map<TopicPartition, String> assignments;
            
            public LoadBalanceReport(Map<String, ConsumerPerformance> performanceMap,
                                   Map<TopicPartition, String> assignments) {
                this.performanceMap = performanceMap;
                this.assignments = assignments;
            }
            
            public double getAverageThroughput() {
                return performanceMap.values().stream()
                    .mapToDouble(ConsumerPerformance::getThroughput)
                    .average()
                    .orElse(0.0);
            }
            
            public double getThroughputVariance() {
                double avg = getAverageThroughput();
                return performanceMap.values().stream()
                    .mapToDouble(p -> Math.pow(p.getThroughput() - avg, 2))
                    .average()
                    .orElse(0.0);
            }
            
            public boolean isWellBalanced() {
                return getThroughputVariance() < 0.1; // 方差小于0.1认为平衡
            }
            
            // Getters
            public Map<String, ConsumerPerformance> getPerformanceMap() { return performanceMap; }
            public Map<TopicPartition, String> getAssignments() { return assignments; }
        }
    }
    
    /**
     * 智能重平衡监听器
     */
    public static class SmartRebalanceListener implements ConsumerRebalanceListener {
        private final AdaptiveLoadBalancer loadBalancer;
        private final String consumerId;
        private final Map<TopicPartition, Long> lastProcessedOffsets;
        
        public SmartRebalanceListener(AdaptiveLoadBalancer loadBalancer, String consumerId) {
            this.loadBalancer = loadBalancer;
            this.consumerId = consumerId;
            this.lastProcessedOffsets = new ConcurrentHashMap<>();
        }
        
        @Override
        public void onPartitionsRevoked(Collection<TopicPartition> partitions) {
            System.out.println("分区被回收: " + partitions + " (消费者: " + consumerId + ")");
            
            // 保存最后处理的偏移量
            for (TopicPartition partition : partitions) {
                Long lastOffset = lastProcessedOffsets.get(partition);
                if (lastOffset != null) {
                    System.out.println("分区 " + partition + " 最后处理偏移量: " + lastOffset);
                }
            }
            
            // 注册消费者到负载均衡器
            loadBalancer.registerConsumer(consumerId);
        }
        
        @Override
        public void onPartitionsAssigned(Collection<TopicPartition> partitions) {
            System.out.println("分区被分配: " + partitions + " (消费者: " + consumerId + ")");
            
            // 智能重新分配分区
            List<TopicPartition> optimalPartitions = loadBalancer.rebalancePartitions(consumerId, partitions);
            
            // 应用智能分配结果（这里只是示例，实际实现需要与Kafka的分配机制结合）
            for (TopicPartition partition : optimalPartitions) {
                System.out.println("消费者 " + consumerId + " 获得分区: " + partition);
            }
        }
        
        public void recordProcessing(TopicPartition partition, long processingTime) {
            lastProcessedOffsets.put(partition, processingTime);
            loadBalancer.recordConsumerPerformance(consumerId, processingTime);
        }
        
        public void recordError() {
            loadBalancer.recordConsumerError(consumerId);
        }
    }
    
    /**
     * 分区感知负载均衡
     */
    public static class PartitionAwareBalancer {
        private final Map<String, Set<TopicPartition>> consumerPartitions;
        private final Map<TopicPartition, Integer> partitionLoad;
        
        public PartitionAwareBalancer() {
            this.consumerPartitions = new ConcurrentHashMap<>();
            this.partitionLoad = new ConcurrentHashMap<>();
        }
        
        /**
         * 基于分区负载的均衡策略
         */
        public Map<String, List<TopicPartition>> calculateOptimalAssignment(Set<String> consumers,
                                                                           Set<TopicPartition> allPartitions) {
            Map<String, List<TopicPartition>> assignment = new HashMap<>();
            
            // 初始化每个消费者的分配列表
            for (String consumer : consumers) {
                assignment.put(consumer, new ArrayList<>());
                consumerPartitions.put(consumer, new HashSet<>());
            }
            
            // 按分区负载排序（负载高的分区优先分配）
            List<TopicPartition> sortedPartitions = allPartitions.stream()
                .sorted((p1, p2) -> Integer.compare(
                    partitionLoad.getOrDefault(p2, 0),
                    partitionLoad.getOrDefault(p1, 0)
                ))
                .collect(ArrayList::new, ArrayList::add, ArrayList::addAll);
            
            // 为每个分区找到最合适的消费者
            for (TopicPartition partition : sortedPartitions) {
                String bestConsumer = findBestConsumerForPartition(partition, consumers);
                if (bestConsumer != null) {
                    assignment.get(bestConsumer).add(partition);
                    consumerPartitions.get(bestConsumer).add(partition);
                }
            }
            
            return assignment;
        }
        
        /**
         * 为分区找到最合适的消费者
         */
        private String findBestConsumerForPartition(TopicPartition partition, Set<String> consumers) {
            String bestConsumer = null;
            int minLoad = Integer.MAX_VALUE;
            
            for (String consumer : consumers) {
                int currentLoad = consumerPartitions.get(consumer).size();
                
                // 考虑分区特性和消费者能力
                int adjustedLoad = calculateAdjustedLoad(consumer, partition, currentLoad);
                
                if (adjustedLoad < minLoad) {
                    minLoad = adjustedLoad;
                    bestConsumer = consumer;
                }
            }
            
            return bestConsumer;
        }
        
        /**
         * 计算调整后的负载（考虑分区特性）
         */
        private int calculateAdjustedLoad(String consumer, TopicPartition partition, int baseLoad) {
            // 基于消费者能力和分区特性的调整
            double adjustmentFactor = 1.0;
            
            // 如果消费者刚刚加入，略微降低其负载
            // 如果消费者性能好，可以增加其负载
            // 等等
            
            return (int) (baseLoad * adjustmentFactor);
        }
        
        /**
         * 更新分区负载
         */
        public void updatePartitionLoad(TopicPartition partition, int loadDelta) {
            partitionLoad.merge(partition, loadDelta, Integer::sum);
        }
        
        /**
         * 获取分配状态
         */
        public PartitionBalanceStatus getBalanceStatus() {
            Map<String, Integer> partitionCounts = new HashMap<>();
            
            for (Map.Entry<String, Set<TopicPartition>> entry : consumerPartitions.entrySet()) {
                partitionCounts.put(entry.getKey(), entry.getValue().size());
            }
            
            return new PartitionBalanceStatus(partitionCounts);
        }
        
        /**
         * 分区平衡状态
         */
        public static class PartitionBalanceStatus {
            private final Map<String, Integer> partitionCounts;
            
            public PartitionBalanceStatus(Map<String, Integer> partitionCounts) {
                this.partitionCounts = new HashMap<>(partitionCounts);
            }
            
            public double getBalanceScore() {
                if (partitionCounts.isEmpty()) return 0.0;
                
                int min = partitionCounts.values().stream().mapToInt(Integer::intValue).min().orElse(0);
                int max = partitionCounts.values().stream().mapToInt(Integer::intValue).max().orElse(0);
                
                return max > 0 ? (double) min / max : 1.0; // 平衡分数：0-1，1表示完全平衡
            }
            
            public boolean isBalanced() {
                return getBalanceScore() > 0.8; // 平衡分数大于0.8认为平衡
            }
            
            public Map<String, Integer> getPartitionCounts() { return partitionCounts; }
        }
    }
    
    /**
     * 负载均衡管理器
     */
    public static class LoadBalanceManager {
        private final AdaptiveLoadBalancer adaptiveBalancer;
        private final PartitionAwareBalancer partitionBalancer;
        
        public LoadBalanceManager() {
            this.adaptiveBalancer = new AdaptiveLoadBalancer();
            this.partitionBalancer = new PartitionAwareBalancer();
        }
        
        /**
         * 综合负载均衡决策
         */
        public LoadBalanceDecision makeLoadBalanceDecision(Set<String> consumers, Set<TopicPartition> allPartitions) {
            AdaptiveLoadBalancer.LoadBalanceReport adaptiveReport = adaptiveBalancer.getLoadBalanceReport();
            PartitionAwareBalancer.PartitionBalanceStatus partitionStatus = partitionBalancer.getBalanceStatus();
            
            // 分析当前状态
            boolean needsRebalancing = !adaptiveReport.isWellBalanced() || !partitionStatus.isBalanced();
            
            Map<String, List<TopicPartition>> newAssignments = null;
            
            if (needsRebalancing) {
                newAssignments = partitionBalancer.calculateOptimalAssignment(consumers, allPartitions);
            }
            
            return new LoadBalanceDecision(needsRebalancing, newAssignments, adaptiveReport, partitionStatus);
        }
        
        /**
         * 负载均衡决策
         */
        public static class LoadBalanceDecision {
            private final boolean needsRebalancing;
            private final Map<String, List<TopicPartition>> newAssignments;
            private final AdaptiveLoadBalancer.LoadBalanceReport adaptiveReport;
            private final PartitionAwareBalancer.PartitionBalanceStatus partitionStatus;
            
            public LoadBalanceDecision(boolean needsRebalancing,
                                     Map<String, List<TopicPartition>> newAssignments,
                                     AdaptiveLoadBalancer.LoadBalanceReport adaptiveReport,
                                     PartitionAwareBalancer.PartitionBalanceStatus partitionStatus) {
                this.needsRebalancing = needsRebalancing;
                this.newAssignments = newAssignments;
                this.adaptiveReport = adaptiveReport;
                this.partitionStatus = partitionStatus;
            }
            
            public String getDecisionReason() {
                if (!needsRebalancing) {
                    return "系统当前负载均衡，无需重平衡";
                }
                
                List<String> reasons = new ArrayList<>();
                
                if (!adaptiveReport.isWellBalanced()) {
                    reasons.add("消费者性能不均衡");
                }
                
                if (!partitionStatus.isBalanced()) {
                    reasons.add("分区分配不均衡");
                }
                
                return String.join(", ", reasons);
            }
            
            // Getters
            public boolean needsRebalancing() { return needsRebalancing; }
            public Map<String, List<TopicPartition>> getNewAssignments() { return newAssignments; }
            public AdaptiveLoadBalancer.LoadBalanceReport getAdaptiveReport() { return adaptiveReport; }
            public PartitionAwareBalancer.PartitionBalanceStatus getPartitionStatus() { return partitionStatus; }
        }
        
        // Getters
        public AdaptiveLoadBalancer getAdaptiveBalancer() { return adaptiveBalancer; }
        public PartitionAwareBalancer getPartitionBalancer() { return partitionBalancer; }
    }
}
```

---

## 偏移量管理与存储机制

### 偏移量管理实现

```java
package com.kafka.tutorial.consumer;

import org.apache.kafka.clients.consumer.*;
import org.apache.kafka.common.TopicPartition;
import org.apache.kafka.common.serialization.StringDeserializer;

import java.util.*;
import java.util.concurrent.*;
import java.util.concurrent.atomic.AtomicBoolean;
import java.util.concurrent.atomic.AtomicLong;
import java.util.concurrent.locks.ReentrantReadWriteLock;
import java.time.Duration;

/**
 * 高级偏移量管理实现
 */
public class OffsetManagement {
    private final String groupId;
    private final OffsetStorageBackend storageBackend;
    private final Map<TopicPartition, OffsetState> offsetStates;
    private final AtomicBoolean autoCommitEnabled;
    private final AtomicLong totalCommits = new AtomicLong(0);
    private final AtomicLong totalCommitErrors = new AtomicLong(0);
    
    public OffsetManagement(String groupId, OffsetStorageBackend storageBackend, boolean autoCommitEnabled) {
        this.groupId = groupId;
        this.storageBackend = storageBackend;
        this.offsetStates = new ConcurrentHashMap<>();
        this.autoCommitEnabled = new AtomicBoolean(autoCommitEnabled);
    }
    
    /**
     * 偏移量存储后端接口
     */
    public interface OffsetStorageBackend {
        void storeOffset(TopicPartition partition, long offset, String metadata);
        OffsetState getOffset(TopicPartition partition);
        void deleteOffset(TopicPartition partition);
        void deleteOffsetsForTopic(String topic);
        Map<TopicPartition, OffsetState> getAllOffsets();
        void close();
    }
    
    /**
     * 内存偏移量存储
     */
    public static class InMemoryOffsetStorage implements OffsetStorageBackend {
        private final Map<TopicPartition, OffsetState> offsets = new ConcurrentHashMap<>();
        
        @Override
        public void storeOffset(TopicPartition partition, long offset, String metadata) {
            offsets.put(partition, new OffsetState(partition, offset, metadata, System.currentTimeMillis()));
        }
        
        @Override
        public OffsetState getOffset(TopicPartition partition) {
            return offsets.get(partition);
        }
        
        @Override
        public void deleteOffset(TopicPartition partition) {
            offsets.remove(partition);
        }
        
        @Override
        public void deleteOffsetsForTopic(String topic) {
            offsets.keySet().removeIf(partition -> partition.topic().equals(topic));
        }
        
        @Override
        public Map<TopicPartition, OffsetState> getAllOffsets() {
            return new HashMap<>(offsets);
        }
        
        @Override
        public void close() {
            offsets.clear();
        }
    }
    
    /**
     * 文件系统偏移量存储
     */
    public static class FileSystemOffsetStorage implements OffsetStorageBackend {
        private final String baseDirectory;
        private final ExecutorService ioExecutor;
        
        public FileSystemOffsetStorage(String baseDirectory) {
            this.baseDirectory = baseDirectory;
            this.ioExecutor = Executors.newFixedThreadPool(2);
            
            // 创建基础目录
            java.nio.file.Path path = java.nio.file.Paths.get(baseDirectory);
            try {
                java.nio.file.Files.createDirectories(path);
            } catch (Exception e) {
                throw new RuntimeException("创建偏移量存储目录失败", e);
            }
        }
        
        @Override
        public void storeOffset(TopicPartition partition, long offset, String metadata) {
            try {
                String fileName = getOffsetFileName(partition);
                java.nio.file.Path filePath = java.nio.file.Paths.get(baseDirectory, fileName);
                
                String content = String.format("%d,%d,%s", System.currentTimeMillis(), offset, metadata);
                java.nio.file.Files.write(filePath, content.getBytes(), 
                    java.nio.file.StandardOpenOption.CREATE, java.nio.file.StandardOpenOption.TRUNCATE_EXISTING);
                
            } catch (Exception e) {
                throw new RuntimeException("存储偏移量失败", e);
            }
        }
        
        @Override
        public OffsetState getOffset(TopicPartition partition) {
            try {
                String fileName = getOffsetFileName(partition);
                java.nio.file.Path filePath = java.nio.file.Paths.get(baseDirectory, fileName);
                
                if (java.nio.file.Files.exists(filePath)) {
                    String content = new String(java.nio.file.Files.readAllBytes(filePath));
                    String[] parts = content.split(",", 3);
                    
                    if (parts.length >= 2) {
                        long lastUpdated = Long.parseLong(parts[0]);
                        long offset = Long.parseLong(parts[1]);
                        String metadata = parts.length > 2 ? parts[2] : "";
                        
                        return new OffsetState(partition, offset, metadata, lastUpdated);
                    }
                }
                
                return null;
            } catch (Exception e) {
                throw new RuntimeException("读取偏移量失败", e);
            }
        }
        
        @Override
        public void deleteOffset(TopicPartition partition) {
            try {
                String fileName = getOffsetFileName(partition);
                java.nio.file.Path filePath = java.nio.file.Paths.get(baseDirectory, fileName);
                java.nio.file.Files.deleteIfExists(filePath);
            } catch (Exception e) {
                throw new RuntimeException("删除偏移量失败", e);
            }
        }
        
        @Override
        public void deleteOffsetsForTopic(String topic) {
            try {
                java.nio.file.Path dir = java.nio.file.Paths.get(baseDirectory);
                java.nio.file.Files.list(dir)
                    .filter(path -> path.toString().contains(topic + "-"))
                    .forEach(path -> {
                        try {
                            java.nio.file.Files.delete(path);
                        } catch (Exception e) {
                            System.err.println("删除文件失败: " + path);
                        }
                    });
            } catch (Exception e) {
                throw new RuntimeException("删除主题偏移量失败", e);
            }
        }
        
        @Override
        public Map<TopicPartition, OffsetState> getAllOffsets() {
            Map<TopicPartition, OffsetState> allOffsets = new HashMap<>();
            
            try {
                java.nio.file.Path dir = java.nio.file.Paths.get(baseDirectory);
                java.nio.file.Files.list(dir)
                    .filter(path -> path.toString().endsWith(".offset"))
                    .forEach(path -> {
                        try {
                            String content = new String(java.nio.file.Files.readAllBytes(path));
                            String[] parts = content.split(",", 3);
                            
                            if (parts.length >= 2) {
                                String fileName = path.getFileName().toString();
                                TopicPartition partition = parseFileNameToPartition(fileName);
                                
                                long lastUpdated = Long.parseLong(parts[0]);
                                long offset = Long.parseLong(parts[1]);
                                String metadata = parts.length > 2 ? parts[2] : "";
                                
                                allOffsets.put(partition, new OffsetState(partition, offset, metadata, lastUpdated));
                            }
                        } catch (Exception e) {
                            System.err.println("解析偏移量文件失败: " + path);
                        }
                    });
            } catch (Exception e) {
                throw new RuntimeException("读取所有偏移量失败", e);
            }
            
            return allOffsets;
        }
        
        private String getOffsetFileName(TopicPartition partition) {
            return String.format("%s-%d.offset", partition.topic(), partition.partition());
        }
        
        private TopicPartition parseFileNameToPartition(String fileName) {
            // 解析文件名格式: topic-partition.offset
            String[] parts = fileName.replace(".offset", "").split("-");
            String topic = parts[0];
            int partition = Integer.parseInt(parts[1]);
            return new TopicPartition(topic, partition);
        }
        
        @Override
        public void close() {
            ioExecutor.shutdown();
        }
    }
    
    /**
     * 数据库偏移量存储
     */
    public static class DatabaseOffsetStorage implements OffsetStorageBackend {
        private final java.sql