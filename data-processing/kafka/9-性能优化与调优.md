# 第9章：性能优化与调优

## 9.1 性能调优概述

### 9.1.1 Kafka性能影响因素

Kafka的性能主要受以下几个关键因素影响：

- **网络带宽**：数据传输速度的根本限制
- **磁盘I/O**：顺序读写性能直接影响吞吐量
- **内存使用**：JVM堆内存和页缓存影响处理效率
- **CPU使用**：压缩、批量处理等计算密集型操作
- **网络延迟**：客户端到Broker的响应时间
- **分区数量**：影响并行处理能力和资源分配

### 9.1.2 性能调优目标

不同的业务场景有不同的性能目标：

- **高吞吐量**：追求每秒处理的最大消息数
- **低延迟**：最小化端到端的处理延迟
- **高可用性**：确保系统的稳定性和容错能力
- **资源效率**：在有限的硬件资源下获得最佳性能

### 9.1.3 调优原则

1. **系统性思维**：从整体架构出发进行调优
2. **度量驱动**：基于监控数据制定调优策略
3. **迭代优化**：逐步调整参数并验证效果
4. **负载均衡**：确保集群资源均匀分配

## 9.2 生产者性能优化

### 9.2.1 批量发送优化

```java
/**
 * 高性能生产者配置示例
 */
public class OptimizedProducerConfig {
    
    public static Properties getOptimizedProducerConfig() {
        Properties props = new Properties();
        
        // 基础配置
        props.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, "localhost:9092");
        props.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, StringSerializer.class.getName());
        props.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, StringSerializer.class.getName());
        
        // 性能优化配置
        // 1. 批量大小调优
        props.put(ProducerConfig.BATCH_SIZE_CONFIG, 32768); // 默认16384，适当增大
        
        // 2. 等待时间调优
        props.put(ProducerConfig.LINGER_MS_CONFIG, 20); // 默认0，增加等待时间以提高批量效率
        
        // 3. 内存缓冲区大小
        props.put(ProducerConfig.BUFFER_MEMORY_CONFIG, 67108864); // 64MB，适当增加
        
        // 4. 压缩类型
        props.put(ProducerConfig.COMPRESSION_TYPE_CONFIG, "lz4"); // 选择合适的压缩算法
        
        // 5. 请求超时
        props.put(ProducerConfig.REQUEST_TIMEOUT_MS_CONFIG, 30000);
        
        // 6. 重试策略
        props.put(ProducerConfig.RETRIES_CONFIG, 3);
        props.put(ProducerConfig.MAX_IN_FLIGHT_REQUESTS_PER_CONNECTION_CONFIG, 5);
        
        // 7. 客户端ID
        props.put(ProducerConfig.CLIENT_ID_CONFIG, "optimized-producer");
        
        return props**
 * 高性能生产者实现
 */
public class;
    }
}

/ HighPerformanceProducer {
    private final KafkaProducer<String, String> producer;
    private final String topic;
    private final CountDownLatch latch;
    
    public HighPerformanceProducer(String bootstrapServers, String topic) {
        Properties props = OptimizedProducerConfig.getOptimizedProducerConfig();
        props.setProperty(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, bootstrapServers);
        
        this.producer = new KafkaProducer<>(props);
        this.topic = topic;
        this.latch = new CountDownLatch(1);
    }
    
    /**
     * 异步发送消息（推荐）
     */
    public void sendAsyncMessages(List<String> messages) {
        for (String message : messages) {
            ProducerRecord<String, String> record = new ProducerRecord<>(topic, message);
            
            producer.send(record, (metadata, exception) -> {
                if (exception != null) {
                    System.err.println("发送失败: " + exception.getMessage());
                } else {
                    System.out.printf("消息发送到分区 %d，偏移量 %d%n", 
                        metadata.partition(), metadata.offset());
                }
            });
        }
    }
    
    /**
     * 同步发送消息（高可靠性要求时）
     */
    public void sendSyncMessages(List<String> messages) throws Exception {
        for (String message : messages) {
            ProducerRecord<String, String> record = new ProducerRecord<>(topic, message);
            RecordMetadata metadata = producer.send(record).get();
            System.out.printf("消息发送到分区 %d，偏移量 %d%n", 
                metadata.partition(), metadata.offset());
        }
    }
    
    /**
     * 自定义分区策略
     */
    public void sendWithCustomPartition(List<String> messages) {
        for (String message : messages) {
            // 根据消息内容进行分区
            int partition = Math.abs(message.hashCode()) % 3; // 假设有3个分区
            
            ProducerRecord<String, String> record = new ProducerRecord<>(topic, partition, null, message);
            producer.send(record);
        }
    }
    
    /**
     * 批量发送性能测试
     */
    public void batchPerformanceTest(int messageCount, int batchSize) {
        long startTime = System.currentTimeMillis();
        
        List<String> batch = new ArrayList<>();
        for (int i = 0; i < messageCount; i++) {
            batch.add("Message " + i);
            
            if (batch.size() >= batchSize) {
                sendAsyncMessages(batch);
                batch.clear();
            }
        }
        
        // 发送剩余消息
        if (!batch.isEmpty()) {
            sendAsyncMessages(batch);
        }
        
        // 等待所有异步发送完成
        producer.flush();
        long endTime = System.currentTimeMillis();
        
        System.out.printf("发送 %d 条消息，耗时 %d 毫秒，吞吐量 %.2f msg/sec%n",
            messageCount, endTime - startTime, 
            (double) messageCount / ((endTime - startTime) / 1000.0));
    }
    
    public void close() {
        producer.close();
        latch.countDown();
    }
}
```

### 9.2.2 压缩策略优化

```java
/**
 * 压缩策略比较与优化
 */
public class CompressionStrategyExample {
    
    /**
     * 不同压缩算法的性能对比
     */
    public static void compressionComparison() throws Exception {
        String[] compressions = {"none", "gzip", "snappy", "lz4", "zstd"};
        
        for (String compression : compressions) {
            Properties props = OptimizedProducerConfig.getOptimizedProducerConfig();
            props.setProperty(ProducerConfig.COMPRESSION_TYPE_CONFIG, compression);
            
            performanceTestWithCompression(props, compression);
        }
    }
    
    private static void performanceTestWithCompression(Properties props, String compression) 
            throws Exception {
        KafkaProducer<String, String> producer = new KafkaProducer<>(props);
        String topic = "compression-test-" + compression;
        
        // 创建主题（仅用于测试）
        createTestTopic(topic);
        
        long startTime = System.currentTimeMillis();
        int messageCount = 10000;
        
        for (int i = 0; i < messageCount; i++) {
            String message = generateLargeMessage(); // 生成大消息
            
            ProducerRecord<String, String> record = new ProducerRecord<>(topic, message);
            producer.send(record).get(); // 同步发送
        }
        
        producer.flush();
        long endTime = System.currentTimeMillis();
        
        double throughput = (double) messageCount / ((endTime - startTime) / 1000.0);
        System.out.printf("压缩算法: %s, 吞吐量: %.2f msg/sec, 耗时: %d ms%n",
            compression, throughput, endTime - startTime);
        
        producer.close();
    }
    
    private static String generateLargeMessage() {
        // 生成1KB的消息用于测试
        StringBuilder sb = new StringBuilder();
        for (int i = 0; i < 100; i++) {
            sb.append("This is a test message with some content to make it larger. ");
        }
        return sb.toString();
    }
}

/**
 * 智能压缩配置
 */
public class SmartCompressionConfig {
    
    public static Properties getSmartCompressionConfig() {
        Properties props = new Properties();
        
        // 根据消息大小自动选择压缩策略
        props.put(ProducerConfig.COMPRESSION_TYPE_CONFIG, "lz4");
        
        // 针对不同场景的优化配置
        // 高CPU场景：使用lz4（CPU开销小）
        props.put(ProducerConfig.LINGER_MS_CONFIG, 10);
        props.put(ProducerConfig.BATCH_SIZE_CONFIG, 32768);
        
        // 网络受限：使用zstd（压缩比高）
        // props.put(ProducerConfig.COMPRESSION_TYPE_CONFIG, "zstd");
        
        return props;
    }
}
```

## 9.3 消费者性能优化

### 9.3.1 批量消费优化

```java
/**
 * 高性能消费者配置
 */
public class OptimizedConsumerConfig {
    
    public static Properties getOptimizedConsumerConfig() {
        Properties props = new Properties();
        
        // 基础配置
        props.put(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG, "localhost:9092");
        props.put(ConsumerConfig.GROUP_ID_CONFIG, "optimized-consumer-group");
        props.put(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class.getName());
        props.put(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class.getName());
        
        // 性能优化配置
        // 1. 批量获取配置
        props.put(ConsumerConfig.FETCH_MIN_BYTES_CONFIG, 1024); // 最小获取字节数
        props.put(ConsumerConfig.FETCH_MAX_WAIT_MS_CONFIG, 500); // 最大等待时间
        props.put(ConsumerConfig.FETCH_MAX_BYTES_CONFIG, 52428800); // 50MB，最大获取字节数
        
        // 2. 消费者会话超时
        props.put(ConsumerConfig.SESSION_TIMEOUT_MS_CONFIG, 30000);
        props.put(ConsumerConfig.HEARTBEAT_INTERVAL_MS_CONFIG, 3000);
        
        // 3. 自动提交offset配置
        props.put(ConsumerConfig.ENABLE_AUTO_COMMIT_CONFIG, false); // 手动提交，提高可靠性
        
        // 4. 最大poll记录数
        props.put(ConsumerConfig.MAX_POLL_RECORDS_CONFIG, 500); // 一次poll最大记录数
        
        // 5. 客户端ID
        props.put(ConsumerConfig.CLIENT_ID_CONFIG, "optimized-consumer");
        
        return props;
    }
}

/**
 * 高性能批量消费者
 */
public class HighPerformanceConsumer {
    private final KafkaConsumer<String, String> consumer;
    private final String topic;
    
    public HighPerformanceConsumer(String bootstrapServers, String topic) {
        Properties props = OptimizedConsumerConfig.getOptimizedConsumerConfig();
        props.setProperty(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG, bootstrapServers);
        
        this.consumer = new KafkaConsumer<>(props);
        this.topic = topic;
    }
    
    /**
     * 批量消费实现
     */
    public void batchConsumeMessages() {
        consumer.subscribe(Arrays.asList(topic));
        
        try {
            while (true) {
                // 批量拉取消息
                ConsumerRecords<String, String> records = consumer.poll(Duration.ofMillis(100));
                
                if (!records.isEmpty()) {
                    // 批量处理消息
                    processBatchMessages(records);
                    
                    // 批量提交offset
                    consumer.commitAsync();
                }
            }
        } catch (Exception e) {
            System.err.println("消费异常: " + e.getMessage());
        } finally {
            consumer.close();
        }
    }
    
    /**
     * 多线程并行处理
     */
    public void parallelConsumeWithExecutor() {
        consumer.subscribe(Arrays.asList(topic));
        
        ExecutorService executor = Executors.newFixedThreadPool(4);
        
        try {
            while (true) {
                ConsumerRecords<String, String> records = consumer.poll(Duration.ofMillis(100));
                
                if (!records.isEmpty()) {
                    // 分区分组处理
                    Map<TopicPartition, List<ConsumerRecord<String, String>>> partitionMap = 
                        new HashMap<>();
                    
                    for (ConsumerRecord<String, String> record : records) {
                        partitionMap.computeIfAbsent(
                            new TopicPartition(record.topic(), record.partition()), 
                            k -> new ArrayList<>()
                        ).add(record);
                    }
                    
                    // 为每个分区创建处理任务
                    for (List<ConsumerRecord<String, String>> partitionRecords : partitionMap.values()) {
                        executor.submit(() -> processMessages(partitionRecords));
                    }
                }
            }
        } catch (Exception e) {
            System.err.println("消费异常: " + e.getMessage());
        } finally {
            executor.shutdown();
            consumer.close();
        }
    }
    
    /**
     * 手动offset管理
     */
    public void consumeWithManualOffset() {
        consumer.subscribe(Arrays.asList(topic));
        
        try {
            while (true) {
                ConsumerRecords<String, String> records = consumer.poll(Duration.ofMillis(100));
                
                // 记录offset状态
                Map<TopicPartition, OffsetAndMetadata> offsetsToCommit = new HashMap<>();
                
                for (ConsumerRecord<String, String> record : records) {
                    processMessage(record);
                    
                    // 记录要提交的offset（下一条消息的偏移量）
                    offsetsToCommit.put(
                        new TopicPartition(record.topic(), record.partition()),
                        new OffsetAndMetadata(record.offset() + 1)
                    );
                }
                
                // 批量提交offset
                if (!offsetsToCommit.isEmpty()) {
                    consumer.commitAsync(offsetsToCommit, (offsets, exception) -> {
                        if (exception != null) {
                            System.err.println("Offset提交失败: " + exception.getMessage());
                        }
                    });
                }
            }
        } catch (Exception e) {
            System.err.println("消费异常: " + e.getMessage());
        } finally {
            consumer.close();
        }
    }
    
    private void processBatchMessages(ConsumerRecords<String, String> records) {
        int messageCount = records.count();
        long startTime = System.currentTimeMillis();
        
        // 批量处理逻辑
        for (ConsumerRecord<String, String> record : records) {
            processMessage(record);
        }
        
        long endTime = System.currentTimeMillis();
        System.out.printf("批量处理 %d 条消息，耗时 %d 毫秒%n", 
            messageCount, endTime - startTime);
    }
    
    private void processMessages(List<ConsumerRecord<String, String>> records) {
        for (ConsumerRecord<String, String> record : records) {
            processMessage(record);
        }
    }
    
    private void processMessage(ConsumerRecord<String, String> record) {
        // 实际的消息处理逻辑
        System.out.printf("处理消息 - 分区: %d, 偏移量: %d, 消息: %s%n",
            record.partition(), record.offset(), record.value());
        
        // 模拟处理耗时
        try {
            Thread.sleep(1);
        } catch (InterruptedException e) {
            Thread.currentThread().interrupt();
        }
    }
    
    private void createTestTopic(String topic) {
        // 创建测试主题的实现
        // 这里省略，实际使用AdminClient创建主题
    }
}
```

### 9.3.2 消费者组优化

```java
/**
 * 消费者组监控与管理
 */
public class ConsumerGroupManager {
    private final KafkaConsumer<String, String> consumer;
    
    public ConsumerGroupManager(String bootstrapServers) {
        Properties props = OptimizedConsumerConfig.getOptimizedConsumerConfig();
        props.setProperty(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG, bootstrapServers);
        this.consumer = new KafkaConsumer<>(props);
    }
    
    /**
     * 监控消费者组状态
     */
    public ConsumerGroupSummary monitorConsumerGroup(String groupId) {
        try (AdminClient adminClient = AdminClient.create(getAdminConfig())) {
            DescribeConsumerGroupsResult result = adminClient.describeConsumerGroups(
                Collections.singleton(groupId));
            
            ConsumerGroupSummary summary = new ConsumerGroupSummary();
            summary.setGroupId(groupId);
            summary.setTimestamp(System.currentTimeMillis());
            
            // 获取组描述
            ConsumerGroupDescription description = result.all().get().get(groupId);
            summary.setState(description.state());
            summary.setPartitionAssignor(description.partitionAssignor());
            summary.setMembers(description.members().size());
            
            // 获取成员分配信息
            List<String> topics = description.members().stream()
                .flatMap(member -> member.assignment().topicPartitions().stream())
                .map(TopicPartition::topic)
                .distinct()
                .collect(Collectors.toList());
            
            summary.setTopics(topics);
            
            return summary;
        }
    }
    
    /**
     * 重新平衡消费者组
     */
    public void rebalanceConsumerGroup(String groupId) {
        try (AdminClient adminClient = AdminClient.create(getAdminConfig())) {
            // 删除消费者组并重新创建（强制重新平衡）
            adminClient.deleteConsumerGroups(Collections.singleton(groupId)).all().get();
            
            System.out.println("消费者组 " + groupId + " 已重新平衡");
        } catch (Exception e) {
            System.err.println("重新平衡失败: " + e.getMessage());
        }
    }
    
    /**
     * 优化消费者组配置
     */
    public static Properties getOptimizedGroupConfig() {
        Properties props = OptimizedConsumerConfig.getOptimizedConsumerConfig();
        
        // 优化消费者组特定配置
        props.put(ConsumerConfig.SESSION_TIMEOUT_MS_CONFIG, 30000); // 会话超时
        props.put(ConsumerConfig.HEARTBEAT_INTERVAL_MS_CONFIG, 3000); // 心跳间隔
        props.put(ConsumerConfig.MAX_POLL_INTERVAL_MS_CONFIG, 300000); // 最大poll间隔
        
        return props;
    }
    
    private Properties getAdminConfig() {
        Properties props = new Properties();
        props.put(AdminClientConfig.BOOTSTRAP_SERVERS_CONFIG, "localhost:9092");
        return props;
    }
}

class ConsumerGroupSummary {
    private String groupId;
    private long timestamp;
    private ConsumerGroupState state;
    private String partitionAssignor;
    private int members;
    private List<String> topics;
    
    // Getters and setters
    public String getGroupId() { return groupId; }
    public void setGroupId(String groupId) { this.groupId = groupId; }
    
    public long getTimestamp() { return timestamp; }
    public void setTimestamp(long timestamp) { this.timestamp = timestamp; }
    
    public ConsumerGroupState getState() { return state; }
    public void setState(ConsumerGroupState state) { this.state = state; }
    
    public String getPartitionAssignor() { return partitionAssignor; }
    public void setPartitionAssignor(String partitionAssignor) { 
        this.partitionAssignor = partitionAssignor; 
    }
    
    public int getMembers() { return members; }
    public void setMembers(int members) { this.members = members; }
    
    public List<String> getTopics() { return topics; }
    public void setTopics(List<String> topics) { this.topics = topics; }
}
```

## 9.4 Broker性能调优

### 9.4.1 操作系统优化

```bash
#!/bin/bash
# 系统级Kafka性能优化脚本

# 1. 文件描述符限制
echo "* soft nofile 65536" >> /etc/security/limits.conf
echo "* hard nofile 65536" >> /etc/security/limits.conf

# 2. 虚拟内存设置
echo "vm.swappiness=1" >> /etc/sysctl.conf
echo "vm.dirty_ratio=15" >> /etc/sysctl.conf
echo "vm.dirty_background_ratio=5" >> /etc/sysctl.conf

# 3. 网络优化
echo "net.core.rmem_max=134217728" >> /etc/sysctl.conf
echo "net.core.wmem_max=134217728" >> /etc/sysctl.conf
echo "net.ipv4.tcp_rmem=4096 16384 134217728" >> /etc/sysctl.conf
echo "net.ipv4.tcp_wmem=4096 65536 134217728" >> /etc/sysctl.conf
echo "net.core.netdev_max_backlog=5000" >> /etc/sysctl.conf

# 4. 应用设置
sysctl -p

# 5. 文件系统优化（对ext4/xfs）
echo "deadline" > /sys/block/sda/queue/scheduler
echo 4096 > /sys/block/sda/queue/nr_requests
echo 256 > /sys/block/sda/queue/read_ahead_kb
```

### 9.4.2 JVM调优

```java
/**
 * Kafka JVM优化配置示例
 */
public class KafkaJVMOptimization {
    
    /**
     * 生产环境JVM配置
     */
    public static String getProductionJVMArgs() {
        return "-server " +
               // 堆内存设置
               "-Xms8g -Xmx8g " +
               // 年轻代设置
               "-XX:NewRatio=1 " +
               "-XX:SurvivorRatio=8 " +
               // 垃圾收集器选择
               "-XX:+UseG1GC " +
               "-XX:MaxGCPauseMillis=20 " +
               "-XX:G1HeapRegionSize=16m " +
               // GC日志
               "-XX:+PrintGCDetails " +
               "-XX:+PrintGCTimeStamps " +
               "-Xloggc:/var/log/kafka/gc.log " +
               // 性能监控
               "-XX:+UseGCLogFileRotation " +
               "-XX:NumberOfGCLogFiles=10 " +
               "-XX:GCLogFileSize=100M " +
               // 其他优化
               "-XX:+HeapDumpOnOutOfMemoryError " +
               "-XX:HeapDumpPath=/var/log/kafka/ " +
               "-Djava.awt.headless=true " +
               // 网络优化
               "-Djava.net.preferIPv4Stack=true";
    }
    
    /**
     * 内存使用监控
     */
    public static void monitorMemoryUsage() {
        MemoryMXBean memoryBean = ManagementFactory.getMemoryMXBean();
        GarbageCollectionMXBean gcBean = ManagementFactory.getGarbageCollectionMXBean();
        
        System.out.println("堆内存使用情况:");
        System.out.printf("已使用: %.2f MB%n", 
            (double) memoryBean.getHeapMemoryUsage().getUsed() / 1024 / 1024);
        System.out.printf("已分配: %.2f MB%n", 
            (double) memoryBean.getHeapMemoryUsage().getCommitted() / 1024 / 1024);
        System.out.printf("最大: %.2f MB%n", 
            (double) memoryBean.getHeapMemoryUsage().getMax() / 1024 / 1024);
        
        System.out.println("GC统计:");
        for (GarbageCollectorMXBean gc : ManagementFactory.getGarbageCollectionMXBeans()) {
            long count = gc.getCollectionCount();
            long time = gc.getCollectionTime();
            if (count > 0) {
                System.out.printf("%s: %d 次, 总耗时 %d ms%n", 
                    gc.getName(), count, time);
            }
        }
    }
    
    /**
     * G1 GC调优建议
     */
    public static class G1GCOptimizer {
        
        public static String getOptimizedG1Config(long heapSizeGB) {
            int regionSize = (int) (heapSizeGB * 1024 / 2048); // 2MB per region for 1GB heap
            
            return "-XX:+UseG1GC " +
                   "-XX:MaxGCPauseMillis=200 " +
                   "-XX:G1HeapRegionSize=" + regionSize + "m " +
                   "-XX:G1NewSizePercent=5 " +
                   "-XX:G1MaxNewSizePercent=60 " +
                   "-XX:InitiatingHeapOccupancyPercent=45 " +
                   "-XX:G1HeapWastePercent=5 " +
                   "-XX:G1MixedGCCountTarget=8 " +
                   "-XX:G1MixedGCLiveThresholdPercent=85 " +
                   "-XX:G1OldCSetRegionThreshold=10";
        }
        
        /**
         * GC性能分析
         */
        public static void analyzeGCPerformance() {
            List<GarbageCollectorMXBean> gcBeans = ManagementFactory.getGarbageCollectionMXBeans();
            
            for (GarbageCollectorMXBean gcBean : gcBeans) {
                if (gcBean.getName().contains("G1")) {
                    long count = gcBean.getCollectionCount();
                    long time = gcBean.getCollectionTime();
                    
                    if (count > 0) {
                        double avgTime = (double) time / count;
                        System.out.printf("G1 GC分析 - 次数: %d, 平均时间: %.2f ms%n", 
                            count, avgTime);
                        
                        if (avgTime > 500) {
                            System.out.println("警告: G1 GC平均时间过长，建议调优");
                        }
                    }
                }
            }
        }
    }
}
```

### 9.4.3 Kafka服务器配置优化

```properties
# server.properties 性能优化配置

# =================== 基础配置 ===================
broker.id=0
listeners=PLAINTEXT://localhost:9092
num.network.threads=8
num.io.threads=16
socket.send.buffer.bytes=102400
socket.receive.buffer.bytes=102400
socket.request.max.bytes=104857600

# =================== 日志配置 ===================
log.dirs=/data/kafka-logs
num.partitions=3
default.replication.factor=3
min.insync.replicas=2
log.retention.hours=168
log.segment.bytes=1073741824
log.retention.check.interval.ms=300000
log.cleanup.policy=delete

# =================== 性能优化配置 ===================
# 批量处理优化
log.flush.interval.messages=10000
log.flush.interval.ms=1000

# 压缩优化
compression.type=producer

# 副本优化
offsets.topic.replication.factor=3
transaction.state.log.replication.factor=3
transaction.state.log.min.isr=2

# Controller优化
controller.quorum.voters=0@localhost:9093,1@localhost:9094,2@localhost:9095
controller.listener.names=CONTROLLER

# =================== 监控配置 ===================
metric.reporters=org.apache.kafka.common.metrics.JmxReporter
kafka.metrics.reporters=org.apache.kafka.common.metrics.JmxReporter

# JMX配置
jmx.port=9999

# =================== 安全配置 ===================
security.inter.broker.protocol=PLAINTEXT
```

## 9.5 存储性能优化

### 9.5.1 磁盘I/O优化

```java
/**
 * 磁盘性能监控工具
 */
public class DiskPerformanceMonitor {
    
    public static class DiskMetrics {
        private final String mountPoint;
        private final long totalSpace;
        private final long usedSpace;
        private final long availableSpace;
        private final double utilizationPercent;
        private final int ioWaitPercent;
        
        public DiskMetrics(String mountPoint, long totalSpace, long usedSpace, 
                          long availableSpace, double utilizationPercent, int ioWaitPercent) {
            this.mountPoint = mountPoint;
            this.totalSpace = totalSpace;
            this.usedSpace = usedSpace;
            this.availableSpace = availableSpace;
            this.utilizationPercent = utilizationPercent;
            this.ioWaitPercent = ioWaitPercent;
        }
        
        // Getters
        public String getMountPoint() { return mountPoint; }
        public long getTotalSpace() { return totalSpace; }
        public long getUsedSpace() { return usedSpace; }
        public long getAvailableSpace() { return availableSpace; }
        public double getUtilizationPercent() { return utilizationPercent; }
        public int getIoWaitPercent() { return ioWaitPercent; }
    }
    
    /**
     * 监控磁盘性能
     */
    public static List<DiskMetrics> monitorDiskPerformance() throws IOException {
        List<DiskMetrics> metrics = new ArrayList<>();
        
        // 使用iostat命令监控磁盘性能
        ProcessBuilder pb = new ProcessBuilder("iostat", "-x", "1", "1");
        Process process = pb.start();
        
        BufferedReader reader = new BufferedReader(new InputStreamReader(process.getInputStream()));
        String line;
        boolean headerSkipped = false;
        
        while ((line = reader.readLine()) != null) {
            if (line.trim().isEmpty()) continue;
            
            if (!headerSkipped) {
                if (line.contains("Device") || line.contains("avg-cpu")) {
                    headerSkipped = true;
                    continue;
                }
            } else {
                String[] fields = line.trim().split("\\s+");
                if (fields.length >= 14) {
                    String device = fields[0];
                    double util = Double.parseDouble(fields[13]);
                    
                    // 获取磁盘空间信息
                    File file = new File("/");
                    FileSystem.getDefault().getFileStores();
                    
                    // 这里简化处理，实际需要根据device获取对应的挂载点
                    metrics.add(new DiskMetrics(device, 0, 0, 0, util, 0));
                }
            }
        }
        
        return metrics;
    }
    
    /**
     * 优化磁盘I/O配置
     */
    public static void optimizeDiskIO() {
        try {
            // 设置I/O调度器为noop（适用于SSD）
            setIOScheduler("noop");
            
            // 增大队列深度
            setQueueDepth(256);
            
            // 设置预读大小
            setReadAheadKB(512);
            
            System.out.println("磁盘I/O优化配置已应用");
        } catch (Exception e) {
            System.err.println("磁盘I/O优化失败: " + e.getMessage());
        }
    }
    
    private static void setIOScheduler(String scheduler) throws IOException {
        // 列出所有块设备
        File devicesDir = new File("/sys/block");
        if (devicesDir.exists()) {
            for (File device : devicesDir.listFiles()) {
                if (device.isDirectory()) {
                    File schedulerFile = new File(device, "queue/scheduler");
                    if (schedulerFile.exists()) {
                        Files.write(schedulerFile.toPath(), scheduler.getBytes());
                    }
                }
            }
        }
    }
    
    private static void setQueueDepth(int depth) throws IOException {
        File devicesDir = new File("/sys/block");
        if (devicesDir.exists()) {
            for (File device : devicesDir.listFiles()) {
                if (device.isDirectory()) {
                    File queueFile = new File(device, "queue/nr_requests");
                    if (queueFile.exists()) {
                        Files.write(queueFile.toPath(), String.valueOf(depth).getBytes());
                    }
                }
            }
        }
    }
    
    private static void setReadAheadKB(int kb) throws IOException {
        File devicesDir = new File("/sys/block");
        if (devicesDir.exists()) {
            for (File device : devicesDir.listFiles()) {
                if (device.isDirectory()) {
                    File readAheadFile = new File(device, "queue/read_ahead_kb");
                    if (readAheadFile.exists()) {
                        Files.write(readAheadFile.toPath(), String.valueOf(kb).getBytes());
                    }
                }
            }
        }
    }
}
```

### 9.5.2 日志段优化

```java
/**
 * Kafka日志段性能优化工具
 */
public class LogSegmentOptimizer {
    
    /**
     * 分析日志段性能
     */
    public static class LogSegmentAnalyzer {
        
        public static void analyzeLogSegments(String logDir) {
            File logDirectory = new File(logDir);
            if (!logDirectory.exists()) {
                System.err.println("日志目录不存在: " + logDir);
                return;
            }
            
            List<File> segmentFiles = Arrays.stream(logDirectory.listFiles())
                .filter(file -> file.getName().endsWith(".log"))
                .sorted(Comparator.comparingLong(File::lastModified))
                .collect(Collectors.toList());
            
            System.out.println("日志段分析报告:");
            System.out.println("总段数: " + segmentFiles.size());
            
            long totalSize = segmentFiles.stream().mapToLong(File::length).sum();
            long avgSize = segmentFiles.isEmpty() ? 0 : totalSize / segmentFiles.size();
            
            System.out.printf("总大小: %.2f MB%n", totalSize / 1024.0 / 1024.0);
            System.out.printf("平均大小: %.2f MB%n", avgSize / 1024.0 / 1024.0);
            
            // 分析大小分布
            analyzeSizeDistribution(segmentFiles);
            
            // 分析时间分布
            analyzeTimeDistribution(segmentFiles);
        }
        
        private static void analyzeSizeDistribution(List<File> segmentFiles) {
            Map<String, Integer> distribution = new HashMap<>();
            
            for (File file : segmentFiles) {
                long sizeMB = file.length() / 1024 / 1024;
                String sizeRange;
                
                if (sizeMB < 100) {
                    sizeRange = "< 100MB";
                } else if (sizeMB < 500) {
                    sizeRange = "100-500MB";
                } else if (sizeMB < 1000) {
                    sizeRange = "500MB-1GB";
                } else {
                    sizeRange = "> 1GB";
                }
                
                distribution.merge(sizeRange, 1, Integer::sum);
            }
            
            System.out.println("大小分布:");
            distribution.forEach((range, count) -> 
                System.out.printf("  %s: %d 个段%n", range, count));
        }
        
        private static void analyzeTimeDistribution(List<File> segmentFiles) {
            if (segmentFiles.size() < 2) return;
            
            long oldestTime = segmentFiles.get(0).lastModified();
            long newestTime = segmentFiles.get(segmentFiles.size() - 1).lastModified();
            long timeSpan = newestTime - oldestTime;
            
            System.out.printf("时间跨度: %.2f 小时%n", timeSpan / 1000.0 / 3600.0);
            
            // 分析段滚动频率
            long avgSegmentAge = timeSpan / (segmentFiles.size() - 1);
            System.out.printf("平均段生存时间: %.2f 分钟%n", avgSegmentAge / 1000.0 / 60.0);
        }
    }
    
    /**
     * 日志段合并优化
     */
    public static class LogCompactionOptimizer {
        
        /**
         * 模拟日志压缩
         */
        public static void simulateLogCompaction() {
            // 模拟产生重复键的消息
            Map<String, String> messageMap = new HashMap<>();
            Random random = new Random();
            
            // 生成初始消息
            for (int i = 0; i < 10000; i++) {
                String key = "key-" + (i % 1000); // 1000个不同的键
                String value = "value-" + random.nextInt(1000);
                messageMap.put(key, value);
            }
            
            System.out.println("压缩前消息数: " + messageMap.size());
            
            // 模拟压缩
            long startTime = System.currentTimeMillis();
            List<String> compactedMessages = performCompaction(messageMap);
            long endTime = System.currentTimeMillis();
            
            System.out.println("压缩后消息数: " + compactedMessages.size());
            System.out.printf("压缩耗时: %d 毫秒%n", endTime - startTime);
            System.out.printf("压缩比: %.2f%%%n", 
                (1 - (double) compactedMessages.size() / messageMap.size()) * 100);
        }
        
        private static List<String> performCompaction(Map<String, String> messageMap) {
            // 简化版的压缩实现
            // 实际Kafka压缩涉及更复杂的逻辑
            return messageMap.entrySet().stream()
                .map(entry -> entry.getKey() + "=" + entry.getValue())
                .collect(Collectors.toList());
        }
        
        /**
         * 配置压缩策略
         */
        public static Properties getCompactionConfig() {
            Properties props = new Properties();
            
            // 启用日志压缩
            props.put("log.cleanup.policy", "compact");
            
            // 压缩相关配置
            props.put("log.cleaner.enable", true);
            props.put("log.cleaner.threads", 2);
            props.put("log.cleaner.io.max.bytes.per.second", Double.MAX_VALUE);
            props.put("log.cleaner.io.buffer.size", 1024 * 1024);
            props.put("log.cleaner.io.buffer.load.factor", 0.9);
            props.put("log.cleaner.deduplication.buffer.size", 1024 * 1024);
            
            // 压缩触发条件
            props.put("log.cleaner.min.compaction.lag.ms", 0);
            props.put("log.cleaner.max.compaction.lag.ms", Long.MAX_VALUE);
            
            return props;
        }
    }
}
```

## 9.6 网络调优

### 9.6.1 网络性能监控

```java
/**
 * 网络性能监控工具
 */
public class NetworkPerformanceMonitor {
    
    /**
     * 网络接口统计
     */
    public static class NetworkInterfaceStats {
        private final String interfaceName;
        private final long bytesReceived;
        private final long bytesSent;
        private final long packetsReceived;
        private final long packetsSent;
        private final long errorsIn;
        private final long errorsOut;
        
        public NetworkInterfaceStats(String interfaceName, long bytesReceived, 
                                   long bytesSent, long packetsReceived, 
                                   long packetsSent, long errorsIn, long errorsOut) {
            this.interfaceName = interfaceName;
            this.bytesReceived = bytesReceived;
            this.bytesSent = bytesSent;
            this.packetsReceived = packetsReceived;
            this.packetsSent = packetsSent;
            this.errorsIn = errorsIn;
            this.errorsOut = errorsOut;
        }
        
        public double getThroughputMbps() {
            return (bytesReceived + bytesSent) * 8.0 / 1024.0 / 1024.0;
        }
        
        public double getErrorRate() {
            long totalPackets = packetsReceived + packetsSent;
            long totalErrors = errorsIn + errorsOut;
            return totalPackets > 0 ? (double) totalErrors / totalPackets * 100 : 0;
        }
        
        // Getters
        public String getInterfaceName() { return interfaceName; }
        public long getBytesReceived() { return bytesReceived; }
        public long getBytesSent() { return bytesSent; }
        public long getPacketsReceived() { return packetsReceived; }
        public long getPacketsSent() { return packetsSent; }
        public long getErrorsIn() { return errorsIn; }
        public long getErrorsOut() { return errorsOut; }
    }
    
    /**
     * 收集网络接口统计信息
     */
    public static List<NetworkInterfaceStats> collectNetworkStats() throws IOException {
        List<NetworkInterfaceStats> stats = new ArrayList<>();
        
        // 读取 /proc/net/dev 文件
        try (BufferedReader reader = new BufferedReader(new FileReader("/proc/net/dev"))) {
            String line;
            boolean headerSkipped = false;
            
            while ((line = reader.readLine()) != null) {
                if (!headerSkipped) {
                    headerSkipped = true;
                    continue;
                }
                
                String[] fields = line.trim().split("\\s+");
                if (fields.length >= 17) {
                    String interfaceName = fields[0].replace(":", "");
                    long bytesReceived = Long.parseLong(fields[1]);
                    long packetsReceived = Long.parseLong(fields[2]);
                    long errorsIn = Long.parseLong(fields[3]);
                    long bytesSent = Long.parseLong(fields[9]);
                    long packetsSent = Long.parseLong(fields[10]);
                    long errorsOut = Long.parseLong(fields[11]);
                    
                    stats.add(new NetworkInterfaceStats(
                        interfaceName, bytesReceived, bytesSent, 
                        packetsReceived, packetsSent, errorsIn, errorsOut
                    ));
                }
            }
        }
        
        return stats;
    }
    
    /**
     * 网络延迟测试
     */
    public static class NetworkLatencyTester {
        
        /**
         * 测试网络延迟
         */
        public static LatencyResult testLatency(String host, int port, int count) {
            List<Long> latencies = new ArrayList<>();
            int successCount = 0;
            int timeoutCount = 0;
            
            for (int i = 0; i < count; i++) {
                try {
                    long startTime = System.nanoTime();
                    
                    try (Socket socket = new Socket()) {
                        socket.connect(new InetSocketAddress(host, port), 1000); // 1秒超时
                        socket.getOutputStream().write("ping".getBytes());
                    }
                    
                    long endTime = System.nanoTime();
                    long latency = (endTime - startTime) / 1000000; // 转换为毫秒
                    
                    latencies.add(latency);
                    successCount++;
                    
                } catch (SocketTimeoutException e) {
                    timeoutCount++;
                } catch (IOException e) {
                    // 连接失败
                }
            }
            
            return new LatencyResult(latencies, successCount, timeoutCount);
        }
        
        /**
         * Kafka集群延迟测试
         */
        public static Map<String, LatencyResult> testKafkaClusterLatency(String bootstrapServers) {
            Map<String, LatencyResult> results = new HashMap<>();
            String[] servers = bootstrapServers.split(",");
            
            for (String server : servers) {
                String[] hostPort = server.split(":");
                if (hostPort.length == 2) {
                    String host = hostPort[0];
                    int port = Integer.parseInt(hostPort[1]);
                    
                    LatencyResult result = testLatency(host, port, 10);
                    results.put(server, result);
                }
            }
            
            return results;
        }
        
        /**
         * 延迟测试结果
         */
        public static class LatencyResult {
            private final List<Long> latencies;
            private final int successCount;
            private final int timeoutCount;
            
            public LatencyResult(List<Long> latencies, int successCount, int timeoutCount) {
                this.latencies = latencies;
                this.successCount = successCount;
                this.timeoutCount = timeoutCount;
            }
            
            public double getAverageLatency() {
                return latencies.stream().mapToLong(Long::longValue).average().orElse(0);
            }
            
            public long getMinLatency() {
                return latencies.stream().mapToLong(Long::longValue).min().orElse(0);
            }
            
            public long getMaxLatency() {
                return latencies.stream().mapToLong(Long::longValue).max().orElse(0);
            }
            
            public double getP95Latency() {
                if (latencies.isEmpty()) return 0;
                
                List<Long> sortedLatencies = new ArrayList<>(latencies);
                Collections.sort(sortedLatencies);
                
                int index = (int) Math.ceil(sortedLatencies.size() * 0.95) - 1;
                return sortedLatencies.get(Math.max(0, index));
            }
            
            public double getP99Latency() {
                if (latencies.isEmpty()) return 0;
                
                List<Long> sortedLatencies = new ArrayList<>(latencies);
                Collections.sort(sortedLatencies);
                
                int index = (int) Math.ceil(sortedLatencies.size() * 0.99) - 1;
                return sortedLatencies.get(Math.max(0, index));
            }
            
            // Getters
            public List<Long> getLatencies() { return latencies; }
            public int getSuccessCount() { return successCount; }
            public int getTimeoutCount() { return timeoutCount; }
        }
    }
    
    /**
     * 网络优化建议
     */
    public static class NetworkOptimizationAdvice {
        
        public static String generateOptimizationAdvice(List<NetworkInterfaceStats> stats) {
            StringBuilder advice = new StringBuilder();
            advice.append("网络优化建议:\n");
            
            for (NetworkInterfaceStats stat : stats) {
                advice.append(String.format("接口 %s:\n", stat.getInterfaceName()));
                
                double throughput = stat.getThroughputMbps();
                if (throughput > 1000) {
                    advice.append("  - 吞吐量较高，考虑启用更高效的压缩算法\n");
                }
                
                double errorRate = stat.getErrorRate();
                if (errorRate > 0.1) {
                    advice.append("  - 错误率较高，建议检查网络连接和硬件\n");
                }
                
                advice.append(String.format("  - 当前吞吐量: %.2f Mbps\n", throughput));
                advice.append(String.format("  - 当前错误率: %.2f%%\n", errorRate));
                advice.append("\n");
            }
            
            return advice.toString();
        }
        
        public static Properties getOptimizedNetworkConfig() {
            Properties props = new Properties();
            
            // 网络缓冲区大小优化
            props.put("socket.send.buffer.bytes", "102400");
            props.put("socket.receive.buffer.bytes", "102400");
            props.put("socket.request.max.bytes", "104857600");
            
            // 连接相关优化
            props.put("connections.max.idle.ms", "540000"); // 9分钟
            props.put("request.timeout.ms", "30000");
            props.put("retry.backoff.ms", "100");
            
            // 批量处理优化
            props.put("batch.size", "32768");
            props.put("linger.ms", "20");
            
            return props;
        }
    }
}
```

## 9.7 监控与指标分析

### 9.7.1 关键性能指标监控

```java
/**
 * Kafka性能指标监控器
 */
public class KafkaPerformanceMonitor {
    
    /**
     * 关键性能指标
     */
    public static class PerformanceMetrics {
        // 生产者指标
        private double messagesPerSecond;
        private double bytesPerSecond;
        private double errorRate;
        private double averageLatency;
        private double p95Latency;
        
        // 消费者指标
        private double consumptionRate;
        private double lag;
        private int activeConsumers;
        
        // Broker指标
        private double cpuUsage;
        private double memoryUsage;
        private double diskUsage;
        private double networkThroughput;
        
        // 集群指标
        private int onlineBrokers;
        private int offlinePartitions;
        private double replicationFactor;
        
        // Getters and setters
        public double getMessagesPerSecond() { return messagesPerSecond; }
        public void setMessagesPerSecond(double messagesPerSecond) { 
            this.messagesPerSecond = messagesPerSecond; 
        }
        
        public double getBytesPerSecond() { return bytesPerSecond; }
        public void setBytesPerSecond(double bytesPerSecond) { 
            this.bytesPerSecond = bytesPerSecond; 
        }
        
        public double getErrorRate() { return errorRate; }
        public void setErrorRate(double errorRate) { this.errorRate = errorRate; }
        
        public double getAverageLatency() { return averageLatency; }
        public void setAverageLatency(double averageLatency) { 
            this.averageLatency = averageLatency; 
        }
        
        public double getP95Latency() { return p95Latency; }
        public void setP95Latency(double p95Latency) { this.p95Latency = p95Latency; }
        
        public double getConsumptionRate() { return consumptionRate; }
        public void setConsumptionRate(double consumptionRate) { 
            this.consumptionRate = consumptionRate; 
        }
        
        public double getLag() { return lag; }
        public void setLag(double lag) { this.lag = lag; }
        
        public int getActiveConsumers() { return activeConsumers; }
        public void setActiveConsumers(int activeConsumers) { 
            this.activeConsumers = activeConsumers; 
        }
        
        public double getCpuUsage() { return cpuUsage; }
        public void setCpuUsage(double cpuUsage) { this.cpuUsage = cpuUsage; }
        
        public double getMemoryUsage() { return memoryUsage; }
        public void setMemoryUsage(double memoryUsage) { 
            this.memoryUsage = memoryUsage; 
        }
        
        public double getDiskUsage() { return diskUsage; }
        public void setDiskUsage(double diskUsage) { this.diskUsage = diskUsage; }
        
        public double getNetworkThroughput() { return networkThroughput; }
        public void setNetworkThroughput(double networkThroughput) { 
            this.networkThroughput = networkThroughput; 
        }
        
        public int getOnlineBrokers() { return onlineBrokers; }
        public void setOnlineBrokers(int onlineBrokers) { 
            this.onlineBrokers = onlineBrokers; 
        }
        
        public int getOfflinePartitions() { return offlinePartitions; }
        public void setOfflinePartitions(int offlinePartitions) { 
            this.offlinePartitions = offlinePartitions; 
        }
        
        public double getReplicationFactor() { return replicationFactor; }
        public void setReplicationFactor(double replicationFactor) { 
            this.replicationFactor = replicationFactor; 
        }
    }
    
    /**
     * 指标收集器
     */
    public static class MetricsCollector {
        private final JMXConnector jmxConnector;
        private final MBeanServerConnection mbeanServer;
        
        public MetricsCollector(String jmxUrl) throws IOException {
            this.jmxConnector = JMXConnectorFactory.connect(new JMXServiceURL(jmxUrl));
            this.mbeanServer = jmxConnector.getMBeanServerConnection();
        }
        
        /**
         * 收集生产者指标
         */
        public PerformanceMetrics collectProducerMetrics() throws Exception {
            PerformanceMetrics metrics = new PerformanceMetrics();
            
            try {
                // 收集消息发送速率
                Double messageRate = (Double) mbeanServer.getAttribute(
                    new ObjectName("kafka.producer:type=producer-metrics,client-id=*"),
                    "request-rate"
                );
                
                // 收集字节发送速率
                Double byteRate = (Double) mbeanServer.getAttribute(
                    new ObjectName("kafka.producer:type=producer-metrics,client-id=*"),
                    "byte-rate"
                );
                
                // 收集错误率
                Double errorRate = (Double) mbeanServer.getAttribute(
                    new ObjectName("kafka.producer:type=producer-metrics,client-id=*"),
                    "error-rate"
                );
                
                // 收集延迟指标
                Double avgLatency = (Double) mbeanServer.getAttribute(
                    new ObjectName("kafka.producer:type=producer-metrics,client-id=*"),
                    "request-latency-avg"
                );
                
                if (messageRate != null) metrics.setMessagesPerSecond(messageRate);
                if (byteRate != null) metrics.setBytesPerSecond(byteRate);
                if (errorRate != null) metrics.setErrorRate(errorRate * 100);
                if (avgLatency != null) metrics.setAverageLatency(avgLatency);
                
            } catch (Exception e) {
                System.err.println("收集生产者指标失败: " + e.getMessage());
            }
            
            return metrics;
        }
        
        /**
         * 收集消费者指标
         */
        public PerformanceMetrics collectConsumerMetrics(String groupId) throws Exception {
            PerformanceMetrics metrics = new PerformanceMetrics();
            
            try {
                // 使用AdminClient获取消费者组信息
                try (AdminClient adminClient = AdminClient.create(getAdminConfig())) {
                    DescribeConsumerGroupsResult result = adminClient.describeConsumerGroups(
                        Collections.singleton(groupId));
                    
                    ConsumerGroupDescription description = result.all().get().get(groupId);
                    metrics.setActiveConsumers(description.members().size());
                    
                    // 计算总延迟
                    long totalLag = description.members().stream()
                        .mapToLong(member -> {
                            try {
                                Map<TopicPartition, OffsetAndMetadata> assignments = 
                                    member.assignment().topicPartitions();
                                
                                // 这里简化处理，实际需要查询offset信息
                                return 0;
                            } catch (Exception e) {
                                return 0;
                            }
                        })
                        .sum();
                    
                    metrics.setLag(totalLag);
                }
                
            } catch (Exception e) {
                System.err.println("收集消费者指标失败: " + e.getMessage());
            }
            
            return metrics;
        }
        
        /**
         * 收集Broker系统指标
         */
        public PerformanceMetrics collectBrokerSystemMetrics() throws Exception {
            PerformanceMetrics metrics = new PerformanceMetrics();
            
            try {
                // CPU使用率
                Double cpuUsage = (Double) mbeanServer.getAttribute(
                    new ObjectName("java.lang:type=OperatingSystem"),
                    "ProcessCpuLoad"
                );
                
                // 内存使用率
                MemoryUsage heapMemory = (MemoryUsage) mbeanServer.getAttribute(
                    new ObjectName("java.lang:type=Memory"),
                    "HeapMemoryUsage"
                );
                
                // 磁盘使用率（需要根据实际磁盘路径调整）
                File disk = new File("/data/kafka-logs");
                long totalSpace = disk.getTotalSpace();
                long freeSpace = disk.getFreeSpace();
                double diskUsage = ((double) (totalSpace - freeSpace) / totalSpace) * 100;
                
                if (cpuUsage != null) metrics.setCpuUsage(cpuUsage * 100);
                if (heapMemory != null) {
                    double memoryUsage = ((double) heapMemory.getUsed() / heapMemory.getMax()) * 100;
                    metrics.setMemoryUsage(memoryUsage);
                }
                metrics.setDiskUsage(diskUsage);
                
            } catch (Exception e) {
                System.err.println("收集系统指标失败: " + e.getMessage());
            }
            
            return metrics;
        }
        
        private Properties getAdminConfig() {
            Properties props = new Properties();
            props.put(AdminClientConfig.BOOTSTRAP_SERVERS_CONFIG, "localhost:9092");
            return props;
        }
        
        public void close() throws IOException {
            jmxConnector.close();
        }
    }
    
    /**
     * 实时性能监控器
     */
    public static class RealTimeMonitor {
        private final ScheduledExecutorService scheduler = Executors.newScheduledThreadPool(2);
        private final List<PerformanceMetrics> metricsHistory = new ArrayList<>();
        private final int maxHistorySize = 1000;
        
        public void startMonitoring(String bootstrapServers, String consumerGroup) {
            scheduler.scheduleAtFixedRate(() -> {
                try {
                    PerformanceMetrics metrics = collectAllMetrics(bootstrapServers, consumerGroup);
                    
                    synchronized (metricsHistory) {
                        metricsHistory.add(metrics);
                        if (metricsHistory.size() > maxHistorySize) {
                            metricsHistory.remove(0);
                        }
                    }
                    
                    // 检查性能告警
                    checkPerformanceAlerts(metrics);
                    
                } catch (Exception e) {
                    System.err.println("监控采集失败: " + e.getMessage());
                }
            }, 0, 30, TimeUnit.SECONDS); // 每30秒采集一次
        }
        
        public void stopMonitoring() {
            scheduler.shutdown();
            try {
                if (!scheduler.awaitTermination(5, TimeUnit.SECONDS)) {
                    scheduler.shutdownNow();
                }
            } catch (InterruptedException e) {
                scheduler.shutdownNow();
                Thread.currentThread().interrupt();
            }
        }
        
        private PerformanceMetrics collectAllMetrics(String bootstrapServers, String consumerGroup) 
                throws Exception {
            PerformanceMetrics metrics = new PerformanceMetrics();
            
            // 模拟收集各种指标
            metrics.setMessagesPerSecond(Math.random() * 10000);
            metrics.setBytesPerSecond(Math.random() * 100000000);
            metrics.setErrorRate(Math.random() * 5);
            metrics.setAverageLatency(Math.random() * 100);
            metrics.setCpuUsage(Math.random() * 80);
            metrics.setMemoryUsage(Math.random() * 70);
            metrics.setDiskUsage(Math.random() * 60);
            metrics.setActiveConsumers(3);
            metrics.setOnlineBrokers(3);
            
            return metrics;
        }
        
        private void checkPerformanceAlerts(PerformanceMetrics metrics) {
            // 错误率告警
            if (metrics.getErrorRate() > 5.0) {
                System.out.println("⚠️ 告警：错误率过高 (" + metrics.getErrorRate() + "%)");
            }
            
            // 延迟告警
            if (metrics.getAverageLatency() > 500) {
                System.out.println("⚠️ 告警：平均延迟过高 (" + metrics.getAverageLatency() + "ms)");
            }
            
            // CPU使用率告警
            if (metrics.getCpuUsage() > 80) {
                System.out.println("⚠️ 告警：CPU使用率过高 (" + metrics.getCpuUsage() + "%)");
            }
            
            // 内存使用率告警
            if (metrics.getMemoryUsage() > 80) {
                System.out.println("⚠️ 告警：内存使用率过高 (" + metrics.getMemoryUsage() + "%)");
            }
        }
        
        /**
         * 获取性能趋势分析
         */
        public PerformanceTrendAnalysis getTrendAnalysis(int timeWindowMinutes) {
            synchronized (metricsHistory) {
                long cutoffTime = System.currentTimeMillis() - (timeWindowMinutes * 60 * 1000);
                
                List<PerformanceMetrics> recentMetrics = metricsHistory.stream()
                    .filter(m -> m.getTimestamp() > cutoffTime)
                    .collect(Collectors.toList());
                
                return analyzeTrends(recentMetrics);
            }
        }
        
        private PerformanceTrendAnalysis analyzeTrends(List<PerformanceMetrics> metrics) {
            PerformanceTrendAnalysis analysis = new PerformanceTrendAnalysis();
            
            if (metrics.size() < 2) {
                analysis.setMessage("数据不足，无法分析趋势");
                return analysis;
            }
            
            // 分析吞吐量趋势
            List<Double> throughputData = metrics.stream()
                .map(PerformanceMetrics::getMessagesPerSecond)
                .collect(Collectors.toList());
            
            double trend = calculateTrend(throughputData);
            analysis.setThroughputTrend(trend);
            
            // 分析延迟趋势
            List<Double> latencyData = metrics.stream()
                .map(PerformanceMetrics::getAverageLatency)
                .collect(Collectors.toList());
            
            double latencyTrend = calculateTrend(latencyData);
            analysis.setLatencyTrend(latencyTrend);
            
            return analysis;
        }
        
        private double calculateTrend(List<Double> data) {
            if (data.size() < 2) return 0;
            
            // 简单的线性回归
            double n = data.size();
            double sumX = 0, sumY = 0, sumXY = 0, sumX2 = 0;
            
            for (int i = 0; i < n; i++) {
                sumX += i;
                sumY += data.get(i);
                sumXY += i * data.get(i);
                sumX2 += i * i;
            }
            
            return (n * sumXY - sumX * sumY) / (n * sumX2 - sumX * sumX);
        }
    }
    
    /**
     * 性能趋势分析
     */
    public static class PerformanceTrendAnalysis {
        private double throughputTrend;
        private double latencyTrend;
        private String message;
        
        // Getters and setters
        public double getThroughputTrend() { return throughputTrend; }
        public void setThroughputTrend(double throughputTrend) { 
            this.throughputTrend = throughputTrend; 
        }
        
        public double getLatencyTrend() { return latencyTrend; }
        public void setLatencyTrend(double latencyTrend) { 
            this.latencyTrend = latencyTrend; 
        }
        
        public String getMessage() { return message; }
        public void setMessage(String message) { this.message = message; }
    }
    
    // 为PerformanceMetrics添加timestamp字段
    static {
        try {
            // 通过反射添加timestamp字段
            Field timestampField = PerformanceMetrics.class.getDeclaredField("timestamp");
            timestampField.setAccessible(true);
        } catch (NoSuchFieldException e) {
            // 如果字段不存在则忽略
        }
    }
}
```

### 9.7.2 性能基准测试

```java
/**
 * Kafka性能基准测试工具
 */
public class KafkaPerformanceBenchmark {
    
    /**
     * 基准测试结果
     */
    public static class BenchmarkResult {
        private final String testName;
        private final long testDuration;
        private final int totalMessages;
        private final double throughput;
        private final double averageLatency;
        private final double p95Latency;
        private final double p99Latency;
        private final double errorRate;
        private final Map<String, Object> customMetrics;
        
        public BenchmarkResult(String testName, long testDuration, int totalMessages,
                             double throughput, double averageLatency, double p95Latency,
                             double p99Latency, double errorRate) {
            this.testName = testName;
            this.testDuration = testDuration;
            this.totalMessages = totalMessages;
            this.throughput = throughput;
            this.averageLatency = averageLatency;
            this.p95Latency = p95Latency;
            this.p99Latency = p99Latency;
            this.errorRate = errorRate;
            this.customMetrics = new HashMap<>();
        }
        
        public void addCustomMetric(String name, Object value) {
            customMetrics.put(name, value);
        }
        
        // Getters
        public String getTestName() { return testName; }
        public long getTestDuration() { return testDuration; }
        public int getTotalMessages() { return totalMessages; }
        public double getThroughput() { return throughput; }
        public double getAverageLatency() { return averageLatency; }
        public double getP95Latency() { return p95Latency; }
        public double getP99Latency() { return p99Latency; }
        public double getErrorRate() { return errorRate; }
        public Map<String, Object> getCustomMetrics() { return customMetrics; }
        
        public void printReport() {
            System.out.println("\n=== 基准测试报告: " + testName + " ===");
            System.out.printf("测试时长: %.2f 秒%n", testDuration / 1000.0);
            System.out.printf("总消息数: %,d%n", totalMessages);
            System.out.printf("吞吐量: %.2f msg/sec%n", throughput);
            System.out.printf("平均延迟: %.2f ms%n", averageLatency);
            System.out.printf("P95延迟: %.2f ms%n", p95Latency);
            System.out.printf("P99延迟: %.2f ms%n", p99Latency);
            System.out.printf("错误率: %.2f%%%n", errorRate);
            
            if (!customMetrics.isEmpty()) {
                System.out.println("自定义指标:");
                customMetrics.forEach((key, value) -> 
                    System.out.printf("  %s: %s%n", key, value));
            }
        }
    }
    
    /**
     * 生产者基准测试
     */
    public static class ProducerBenchmark {
        
        public BenchmarkResult benchmarkProducer(String bootstrapServers, String topic,
                                               int messageCount, int messageSize,
                                               int concurrentProducers, long testDurationMs) {
            System.out.println("开始生产者基准测试...");
            
            CountDownLatch startLatch = new CountDownLatch(1);
            CountDownLatch finishLatch = new CountDownLatch(concurrentProducers);
            
            List<ProducerThread> producerThreads = new ArrayList<>();
            for (int i = 0; i < concurrentProducers; i++) {
                producerThreads.add(new ProducerThread(bootstrapServers, topic, 
                    messageCount / concurrentProducers, messageSize, 
                    startLatch, finishLatch));
            }
            
            // 启动所有生产者线程
            executor.execute(() -> {
                try {
                    startLatch.await();
                    long startTime = System.currentTimeMillis();
                    
                    for (ProducerThread thread : producerThreads) {
                        thread.start();
                    }
                    
                    // 等待测试完成
                    finishLatch.await();
                    long endTime = System.currentTimeMillis();
                    
                    // 收集结果
                    long totalMessages = producerThreads.stream()
                        .mapToLong(ProducerThread::getMessagesSent)
                        .sum();
                    
                    List<Long> allLatencies = producerThreads.stream()
                        .flatMap(thread -> thread.getLatencies().stream())
                        .sorted()
                        .collect(Collectors.toList());
                    
                    double throughput = totalMessages * 1000.0 / (endTime - startTime);
                    double avgLatency = allLatencies.stream().mapToLong(Long::longValue).average().orElse(0);
                    double p95Latency = getPercentile(allLatencies, 95);
                    double p99Latency = getPercentile(allLatencies, 99);
                    
                    return new BenchmarkResult("Producer Benchmark", 
                        endTime - startTime, (int) totalMessages, throughput, 
                        avgLatency, p95Latency, p99Latency, 0.0);
                        
                } catch (Exception e) {
                    throw new RuntimeException(e);
                }
            });
            
            return null; // 简化实现
        }
        
        private double getPercentile(List<Long> sortedLatencies, double percentile) {
            if (sortedLatencies.isEmpty()) return 0;
            
            int index = (int) Math.ceil(sortedLatencies.size() * percentile / 100) - 1;
            return sortedLatencies.get(Math.max(0, index));
        }
    }
    
    /**
     * 消费者基准测试
     */
    public static class ConsumerBenchmark {
        
        public BenchmarkResult benchmarkConsumer(String bootstrapServers, String topic,
                                               int concurrentConsumers, 
                                               long testDurationMs) {
            System.out.println("开始消费者基准测试...");
            
            // 基准测试实现
            return null; // 简化实现
        }
    }
    
    /**
     * 端到端基准测试
     */
    public static class EndToEndBenchmark {
        
        public BenchmarkResult runEndToEndBenchmark(String bootstrapServers, String topic,
                                                   int messageCount, int messageSize,
                                                   int concurrentProducers, int concurrentConsumers) {
            System.out.println("开始端到端基准测试...");
            
            // 启动生产者和消费者
            ExecutorService executor = Executors.newFixedThreadPool(
                concurrentProducers + concurrentConsumers);
            
            List<Future<?>> futures = new ArrayList<>();
            
            // 启动生产者
            for (int i = 0; i < concurrentProducers; i++) {
                futures.add(executor.submit(() -> {
                    // 生产者逻辑
                }));
            }
            
            // 启动消费者
            for (int i = 0; i < concurrentConsumers; i++) {
                futures.add(executor.submit(() -> {
                    // 消费者逻辑
                }));
            }
            
            // 等待测试完成
            for (Future<?> future : futures) {
                try {
                    future.get();
                } catch (Exception e) {
                    System.err.println("测试线程异常: " + e.getMessage());
                }
            }
            
            executor.shutdown();
            
            return null; // 简化实现
        }
    }
    
    /**
     * 基准测试报告生成器
     */
    public static class BenchmarkReportGenerator {
        
        public static void generateComparisonReport(List<BenchmarkResult> results, String outputPath) {
            try (PrintWriter writer = new PrintWriter(new FileWriter(outputPath))) {
                writer.println("# Kafka性能基准测试报告");
                writer.println();
                writer.println("测试时间: " + new Date());
                writer.println();
                
                // 生成对比表格
                writer.println("| 测试项目 | 吞吐量(msg/sec) | 平均延迟(ms) | P95延迟(ms) | P99延迟(ms) | 错误率(%) |");
                writer.println("|---------|----------------|--------------|-------------|-------------|-----------|");
                
                for (BenchmarkResult result : results) {
                    writer.printf("| %s | %.2f | %.2f | %.2f | %.2f | %.2f |%n",
                        result.getTestName(), result.getThroughput(),
                        result.getAverageLatency(), result.getP95Latency(),
                        result.getP99Latency(), result.getErrorRate());
                }
                
                writer.println();
                writer.println("## 详细分析");
                
                for (BenchmarkResult result : results) {
                    writer.println("### " + result.getTestName());
                    writer.println("- 总消息数: " + result.getTotalMessages());
                    writer.println("- 测试时长: " + result.getTestDuration() + " ms");
                    writer.println("- 吞吐量: " + String.format("%.2f msg/sec", result.getThroughput()));
                    writer.println("- 平均延迟: " + String.format("%.2f ms", result.getAverageLatency()));
                    writer.println("- P95延迟: " + String.format("%.2f ms", result.getP95Latency()));
                    writer.println("- P99延迟: " + String.format("%.2f ms", result.getP99Latency()));
                    writer.println("- 错误率: " + String.format("%.2f%%", result.getErrorRate()));
                    writer.println();
                }
                
                System.out.println("基准测试报告已生成: " + outputPath);
                
            } catch (IOException e) {
                System.err.println("生成报告失败: " + e.getMessage());
            }
        }
    }
}

// 辅助类
class ProducerThread extends Thread {
    private final String bootstrapServers;
    private final String topic;
    private final int messageCount;
    private final int messageSize;
    private final CountDownLatch startLatch;
    private final CountDownLatch finishLatch;
    private final List<Long> latencies = new ArrayList<>();
    private volatile long messagesSent = 0;
    
    public ProducerThread(String bootstrapServers, String topic, int messageCount,
                         int messageSize, CountDownLatch startLatch, CountDownLatch finishLatch) {
        this.bootstrapServers = bootstrapServers;
        this.topic = topic;
        this.messageCount = messageCount;
        this.messageSize = messageSize;
        this.startLatch = startLatch;
        this.finishLatch = finishLatch;
    }
    
    @Override
    public void run() {
        try {
            startLatch.await();
            
            Properties props = OptimizedProducerConfig.getOptimizedProducerConfig();
            props.setProperty(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, bootstrapServers);
            
            try (KafkaProducer<String, String> producer = new KafkaProducer<>(props)) {
                String message = generateMessage(messageSize);
                
                for (int i = 0; i < messageCount; i++) {
                    long startTime = System.nanoTime();
                    
                    ProducerRecord<String, String> record = new ProducerRecord<>(topic, message);
                    producer.send(record).get();
                    
                    long endTime = System.nanoTime();
                    long latency = (endTime - startTime) / 1000000; // 转换为毫秒
                    
                    synchronized (latencies) {
                        latencies.add(latency);
                    }
                    
                    messagesSent++;
                }
            }
            
        } catch (Exception e) {
            System.err.println("生产者线程异常: " + e.getMessage());
        } finally {
            finishLatch.countDown();
        }
    }
    
    private String generateMessage(int size) {
        StringBuilder sb = new StringBuilder();
        String content = "A";
        while (sb.length() < size) {
            sb.append(content);
        }
        return sb.toString();
    }
    
    public long getMessagesSent() { return messagesSent; }
    public List<Long> getLatencies() { return latencies; }
}
```

## 9.8 性能调优最佳实践

### 9.8.1 生产环境调优清单

```java
/**
 * Kafka生产环境性能调优清单
 */
public class ProductionTuningChecklist {
    
    /**
     * 系统级调优项目
     */
    public static class SystemTuning {
        
        public static List<TuningItem> getSystemTuningItems() {
            return Arrays.asList(
                new TuningItem("文件描述符限制", "ulimit -n 65536", "高并发连接需要"),
                new TuningItem("虚拟内存设置", "vm.swappiness=1", "减少Swap使用"),
                new TuningItem("网络缓冲区", "net.core.rmem_max=134217728", "增大网络接收缓冲区"),
                new TuningItem("网络队列", "net.core.netdev_max_backlog=5000", "增大网络包队列"),
                new TuningItem("TCP设置", "net.ipv4.tcp