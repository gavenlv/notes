# 第3章：核心架构与组件详解

## 目录
1. [Kafka整体架构](#kafka整体架构)
2. [Broker内部组件](#broker内部组件)
3. [分区副本机制](#分区副本机制)
4. [Leader选举算法](#leader选举算法)
5. [数据一致性保证](#数据一致性保证)
6. [控制器(Controller)](#控制器controller)
7. [分区重分配](#分区重分配)
8. [性能优化原理](#性能优化原理)

## Kafka整体架构

### 架构概览
```
┌─────────────────────────────────────────────────────────────┐
│                        Kafka集群                            │
├─────────────────────────────────────────────────────────────┤
│  ┌─────────────┐  ┌─────────────┐  ┌─────────────┐          │
│  │   Broker 1  │  │   Broker 2  │  │   Broker 3  │          │
│  ├─────────────┤  ├─────────────┤  ├─────────────┤          │
│  │Partition 0  │  │Partition 1  │  │Partition 2  │          │
│  │Partition 1  │  │Partition 2  │  │Partition 0  │          │
│  │Partition 2  │  │Partition 0  │  │Partition 1  │          │
│  └─────────────┘  └─────────────┘  └─────────────┘          │
├─────────────────────────────────────────────────────────────┤
│                     Zookeeper集群                           │
│  ┌─────────────┐  ┌─────────────┐  ┌─────────────┐          │
│  │     ZK1     │  │     ZK2     │  │     ZK3     │          │
│  └─────────────┘  └─────────────┘  └─────────────┘          │
└─────────────────────────────────────────────────────────────┘

外部客户端：
┌─────────────┐  ┌─────────────┐  ┌─────────────┐
│  Producer   │  │  Consumer   │  │ Admin Tools │
└─────────────┘  └─────────────┘  └─────────────┘
```

### 架构特点
```
1. 分布式架构：
   - 多Broker节点支持水平扩展
   - 数据自动分区和分布
   - 负载均衡和容错

2. 存储架构：
   - 顺序写磁盘，高性能
   - 分段日志文件管理
   - 零拷贝技术优化

3. 通信协议：
   - 基于TCP的二进制协议
   - 异步非阻塞I/O
   - 批量处理优化

4. 容错机制：
   - 多副本冗余
   - 自动故障检测
   - Leader选举恢复
```

## Broker内部组件

### Broker架构图
```
┌─────────────────────────────────────────────────────────────┐
│                        Broker进程                           │
├─────────────────────────────────────────────────────────────┤
│  ┌─────────────┐  ┌─────────────┐  ┌─────────────┐          │
│  │   网络层    │  │   存储层    │  │   协调器    │          │
│  ├─────────────┤  ├─────────────┤  ├─────────────┤          │
│  │• Acceptor   │  │• Log Manager│  │• Group Coord│          │
│  │• Processor  │  │• Index      │  │• Offset Coord│         │
│  │• RequestHandler│ │• Compaction │  │• Txn Coord  │          │
│  └─────────────┘  └─────────────┘  └─────────────┘          │
├─────────────────────────────────────────────────────────────┤
│  ┌─────────────┐  ┌─────────────┐  ┌─────────────┐          │
│  │   副本管理   │  │  控制器     │  │  监控指标   │          │
│  ├─────────────┤  ├─────────────┤  ├─────────────┤          │
│  │• ReplicaMgr │  │• Controller │  │• Metrics    │          │
│  │• Partition  │  │• LeaderElection│ │• JMX       │          │
│  │• ISR        │  │• BrokerLife │  │• HealthCheck│         │
│  └─────────────┘  └─────────────┘  └─────────────┘          │
└─────────────────────────────────────────────────────────────┘
```

### 核心组件详解

#### 1. 网络层 (Network Layer)
```java
// 接收器：监听端口，接受连接
class Acceptor {
    private ServerSocketChannel serverChannel;
    
    public void run() {
        while (isRunning) {
            // 接受新连接
            SocketChannel channel = serverChannel.accept();
            
            // 创建Processor处理请求
            Processor processor = getIdleProcessor();
            processor.accept(channel);
        }
    }
}

// 处理器：处理具体请求
class Processor extends Thread {
    private BlockingQueue<SocketChannel> newConnections;
    
    public void run() {
        while (isRunning) {
            // 从连接队列中取出Channel
            SocketChannel channel = newConnections.poll();
            
            // 读取请求
            RequestHeader header = readRequest(channel);
            
            // 分发到RequestHandler
            RequestHandler handler = getHandler(header.apiKey());
            handler.handle(channel, header, request);
        }
    }
}

// 请求处理器：执行业务逻辑
class RequestHandler {
    public void handle(SocketChannel channel, RequestHeader header, Request request) {
        switch (header.apiKey()) {
            case PRODUCE:
                handleProduce(channel, request);
                break;
            case FETCH:
                handleFetch(channel, request);
                break;
            case METADATA:
                handleMetadata(channel, request);
                break;
            // ... 其他API
        }
    }
}
```

#### 2. 存储层 (Storage Layer)
```java
// 日志管理器
class LogManager {
    private ConcurrentHashMap<TopicPartition, Log> logs;
    
    public Log getLog(TopicPartition tp) {
        return logs.computeIfAbsent(tp, k -> new Log(dir, config));
    }
    
    public void append(RecordBatch batch) {
        Log log = getLog(batch.topicPartition());
        log.append(batch);
    }
}

// 日志：物理存储管理
class Log {
    private List<LogSegment> segments;
    private long nextOffset = 0;
    
    public void append(RecordBatch batch) {
        // 1. 检查是否需要创建新Segment
        if (needsNewSegment()) {
            createNewSegment();
        }
        
        // 2. 写入数据
        LogSegment segment = segments.get(segments.size() - 1);
        long offset = segment.append(batch);
        
        // 3. 更新索引
        updateIndex(batch.records(), offset);
        
        // 4. 触发刷盘（可选）
        if (shouldFlush()) {
            segment.flush();
        }
    }
}

// 日志段：物理文件
class LogSegment {
    private File file;
    private MappedByteBuffer indexBuffer;
    private MappedByteBuffer logBuffer;
    
    public long append(RecordBatch batch) {
        // 1. 计算offset
        long baseOffset = log.sizeInBytes() / 1024 / 1024;
        long relativeOffset = batch.lastOffset() - baseOffset;
        
        // 2. 写入数据文件
        logBuffer.put(batch.bytes());
        
        // 3. 更新索引
        IndexEntry entry = new IndexEntry(relativeOffset, logBuffer.position());
        indexBuffer.put(entry.bytes());
        
        return batch.lastOffset();
    }
}
```

#### 3. 副本管理器 (Replica Manager)
```java
// 副本管理器
class ReplicaManager {
    private Map<TopicPartition, Partition> partitions;
    
    public void becomeLeader(TopicPartition tp, List<Replica> replicas, ISR isr) {
        Partition partition = partitions.get(tp);
        partition.becomeLeader(replicas, isr);
    }
    
    public void becomeFollower(TopicPartition tp, List<Replica> replicas, int leaderId) {
        Partition partition = partitions.get(tp);
        partition.becomeFollower(replicas, leaderId);
    }
}

// 分区：管理副本状态
class Partition {
    private List<Replica> replicas;
    private Replica leaderReplica;
    private ISR isr;
    private ControllerEpoch controllerEpoch;
    
    public void becomeLeader(List<Replica> replicas, ISR isr) {
        this.replicas = replicas;
        this.leaderReplica = findLocalReplica();
        this.isr = isr;
        
        // 1. 将所有副本设为Follower
        replicas.forEach(r -> r.setFollowerState());
        
        // 2. 当前broker成为Leader
        this.leaderReplica.setLeaderState();
        
        // 3. 通知所有副本状态变化
        notifyReplicas();
    }
}

// 副本：单个副本状态
class Replica {
    private int brokerId;
    private TopicPartition topicPartition;
    private Log log;
    private ReplicaState state;
    
    public enum ReplicaState {
        Online,          // 正常在线
        Offline,         // 离线状态
        ReplicaDeletionStarted,  // 开始删除
        ReplicaDeletionSuccessful, // 删除成功
        ReplicaDeletionFailed    // 删除失败
    }
    
    public void setFollowerState() {
        this.state = ReplicaState.Online;
        startFetcher();  // 开始从Leader同步数据
    }
}
```

#### 4. 控制器 (Controller)
```java
// 控制器
class Controller {
    private BrokerControllerId brokerId;
    private Map<Integer, Broker> brokers;
    private Map<TopicPartition, Partition> partitions;
    
    public void onBrokerFailure(List<Integer> deadBrokers) {
        // 1. 更新broker状态
        deadBrokers.forEach(brokerId -> {
            Broker broker = brokers.get(brokerId);
            broker.setState(BrokerState.Offline);
        });
        
        // 2. 为受影响的分区选举新的Leader
        deadBrokers.forEach(brokerId -> {
            List<TopicPartition> affectedPartitions = getPartitionsForDeadBroker(brokerId);
            
            affectedPartitions.forEach(tp -> {
                try {
                    electNewLeader(tp);
                } catch (Exception e) {
                    log.error("Failed to elect leader for {}", tp, e);
                }
            });
        });
        
        // 3. 更新元数据
        updateMetadata();
    }
    
    private void electNewLeader(TopicPartition tp) throws Exception {
        Partition partition = partitions.get(tp);
        
        // 1. 获取当前ISR
        List<Replica> isr = partition.getInSyncReplicas();
        
        // 2. 从ISR中选举新Leader（按brokerId排序）
        if (!isr.isEmpty()) {
            Replica newLeader = isr.stream()
                .filter(r -> r.brokerId() != brokerId)
                .min(Comparator.comparing(Replica::brokerId))
                .orElseThrow(() -> new RuntimeException("No available replica"));
            
            // 3. 更新分区状态
            partition.becomeLeader(isr, tp);
            
            // 4. 通知新Leader
            sendLeadershipChange(newLeader.brokerId(), tp);
        }
    }
}
```

## 分区副本机制

### 副本分配策略
```java
// 副本分配算法
class ReplicaAssignmentStrategy {
    
    // 随机分配副本
    public Map<TopicPartition, List<Integer>> assignReplicasRandomly(
            int partitions, int replicationFactor, List<Integer> brokers) {
        
        Map<TopicPartition, List<Integer>> assignment = new HashMap<>();
        Random random = new Random();
        
        for (int p = 0; p < partitions; p++) {
            List<Integer> replicas = new ArrayList<>();
            List<Integer> availableBrokers = new ArrayList<>(brokers);
            
            // 选择Leader副本
            int leaderBroker = availableBrokers.get(random.nextInt(availableBrokers.size()));
            replicas.add(leaderBroker);
            availableBrokers.remove(Integer.valueOf(leaderBroker));
            
            // 选择Follower副本
            for (int r = 1; r < replicationFactor && !availableBrokers.isEmpty(); r++) {
                int followerBroker = availableBrokers.get(random.nextInt(availableBrokers.size()));
                replicas.add(followerBroker);
                availableBrokers.remove(Integer.valueOf(followerBroker));
            }
            
            assignment.put(new TopicPartition("topic", p), replicas);
        }
        
        return assignment;
    }
    
    // 轮询分配副本（保证负载均衡）
    public Map<TopicPartition, List<Integer>> assignReplicasRoundRobin(
            int partitions, int replicationFactor, List<Integer> brokers) {
        
        Map<TopicPartition, List<Integer>> assignment = new HashMap<>();
        int currentBroker = 0;
        
        for (int p = 0; p < partitions; p++) {
            List<Integer> replicas = new ArrayList<>();
            
            // Leader副本
            int leaderBroker = brokers.get(currentBroker % brokers.size());
            replicas.add(leaderBroker);
            
            // Follower副本（选择后续broker）
            for (int r = 1; r < replicationFactor; r++) {
                int followerBroker = brokers.get((currentBroker + r) % brokers.size());
                replicas.add(followerBroker);
            }
            
            assignment.put(new TopicPartition("topic", p), replicas);
            currentBroker++;
        }
        
        return assignment;
    }
}
```

### ISR机制
```java
// In-Sync Replicas (ISR) 管理
class ISRManager {
    
    public void updateISR(TopicPartition tp, long currentTimeMs) {
        Partition partition = partitions.get(tp);
        List<Replica> allReplicas = partition.getAllReplicas();
        List<Replica> newISR = new ArrayList<>();
        
        for (Replica replica : allReplicas) {
            // 检查副本是否与ZooKeeper有会话
            if (isZooKeeperSessionValid(replica.brokerId())) {
                // 检查副本是否与Leader保持同步
                if (isReplicaInSync(replica, currentTimeMs)) {
                    newISR.add(replica);
                } else {
                    // 从ISR中移除
                    log.debug("Removing replica {} from ISR for {}", replica.brokerId(), tp);
                }
            } else {
                // ZooKeeper会话超时，从ISR中移除
                log.debug("Removing replica {} from ISR due to ZooKeeper session timeout", replica.brokerId());
            }
        }
        
        // 更新ISR
        partition.setISR(newISR);
    }
    
    private boolean isReplicaInSync(Replica replica, long currentTimeMs) {
        // 1. 检查副本的日志是否落后
        long replicaLag = getReplicaLag(replica);
        if (replicaLag > replica.config.getMinInSyncReplicaLag()) {
            return false;
        }
        
        // 2. 检查副本是否超时
        long lastFetchTime = getLastFetchTime(replica);
        if (currentTimeMs - lastFetchTime > replica.config.getReplicaMaxLagTimeMs()) {
            return false;
        }
        
        return true;
    }
}

// 示例：副本同步检查
public class ReplicaSyncCheck {
    
    public void periodicSyncCheck() {
        for (TopicPartition tp : allPartitions) {
            Partition partition = partitions.get(tp);
            Replica leader = partition.getLeader();
            
            // 检查每个Follower的同步状态
            for (Replica follower : partition.getFollowers()) {
                // 计算副本延迟
                long replicaLag = leader.logEndOffset() - follower.logEndOffset();
                
                // 更新副本状态
                if (replicaLag > config.getReplicaLagMaxMessages()) {
                    // 副本严重滞后，标记为同步异常
                    log.warn("Replica {} is lagging behind leader by {} messages for {}", 
                            follower.brokerId(), replicaLag, tp);
                }
            }
        }
    }
}
```

## Leader选举算法

### 选举算法详解
```java
// Leader选举算法
class LeaderElectionAlgorithm {
    
    // 基于优先级的选举算法
    public Replica electLeader(List<Replica> isr, Map<Integer, Integer> brokerPriorities) {
        // 1. 过滤出可用的副本
        List<Replica> availableReplicas = isr.stream()
            .filter(Replica::isOnline)
            .collect(Collectors.toList());
        
        if (availableReplicas.isEmpty()) {
            throw new NoReplicaAvailableException("No replicas available for election");
        }
        
        // 2. 按优先级排序（数值越小优先级越高）
        availableReplicas.sort(Comparator.comparing(r -> 
            brokerPriorities.getOrDefault(r.brokerId(), Integer.MAX_VALUE)));
        
        // 3. 选择优先级最高的副本作为新Leader
        Replica newLeader = availableReplicas.get(0);
        
        log.info("Elected leader {} for partition {}", newLeader.brokerId(), 
                newLeader.topicPartition());
        
        return newLeader;
    }
    
    // 基于轮询的选举算法
    public Replica electLeaderRoundRobin(Map<Integer, Integer> brokerRoundRobinIndex) {
        int totalBrokers = brokers.size();
        int currentRoundRobin = getCurrentRoundRobinIndex();
        
        // 尝试每个broker作为候选Leader
        for (int i = 0; i < totalBrokers; i++) {
            int candidateBrokerId = (currentRoundRobin + i) % totalBrokers;
            
            if (isBrokerAvailable(candidateBrokerId)) {
                updateRoundRobinIndex(candidateBrokerId + 1);
                return getReplica(candidateBrokerId);
            }
        }
        
        throw new NoReplicaAvailableException("No suitable leader found");
    }
    
    // 基于偏移量的选举算法
    public Replica electLeaderByOffset(List<Replica> isr) {
        // 选择日志最全的副本作为Leader
        return isr.stream()
            .filter(Replica::isOnline)
            .max(Comparator.comparing(Replica::logEndOffset))
            .orElseThrow(() -> new NoReplicaAvailableException("No available replica"));
    }
}
```

### 选举触发场景
```java
// 选举触发条件
public class ElectionTriggers {
    
    // 1. Broker故障触发选举
    public void onBrokerFailure(int failedBrokerId) {
        List<TopicPartition> affectedPartitions = 
            getPartitionsWithBroker(failedBrokerId);
        
        for (TopicPartition tp : affectedPartitions) {
            Partition partition = partitions.get(tp);
            
            // 如果故障的Broker是Leader，需要重新选举
            if (partition.getLeader().brokerId() == failedBrokerId) {
                try {
                    electNewLeader(tp);
                } catch (Exception e) {
                    log.error("Leader election failed for {}", tp, e);
                }
            }
        }
    }
    
    // 2. 副本恢复触发选举
    public void onReplicaRecovery(int recoveredBrokerId) {
        List<TopicPartition> affectedPartitions = 
            getPartitionsWithBroker(recoveredBrokerId);
        
        for (TopicPartition tp : affectedPartitions) {
            Partition partition = partitions.get(tp);
            
            // 如果有更合适的Leader候选者，重新选举
            if (shouldTriggerElection(tp)) {
                try {
                    electNewLeader(tp);
                } catch (Exception e) {
                    log.error("Leader re-election failed for {}", tp, e);
                }
            }
        }
    }
    
    // 3. 控制器切换触发选举
    public void onControllerSwitch(int newControllerId) {
        // 控制器切换后，需要对所有分区重新进行Leader选举
        // 避免不同控制器之间的状态不一致
        for (TopicPartition tp : allPartitions) {
            try {
                verifyLeaderForPartition(tp);
            } catch (Exception e) {
                log.error("Leader verification failed for {}", tp, e);
            }
        }
    }
}
```

## 数据一致性保证

### 写入一致性
```java
// 生产者写入策略
class ProducerConsistency {
    
    // acks=0: 发送后不等待确认
    public void sendWithAcks0(ProducerRecord record) {
        producer.send(record, new Callback() {
            @Override
            public void onCompletion(RecordMetadata metadata, Exception exception) {
                if (exception != null) {
                    // 发送失败，但不等确认
                    log.warn("Send failed but acks=0, continuing", exception);
                }
            }
        });
    }
    
    // acks=1: 等待Leader确认
    public void sendWithAcks1(ProducerRecord record) {
        producer.send(record, new Callback() {
            @Override
            public void onCompletion(RecordMetadata metadata, Exception exception) {
                if (exception != null) {
                    // Leader确认失败，数据可能丢失
                    log.error("Leader acknowledgment failed", exception);
                    handleDataLoss(record);
                }
            }
        });
    }
    
    // acks=all: 等待ISR中所有副本确认
    public void sendWithAcksAll(ProducerRecord record) {
        producer.send(record, new Callback() {
            @Override
            public void onCompletion(RecordMetadata metadata, Exception exception) {
                if (exception != null) {
                    // 所有副本确认失败，数据没有丢失
                    log.warn("ISR acknowledgment failed", exception);
                    handleDataInconsistency(record);
                }
            }
        });
    }
}

// Broker端一致性检查
class BrokerConsistencyManager {
    
    public void checkAndEnsureConsistency(TopicPartition tp) {
        Partition partition = partitions.get(tp);
        
        // 1. 检查Leader副本的日志完整性
        if (isLogCorrupted(partition.getLeader())) {
            // 标记分区为不可用
            partition.setCorrupt();
            triggerLeaderElection(tp);
            return;
        }
        
        // 2. 检查Follower副本的同步状态
        List<Replica> followers = partition.getFollowers();
        for (Replica follower : followers) {
            // 如果Follower严重滞后，进行数据恢复
            if (isFollowerOutOfSync(follower)) {
                triggerReplicaRecovery(follower);
            }
        }
        
        // 3. 检查ISR的有效性
        if (!isISRValid(partition.getISR())) {
            updateISR(tp);
        }
    }
}
```

### 读取一致性
```java
// 消费者读取策略
class ConsumerConsistency {
    
    // 可重复读（Repetable Read）
    public void consumeWithRepeatableRead(String groupId, TopicPartition tp) {
        ConsumerConfig config = new ConsumerConfig();
        config.setProperty(ConsumerConfig.ENABLE_AUTO_COMMIT_CONFIG, "false"); // 手动提交
        config.setProperty(ConsumerConfig.ISOLATION_LEVEL_CONFIG, "read_committed"); // 只读已提交
        
        KafkaConsumer<String, String> consumer = new KafkaConsumer<>(config);
        consumer.assign(Arrays.asList(tp));
        
        while (true) {
            ConsumerRecords<String, String> records = consumer.poll(Duration.ofMillis(100));
            
            for (ConsumerRecord<String, String> record : records) {
                // 处理消息
                processRecord(record);
                
                // 批量提交offset，避免重复消费
                consumer.commitAsync();
            }
        }
    }
    
    // 读已提交（Read Committed）
    public void consumeWithReadCommitted(String groupId, TopicPartition tp) {
        ConsumerConfig config = new ConsumerConfig();
        config.setProperty(ConsumerConfig.ISOLATION_LEVEL_CONFIG, "read_committed");
        
        KafkaConsumer<String, String> consumer = new KafkaConsumer<>(config);
        consumer.assign(Arrays.asList(tp));
        
        // 确保只读取已提交的消息
        long lastCommittedOffset = getLastCommittedOffset(groupId, tp);
        
        consumer.seek(tp, lastCommittedOffset);
        
        while (true) {
            ConsumerRecords<String, String> records = consumer.poll(Duration.ofMillis(100));
            // 只处理已提交的消息
            processCommittedRecords(records);
        }
    }
}

// 事务性处理
public class TransactionalProcessing {
    
    public void processWithTransaction() {
        KafkaProducer<String, String> producer = new KafkaProducer<>(config);
        producer.initTransactions();
        
        try {
            producer.beginTransaction();
            
            // 生产消息到输出主题
            producer.send(new ProducerRecord<>("output-topic", "key", "value"));
            
            // 提交offset到特殊主题
            producer.sendOffsetsToTransaction(getCurrentOffsets(), "group-id");
            
            // 提交事务
            producer.commitTransaction();
            
        } catch (Exception e) {
            // 回滚事务
            producer.abortTransaction();
            log.error("Transaction failed", e);
        } finally {
            producer.close();
        }
    }
}
```

## 控制器(Controller)

### 控制器职责
```java
// 控制器核心职责
class ControllerResponsibilities {
    
    // 1. Broker管理
    public void manageBrokers() {
        // 监听Broker状态变化
        registerBrokerChangeListener();
        
        // 处理Broker启动/关闭
        onBrokerStart(brokerId);
        onBrokerShutdown(brokerId);
        
        // 管理Broker元数据
        updateBrokerMetadata(brokerId, metadata);
    }
    
    // 2. 分区管理
    public void managePartitions() {
        // 监听分区变化
        registerPartitionChangeListener();
        
        // 处理分区重分配
        reassignPartitions(partitionReassignment);
        
        // 监听Leader选举
        onLeaderElection(partition, newLeader);
    }
    
    // 3. 副本管理
    public void manageReplicas() {
        // 监听副本状态变化
        registerReplicaChangeListener();
        
        // 管理ISR变化
        onISRChange(partition, newISR);
        
        // 处理副本同步
        onReplicaSync(partition, replicaId);
    }
    
    // 4. 元数据管理
    public void manageMetadata() {
        // 更新集群元数据
        updateClusterMetadata();
        
        // 广播元数据变更
        broadcastMetadataUpdate();
        
        // 处理元数据请求
        handleMetadataRequest(request);
    }
}

// 控制器选举
public class ControllerElection {
    
    private static final String CONTROLLER_ZNODE = "/controller";
    
    public void tryBecomeController() {
        // 1. 检查当前控制器状态
        String currentControllerData = zkClient.getData(CONTROLLER_ZNODE);
        
        if (currentControllerData == null) {
            // 没有控制器，可以竞争
            if (tryCreateControllerNode()) {
                becomeController();
            }
        } else {
            ControllerData controllerData = parseControllerData(currentControllerData);
            
            // 检查当前控制器是否已经失效
            if (isControllerExpired(controllerData)) {
                // 删除旧控制器节点
                if (tryDeleteControllerNode(controllerData.controllerEpoch)) {
                    // 创建新的控制器节点
                    if (tryCreateControllerNode()) {
                        becomeController();
                    }
                }
            }
        }
    }
    
    private boolean tryCreateControllerNode() {
        try {
            ControllerData controllerData = new ControllerData(
                brokerId, 
                System.currentTimeMillis(),
                getCurrentEpoch() + 1
            );
            
            zkClient.create(CONTROLLER_ZNODE, controllerData.toBytes());
            return true;
        } catch (NodeExistsException e) {
            return false;
        }
    }
}
```

## 分区重分配

### 分区重分配算法
```java
// 分区重分配管理器
public class PartitionReassignmentManager {
    
    public PartitionReassignment executeReassignment(PartitionReassignment reassignment) {
        // 1. 验证重分配计划
        validateReassignmentPlan(reassignment);
        
        // 2. 创建重分配任务
        ReassignmentTask task = createReassignmentTask(reassignment);
        
        // 3. 执行重分配
        return executeReassignment(task);
    }
    
    private PartitionReassignment executeReassignment(ReassignmentTask task) {
        Map<TopicPartition, List<Integer>> currentAssignment = getCurrentAssignment();
        Map<TopicPartition, List<Integer>> targetAssignment = task.getTargetAssignment();
        
        // 1. 将目标副本添加到ISR中
        for (TopicPartition tp : task.getPartitionsToReassign()) {
            List<Integer> targetReplicas = targetAssignment.get(tp);
            
            for (int replicaId : targetReplicas) {
                // 将新副本添加到ISR
                addReplicaToISR(tp, replicaId);
                
                // 开始同步数据
                startReplicaSync(tp, replicaId);
            }
        }
        
        // 2. 等待所有副本同步完成
        waitForReplicaSync(task.getPartitionsToReassign());
        
        // 3. 移除旧副本
        for (TopicPartition tp : task.getPartitionsToReassign()) {
            List<Integer> currentReplicas = currentAssignment.get(tp);
            List<Integer> targetReplicas = targetAssignment.get(tp);
            
            // 找出需要移除的副本
            List<Integer> replicasToRemove = currentReplicas.stream()
                .filter(replicaId -> !targetReplicas.contains(replicaId))
                .collect(Collectors.toList());
            
            for (int replicaId : replicasToRemove) {
                removeReplica(tp, replicaId);
            }
        }
        
        // 4. 更新分区Leader
        updatePartitionLeaders(task.getPartitionsToReassign());
        
        return new PartitionReassignment(targetAssignment);
    }
}
```

## 性能优化原理

### 顺序写磁盘
```java
// 顺序写优化
class SequentialDiskWrite {
    
    public void demonstrateSequentialWrite() {
        File logFile = new File("kafka-logs/topic-partition-0/00000000000000000000.log");
        
        // 传统随机写：每次都要寻道
        try (RandomAccessFile raf = new RandomAccessFile(logFile, "rw")) {
            for (int i = 0; i < 10000; i++) {
                raf.seek(raf.length()); // 寻道到文件末尾
                raf.writeInt(i);        // 写入数据
            }
            // 总时间：~30秒
        }
        
        // 顺序写：一次寻道，持续写入
        try (RandomAccessFile raf = new RandomAccessFile(logFile, "rw")) {
            for (int i = 0; i < 10000; i++) {
                raf.writeInt(i);        // 直接写入当前位置
            }
            // 总时间：~0.1秒
        }
    }
}
```

### 零拷贝技术
```java
// 零拷贝实现
class ZeroCopyOptimization {
    
    public void demonstrateZeroCopy() {
        // 传统方式：多次数据复制
        // 用户空间 → 内核空间 → 应用程序 → 网络缓冲区 → 网络接口
        
        // 零拷贝方式：直接数据传输
        // 磁盘 → 内核缓冲区 → 网络缓冲区 → 网络接口
        
        // Java NIO零拷贝实现
        FileChannel fileChannel = new RandomAccessFile(file, "r").getChannel();
        
        // 使用sendfile系统调用
        fileChannel.transferTo(position, length, socketChannel);
        
        // 或者使用splice系统调用
        fileChannel.spliceTo(socketChannel, position, length, SPLICE_F_MOVE);
    }
    
    public void demonstrateBufferedRead() {
        // 传统方式：单个字节读取
        try (FileInputStream fis = new FileInputStream(file)) {
            int b;
            while ((b = fis.read()) != -1) {
                processByte(b); // 每个字节都需要系统调用
            }
        }
        
        // 缓冲区方式：批量读取
        try (BufferedInputStream bis = new BufferedInputStream(
                new FileInputStream(file))) {
            byte[] buffer = new byte[8192];
            int bytesRead;
            while ((bytesRead = bis.read(buffer)) != -1) {
                processBytes(buffer, bytesRead); // 批量处理
            }
        }
    }
}
```

### 批量处理
```java
// 批量处理优化
class BatchProcessing {
    
    // 消息批次
    public class MessageBatch {
        private List<Message> messages;
        private long maxBatchSize = 16384; // 16KB
        private long maxBatchTime = 100;   // 100ms
        
        public void addMessage(Message message) {
            messages.add(message);
            
            // 检查是否需要发送批次
            if (shouldSendBatch()) {
                sendBatch();
            }
        }
        
        private boolean shouldSendBatch() {
            // 1. 批次大小达到阈值
            if (getBatchSize() >= maxBatchSize) {
                return true;
            }
            
            // 2. 批次时间达到阈值
            if (getBatchAge() >= maxBatchTime) {
                return true;
            }
            
            return false;
        }
        
        private void sendBatch() {
            // 一次性发送整个批次，减少网络往返次数
            producer.send(messages);
            messages.clear();
        }
    }
}
```

## 本章小结

### 核心概念总结
```
1. 架构设计：分布式、多层次、高性能
2. 核心组件：网络层、存储层、控制器、副本管理器
3. 副本机制：多副本冗余、ISR管理、Leader选举
4. 一致性保证：写入一致性、读取一致性、事务支持
5. 性能优化：顺序写、零拷贝、批量处理
```

### 设计亮点
```
1. 高性能：顺序写 + 零拷贝 + 批量处理
2. 高可用：多副本 + 自动选举 + 容错恢复
3. 高扩展：分布式架构 + 动态负载均衡
4. 强一致：ISR机制 + 事务支持 + 幂等保证
```

### 学习建议
```
1. 理解架构要有层次感，从宏观到微观
2. 重点掌握副本机制和一致性保证
3. 理解性能优化的底层原理
4. 为后续的开发实践打下理论基础
```

### 下一章预告
第4章《主题与分区管理》将详细介绍：
- 主题创建和配置参数
- 分区策略和负载均衡
- 日志分段和清理机制
- 数据保留和压缩策略
- 主题管理和运维操作

### 思考练习
```
1. 画出完整的Kafka架构图，标明各组件关系
2. 解释ISR机制的工作原理和重要性
3. 分析不同一致性级别对性能的影响
4. 设计一个3节点集群的副本分配方案
5. 说明为什么Kafka能支持高吞吐量的底层原理
```

---

**重要提醒**：
- 架构理解是Kafka掌握的关键基础
- 性能优化要理解底层原理，不能只记配置
- 实际生产中要根据业务需求选择合适的一致性级别
- 监控和调优需要基于对内部机制的理解