# 第5章：生产者开发实战

## 目录
1. [生产者基础概念与API](#生产者基础概念与API)
2. [消息发送模式详解](#消息发送模式详解)
3. [分区策略与负载均衡](#分区策略与负载均衡)
4. [批量处理与压缩优化](#批量处理与压缩优化)
5. [事务性发送与幂等性](#事务性发送与幂等性)
6. [错误处理与重试机制](#错误处理与重试机制)
7. [生产性能监控与调优](#生产性能监控与调优)
8. [高级生产者特性](#高级生产者特性)
9. [实战项目：订单消息系统](#实战项目订单消息系统)
10. [最佳实践与常见问题](#最佳实践与常见问题)

---

## 生产者基础概念与API

### Kafka生产者核心概念

Kafka生产者是负责将数据发送到Kafka集群的客户端应用程序。它是构建数据流应用的关键组件，负责将数据按照指定的主题和分区策略发送到Kafka broker。

#### 生产者的核心职责：
1. **消息封装**：将业务数据封装成ProducerRecord
2. **序列化**：将消息转换为字节数组
3. **分区决策**：确定消息发送的目标分区
4. **批量处理**：将消息按照批量大小和时间进行聚合
5. **压缩**：对批量数据进行压缩以提高传输效率
6. **可靠性保证**：确保消息按照配置的确认机制送达

### KafkaProducer API架构

```java
package com.kafka.tutorial.producer;

import org.apache.kafka.clients.producer.*;
import org.apache.kafka.common.serialization.StringSerializer;
import org.apache.kafka.common.header.Header;
import org.apache.kafka.common.header.Headers;
import org.apache.kafka.common.record.CompressionType;

import java.util.*;
import java.util.concurrent.Future;
import java.util.concurrent.TimeUnit;
import java.util.concurrent.atomic.AtomicBoolean;
import java.util.concurrent.atomic.AtomicLong;

/**
 * 基础Kafka生产者实现
 */
public class BasicKafkaProducer {
    private final KafkaProducer<String, String> producer;
    private final String clientId;
    private final AtomicBoolean closed = new AtomicBoolean(false);
    private final AtomicLong messagesSent = new AtomicLong(0);
    private final AtomicLong sendErrors = new AtomicLong(0);
    
    public BasicKafkaProducer(Properties configs) {
        this.clientId = configs.getProperty(ProducerConfig.CLIENT_ID_CONFIG, "basic-producer");
        this.producer = new KafkaProducer<>(configs);
    }
    
    /**
     * 创建基础生产者配置
     */
    public static Properties createBasicConfigs() {
        Properties configs = new Properties();
        
        // 基础连接配置
        configs.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, "localhost:9092");
        configs.put(ProducerConfig.CLIENT_ID_CONFIG, "basic-producer");
        
        // 序列化配置
        configs.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, StringSerializer.class.getName());
        configs.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, StringSerializer.class.getName());
        
        // 基础性能配置
        configs.put(ProducerConfig.BATCH_SIZE_CONFIG, 16384); // 16KB
        configs.put(ProducerConfig.LINGER_MS_CONFIG, 0);
        configs.put(ProducerConfig.BUFFER_MEMORY_CONFIG, 33554432); // 32MB
        
        // 可靠性配置
        configs.put(ProducerConfig.ACKS_CONFIG, "1");
        
        return configs;
    }
    
    /**
     * 发送同步消息
     */
    public void sendSynchronousMessage(String topic, String key, String value) {
        if (closed.get()) {
            throw new IllegalStateException("Producer has been closed");
        }
        
        ProducerRecord<String, String> record = new ProducerRecord<>(topic, key, value);
        
        try {
            // 同步发送，阻塞等待结果
            RecordMetadata metadata = producer.send(record).get(30, TimeUnit.SECONDS);
            
            messagesSent.incrementAndGet();
            System.out.println("消息发送成功: " +
                "Topic: " + metadata.topic() + 
                ", Partition: " + metadata.partition() + 
                ", Offset: " + metadata.offset() + 
                ", Timestamp: " + metadata.timestamp());
                
        } catch (Exception e) {
            sendErrors.incrementAndGet();
            System.err.println("消息发送失败: " + e.getMessage());
            throw new RuntimeException("Failed to send message", e);
        }
    }
    
    /**
     * 发送异步消息
     */
    public void sendAsynchronousMessage(String topic, String key, String value) {
        if (closed.get()) {
            throw new IllegalStateException("Producer has been closed");
        }
        
        ProducerRecord<String, String> record = new ProducerRecord<>(topic, key, value);
        
        // 异步发送，使用回调处理结果
        producer.send(record, new Callback() {
            @Override
            public void onCompletion(RecordMetadata metadata, Exception exception) {
                if (exception != null) {
                    sendErrors.incrementAndGet();
                    System.err.println("异步消息发送失败: " + exception.getMessage());
                } else {
                    messagesSent.incrementAndGet();
                    System.out.println("异步消息发送成功: " +
                        "Topic: " + metadata.topic() + 
                        ", Partition: " + metadata.partition() + 
                        ", Offset: " + metadata.offset());
                }
            }
        });
    }
    
    /**
     * 发送带自定义头部的消息
     */
    public void sendMessageWithHeaders(String topic, String key, String value, Map<String, String> headers) {
        if (closed.get()) {
            throw new IllegalStateException("Producer has been closed");
        }
        
        // 创建带头部的新记录
        ProducerRecord<String, String> record = new ProducerRecord<>(topic, null, key, value) {
            @Override
            public Headers headers() {
                Headers headers = new RecordHeaders();
                
                // 添加自定义头部
                if (headers != null) {
                    headers.forEach(header -> {
                        if (header.key().equals("source")) {
                            ((RecordHeaders) headers).remove("source"); // 移除重复
                        }
                        headers.add(header);
                    });
                }
                
                // 添加默认头部
                headers.add(new RecordHeader("source", "kafka-tutorial".getBytes()));
                headers.add(new RecordHeader("version", "1.0".getBytes()));
                headers.add(new RecordHeader("timestamp", String.valueOf(System.currentTimeMillis()).getBytes()));
                
                return headers;
            }
        };
        
        producer.send(record);
    }
    
    /**
     * 批量发送消息
     */
    public void sendBatchMessages(String topic, List<String> messages) {
        if (closed.get()) {
            throw new IllegalStateException("Producer has been closed");
        }
        
        // 使用事务性发送
        producer.beginTransaction();
        
        try {
            List<Future<RecordMetadata>> futures = new ArrayList<>();
            
            for (int i = 0; i < messages.size(); i++) {
                String message = messages.get(i);
                String key = "batch-key-" + i;
                
                ProducerRecord<String, String> record = new ProducerRecord<>(topic, key, message);
                Future<RecordMetadata> future = producer.send(record);
                futures.add(future);
            }
            
            // 等待所有消息发送完成
            for (Future<RecordMetadata> future : futures) {
                future.get(10, TimeUnit.SECONDS);
            }
            
            producer.commitTransaction();
            messagesSent.addAndGet(messages.size());
            
            System.out.println("批量发送成功: " + messages.size() + " 条消息");
            
        } catch (Exception e) {
            producer.abortTransaction();
            sendErrors.addAndGet(messages.size());
            System.err.println("批量发送失败: " + e.getMessage());
            throw new RuntimeException("Failed to send batch messages", e);
        }
    }
    
    /**
     * 发送带时间戳的消息
     */
    public void sendMessageWithTimestamp(String topic, String key, String value, long timestamp) {
        if (closed.get()) {
            throw new IllegalStateException("Producer has been closed");
        }
        
        ProducerRecord<String, String> record = new ProducerRecord<>(topic, null, timestamp, key, value);
        producer.send(record);
    }
    
    /**
     * 获取生产者统计信息
     */
    public ProducerStats getStats() {
        return new ProducerStats(
            clientId,
            messagesSent.get(),
            sendErrors.get(),
            closed.get(),
            System.currentTimeMillis()
        );
    }
    
    /**
     * 优雅关闭生产者
     */
    public void close() {
        if (closed.compareAndSet(false, true)) {
            try {
                producer.flush(); // 刷新所有待发送的消息
                producer.close(30, TimeUnit.SECONDS);
                System.out.println("生产者已关闭: " + clientId);
            } catch (Exception e) {
                System.err.println("关闭生产者时发生错误: " + e.getMessage());
            }
        }
    }
    
    /**
     * 生产者统计信息
     */
    public static class ProducerStats {
        private final String clientId;
        private final long messagesSent;
        private final long sendErrors;
        private final boolean closed;
        private final long timestamp;
        
        public ProducerStats(String clientId, long messagesSent, long sendErrors, boolean closed, long timestamp) {
            this.clientId = clientId;
            this.messagesSent = messagesSent;
            this.sendErrors = sendErrors;
            this.closed = closed;
            this.timestamp = timestamp;
        }
        
        public double getErrorRate() {
            return messagesSent > 0 ? (double) sendErrors / messagesSent : 0.0;
        }
        
        // Getters
        public String getClientId() { return clientId; }
        public long getMessagesSent() { return messagesSent; }
        public long getSendErrors() { return sendErrors; }
        public boolean isClosed() { return closed; }
        public long getTimestamp() { return timestamp; }
        
        @Override
        public String toString() {
            return String.format("ProducerStats{clientId='%s', sent=%d, errors=%d, errorRate=%.2f%%, closed=%s}", 
                clientId, messagesSent, sendErrors, getErrorRate() * 100, closed);
        }
    }
}
```

### ProducerRecord详解

```java
package com.kafka.tutorial.producer;

import org.apache.kafka.clients.producer.ProducerRecord;
import org.apache.kafka.common.header.Header;
import org.apache.kafka.common.header.Headers;
import org.apache.kafka.common.header.internals.RecordHeaders;
import org.apache.kafka.common.record.CompressionType;

import java.util.*;
import java.util.concurrent.atomic.AtomicInteger;

/**
 * ProducerRecord的高级用法和自定义实现
 */
public class AdvancedRecordCreation {
    
    /**
     * 创建带有复杂结构的消息
     */
    public static ProducerRecord<String, String> createJsonMessage(String topic, String key, Object jsonObject) {
        return new ProducerRecord<>(
            topic,
            key,
            jsonObject.toString()
        );
    }
    
    /**
     * 创建带有压缩的消息
     */
    public static ProducerRecord<String, String> createCompressedMessage(String topic, String key, String value) {
        ProducerRecord<String, String> record = new ProducerRecord<>(topic, key, value);
        
        // 注意：压缩类型通常在生产者配置中设置，而不是单个记录
        // 这里展示如何添加压缩相关的元数据
        
        return record;
    }
    
    /**
     * 创建批处理用的自定义记录
     */
    public static List<ProducerRecord<String, String>> createBatchRecords(String topic, List<String> messages) {
        List<ProducerRecord<String, String>> records = new ArrayList<>();
        AtomicInteger counter = new AtomicInteger(0);
        
        for (String message : messages) {
            String key = "batch-" + counter.incrementAndGet();
            
            ProducerRecord<String, String> record = new ProducerRecord<>(topic, key, message) {
                @Override
                public Headers headers() {
                    Headers headers = new RecordHeaders();
                    headers.add(new RecordHeader("batch-id", UUID.randomUUID().toString().getBytes()));
                    headers.add(new RecordHeader("sequence", String.valueOf(counter.get()).getBytes()));
                    headers.add(new RecordHeader("created-at", String.valueOf(System.currentTimeMillis()).getBytes()));
                    return headers;
                }
            };
            
            records.add(record);
        }
        
        return records;
    }
    
    /**
     * 创建分区特定的消息
     */
    public static Producer> createPartitionedMessageRecord<String, String(String topic, int partition, String key, String value) {
        return new ProducerRecord<>(topic, partition, null, key, value);
    }
    
    /**
     * 创建带有时间戳的消息
     */
    public static ProducerRecord<String, String> createTimestampedMessage(String topic, long timestamp, String key, String value) {
        return new ProducerRecord<>(topic, null, timestamp, key, value);
    }
    
    /**
     * 创建带有多层头部的消息
     */
    public static ProducerRecord<String, String> createMessageWithComplexHeaders(String topic, String key, String value, 
                                                                                Map<String, Object> metadata) {
        ProducerRecord<String, String> record = new ProducerRecord<>(topic, key, value);
        
        Headers headers = new RecordHeaders();
        
        // 添加业务元数据
        metadata.forEach((k, v) -> {
            headers.add(new RecordHeader("biz-" + k, v.toString().getBytes()));
        });
        
        // 添加系统元数据
        headers.add(new RecordHeader("system-version", "1.0.0".getBytes()));
        headers.add(new RecordHeader("environment", "production".getBytes()));
        headers.add(new RecordHeader("correlation-id", UUID.randomUUID().toString().getBytes()));
        
        return record;
    }
    
    /**
     * 创建键值对消息处理器
     */
    public static class KeyValueProcessor {
        private final Map<String, String> keyValuePairs;
        
        public KeyValueProcessor(Map<String, String> keyValuePairs) {
            this.keyValuePairs = new HashMap<>(keyValuePairs);
        }
        
        public ProducerRecord<String, String> createMessage(String topic, String value) {
            if (keyValuePairs.isEmpty()) {
                throw new IllegalArgumentException("Key-value pairs cannot be empty");
            }
            
            // 选择第一个键作为主键
            String primaryKey = keyValuePairs.keySet().iterator().next();
            String primaryValue = keyValuePairs.get(primaryKey);
            
            // 将所有键值对序列化为JSON作为值
            String compositeValue = buildCompositeValue(value);
            
            return new ProducerRecord<>(topic, primaryKey, compositeValue);
        }
        
        private String buildCompositeValue(String userValue) {
            StringBuilder json = new StringBuilder();
            json.append("{\"userValue\":\"").append(userValue.replace("\"", "\\\"")).append("\"");
            json.append(",\"metadata\":{");
            
            keyValuePairs.forEach((k, v) -> {
                json.append("\"").append(k).append("\":\"").append(v.replace("\"", "\\\"")).append("\",");
            });
            
            // 移除最后的逗号
            if (json.charAt(json.length() - 1) == ',') {
                json.deleteCharAt(json.length() - 1);
            }
            
            json.append("}}");
            return json.toString();
        }
        
        // 添加键值对
        public void addKeyValuePair(String key, String value) {
            this.keyValuePairs.put(key, value);
        }
    }
    
    /**
     * 消息模板生成器
     */
    public static class MessageTemplate {
        private final String topic;
        private final String template;
        private final Map<String, Object> defaultHeaders;
        
        public MessageTemplate(String topic, String template, Map<String, Object> defaultHeaders) {
            this.topic = topic;
            this.template = template;
            this.defaultHeaders = new HashMap<>(defaultHeaders);
        }
        
        public ProducerRecord<String, String> createMessage(String key, Map<String, Object> variables) {
            String value = buildValue(variables);
            
            ProducerRecord<String, String> record = new ProducerRecord<>(topic, key, value);
            
            // 添加默认头部
            Headers headers = new RecordHeaders();
            defaultHeaders.forEach((k, v) -> {
                headers.add(new RecordHeader(k, v.toString().getBytes()));
            });
            
            return record;
        }
        
        private String buildValue(Map<String, Object> variables) {
            String value = template;
            
            for (Map.Entry<String, Object> entry : variables.entrySet()) {
                String placeholder = "{" + entry.getKey() + "}";
                value = value.replace(placeholder, entry.getValue().toString());
            }
            
            return value;
        }
    }
}
```

---

## 消息发送模式详解

### 同步发送模式

同步发送是最简单直接的发送方式，适用于对消息顺序和可靠性要求较高的场景。

```java
package com.kafka.tutorial.producer;

import org.apache.kafka.clients.producer.*;
import org.apache.kafka.common.serialization.StringSerializer;
import org.apache.kafka.common.header.Headers;
import org.apache.kafka.common.record.CompressionType;

import java.util.*;
import java.util.concurrent.*;
import java.util.concurrent.atomic.AtomicInteger;
import java.util.concurrent.atomic.AtomicLong;

/**
 * 同步发送模式实现
 */
public class SynchronousProducer {
    private final KafkaProducer<String, String> producer;
    private final ExecutorService executorService;
    private final int maxConcurrentSends;
    private final AtomicInteger activeSends = new AtomicInteger(0);
    private final AtomicLong totalSendTime = new AtomicLong(0);
    
    public SynchronousProducer(Properties configs, int maxConcurrentSends) {
        this.maxConcurrentSends = maxConcurrentSends;
        this.executorService = Executors.newFixedThreadPool(maxConcurrentSends);
        this.producer = new KafkaProducer<>(configs);
    }
    
    /**
     * 基础同步发送
     */
    public SendResult sendSynchronousMessage(String topic, String key, String value) {
        if (activeSends.get() >= maxConcurrentSends) {
            throw new RuntimeException("达到最大并发发送限制: " + maxConcurrentSends);
        }
        
        activeSends.incrementAndGet();
        long startTime = System.currentTimeMillis();
        
        try {
            ProducerRecord<String, String> record = new ProducerRecord<>(topic, key, value);
            
            // 同步发送，等待结果
            RecordMetadata metadata = producer.send(record).get(30, TimeUnit.SECONDS);
            
            long endTime = System.currentTimeMillis();
            totalSendTime.addAndGet(endTime - startTime);
            
            return new SendResult(true, metadata, null, endTime - startTime);
            
        } catch (Exception e) {
            long endTime = System.currentTimeMillis();
            return new SendResult(false, null, e, endTime - startTime);
        } finally {
            activeSends.decrementAndGet();
        }
    }
    
    /**
     * 带重试的同步发送
     */
    public SendResult sendSynchronousWithRetry(String topic, String key, String value, int maxRetries) {
        int retryCount = 0;
        Exception lastException = null;
        long startTime = System.currentTimeMillis();
        
        while (retryCount <= maxRetries) {
            try {
                SendResult result = sendSynchronousMessage(topic, key, value);
                
                if (result.isSuccess()) {
                    return result;
                } else {
                    lastException = result.getException();
                    retryCount++;
                    
                    if (retryCount <= maxRetries) {
                        // 指数退避重试
                        long backoffMs = (long) Math.pow(2, retryCount - 1) * 1000;
                        Thread.sleep(backoffMs);
                    }
                }
                
            } catch (InterruptedException e) {
                Thread.currentThread().interrupt();
                lastException = e;
                break;
            } catch (Exception e) {
                lastException = e;
                retryCount++;
                
                if (retryCount <= maxRetries) {
                    try {
                        long backoffMs = (long) Math.pow(2, retryCount - 1) * 1000;
                        Thread.sleep(backoffMs);
                    } catch (InterruptedException ie) {
                        Thread.currentThread().interrupt();
                        break;
                    }
                }
            }
        }
        
        return new SendResult(false, null, lastException, System.currentTimeMillis() - startTime);
    }
    
    /**
     * 批量同步发送
     */
    public BatchSendResult sendBatchSynchronous(String topic, List<String> messages) {
        if (messages.isEmpty()) {
            return new BatchSendResult(Collections.emptyList(), Collections.emptyList());
        }
        
        List<Future<SendResult>> futures = new ArrayList<>();
        long startTime = System.currentTimeMillis();
        
        try {
            // 提交所有发送任务
            for (String message : messages) {
                String key = "batch-" + UUID.randomUUID().toString();
                Future<SendResult> future = executorService.submit(() -> {
                    return sendSynchronousMessage(topic, key, message);
                });
                futures.add(future);
            }
            
            // 等待所有任务完成
            List<SendResult> results = new ArrayList<>();
            List<Exception> exceptions = new ArrayList<>();
            
            for (int i = 0; i < futures.size(); i++) {
                try {
                    SendResult result = futures.get(i).get(60, TimeUnit.SECONDS);
                    results.add(result);
                    
                    if (!result.isSuccess() && result.getException() != null) {
                        exceptions.add(result.getException());
                    }
                    
                } catch (Exception e) {
                    exceptions.add(e);
                    results.add(new SendResult(false, null, e, 0));
                }
            }
            
            long endTime = System.currentTimeMillis();
            
            return new BatchSendResult(results, exceptions, endTime - startTime);
            
        } catch (Exception e) {
            return new BatchSendResult(Collections.emptyList(), Arrays.asList(e), 0);
        }
    }
    
    /**
     * 发送结果类
     */
    public static class SendResult {
        private final boolean success;
        private final RecordMetadata metadata;
        private final Exception exception;
        private final long sendTimeMs;
        
        public SendResult(boolean success, RecordMetadata metadata, Exception exception, long sendTimeMs) {
            this.success = success;
            this.metadata = metadata;
            this.exception = exception;
            this.sendTimeMs = sendTimeMs;
        }
        
        public boolean isSuccess() { return success; }
        public RecordMetadata getMetadata() { return metadata; }
        public Exception getException() { return exception; }
        public long getSendTimeMs() { return sendTimeMs; }
    }
    
    /**
     * 批量发送结果类
     */
    public static class BatchSendResult {
        private final List<SendResult> results;
        private final List<Exception> exceptions;
        private final long totalTimeMs;
        private final int successCount;
        private final int failureCount;
        
        public BatchSendResult(List<SendResult> results, List<Exception> exceptions) {
            this(results, exceptions, 0);
        }
        
        public BatchSendResult(List<SendResult> results, List<Exception> exceptions, long totalTimeMs) {
            this.results = new ArrayList<>(results);
            this.exceptions = new ArrayList<>(exceptions);
            this.totalTimeMs = totalTimeMs;
            
            this.successCount = (int) results.stream().filter(SendResult::isSuccess).count();
            this.failureCount = results.size() - successCount;
        }
        
        public double getSuccessRate() {
            return results.isEmpty() ? 0.0 : (double) successCount / results.size();
        }
        
        public double getAverageSendTimeMs() {
            return successCount > 0 ? (double) totalTimeMs / results.size() : 0.0;
        }
        
        // Getters
        public List<SendResult> getResults() { return results; }
        public List<Exception> getExceptions() { return exceptions; }
        public long getTotalTimeMs() { return totalTimeMs; }
        public int getSuccessCount() { return successCount; }
        public int getFailureCount() { return failureCount; }
        
        @Override
        public String toString() {
            return String.format("BatchSendResult{success=%d, failure=%d, successRate=%.2f%%, avgTime=%.2fms}", 
                successCount, failureCount, getSuccessRate() * 100, getAverageSendTimeMs());
        }
    }
    
    /**
     * 获取性能统计
     */
    public ProducerPerformanceStats getPerformanceStats() {
        return new ProducerPerformanceStats(
            maxConcurrentSends,
            activeSends.get(),
            totalSendTime.get(),
            getAverageSendTime(),
            getAverageConcurrentSends()
        );
    }
    
    private double getAverageSendTime() {
        return totalSendTime.get() > 0 ? (double) totalSendTime.get() / getTotalSends() : 0.0;
    }
    
    private long getTotalSends() {
        return totalSendTime.get() / (totalSendTime.get() > 0 ? (long) getAverageSendTime() : 1);
    }
    
    private double getAverageConcurrentSends() {
        return (maxConcurrentSends + activeSends.get()) / 2.0;
    }
    
    /**
     * 关闭生产者
     */
    public void close() {
        executorService.shutdown();
        try {
            if (!executorService.awaitTermination(30, TimeUnit.SECONDS)) {
                executorService.shutdownNow();
            }
        } catch (InterruptedException e) {
            executorService.shutdownNow();
            Thread.currentThread().interrupt();
        }
        
        producer.close();
    }
    
    /**
     * 生产者性能统计
     */
    public static class ProducerPerformanceStats {
        private final int maxConcurrentSends;
        private final int activeSends;
        private final long totalSendTime;
        private final double averageSendTime;
        private final double averageConcurrentSends;
        
        public ProducerPerformanceStats(int maxConcurrentSends, int activeSends, long totalSendTime, 
                                      double averageSendTime, double averageConcurrentSends) {
            this.maxConcurrentSends = maxConcurrentSends;
            this.activeSends = activeSends;
            this.totalSendTime = totalSendTime;
            this.averageSendTime = averageSendTime;
            this.averageConcurrentSends = averageConcurrentSends;
        }
        
        public double getThroughputPerSecond() {
            return averageConcurrentSends / averageSendTime * 1000;
        }
        
        // Getters
        public int getMaxConcurrentSends() { return maxConcurrentSends; }
        public int getActiveSends() { return activeSends; }
        public long getTotalSendTime() { return totalSendTime; }
        public double getAverageSendTime() { return averageSendTime; }
        public double getAverageConcurrentSends() { return averageConcurrentSends; }
        
        @Override
        public String toString() {
            return String.format("ProducerPerformanceStats{maxConcurrent=%d, active=%d, totalTime=%dms, avgTime=%.2fms, throughput=%.2fmsg/s}", 
                maxConcurrentSends, activeSends, totalSendTime, averageSendTime, getThroughputPerSecond());
        }
    }
}
```

### 异步发送模式

异步发送是Kafka生产者的默认模式，提供了最佳的吞吐量。

```java
package com.kafka.tutorial.producer;

import org.apache.kafka.clients.producer.*;
import org.apache.kafka.common.serialization.StringSerializer;

import java.util.*;
import java.util.concurrent.*;
import java.util.concurrent.atomic.AtomicLong;
import java.util.concurrent.atomic.AtomicReference;
import java.util.concurrent.locks.ReentrantReadWriteLock;

/**
 * 异步发送模式实现
 */
public class AsynchronousProducer {
    private final KafkaProducer<String, String> producer;
    private final ExecutorService callbackExecutor;
    private final int maxPendingCallbacks;
    private final BlockingQueue<CallbackFuture> pendingCallbacks;
    private final AtomicLong totalCallbacks = new AtomicLong(0);
    private final AtomicLong successCallbacks = new AtomicLong(0);
    private final AtomicLong errorCallbacks = new AtomicLong(0);
    private final AtomicReference<Thread> mainThread = new AtomicReference<>();
    private final ReentrantReadWriteLock statsLock = new ReentrantReadWriteLock();
    private final AtomicLong totalResponseTime = new AtomicLong(0);
    
    public AsynchronousProducer(Properties configs, int callbackThreads, int maxPendingCallbacks) {
        this.maxPendingCallbacks = maxPendingCallbacks;
        this.callbackExecutor = Executors.newFixedThreadPool(callbackThreads);
        this.pendingCallbacks = new LinkedBlockingQueue<>(maxPendingCallbacks);
        this.producer = new KafkaProducer<>(configs);
    }
    
    /**
     * 异步发送基础实现
     */
    public CompletableFuture<AsyncSendResult> sendAsyncMessage(String topic, String key, String value) {
        return sendAsyncMessage(topic, key, value, null);
    }
    
    /**
     * 带自定义回调的异步发送
     */
    public CompletableFuture<AsyncSendResult> sendAsyncMessage(String topic, String key, String value, 
                                                              AsyncCallback customCallback) {
        CompletableFuture<AsyncSendResult> future = new CompletableFuture<>();
        long sendTime = System.currentTimeMillis();
        
        ProducerRecord<String, String> record = new ProducerRecord<>(topic, key, value);
        
        try {
            producer.send(record, (metadata, exception) -> {
                callbackExecutor.submit(() -> {
                    long responseTime = System.currentTimeMillis() - sendTime;
                    updateResponseTimeStats(responseTime);
                    
                    if (exception != null) {
                        errorCallbacks.incrementAndGet();
                        AsyncSendResult result = new AsyncSendResult(false, metadata, exception, responseTime);
                        future.complete(result);
                        
                        if (customCallback != null) {
                            customCallback.onFailure(exception, result);
                        }
                    } else {
                        successCallbacks.incrementAndGet();
                        AsyncSendResult result = new AsyncSendResult(true, metadata, null, responseTime);
                        future.complete(result);
                        
                        if (customCallback != null) {
                            customCallback.onSuccess(metadata, result);
                        }
                    }
                    
                    totalCallbacks.incrementAndGet();
                });
            });
            
        } catch (Exception e) {
            AsyncSendResult result = new AsyncSendResult(false, null, e, System.currentTimeMillis() - sendTime);
            future.complete(result);
        }
        
        return future;
    }
    
    /**
     * 批量异步发送
     */
    public CompletableFuture<BatchAsyncSendResult> sendBatchAsync(String topic, List<String> messages) {
        return sendBatchAsync(topic, messages, null);
    }
    
    /**
     * 带回调的批量异步发送
     */
    public CompletableFuture<BatchAsyncSendResult> sendBatchAsync(String topic, List<String> messages, 
                                                                BatchAsyncCallback batchCallback) {
        CompletableFuture<BatchAsyncSendResult> future = new CompletableFuture<>();
        
        if (messages.isEmpty()) {
            future.complete(new BatchAsyncSendResult(Collections.emptyList(), 0));
            return future;
        }
        
        List<CompletableFuture<AsyncSendResult>> futures = new ArrayList<>();
        Map<String, Integer> messageIndexMap = new HashMap<>();
        
        // 创建所有发送任务
        for (int i = 0; i < messages.size(); i++) {
            String message = messages.get(i);
            String key = "async-batch-" + i;
            messageIndexMap.put(key, i);
            
            CompletableFuture<AsyncSendResult> sendFuture = sendAsyncMessage(topic, key, message, 
                new AsyncCallback() {
                    @Override
                    public void onSuccess(RecordMetadata metadata, AsyncSendResult result) {
                        if (batchCallback != null) {
                            batchCallback.onIndividualSuccess(i, metadata, result);
                        }
                    }
                    
                    @Override
                    public void onFailure(Exception exception, AsyncSendResult result) {
                        if (batchCallback != null) {
                            batchCallback.onIndividualFailure(i, exception, result);
                        }
                    }
                });
            
            futures.add(sendFuture);
        }
        
        // 等待所有任务完成
        CompletableFuture.allOf(futures.toArray(new CompletableFuture[0]))
            .thenRun(() -> {
                List<AsyncSendResult> results = new ArrayList<>();
                int successCount = 0;
                int failureCount = 0;
                long totalResponseTime = 0;
                
                for (CompletableFuture<AsyncSendResult> cf : futures) {
                    try {
                        AsyncSendResult result = cf.get();
                        results.add(result);
                        
                        if (result.isSuccess()) {
                            successCount++;
                        } else {
                            failureCount++;
                        }
                        
                        totalResponseTime += result.getResponseTimeMs();
                        
                    } catch (Exception e) {
                        results.add(new AsyncSendResult(false, null, e, 0));
                        failureCount++;
                    }
                }
                
                BatchAsyncSendResult batchResult = new BatchAsyncSendResult(
                    results, totalResponseTime, successCount, failureCount);
                
                if (batchCallback != null) {
                    batchCallback.onBatchComplete(batchResult);
                }
                
                future.complete(batchResult);
            })
            .exceptionally(ex -> {
                BatchAsyncSendResult errorResult = new BatchAsyncSendResult(
                    Collections.emptyList(), 0, 0, futures.size());
                future.completeExceptionally(ex);
                return null;
            });
        
        return future;
    }
    
    /**
     * 异步发送与重试机制
     */
    public CompletableFuture<AsyncSendResult> sendAsyncWithRetry(String topic, String key, String value, 
                                                               int maxRetries, long baseBackoffMs) {
        return sendAsyncWithRetry(topic, key, value, maxRetries, baseBackoffMs, 0);
    }
    
    private CompletableFuture<AsyncSendResult> sendAsyncWithRetry(String topic, String key, String value, 
                                                                int maxRetries, long baseBackoffMs, int currentRetry) {
        return sendAsyncMessage(topic, key, value, new AsyncCallback() {
            @Override
            public void onSuccess(RecordMetadata metadata, AsyncSendResult result) {
                // 成功，无需重试
            }
            
            @Override
            public void onFailure(Exception exception, AsyncSendResult result) {
                if (currentRetry < maxRetries) {
                    long backoffTime = baseBackoffMs * (1L << currentRetry); // 指数退避
                    
                    try {
                        Thread.sleep(backoffTime);
                        
                        // 重试发送
                        sendAsyncWithRetry(topic, key, value, maxRetries, baseBackoffMs, currentRetry + 1);
                        
                    } catch (InterruptedException e) {
                        Thread.currentThread().interrupt();
                    }
                }
            }
        });
    }
    
    /**
     * 异步发送与限流
     */
    public CompletableFuture<AsyncSendResult> sendAsyncWithRateLimit(String topic, String key, String value,
                                                                    int maxRequestsPerSecond) {
        return CompletableFuture.supplyAsync(() -> {
            // 限流逻辑
            try {
                Thread.sleep(1000 / maxRequestsPerSecond);
            } catch (InterruptedException e) {
                Thread.currentThread().interrupt();
            }
            
            return sendAsyncMessage(topic, key, value);
        }).thenCompose(completableFuture -> completableFuture);
    }
    
    /**
     * 异步消息发送结果
     */
    public static class AsyncSendResult {
        private final boolean success;
        private final RecordMetadata metadata;
        private final Exception exception;
        private final long responseTimeMs;
        
        public AsyncSendResult(boolean success, RecordMetadata metadata, Exception exception, long responseTimeMs) {
            this.success = success;
            this.metadata = metadata;
            this.exception = exception;
            this.responseTimeMs = responseTimeMs;
        }
        
        public boolean isSuccess() { return success; }
        public RecordMetadata getMetadata() { return metadata; }
        public Exception getException() { return exception; }
        public long getResponseTimeMs() { return responseTimeMs; }
    }
    
    /**
     * 批量异步发送结果
     */
    public static class BatchAsyncSendResult {
        private final List<AsyncSendResult> results;
        private final long totalResponseTime;
        private final int successCount;
        private final int failureCount;
        private final double successRate;
        private final double averageResponseTime;
        
        public BatchAsyncSendResult(List<AsyncSendResult> results, long totalResponseTime, 
                                   int successCount, int failureCount) {
            this.results = new ArrayList<>(results);
            this.totalResponseTime = totalResponseTime;
            this.successCount = successCount;
            this.failureCount = failureCount;
            this.successRate = results.isEmpty() ? 0.0 : (double) successCount / results.size();
            this.averageResponseTime = results.isEmpty() ? 0.0 : (double) totalResponseTime / results.size();
        }
        
        // Getters
        public List<AsyncSendResult> getResults() { return results; }
        public long getTotalResponseTime() { return totalResponseTime; }
        public int getSuccessCount() { return successCount; }
        public int getFailureCount() { return failureCount; }
        public double getSuccessRate() { return successRate; }
        public double getAverageResponseTime() { return averageResponseTime; }
        
        @Override
        public String toString() {
            return String.format("BatchAsyncSendResult{success=%d, failure=%d, successRate=%.2f%%, avgTime=%.2fms}", 
                successCount, failureCount, successRate * 100, averageResponseTime);
        }
    }
    
    /**
     * 异步回调接口
     */
    public interface AsyncCallback {
        void onSuccess(RecordMetadata metadata, AsyncSendResult result);
        void onFailure(Exception exception, AsyncSendResult result);
    }
    
    /**
     * 批量异步回调接口
     */
    public interface BatchAsyncCallback {
        void onIndividualSuccess(int index, RecordMetadata metadata, AsyncSendResult result);
        void onIndividualFailure(int index, Exception exception, AsyncSendResult result);
        void onBatchComplete(BatchAsyncSendResult result);
    }
    
    /**
     * 更新响应时间统计
     */
    private void updateResponseTimeStats(long responseTime) {
        statsLock.writeLock().lock();
        try {
            totalResponseTime.addAndGet(responseTime);
        } finally {
            statsLock.writeLock().unlock();
        }
    }
    
    /**
     * 获取性能统计
     */
    public AsyncProducerStats getAsyncStats() {
        statsLock.readLock().lock();
        try {
            return new AsyncProducerStats(
                totalCallbacks.get(),
                successCallbacks.get(),
                errorCallbacks.get(),
                totalResponseTime.get(),
                getAverageResponseTime()
            );
        } finally {
            statsLock.readLock().unlock();
        }
    }
    
    private double getAverageResponseTime() {
        long total = totalResponseTime.get();
        long totalCalls = totalCallbacks.get();
        return totalCalls > 0 ? (double) total / totalCalls : 0.0;
    }
    
    /**
     * 异步生产者统计
     */
    public static class AsyncProducerStats {
        private final long totalCallbacks;
        private final long successCallbacks;
        private final long errorCallbacks;
        private final long totalResponseTime;
        private final double averageResponseTime;
        private final double errorRate;
        
        public AsyncProducerStats(long totalCallbacks, long successCallbacks, long errorCallbacks,
                                long totalResponseTime, double averageResponseTime) {
            this.totalCallbacks = totalCallbacks;
            this.successCallbacks = successCallbacks;
            this.errorCallbacks = errorCallbacks;
            this.totalResponseTime = totalResponseTime;
            this.averageResponseTime = averageResponseTime;
            this.errorRate = totalCallbacks > 0 ? (double) errorCallbacks / totalCallbacks : 0.0;
        }
        
        public double getThroughputPerSecond() {
            return averageResponseTime > 0 ? 1000.0 / averageResponseTime : 0.0;
        }
        
        // Getters
        public long getTotalCallbacks() { return totalCallbacks; }
        public long getSuccessCallbacks() { return successCallbacks; }
        public long getErrorCallbacks() { return errorCallbacks; }
        public long getTotalResponseTime() { return totalResponseTime; }
        public double getAverageResponseTime() { return averageResponseTime; }
        public double getErrorRate() { return errorRate; }
        
        @Override
        public String toString() {
            return String.format("AsyncProducerStats{total=%d, success=%d, error=%d, avgTime=%.2fms, throughput=%.2fmsg/s, errorRate=%.2f%%}", 
                totalCallbacks, successCallbacks, errorCallbacks, averageResponseTime, getThroughputPerSecond(), errorRate * 100);
        }
    }
    
    /**
     * 关闭生产者
     */
    public void close() {
        callbackExecutor.shutdown();
        try {
            if (!callbackExecutor.awaitTermination(30, TimeUnit.SECONDS)) {
                callbackExecutor.shutdownNow();
            }
        } catch (InterruptedException e) {
            callbackExecutor.shutdownNow();
            Thread.currentThread().interrupt();
        }
        
        producer.close();
    }
}
```

---

## 分区策略与负载均衡

### 自定义分区策略实现

```java
package com.kafka.tutorial.partitioner;

import org.apache.kafka.clients.producer.Partitioner;
import org.apache.kafka.common.Cluster;
import org.apache.kafka.common.utils.Utils;

import java.util.List;
import java.util.Map;
import java.util.concurrent.atomic.AtomicInteger;
import java.util.concurrent.ConcurrentHashMap;

/**
 * 自定义分区策略实现
 */
public class CustomPartitionStrategies {
    
    /**
     * 基于轮询的分区策略
     */
    public static class RoundRobinPartitioner implements Partitioner {
        private final AtomicInteger partitionCounter = new AtomicInteger(0);
        private volatile int numPartitions = 0;
        
        @Override
        public void configure(Map<String, ?> configs) {
            // 配置初始化
        }
        
        @Override
        public int partition(String topic, Object keyObj, byte[] keyBytes, Object valueObj, byte[] valueBytes, Cluster cluster) {
            List partitions = cluster.partitionsForTopic(topic);
            numPartitions = partitions.size();
            
            if (keyBytes == null) {
                // 随机分配
                return Utils.toPositive(partitionCounter.getAndIncrement()) % numPartitions;
            } else {
                // 使用键的哈希值确定分区
                return Utils.toPositive(Utils.murmur2(keyBytes)) % numPartitions;
            }
        }
        
        @Override
        public void close() {
            // 清理资源
        }
    }
    
    /**
     * 基于范围分区的策略
     */
    public static class RangePartitioner implements Partitioner {
        private final AtomicInteger rangeStart = new AtomicInteger(0);
        private volatile int numPartitions = 0;
        
        @Override
        public void configure(Map<String, ?> configs) {
            // 配置初始化
        }
        
        @Override
        public int partition(String topic, Object keyObj, byte[] keyBytes, Object valueObj, byte[] valueBytes, Cluster cluster) {
            List partitions = cluster.partitionsForTopic(topic);
            numPartitions = partitions.size();
            
            if (keyBytes == null) {
                // 范围分配
                return Utils.toPositive(rangeStart.getAndIncrement()) % numPartitions;
            } else {
                // 基于键的范围分区
                int keyHash = Utils.toPositive(Utils.murmur2(keyBytes));
                return keyHash % numPartitions;
            }
        }
        
        @Override
        public void close() {
            // 清理资源
        }
    }
    
    /**
     * 基于业务逻辑的分区策略
     */
    public static class BusinessLogicPartitioner implements Partitioner {
        private final Map<String, Integer> businessKeyToPartition = new ConcurrentHashMap<>();
        
        @Override
        public void configure(Map<String, ?> configs) {
            // 配置初始化，可以从配置中读取业务规则
        }
        
        @Override
        public int partition(String topic, Object keyObj, byte[] keyBytes, Object valueObj, byte[] valueBytes, Cluster cluster) {
            List partitions = cluster.partitionsForTopic(topic);
            int numPartitions = partitions.size();
            
            if (keyBytes == null) {
                // 如果没有键，随机分配
                return (int) (Math.random() * numPartitions);
            }
            
            String key = new String(keyBytes);
            
            // 基于业务键的分区逻辑
            if (key.startsWith("user_")) {
                // 用户相关数据分配到特定分区范围
                String userId = key.substring(5);
                int hash = userId.hashCode();
                return Math.abs(hash) % (numPartitions / 2);
                
            } else if (key.startsWith("order_")) {
                // 订单相关数据分配到另一分区范围
                String orderId = key.substring(6);
                int hash = orderId.hashCode();
                return (numPartitions / 2) + (Math.abs(hash) % (numPartitions / 2));
                
            } else {
                // 默认分区逻辑
                return Math.abs(key.hashCode()) % numPartitions;
            }
        }
        
        @Override
        public void close() {
            // 清理资源
        }
    }
    
    /**
     * 基于地理位置的分区策略
     */
    public static class GeoLocationPartitioner implements Partitioner {
        private final Map<String, String> regionToPartitionPrefix = Map.of(
            "beijing", "bj",
            "shanghai", "sh",
            "guangzhou", "gz",
            "shenzhen", "sz"
        );
        
        @Override
        public void configure(Map<String, ?> configs) {
            // 配置初始化
        }
        
        @Override
        public int partition(String topic, Object keyObj, byte[] keyBytes, Object valueObj, byte[] valueBytes, Cluster cluster) {
            List partitions = cluster.partitionsForTopic(topic);
            int numPartitions = partitions.size();
            
            if (keyBytes == null) {
                return (int) (Math.random() * numPartitions);
            }
            
            String key = new String(keyBytes);
            
            // 从键中提取地区信息
            String region = extractRegionFromKey(key);
            if (region != null) {
                // 映射到特定分区范围
                return mapRegionToPartition(region, numPartitions);
            }
            
            return Math.abs(key.hashCode()) % numPartitions;
        }
        
        private String extractRegionFromKey(String key) {
            // 假设键格式为 "region_city_userId" 或 "user_region_city"
            if (key.contains("_")) {
                String[] parts = key.split("_");
                for (String part : parts) {
                    if (regionToPartitionPrefix.containsKey(part.toLowerCase())) {
                        return part.toLowerCase();
                    }
                }
            }
            return null;
        }
        
        private int mapRegionToPartition(String region, int numPartitions) {
            String prefix = regionToPartitionPrefix.get(region);
            if (prefix != null) {
                // 使用前缀的哈希值映射到特定分区
                return Math.abs(prefix.hashCode()) % numPartitions;
            }
            
            return 0;
        }
        
        @Override
        public void close() {
            // 清理资源
        }
    }
    
    /**
     * 负载均衡分区器
     */
    public static class LoadBalancedPartitioner implements Partitioner {
        private final AtomicInteger[] partitionLoads;
        private final AtomicLong totalPartitions = new AtomicLong(0);
        private volatile int numPartitions = 0;
        
        public LoadBalancedPartitioner() {
            this.partitionLoads = new AtomicInteger[0]; // 动态初始化
        }
        
        @Override
        public void configure(Map<String, ?> configs) {
            // 配置初始化
        }
        
        @Override
        public int partition(String topic, Object keyObj, byte[] keyBytes, Object valueObj, byte[] valueBytes, Cluster cluster) {
            List partitions = cluster.partitionsForTopic(topic);
            numPartitions = partitions.size();
            
            // 动态调整分区负载数组大小
            if (partitionLoads.length != numPartitions) {
                AtomicInteger[] newLoads = new AtomicInteger[numPartitions];
                for (int i = 0; i < numPartitions; i++) {
                    if (i < partitionLoads.length) {
                        newLoads[i] = partitionLoads[i];
                    } else {
                        newLoads[i] = new AtomicInteger(0);
                    }
                }
                this.partitionLoads = newLoads;
            }
            
            if (keyBytes == null) {
                // 随机选择一个负载较轻的分区
                return selectLeastLoadedPartition();
            } else {
                // 基于键的哈希值选择分区
                int keyHash = Utils.toPositive(Utils.murmur2(keyBytes));
                return Math.abs(keyHash) % numPartitions;
            }
        }
        
        private int selectLeastLoadedPartition() {
            int minLoad = Integer.MAX_VALUE;
            int minPartition = 0;
            
            for (int i = 0; i < partitionLoads.length; i++) {
                int load = partitionLoads[i].get();
                if (load < minLoad) {
                    minLoad = load;
                    minPartition = i;
                }
            }
            
            // 增加该分区的负载计数
            partitionLoads[minPartition].incrementAndGet();
            totalPartitions.incrementAndGet();
            
            return minPartition;
        }
        
        @Override
        public void close() {
            // 清理资源
        }
    }
    
    /**
     * 一致性哈希分区策略
     */
    public static class ConsistentHashPartitioner implements Partitioner {
        private final int virtualNodes = 160; // 虚拟节点数量
        
        @Override
        public void configure(Map<String, ?> configs) {
            // 配置初始化
        }
        
        @Override
        public int partition(String topic, Object keyObj, byte[] keyBytes, Object valueObj, byte[] valueBytes, Cluster cluster) {
            List partitions = cluster.partitionsForTopic(topic);
            int numPartitions = partitions.size();
            
            if (keyBytes == null) {
                return (int) (Math.random() * numPartitions);
            }
            
            String key = new String(keyBytes);
            int hash = Utils.toPositive(Utils.murmur2(keyBytes));
            
            // 返回基于一致性哈希的分区
            return Math.abs(hash) % numPartitions;
        }
        
        @Override
        public void close() {
            // 清理资源
        }
    }
    
    /**
     * 分区策略工厂
     */
    public static class PartitionStrategyFactory {
        
        public static Partitioner createPartitioner(String strategyType) {
            switch (strategyType.toLowerCase()) {
                case "roundrobin":
                    return new RoundRobinPartitioner();
                case "range":
                    return new RangePartitioner();
                case "business":
                    return new BusinessLogicPartitioner();
                case "geo":
                    return new GeoLocationPartitioner();
                case "loadbalance":
                    return new LoadBalancedPartitioner();
                case "consistent":
                    return new ConsistentHashPartitioner();
                default:
                    return null;
            }
        }
        
        public static Properties configureProducerWithPartitioner(String strategyType, Properties baseConfigs) {
            Partitioner partitioner = createPartitioner(strategyType);
            if (partitioner != null) {
                baseConfigs.put(ProducerConfig.PARTITIONER_CLASS_CONFIG, partitioner.getClass().getName());
            }
            return baseConfigs;
        }
    }
}
```

### 分区性能测试和优化

```java
package com.kafka.tutorial.partitioner;

import org.apache.kafka.clients.producer.*;
import org.apache.kafka.common.serialization.StringSerializer;

import java.util.*;
import java.util.concurrent.*;
import java.util.concurrent.atomic.AtomicInteger;
import java.util.concurrent.atomic.AtomicLong;
import java.util.stream.Collectors;

/**
 * 分区性能测试和优化工具
 */
public class PartitionPerformanceTest {
    
    /**
     * 分区负载测试
     */
    public static class PartitionLoadTest {
        private final KafkaProducer<String, String> producer;
        private final String topic;
        private final int numPartitions;
        private final Map<Integer, AtomicLong> partitionMessageCount = new ConcurrentHashMap<>();
        private final Map<Integer, AtomicLong> partitionByteCount = new ConcurrentHashMap<>();
        
        public PartitionLoadTest(Properties configs, String topic, int numPartitions) {
            this.producer = new KafkaProducer<>(configs);
            this.topic = topic;
            this.numPartitions = numPartitions;
            
            // 初始化分区统计
            for (int i = 0; i < numPartitions; i++) {
                partitionMessageCount.put(i, new AtomicLong(0));
                partitionByteCount.put(i, new AtomicLong(0));
            }
        }
        
        /**
         * 测试分区负载分布
         */
        public PartitionLoadTestResult testPartitionDistribution(int messageCount, int threadCount) throws Exception {
            ExecutorService executor = Executors.newFixedThreadPool(threadCount);
            CountDownLatch latch = new CountDownLatch(threadCount);
            
            long startTime = System.currentTimeMillis();
            
            // 启动多线程发送消息
            for (int t = 0; t < threadCount; t++) {
                final int threadId = t;
                executor.submit(() -> {
                    try {
                        int messagesPerThread = messageCount / threadCount;
                        
                        for (int i = 0; i < messagesPerThread; i++) {
                            String key = "key-" + threadId + "-" + i;
                            String value = "test-message-" + System.currentTimeMillis() + "-" + i;
                            
                            // 获取分配的分区
                            int partition = producer.partitionsFor(topic).size() > 0 ? 
                                Math.abs(key.hashCode()) % producer.partitionsFor(topic).size() : 0;
                            
                            ProducerRecord<String, String> record = new ProducerRecord<>(topic, partition, key, value);
                            
                            // 发送消息
                            RecordMetadata metadata = producer.send(record).get();
                            
                            // 更新统计
                            updatePartitionStats(partition, value.length());
                        }
                    } catch (Exception e) {
                        e.printStackTrace();
                    } finally {
                        latch.countDown();
                    }
                });
            }
            
            latch.await();
            long endTime = System.currentTimeMillis();
            executor.shutdown();
            
            return analyzeLoadDistribution(endTime - startTime);
        }
        
        private void updatePartitionStats(int partition, int messageSize) {
            partitionMessageCount.get(partition).incrementAndGet();
            partitionByteCount.get(partition).addAndGet(messageSize);
        }
        
        private PartitionLoadTestResult analyzeLoadDistribution(long duration) {
            long totalMessages = partitionMessageCount.values().stream()
                .mapToLong(AtomicLong::get).sum();
            long totalBytes = partitionByteCount.values().stream()
                .mapToLong(AtomicLong::get).sum();
            
            double avgMessagesPerPartition = totalMessages / (double) numPartitions;
            double avgBytesPerPartition = totalBytes / (double) numPartitions;
            
            double variance = 0.0;
            for (AtomicLong count : partitionMessageCount.values()) {
                double diff = count.get() - avgMessagesPerPartition;
                variance += diff * diff;
            }
            variance /= numPartitions;
            
            double stdDev = Math.sqrt(variance);
            double coefficientOfVariation = stdDev / avgMessagesPerPartition;
            
            return new PartitionLoadTestResult(
                topic,
                numPartitions,
                totalMessages,
                totalBytes,
                duration,
                partitionMessageCount,
                partitionByteCount,
                coefficientOfVariation,
                totalMessages / (duration / 1000.0)
            );
        }
        
        public void close() {
            producer.close();
        }
    }
    
    /**
     * 分区负载测试结果
     */
    public static class PartitionLoadTestResult {
        private final String topic;
        private final int numPartitions;
        private final long totalMessages;
        private final long totalBytes;
        private final long durationMs;
        private final Map<Integer, AtomicLong> messageCountPerPartition;
        private final Map<Integer, AtomicLong> byteCountPerPartition;
        private final double coefficientOfVariation;
        private final double throughput;
        
        public PartitionLoadTestResult(String topic, int numPartitions, long totalMessages, long totalBytes,
                                     long durationMs, Map<Integer, AtomicLong> messageCountPerPartition,
                                     Map<Integer, AtomicLong> byteCountPerPartition, 
                                     double coefficientOfVariation, double throughput) {
            this.topic = topic;
            this.numPartitions = numPartitions;
            this.totalMessages = totalMessages;
            this.totalBytes = totalBytes;
            this.durationMs = durationMs;
            this.messageCountPerPartition = new HashMap<>(messageCountPerPartition);
            this.byteCountPerPartition = new HashMap<>(byteCountPerPartition);
            this.coefficientOfVariation = coefficientOfVariation;
            this.throughput = throughput;
        }
        
        public boolean isLoadBalanced() {
            return coefficientOfVariation < 0.1; // 变异系数小于10%认为平衡
        }
        
        public Map<Integer, Double> getMessageDistribution() {
            double total =.values().stream()
 messageCountPerPartition                .mapToLong(AtomicLong::get).sum();
            
            return messageCountPerPartition.entrySet().stream()
                .collect(Collectors.toMap(
                    Map.Entry::getKey,
                    e -> e.getValue().get() / total
                ));
        }
        
        public Map<Integer, Double> getByteDistribution() {
            double total = byteCountPerPartition.values().stream()
                .mapToLong(AtomicLong::get).sum();
            
            return byteCountPerPartition.entrySet().stream()
                .collect(Collectors.toMap(
                    Map.Entry::getKey,
                    e -> e.getValue().get() / total
                ));
        }
        
        public List<PartitionRecommendation> getRecommendations() {
            List<PartitionRecommendation> recommendations = new ArrayList<>();
            
            if (!isLoadBalanced()) {
                recommendations.add(new PartitionRecommendation(
                    "LOAD_BALANCE",
                    "分区负载不均匀，变异系数: " + String.format("%.2f", coefficientOfVariation),
                    "建议使用更均匀的分区键或自定义分区策略"
                ));
            }
            
            // 基于吞吐量分析
            if (throughput < 1000) {
                recommendations.add(new PartitionRecommendation(
                    "THROUGHPUT",
                    "吞吐量较低: " + String.format("%.0f msg/s", throughput),
                    "建议增加分区数或优化生产者配置"
                ));
            }
            
            return recommendations;
        }
        
        @Override
        public String toString() {
            StringBuilder sb = new StringBuilder();
            sb.append("=== 分区负载测试结果 ===\n");
            sb.append("主题: ").append(topic).append("\n");
            sb.append("分区数: ").append(numPartitions).append("\n");
            sb.append("总消息数: ").append(totalMessages).append("\n");
            sb.append("总字节数: ").append(totalBytes).append("\n");
            sb.append("测试时长: ").append(durationMs).append("ms\n");
            sb.append("吞吐量: ").append(String.format("%.0f msg/s", throughput)).append("\n");
            sb.append("变异系数: ").append(String.format("%.4f", coefficientOfVariation)).append("\n");
            sb.append("负载均衡: ").append(isLoadBalanced() ? "是" : "否").append("\n");
            
            sb.append("\n各分区消息分布:\n");
            messageCountPerPartition.forEach((partition, count) -> {
                double percentage = count.get() * 100.0 / totalMessages;
                sb.append("  分区 ").append(partition).append(": ")
                  .append(count.get()).append(" 条消息 (")
                  .append(String.format("%.2f%%", percentage)).append(")\n");
            });
            
            return sb.toString();
        }
        
        // Getters
        public String getTopic() { return topic; }
        public int getNumPartitions() { return numPartitions; }
        public long getTotalMessages() { return totalMessages; }
        public long getTotalBytes() { return totalBytes; }
        public long getDurationMs() { return durationMs; }
        public Map<Integer, AtomicLong> getMessageCountPerPartition() { return messageCountPerPartition; }
        public Map<Integer, AtomicLong> getByteCountPerPartition() { return byteCountPerPartition; }
        public double getCoefficientOfVariation() { return coefficientOfVariation; }
        public double getThroughput() { return throughput; }
    }
    
    /**
     * 分区建议
     */
    public static class PartitionRecommendation {
        private final String type;
        private final String issue;
        private final String suggestion;
        
        public PartitionRecommendation(String type, String issue, String suggestion) {
            this.type = type;
            this.issue = issue;
            this.suggestion = suggestion;
        }
        
        public String getType() { return type; }
        public String getIssue() { return issue; }
        public String getSuggestion() { return suggestion; }
        
        @Override
        public String toString() {
            return String.format("[%s] %s -> %s", type, issue, suggestion);
        }
    }
    
    /**
     * 分区策略性能对比
     */
    public static class PartitionStrategyComparison {
        
        public static Map<String, PartitionPerformanceMetrics> compareStrategies(
                String topic, Map<String, Partitioner> strategies, 
                int messageCount, int threadCount) {
            
            Map<String, PartitionPerformanceMetrics> results = new HashMap<>();
            
            for (Map.Entry<String, Partitioner> entry : strategies.entrySet()) {
                String strategyName = entry.getKey();
                Partitioner partitioner = entry.getValue();
                
                try {
                    Properties configs = createProducerConfigs();
                    configs.put(ProducerConfig.PARTITIONER_CLASS_CONFIG, partitioner.getClass().getName());
                    
                    PartitionLoadTest test = new PartitionLoadTest(configs, topic, getPartitionCount(topic));
                    PartitionLoadTestResult result = test.testPartitionDistribution(messageCount, threadCount);
                    
                    results.put(strategyName, new PartitionPerformanceMetrics(
                        strategyName,
                        result.getThroughput(),
                        result.getCoefficientOfVariation(),
                        result.isLoadBalanced(),
                        result.getRecommendations()
                    ));
                    
                    test.close();
                    
                } catch (Exception e) {
                    results.put(strategyName, new PartitionPerformanceMetrics(
                        strategyName, 0, 0, false, 
                        Arrays.asList(new PartitionRecommendation("ERROR", e.getMessage(), "检查配置"))
                    ));
                }
            }
            
            return results;
        }
        
        private static int getPartitionCount(String topic) {
            // 这里应该查询实际的分区数
            return 6; // 假设值为6
        }
        
        private static Properties createProducerConfigs() {
            Properties configs = new Properties();
            configs.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, "localhost:9092");
            configs.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, StringSerializer.class.getName());
            configs.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, StringSerializer.class.getName());
            configs.put(ProducerConfig.BATCH_SIZE_CONFIG, 16384);
            configs.put(ProducerConfig.LINGER_MS_CONFIG, 0);
            configs.put(ProducerConfig.BUFFER_MEMORY_CONFIG, 33554432);
            configs.put(ProducerConfig.ACKS_CONFIG, "1");
            return configs;
        }
    }
    
    /**
     * 分区性能指标
     */
    public static class PartitionPerformanceMetrics {
        private final String strategyName;
        private final double throughput;
        private final double loadBalanceScore;
        private final boolean loadBalanced;
        private final List<PartitionRecommendation> recommendations;
        
        public PartitionPerformanceMetrics(String strategyName, double throughput, double loadBalanceScore,
                                         boolean loadBalanced, List<PartitionRecommendation> recommendations) {
            this.strategyName = strategyName;
            this.throughput = throughput;
            this.loadBalanceScore = loadBalanced ? loadBalanceScore : 1 - loadBalanceScore;
            this.loadBalanced = loadBalanced;
            this.recommendations = new ArrayList<>(recommendations);
        }
        
        public double getOverallScore() {
            // 综合评分：负载均衡权重70%，吞吐量权重30%
            double normalizedLoadBalance = 1 - loadBalanceScore; // 变异系数越小越好
            return normalizedLoadBalance * 0.7 + Math.min(throughput / 10000.0, 1.0) * 0.3;
        }
        
        // Getters
        public String getStrategyName() { return strategyName; }
        public double getThroughput() { return throughput; }
        public double getLoadBalanceScore() { return loadBalanceScore; }
        public boolean isLoadBalanced() { return loadBalanced; }
        public List<PartitionRecommendation> getRecommendations() { return recommendations; }
        
        @Override
        public String toString() {
            return String.format("%s: 吞吐=%.0f msg/s, 负载均衡=%s, 评分=%.3f", 
                strategyName, throughput, loadBalanced ? "是" : "否", getOverallScore());
        }
    }
}
```

---

## 批量处理与压缩优化

### 高效批量发送实现

```java
package com.kafka.tutorial.producer;

import org.apache.kafka.clients.producer.*;
import org.apache.kafka.common.serialization.StringSerializer;
import org.apache.kafka.common.record.CompressionType;

import java.util.*;
import java.util.concurrent.*;
import java.util.concurrent.atomic.AtomicInteger;
import java.util.concurrent.atomic.AtomicLong;
import java.util.concurrent.atomic.AtomicReference;
import java.util.concurrent.locks.ReentrantReadWriteLock;

/**
 * 高效批量处理生产者
 */
public class BatchProducer {
    private final KafkaProducer<String, String> producer;
    private final BatchConfig batchConfig;
    private final ExecutorService batchExecutor;
    private final BlockingQueue<BatchMessage> pendingBatches;
    private final AtomicLong totalBatchesProcessed = new AtomicLong(0);
    private final AtomicLong totalMessagesProcessed = new AtomicLong(0);
    private final AtomicLong totalCompressionBytes = new AtomicLong(0);
    private final AtomicLong totalOriginalBytes = new AtomicLong(0);
    
    // 批量统计
    private final AtomicReference<BatchStatistics> batchStats = new AtomicReference<>(
        new BatchStatistics(0, 0, 0, 0, 0));
    
    public BatchProducer(Properties configs, BatchConfig batchConfig) {
        this.batchConfig = batchConfig;
        this.batchExecutor = Executors.newFixedThreadPool(batchConfig.getBatchThreads());
        this.producer = new KafkaProducer<>(configs);
        this.pendingBatches = new LinkedBlockingQueue<>(batchConfig.getMaxPendingBatches());
        
        // 启动批量处理线程
        startBatchProcessing();
    }
    
    /**
     * 批量消息配置
     */
    public static class BatchConfig {
        private final int batchSize;
        private final long batchTimeoutMs;
        private final int batchThreads;
        private final int maxPendingBatches;
        private final CompressionType compressionType;
        private final boolean enableBatching;
        
        private BatchConfig(Builder builder) {
            this.batchSize = builder.batchSize;
            this.batchTimeoutMs = builder.batchTimeoutMs;
            this.batchThreads = builder.batchThreads;
            this.maxPendingBatches = builder.maxPendingBatches;
            this.compressionType = builder.compressionType;
            this.enableBatching = builder.enableBatching;
        }
        
        public static class Builder {
            private int batchSize = 100;
            private long batchTimeoutMs = 5000;
            private int batchThreads = 4;
            private int maxPendingBatches = 100;
            private CompressionType compressionType = CompressionType.GZIP;
            private boolean enableBatching = true;
            
            public Builder batchSize(int batchSize) {
                this.batchSize = batchSize;
                return this;
            }
            
            public Builder batchTimeoutMs(long batchTimeoutMs) {
                this.batchTimeoutMs = batchTimeoutMs;
                return this;
            }
            
            public Builder batchThreads(int batchThreads) {
                this.batchThreads = batchThreads;
                return this;
            }
            
            public Builder maxPendingBatches(int maxPendingBatches) {
                this.maxPendingBatches = maxPendingBatches;
                return this;
            }
            
            public Builder compressionType(CompressionType compressionType) {
                this.compressionType = compressionType;
                return this;
            }
            
            public Builder enableBatching(boolean enableBatching) {
                this.enableBatching = enableBatching;
                return this;
            }
            
            public BatchConfig build() {
                return new BatchConfig(this);
            }
        }
        
        // Getters
        public int getBatchSize() { return batchSize; }
        public long getBatchTimeoutMs() { return batchTimeoutMs; }
        public int getBatchThreads() { return batchThreads; }
        public int getMaxPendingBatches() { return maxPendingBatches; }
        public CompressionType getCompressionType() { return compressionType; }
        public boolean isEnableBatching() { return enableBatching; }
    }
    
    /**
     * 批量消息类
     */
    public static class BatchMessage {
        private final String topic;
        private final String key;
        private final String value;
        private final long timestamp;
        private final int priority;
        private final Map<String, String> headers;
        
        public BatchMessage(String topic, String key, String value, Map<String, String> headers) {
            this(topic, key, value, headers, 0);
        }
        
        public BatchMessage(String topic, String key, String value, Map<String, String> headers, int priority) {
            this.topic = topic;
            this.key = key;
            this.value = value;
            this.timestamp = System.currentTimeMillis();
            this.priority = priority;
            this.headers = headers != null ? new HashMap<>(headers) : new HashMap<>();
        }
        
        public ProducerRecord<String, String> toProducerRecord() {
            Headers kafkaHeaders = new RecordHeaders();
            headers.forEach((k, v) -> kafkaHeaders.add(new RecordHeader(k, v.getBytes())));
            headers.put("batch-timestamp", String.valueOf(timestamp));
            
            return new ProducerRecord<>(topic, null, timestamp, key, value) {
                @Override
                public Headers headers() {
                    return kafkaHeaders;
                }
            };
        }
        
        public int getEstimatedSize() {
            // 估算消息大小
            return (key != null ? key.length() : 0) + 
                   (value != null ? value.length() : 0) + 
                   headers.values().stream().mapToInt(String::length).sum() + 100; // 额外100字节用于元数据
        }
        
        // Getters
        public String getTopic() { return topic; }
        public String getKey() { return key; }
        public String getValue() { return value; }
        public long getTimestamp() { return timestamp; }
        public int getPriority() { return priority; }
        public Map<String, String> getHeaders() { return headers; }
    }
    
    /**
     * 批量发送结果
     */
    public static class BatchSendResult {
        private final boolean success;
        private final List<BatchMessage> messages;
        private final List<RecordMetadata> metadataList;
        private final Exception exception;
        private final long processingTime;
        private final long compressedBytes;
        private final long originalBytes;
        private final double compressionRatio;
        
        public BatchSendResult(boolean success, List<BatchMessage> messages, List<RecordMetadata> metadataList,
                             Exception exception, long processingTime, long compressedBytes, long originalBytes) {
            this.success = success;
            this.messages = new ArrayList<>(messages);
            this.metadataList = metadataList != null ? new ArrayList<>(metadataList) : null;
            this.exception = exception;
            this.processingTime = processingTime;
            this.compressedBytes = compressedBytes;
            this.originalBytes = originalBytes;
            this.compressionRatio = originalBytes > 0 ? (double) compressedBytes / originalBytes : 1.0;
        }
        
        public boolean isSuccess() { return success; }
        public List<BatchMessage> getMessages() { return messages; }
        public List<RecordMetadata> getMetadataList() { return metadataList; }
        public Exception getException() { return exception; }
        public long getProcessingTime() { return processingTime; }
        public long getCompressedBytes() { return compressedBytes; }
        public long getOriginalBytes() { return originalBytes; }
        public double getCompressionRatio() { return compressionRatio; }
        public double getCompressionSavings() { return 1.0 - compressionRatio; }
        
        @Override
        public String to