# 第4章：主题与分区管理

## 目录
1. [主题（Topic）概念详解](#主题概念详解)
2. [分区（Partition）原理与策略](#分区原理与策略)
3. [主题的创建与管理](#主题的创建与管理)
4. [分区管理与重分配](#分区管理与重分配)
5. [分区策略深入分析](#分区策略深入分析)
6. [副本机制与ISR管理](#副本机制与ISR管理)
7. [主题配置深度解析](#主题配置深度解析)
8. [实战代码示例](#实战代码示例)
9. [最佳实践与优化](#最佳实践与优化)
10. [常见问题与解决方案](#常见问题与解决方案)

---

## 主题概念详解

### 什么是主题？

主题（Topic）是Kafka中数据分类的基本单位，类似于数据库中的表或文件系统中的目录。每个主题代表一类特定的数据流，比如用户行为日志、交易数据、传感器数据等。

#### 主题的核心特性：
- **逻辑隔离**：不同主题的数据相互独立，不会混淆
- **多生产者支持**：多个生产者可以向同一主题发送数据
- **多消费者支持**：多个消费者可以同时从同一主题读取数据
- **持久化存储**：消息会持久化保存在磁盘上，直到过期
- **可配置保留**：可以设置消息的保留时间和大小

### 主题命名规范

主题命名应该遵循以下最佳实践：

```
✓ 好的命名：
- user_behavior_logs
- transaction_events
- sensor_data_temperature
- payment_notifications
- inventory_updates

✗ 避免的命名：
- Topic1, Topic2 (无意义名称)
- User Data (包含空格)
- logs.* (包含特殊字符)
- very_long_topic_name_that_violates_kafka_limits
```

#### 命名规则和限制：
- 字符集：字母、数字、点号（.）、下划线（_）、连字符（-）
- 长度限制：1-249个字符
- 不能包含点号（.）和下划线（_）同时出现
- 不能以点号（.）或连字符（-）开头或结尾
- 保留名称：__consumer_offsets, __transaction_state

---

## 分区原理与策略

### 分区（Partition）的本质

分区是主题内部的逻辑划分，是Kafka实现高并发和高吞吐的核心机制。每个主题可以分为多个分区，每个分区是一个有序的、不可变的消息序列。

#### 分区的关键作用：
1. **并发处理**：分区允许消费者组内的多个消费者并行读取数据
2. **水平扩展**：可以通过增加分区数来提升处理能力
3. **顺序保证**：同一分区内的消息是有序的
4. **负载均衡**：生产者可以向不同分区发送消息实现负载均衡

### 分区分配策略

#### 1. Range分区策略（默认）
- **适用场景**：每个主题的分区数量可以被消费者数量整除
- **分配方式**：将分区范围分配给消费者
- **潜在问题**：可能导致分配不均衡

#### 2. RoundRobin分区策略
- **适用场景**：所有主题的分区数量和消费者订阅情况相同
- **分配方式**：轮询分配所有主题的分区
- **优势**：分配更加均匀

#### 3. Sticky分区策略
- **目标**：尽量保持现有的分配不变
- **优势**：减少分区重新分配的开销

---

## 主题的创建与管理

### 通过命令行创建主题

#### 基本创建命令
```bash
# 创建一个简单主题
kafka-topics.sh --create \
  --topic user_events \
  --bootstrap-server localhost:9092 \
  --partitions 3 \
  --replication-factor 1

# 创建带配置的主题
kafka-topics.sh --create \
  --topic advanced_topic \
  --bootstrap-server localhost:9092 \
  --partitions 6 \
  --replication-factor 3 \
  --config cleanup.policy=delete \
  --config retention.ms=604800000 \
  --config min.insync.replicas=2
```

#### 高级创建选项
```bash
# 创建带压缩和删除策略的主题
kafka-topics.sh --create \
  --topic compressed_logs \
  --bootstrap-server localhost:9092 \
  --partitions 12 \
  --replication-factor 3 \
  --config cleanup.policy=compact \
  --config compression.type=gzip \
  --config min.cleanable.dirty.ratio=0.5
```

### 主题管理操作

#### 查看主题列表
```bash
# 列出所有主题
kafka-topics.sh --list --bootstrap-server localhost:9092

# 列出以特定前缀开头的主题
kafka-topics.sh --list --bootstrap-server localhost:9092 | grep "^user_"

# 列出主题详细信息
kafka-topics.sh --describe --bootstrap-server localhost:9092
```

#### 修改主题配置
```bash
# 修改主题的保留时间
kafka-configs.sh --alter \
  --topic user_events \
  --bootstrap-server localhost:9092 \
  --add-config retention.ms=2592000000

# 修改主题的压缩类型
kafka-configs.sh --alter \
  --topic user_events \
  --bootstrap-server localhost:9092 \
  --add-config compression.type=lz4

# 删除主题配置
kafka-configs.sh --alter \
  --topic user_events \
  --bootstrap-server localhost:9092 \
  --delete-config retention.ms
```

#### 删除主题
```bash
# 删除主题（谨慎操作！）
kafka-topics.sh --delete \
  --topic test_topic \
  --bootstrap-server localhost:9092

# 批量删除测试主题
kafka-topics.sh --list --bootstrap-server localhost:9092 | grep "^test_" | xargs -I {} kafka-topics.sh --delete --topic {} --bootstrap-server localhost:9092
```

---

## 分区管理与重分配

### 增加分区数

#### 基本扩容操作
```bash
# 为现有主题增加分区
kafka-topics.sh --alter \
  --topic user_events \
  --bootstrap-server localhost:9092 \
  --partitions 6

# 验证分区增加结果
kafka-topics.sh --describe \
  --topic user_events \
  --bootstrap-server localhost:9092
```

#### 分区重分配计划

分区重分配是一个复杂操作，需要先制定计划，然后执行。

```bash
# 生成重分配计划
kafka-reassign-partitions.sh \
  --bootstrap-server localhost:9092 \
  --broker-list "0,1,2,3" \
  --topics-to-move-json-file topics-to-move.json \
  --generate

# 创建要移动的主题列表文件
echo '{
  "topics": [
    {"topic": "user_events"},
    {"topic": "transaction_data"}
  ],
  "version": 1
}' > topics-to-move.json

# 使用生成的计划执行重分配
kafka-reassign-partitions.sh \
  --bootstrap-server localhost:9092 \
  --reassignment-json-file expand-cluster-reassignment.json \
  --execute

# 验证重分配状态
kafka-reassign-partitions.sh \
  --bootstrap-server localhost:9092 \
  --reassignment-json-file expand-cluster-reassignment.json \
  --verify
```

### 分区Leader选举

#### 手动Leader选举
```bash
# 查看当前分区状态
kafka-topics.sh --describe \
  --topic user_events \
  --bootstrap-server localhost:9092

# 手动执行Leader选举（如果需要）
# 注意：这通常在Leader失效时自动进行
kafka-leader-election.sh \
  --bootstrap-server localhost:9092 \
  --topic user_events \
  --partition 0 \
  --election-type PREFERRED
```

---

## 分区策略深入分析

### Range策略详解

Range策略是默认的分区分配策略，它的工作原理是：

1. 对每个主题，按照分区编号排序
2. 将分区范围分配给消费者
3. 如果分区数不能被消费者数整除，前几个消费者会多分配分区

**示例场景**：
- 主题A有7个分区（0-6）
- 消费者组有3个消费者（C0, C1, C2）

**分配结果**：
- C0：分区0,1,2（3个分区）
- C1：分区3,4（2个分区）
- C2：分区5,6（2个分区）

### RoundRobin策略详解

RoundRobin策略的工作原理：
1. 将所有主题的所有分区排序
2. 轮询分配给每个消费者
3. 要求所有消费者订阅相同的主题

**示例场景**：
- 主题A有分区0,1
- 主题B有分区0,1
- 消费者C0, C1

**分配结果**：
- C0：主题A分区0, 主题B分区1
- C1：主题A分区1, 主题B分区0

### Sticky策略详解

Sticky策略的目标是最小化分区重分配：
1. 尽可能保持现有的分配不变
2. 只在必要时进行最小化调整
3. 适用于重新平衡场景

---

## 副本机制与ISR管理

### 副本（Replication）基础

Kafka使用副本机制来保证数据的可靠性和可用性：

#### 副本类型：
- **Leader副本**：处理所有读写请求
- **Follower副本**：被动复制Leader数据
- **ISR**：In-Sync Replicas，同步副本集合

### ISR管理

#### ISR的组成
ISR包含与ZooKeeper保持会话同步的副本：
- Leader总是包含在ISR中
- Follower必须定期向Leader发送fetch请求
- 滞后超过阈值的Follower会被移出ISR

#### ISR相关配置
```properties
# 副本滞后监控阈值
replica.lag.time.max.ms=10000
replica.lag.max.messages=4000

# 最小ISR数量
min.insync.replicas=2

# 副本抓取配置
replica.fetch.max.bytes=1048576
replica.fetch.wait.max.ms=500
```

#### ISR监控
```bash
# 查看主题的ISR状态
kafka-topics.sh --describe \
  --topic user_events \
  --bootstrap-server localhost:9092

# 查看详细副本信息
kafka-run-class.sh kafka.tools.GetOffsetShell \
  --broker-list localhost:9092 \
  --topic user_events \
  --time -1
```

---

## 主题配置深度解析

### 核心配置参数

#### 保留策略配置
```properties
# 时间保留（7天）
retention.ms=604800000

# 大小保留（1GB）
retention.bytes=1073741824

# 清理策略
cleanup.policy=delete  # delete 或 compact

# 最小清理时间（60秒）
log.retention.check.interval.ms=60000
```

#### 存储配置
```properties
# 分段大小（1GB）
log.segment.bytes=1073741824

# 分段文件检查间隔（5分钟）
log.roll.hours=168

# 压缩配置
compression.type=producer  # producer, gzip, snappy, lz4, zstd

# 压缩脏数据比率（0.5）
min.cleanable.dirty.ratio=0.5
```

#### 性能配置
```properties
# 批量大小
batch.size=16384

# 发送等待时间
linger.ms=5

# 最小同步副本数
min.insync.replicas=2

# acks配置
acks=all
```

### 主题配置模板

#### 高吞吐量日志主题
```properties
cleanup.policy=delete
retention.ms=86400000  # 1天
compression.type=gzip
batch.size=32768
linger.ms=20
```

#### 关键事务数据主题
```properties
cleanup.policy=compact
retention.ms=-1  # 永久保留
min.insync.replicas=2
compression.type=lz4
```

#### 监控指标主题
```properties
cleanup.policy=delete
retention.ms=604800000  # 7天
compression.type=snappy
log.segment.bytes=536870912  # 512MB
```

---

## 实战代码示例

### 1. 主题管理Java API

```java
package com.kafka.tutorial.admin;

import org.apache.kafka.clients.admin.*;
import org.apache.kafka.common.config.ConfigResource;
import org.apache.kafka.common.errors.TopicExistsException;
import org.apache.kafka.common.errors.UnknownTopicOrNamespaceException;

import java.util.*;
import java.util.concurrent.ExecutionException;
import java.util.concurrent.TimeUnit;

/**
 * Kafka主题管理工具类
 */
public class TopicManager {
    private final KafkaAdminClient adminClient;

    public TopicManager(Properties props) {
        this.adminClient = (KafkaAdminClient) KafkaAdminClient.create(props);
    }

    /**
     * 创建主题
     */
    public void createTopic(String topicName, int partitions, int replicationFactor, 
                           Map<String, String> configs) {
        try {
            // 构建主题配置
            NewTopic topic = new NewTopic(topicName, partitions, (short) replicationFactor);
            if (configs != null && !configs.isEmpty()) {
                topic.configs(configs);
            }

            // 执行创建操作
            CreateTopicsResult result = adminClient.createTopics(Collections.singleton(topic));
            
            // 等待创建完成
            result.all().get(30, TimeUnit.SECONDS);
            
            System.out.println("主题创建成功: " + topicName);
            
        } catch (TopicExistsException e) {
            System.out.println("主题已存在: " + topicName);
        } catch (Exception e) {
            throw new RuntimeException("创建主题失败: " + topicName, e);
        }
    }

    /**
     * 删除主题
     */
    public void deleteTopic(String topicName) {
        try {
            DeleteTopicsResult result = adminClient.deleteTopics(Collections.singleton(topicName));
            result.all().get(30, TimeUnit.SECONDS);
            System.out.println("主题删除成功: " + topicName);
        } catch (UnknownTopicOrNamespaceException e) {
            System.out.println("主题不存在: " + topicName);
        } catch (Exception e) {
            throw new RuntimeException("删除主题失败: " + topicName, e);
        }
    }

    /**
     * 列出所有主题
     */
    public List<String> listTopics() {
        try {
            ListTopicsResult result = adminClient.listTopics();
            Set<String> topics = result.names().get(30, TimeUnit.SECONDS);
            return new ArrayList<>(topics);
        } catch (Exception e) {
            throw new RuntimeException("获取主题列表失败", e);
        }
    }

    /**
     * 获取主题详细信息
     */
    public TopicDescription describeTopic(String topicName) {
        try {
            DescribeTopicsResult result = adminClient.describeTopics(Collections.singleton(topicName));
            Map<String, TopicDescription> descriptions = result.all().get(30, TimeUnit.SECONDS);
            return descriptions.get(topicName);
        } catch (Exception e) {
            throw new RuntimeException("获取主题信息失败: " + topicName, e);
        }
    }

    /**
     * 修改主题分区数
     */
    public void increasePartitions(String topicName, int newPartitions) {
        try {
            Map<NewPartitions, NewPartitions.IncreaseTo> newPartitionMap = 
                Collections.singletonMap(
                    NewPartitions.increaseTo(topicName, newPartitions),
                    NewPartitions.IncreaseTo.of(newPartitions)
                );
            
            CreatePartitionsResult result = adminClient.createPartitions(newPartitionMap);
            result.all().get(30, TimeUnit.SECONDS);
            
            System.out.println("分区增加成功: " + topicName + " -> " + newPartitions);
        } catch (Exception e) {
            throw new RuntimeException("增加分区失败: " + topicName, e);
        }
    }

    /**
     * 获取主题配置
     */
    public Config describeTopicConfig(String topicName) {
        try {
            ConfigResource configResource = new ConfigResource(
                ConfigResource.Type.TOPIC, topicName);
            
            DescribeConfigsResult result = adminClient.describeConfigs(
                Collections.singleton(configResource));
            
            Map<ConfigResource, Config> configs = result.all().get(30, TimeUnit.SECONDS);
            return configs.get(configResource);
        } catch (Exception e) {
            throw new RuntimeException("获取主题配置失败: " + topicName, e);
        }
    }

    /**
     * 修改主题配置
     */
    public void alterTopicConfig(String topicName, Map<String, String> configs) {
        try {
            ConfigResource configResource = new ConfigResource(
                ConfigResource.Type.TOPIC, topicName);
            
            Collection<AlterConfigOp> alterConfigOps = new ArrayList<>();
            configs.forEach((key, value) -> 
                alterConfigOps.add(new AlterConfigOp(
                    new ConfigEntry(key, value), 
                    AlterConfigOp.OpType.SET)));
            
            Map<ConfigResource, Collection<AlterConfigOp>> alterConfigMap = 
                Collections.singletonMap(configResource, alterConfigOps);
            
            AlterConfigsResult result = adminClient.incrementalAlterConfigs(alterConfigMap);
            result.all().get(30, TimeUnit.SECONDS);
            
            System.out.println("主题配置修改成功: " + topicName);
        } catch (Exception e) {
            throw new RuntimeException("修改主题配置失败: " + topicName, e);
        }
    }

    /**
     * 批量创建主题
     */
    public void createTopicsBatch(List<TopicCreationRequest> requests) {
        try {
            List<NewTopic> newTopics = new ArrayList<>();
            Map<String, String> allConfigs = new HashMap<>();
            
            for (TopicCreationRequest request : requests) {
                NewTopic topic = new NewTopic(request.getName(), 
                                            request.getPartitions(), 
                                            request.getReplicationFactor());
                if (request.getConfigs() != null) {
                    topic.configs(request.getConfigs());
                }
                newTopics.add(topic);
            }
            
            CreateTopicsResult result = adminClient.createTopics(newTopics);
            result.all().get(60, TimeUnit.SECONDS);
            
            System.out.println("批量主题创建成功，共" + requests.size() + "个主题");
        } catch (Exception e) {
            throw new RuntimeException("批量创建主题失败", e);
        }
    }

    /**
     * 清理过期的消费者组偏移
     */
    public void cleanOldConsumerOffsets() {
        try {
            // 清理__consumer_offsets主题中的过期数据
            Map<String, String> configs = new HashMap<>();
            configs.put("cleanup.policy", "compact");
            
            ConfigResource configResource = new ConfigResource(
                ConfigResource.Type.TOPIC, "__consumer_offsets");
            
            Collection<AlterConfigOp> alterConfigOps = Collections.singletonList(
                new AlterConfigOp(new ConfigEntry("cleanup.policy", "compact"), 
                                AlterConfigOp.OpType.SET));
            
            Map<ConfigResource, Collection<AlterConfigOp>> alterConfigMap = 
                Collections.singletonMap(configResource, alterConfigOps);
            
            AlterConfigsResult result = adminClient.incrementalAlterConfigs(alterConfigMap);
            result.all().get(30, TimeUnit.SECONDS);
            
            System.out.println("消费者偏移清理配置更新成功");
        } catch (Exception e) {
            throw new RuntimeException("清理消费者偏移失败", e);
        }
    }

    /**
     * 关闭客户端
     */
    public void close() {
        if (adminClient != null) {
            adminClient.close();
        }
    }

    /**
     * 主题创建请求类
     */
    public static class TopicCreationRequest {
        private String name;
        private int partitions;
        private int replicationFactor;
        private Map<String, String> configs;

        public TopicCreationRequest(String name, int partitions, int replicationFactor) {
            this.name = name;
            this.partitions = partitions;
            this.replicationFactor = replicationFactor;
        }

        // Getters and setters
        public String getName() { return name; }
        public int getPartitions() { return partitions; }
        public int getReplicationFactor() { return replicationFactor; }
        public Map<String, String> getConfigs() { return configs; }
        
        public void setConfigs(Map<String, String> configs) {
            this.configs = configs;
        }
    }
}
```

### 2. 分区策略测试示例

```java
package com.kafka.tutorial.partitions;

import org.apache.kafka.clients.consumer.*;
import org.apache.kafka.clients.producer.*;
import org.apache.kafka.common.TopicPartition;
import org.apache.kafka.common.serialization.StringDeserializer;
import org.apache.kafka.common.serialization.StringSerializer;

import java.util.*;
import java.util.concurrent.TimeUnit;

/**
 * 分区策略测试和演示
 */
public class PartitionStrategyDemo {

    private static final String BOOTSTRAP_SERVERS = "localhost:9092";
    
    /**
     * Range分区策略演示
     */
    public static void demonstrateRangePartitioning() {
        System.out.println("=== Range分区策略演示 ===");
        
        // 创建具有不同分区数的主题进行测试
        String[] topics = {"range-topic-1", "range-topic-2"};
        int[] partitionCounts = {3, 6};
        
        try (AdminClient adminClient = AdminClient.create(getAdminProps())) {
            // 创建测试主题
            for (int i = 0; i < topics.length; i++) {
                NewTopic topic = new NewTopic(topics[i], partitionCounts[i], (short) 1);
                adminClient.createTopics(Collections.singleton(topic)).all().get();
            }
        } catch (Exception e) {
            e.printStackTrace();
        }
        
        // 测试Range分配策略
        Properties consumerProps = getConsumerProps();
        consumerProps.put(ConsumerConfig.PARTITION_ASSIGNMENT_STRATEGY_CONFIG, 
                         "org.apache.kafka.clients.consumer.RangeAssignor");
        
        // 创建3个消费者来演示Range分配
        testPartitionAssignment(consumerProps, topics, 3, "Range");
    }
    
    /**
     * RoundRobin分区策略演示
     */
    public static void demonstrateRoundRobinPartitioning() {
        System.out.println("=== RoundRobin分区策略演示 ===");
        
        String topic = "rr-topic";
        
        // 创建订阅相同主题的消费者
        Properties consumerProps = getConsumerProps();
        consumerProps.put(ConsumerConfig.PARTITION_ASSIGNMENT_STRATEGY_CONFIG, 
                         "org.apache.kafka.clients.consumer.RoundRobinAssignor");
        
        // 测试RoundRobin分配策略
        String[] topics = {topic, "rr-topic-2"};
        testPartitionAssignment(consumerProps, topics, 3, "RoundRobin");
    }
    
    /**
     * Sticky分区策略演示
     */
    public static void demonstrateStickyPartitioning() {
        System.out.println("=== Sticky分区策略演示 ===");
        
        String topic = "sticky-topic";
        
        Properties consumerProps = getConsumerProps();
        consumerProps.put(ConsumerConfig.PARTITION_ASSIGNMENT_STRATEGY_CONFIG, 
                         "org.apache.kafka.clients.consumer.StickyAssignor");
        
        // 测试Sticky分配策略
        String[] topics = {topic, "sticky-topic-2"};
        testPartitionAssignment(consumerProps, topics, 3, "Sticky");
    }
    
    /**
     * 测试分区分配策略
     */
    private static void testPartitionAssignment(Properties consumerProps, 
                                               String[] topics, 
                                               int consumerCount,
                                               String strategyName) {
        List<String> consumerIds = new ArrayList<>();
        Map<String, List<TopicPartition>> assignments = new HashMap<>();
        
        // 模拟消费者组订阅和分区分配
        for (int i = 0; i < consumerCount; i++) {
            String consumerId = strategyName + "-consumer-" + i;
            consumerIds.add(consumerId);
            
            // 这里模拟Kafka的分区分配算法逻辑
            List<TopicPartition> partitionAssignment = 
                simulatePartitionAssignment(topics, consumerIds, consumerId);
            
            assignments.put(consumerId, partitionAssignment);
        }
        
        // 打印分配结果
        System.out.println("分区分配结果:");
        assignments.forEach((consumer, partitions) -> {
            System.out.println("  " + consumer + ": " + 
                             partitions.stream()
                                       .map(p -> p.topic() + "-" + p.partition())
                                       .reduce((a, b) -> a + ", " + b)
                                       .orElse("无分区"));
        });
        System.out.println();
    }
    
    /**
     * 模拟分区分配算法（简化版）
     */
    private static List<TopicPartition> simulatePartitionAssignment(String[] topics, 
                                                                    List<String> consumerIds, 
                                                                    String currentConsumer) {
        List<TopicPartition> assignedPartitions = new ArrayList<>();
        int consumerIndex = consumerIds.indexOf(currentConsumer);
        
        for (String topic : topics) {
            try (AdminClient adminClient = AdminClient.create(getAdminProps())) {
                TopicDescription description = adminClient.describeTopics(Arrays.asList(topic))
                    .all().get().get(topic);
                int partitions = description.partitions().size();
                
                // 简化版的Range分配算法
                int consumersPerTopic = consumerIds.size();
                int startIndex = consumerIndex % consumersPerTopic;
                
                for (int partition = startIndex; 
                     partition < partitions; 
                     partition += consumersPerTopic) {
                    assignedPartitions.add(new TopicPartition(topic, partition));
                }
                
            } catch (Exception e) {
                e.printStackTrace();
            }
        }
        
        return assignedPartitions;
    }
    
    /**
     * 分区键哈希演示
     */
    public static void demonstratePartitionKeyHashing() {
        System.out.println("=== 分区键哈希演示 ===");
        
        String topic = "user-events";
        Properties producerProps = getProducerProps();
        
        try (AdminClient adminClient = AdminClient.create(getAdminProps());
             KafkaProducer<String, String> producer = new KafkaProducer<>(producerProps)) {
            
            // 创建主题
            NewTopic topicObj = new NewTopic(topic, 6, (short) 1);
            adminClient.createTopics(Collections.singleton(topicObj)).all().get();
            
            // 发送不同用户ID的消息，观察分区分布
            Map<Integer, Integer> partitionDistribution = new HashMap<>();
            
            for (int i = 0; i < 100; i++) {
                String userId = "user" + (i % 15); // 15个用户，每个用户发送多条消息
                String message = "用户 " + userId + " 的第 " + (i / 15 + 1) + " 次操作";
                
                ProducerRecord<String, String> record = 
                    new ProducerRecord<>(topic, userId, message);
                
                // 获取分区
                Integer partition = record.partition();
                partitionDistribution.merge(partition, 1, Integer::sum);
                
                // 发送消息（可选）
                // producer.send(record);
            }
            
            // 打印分区分布
            System.out.println("分区消息分布:");
            partitionDistribution.forEach((partition, count) -> 
                System.out.println("  分区 " + partition + ": " + count + " 条消息"));
                
        } catch (Exception e) {
            e.printStackTrace();
        }
    }
    
    /**
     * 批量消息发送和分区测试
     */
    public static void demonstrateBatchMessagePartitioning() {
        System.out.println("=== 批量消息分区测试 ===");
        
        String topic = "batch-test-topic";
        Properties producerProps = getProducerProps();
        producerProps.put(ProducerConfig.BATCH_SIZE_CONFIG, 32768);
        producerProps.put(ProducerConfig.LINGER_MS_CONFIG, 100);
        
        try (AdminClient adminClient = AdminClient.create(getAdminProps());
             KafkaProducer<String, String> producer = new KafkaProducer<>(producerProps)) {
            
            // 创建主题
            NewTopic topicObj = new NewTopic(topic, 4, (short) 1);
            adminClient.createTopics(Collections.singleton(topicObj)).all().get();
            
            // 发送批量消息
            List<ProducerRecord<String, String>> records = new ArrayList<>();
            Random random = new Random();
            
            for (int i = 0; i < 1000; i++) {
                String key = "batch-key-" + random.nextInt(20);
                String message = "批量消息 " + i;
                records.add(new ProducerRecord<>(topic, key, message));
            }
            
            // 批量发送并收集分区统计
            Map<Integer, Integer> batchPartitionStats = new HashMap<>();
            List<Future<RecordMetadata>> futures = new ArrayList<>();
            
            for (ProducerRecord<String, String> record : records) {
                Future<RecordMetadata> future = producer.send(record, (metadata, exception) -> {
                    if (exception != null) {
                        System.err.println("发送失败: " + exception.getMessage());
                    }
                });
                futures.add(future);
                
                // 记录分区统计
                batchPartitionStats.merge(record.partition(), 1, Integer::sum);
            }
            
            // 等待所有消息发送完成
            for (Future<RecordMetadata> future : futures) {
                try {
                    future.get(10, TimeUnit.SECONDS);
                } catch (Exception e) {
                    System.err.println("等待消息发送完成时出错: " + e.getMessage());
                }
            }
            
            // 打印统计结果
            System.out.println("批量发送分区分布:");
            batchPartitionStats.forEach((partition, count) -> 
                System.out.println("  分区 " + partition + ": " + count + " 条消息"));
                
        } catch (Exception e) {
            e.printStackTrace();
        }
    }
    
    private static Properties getAdminProps() {
        Properties props = new Properties();
        props.put(AdminClientConfig.BOOTSTRAP_SERVERS_CONFIG, BOOTSTRAP_SERVERS);
        return props;
    }
    
    private static Properties getProducerProps() {
        Properties props = new Properties();
        props.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, BOOTSTRAP_SERVERS);
        props.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, StringSerializer.class);
        props.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, StringSerializer.class);
        props.put(ProducerConfig.ACKS_CONFIG, "all");
        return props;
    }
    
    private static Properties getConsumerProps() {
        Properties props = new Properties();
        props.put(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG, BOOTSTRAP_SERVERS);
        props.put(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class);
        props.put(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class);
        props.put(ConsumerConfig.GROUP_ID_CONFIG, "test-group");
        props.put(ConsumerConfig.AUTO_OFFSET_RESET_CONFIG, "earliest");
        return props;
    }
    
    public static void main(String[] args) {
        demonstrateRangePartitioning();
        demonstrateRoundRobinPartitioning();
        demonstrateStickyPartitioning();
        demonstratePartitionKeyHashing();
        demonstrateBatchMessagePartitioning();
    }
}
```

### 3. 主题监控和状态检查

```java
package com.kafka.tutorial.monitoring;

import org.apache.kafka.clients.admin.*;
import org.apache.kafka.common.TopicPartitionInfo;
import org.apache.kafka.common.config.ConfigResource;

import java.util.*;
import java.util.concurrent.TimeUnit;
import java.util.stream.Collectors;

/**
 * 主题监控和状态检查工具
 */
public class TopicMonitoring {
    private final KafkaAdminClient adminClient;

    public TopicMonitoring(Properties props) {
        this.adminClient = (KafkaAdminClient) KafkaAdminClient.create(props);
    }

    /**
     * 获取集群健康状态概览
     */
    public ClusterHealthStatus getClusterHealthStatus() {
        ClusterHealthStatus status = new ClusterHealthStatus();
        
        try {
            // 获取集群信息
            DescribeClusterResult clusterResult = adminClient.describeCluster();
            String clusterId = clusterResult.clusterId().get(30, TimeUnit.SECONDS);
            int brokerCount = clusterResult.nodes().get(30, TimeUnit.SECONDS).size();
            
            status.setClusterId(clusterId);
            status.setBrokerCount(brokerCount);
            
            // 获取所有主题
            Set<String> topics = adminClient.listTopics().names().get(30, TimeUnit.SECONDS);
            Set<String> internalTopics = adminClient.describeTopics(topics).all().get(30, TimeUnit.SECONDS)
                .values().stream()
                .filter(desc -> desc.isInternal())
                .map(TopicDescription::name)
                .collect(Collectors.toSet());
            
            status.setTotalTopics(topics.size() - internalTopics.size());
            status.setInternalTopics(internalTopics.size());
            
            // 分析主题状态
            Map<String, TopicStatus> topicStatuses = new HashMap<>();
            for (String topic : topics) {
                if (internalTopics.contains(topic)) continue; // 跳过内部主题
                
                TopicStatus topicStatus = analyzeTopicStatus(topic);
                topicStatuses.put(topic, topicStatus);
            }
            status.setTopicStatuses(topicStatuses);
            
        } catch (Exception e) {
            status.setError("获取集群状态失败: " + e.getMessage());
        }
        
        return status;
    }

    /**
     * 分析单个主题的状态
     */
    private TopicStatus analyzeTopicStatus(String topicName) {
        TopicStatus status = new TopicStatus();
        status.setTopicName(topicName);
        
        try {
            // 获取主题描述
            TopicDescription description = adminClient.describeTopics(Collections.singleton(topicName))
                .all().get(30, TimeUnit.SECONDS).get(topicName);
            
            status.setPartitionCount(description.partitions().size());
            status.setReplicationFactor(description.partitions().isEmpty() ? 0 : 
                description.partitions().get(0).replicas().size());
            status.setInternal(description.isInternal());
            
            // 分析分区状态
            int underReplicatedPartitions = 0;
            int offlinePartitions = 0;
            Set<String> leaderNodes = new HashSet<>();
            Set<String> replicaNodes = new HashSet<>();
            
            for (TopicPartitionInfo partition : description.partitions()) {
                // Leader信息
                if (partition.leader() != null) {
                    leaderNodes.add(String.valueOf(partition.leader().id()));
                }
                
                // 副本信息
                for (Node replica : partition.replicas()) {
                    replicaNodes.add(String.valueOf(replica.id()));
                    
                    if (replica.id() == partition.leader().id()) {
                        continue; // Leader不需要检查ISR
                    }
                    
                    // 检查副本是否在ISR中
                    boolean isInSync = partition.isr().stream()
                        .anyMatch(node -> node.id() == replica.id());
                    
                    if (!isInSync) {
                        underReplicatedPartitions++;
                    }
                }
                
                // 检查离线分区
                if (partition.leader() == null) {
                    offlinePartitions++;
                }
            }
            
            status.setLeaderNodeCount(leaderNodes.size());
            status.setReplicaNodeCount(replicaNodes.size());
            status.setUnderReplicatedPartitions(underReplicatedPartitions);
            status.setOfflinePartitions(offlinePartitions);
            
            // 计算主题健康状态
            List<String> issues = new ArrayList<>();
            if (underReplicatedPartitions > 0) {
                issues.add("存在" + underReplicatedPartitions + "个未同步分区");
            }
            if (offlinePartitions > 0) {
                issues.add("存在" + offlinePartitions + "个离线分区");
            }
            
            status.setHealthy(issues.isEmpty());
            status.setIssues(issues);
            
        } catch (Exception e) {
            status.setError("分析主题状态失败: " + e.getMessage());
        }
        
        return status;
    }

    /**
     * 获取主题配置信息
     */
    public TopicConfigInfo getTopicConfigInfo(String topicName) {
        TopicConfigInfo configInfo = new TopicConfigInfo();
        configInfo.setTopicName(topicName);
        
        try {
            ConfigResource configResource = new ConfigResource(
                ConfigResource.Type.TOPIC, topicName);
            
            DescribeConfigsResult result = adminClient.describeConfigs(
                Collections.singleton(configResource));
            
            Config config = result.all().get(30, TimeUnit.SECONDS).get(configResource);
            
            Map<String, String> configs = new HashMap<>();
            for (ConfigEntry entry : config.entries()) {
                if (!entry.isDefault() && entry.value() != null) {
                    configs.put(entry.name(), entry.value());
                }
            }
            configInfo.setConfigs(configs);
            
            // 分析配置合理性
            List<String> recommendations = analyzeConfigRecommendations(configs);
            configInfo.setRecommendations(recommendations);
            
        } catch (Exception e) {
            configInfo.setError("获取主题配置失败: " + e.getMessage());
        }
        
        return configInfo;
    }

    /**
     * 分析配置建议
     */
    private List<String> analyzeConfigRecommendations(Map<String, String> configs) {
        List<String> recommendations = new ArrayList<>();
        
        // 保留策略分析
        String retentionMs = configs.get("retention.ms");
        if (retentionMs != null) {
            try {
                long retention = Long.parseLong(retentionMs);
                if (retention > 604800000) { // 7天
                    recommendations.add("保留时间较长(" + (retention / 86400000) + "天)，建议评估存储需求");
                }
                if (retention < 86400000) { // 1天
                    recommendations.add("保留时间较短(" + (retention / 3600000) + "小时)，确保满足业务需求");
                }
            } catch (NumberFormatException e) {
                recommendations.add("保留时间配置格式错误");
            }
        }
        
        // 压缩配置分析
        String compression = configs.get("compression.type");
        if (compression != null && !"none".equals(compression)) {
            recommendations.add("已启用压缩: " + compression + "，注意CPU开销");
        }
        
        // 最小同步副本分析
        String minISR = configs.get("min.insync.replicas");
        if (minISR != null) {
            try {
                int minIsr = Integer.parseInt(minISR);
                if (minIsr == 1) {
                    recommendations.push("最小同步副本为1，安全性较低");
                }
            } catch (NumberFormatException e) {
                recommendations.add("min.insync.replicas配置格式错误");
            }
        }
        
        return recommendations;
    }

    /**
     * 生成主题监控报告
     */
    public String generateMonitoringReport() {
        StringBuilder report = new StringBuilder();
        
        report.append("=== Kafka集群主题监控报告 ===\n\n");
        
        // 集群概览
        ClusterHealthStatus clusterStatus = getClusterHealthStatus();
        report.append("集群概览:\n");
        report.append("  集群ID: ").append(clusterStatus.getClusterId()).append("\n");
        report.append("  Broker数量: ").append(clusterStatus.getBrokerCount()).append("\n");
        report.append("  主题数量: ").append(clusterStatus.getTotalTopics()).append("\n");
        report.append("  内部主题数量: ").append(clusterStatus.getInternalTopics()).append("\n\n");
        
        if (clusterStatus.getTopicStatuses() != null) {
            report.append("主题状态详情:\n");
            int healthyTopics = 0;
            int totalPartitions = 0;
            int totalUnderReplicated = 0;
            int totalOffline = 0;
            
            for (TopicStatus status : clusterStatus.getTopicStatuses().values()) {
                totalPartitions += status.getPartitionCount();
                totalUnderReplicated += status.getUnderReplicatedPartitions();
                totalOffline += status.getOfflinePartitions();
                
                if (status.isHealthy()) {
                    healthyTopics++;
                } else {
                    report.append("  ⚠️ ").append(status.getTopicName()).append(":\n");
                    for (String issue : status.getIssues()) {
                        report.append("      - ").append(issue).append("\n");
                    }
                }
            }
            
            report.append("\n统计汇总:\n");
            report.append("  健康主题: ").append(healthyTopics)
                  .append("/").append(clusterStatus.getTotalTopics()).append("\n");
            report.append("  总分区数: ").append(totalPartitions).append("\n");
            report.append("  未同步分区: ").append(totalUnderReplicated).append("\n");
            report.append("  离线分区: ").append(totalOffline).append("\n");
        }
        
        return report.toString();
    }

    /**
     * 清理主题数据（开发测试用）
     */
    public void cleanupTopicData(String topicName) {
        try {
            // 获取主题描述
            TopicDescription description = adminClient.describeTopics(Collections.singleton(topicName))
                .all().get(30, TimeUnit.SECONDS).get(topicName);
            
            int originalPartitions = description.partitions().size();
            
            // 备份原配置
            TopicConfigInfo configInfo = getTopicConfigInfo(topicName);
            
            // 删除并重建主题
            adminClient.deleteTopics(Collections.singleton(topicName)).all().get();
            
            Thread.sleep(2000); // 等待删除完成
            
            // 重新创建主题
            NewTopic newTopic = new NewTopic(topicName, originalPartitions, (short) 1);
            if (!configInfo.getConfigs().isEmpty()) {
                newTopic.configs(configInfo.getConfigs());
            }
            
            adminClient.createTopics(Collections.singleton(newTopic)).all().get();
            
            System.out.println("主题数据清理完成: " + topicName);
            
        } catch (Exception e) {
            throw new RuntimeException("清理主题数据失败: " + topicName, e);
        }
    }

    public void close() {
        if (adminClient != null) {
            adminClient.close();
        }
    }

    // 数据类定义
    public static class ClusterHealthStatus {
        private String clusterId;
        private int brokerCount;
        private int totalTopics;
        private int internalTopics;
        private Map<String, TopicStatus> topicStatuses;
        private String error;

        // Getters and setters
        public String getClusterId() { return clusterId; }
        public void setClusterId(String clusterId) { this.clusterId = clusterId; }
        public int getBrokerCount() { return brokerCount; }
        public void setBrokerCount(int brokerCount) { this.brokerCount = brokerCount; }
        public int getTotalTopics() { return totalTopics; }
        public void setTotalTopics(int totalTopics) { this.totalTopics = totalTopics; }
        public int getInternalTopics() { return internalTopics; }
        public void setInternalTopics(int internalTopics) { this.internalTopics = internalTopics; }
        public Map<String, TopicStatus> getTopicStatuses() { return topicStatuses; }
        public void setTopicStatuses(Map<String, TopicStatus> topicStatuses) { this.topicStatuses = topicStatuses; }
        public String getError() { return error; }
        public void setError(String error) { this.error = error; }
    }

    public static class TopicStatus {
        private String topicName;
        private int partitionCount;
        private int replicationFactor;
        private boolean internal;
        private boolean healthy;
        private int leaderNodeCount;
        private int replicaNodeCount;
        private int underReplicatedPartitions;
        private int offlinePartitions;
        private List<String> issues;
        private String error;

        // Getters and setters
        public String getTopicName() { return topicName; }
        public void setTopicName(String topicName) { this.topicName = topicName; }
        public int getPartitionCount() { return partitionCount; }
        public void setPartitionCount(int partitionCount) { this.partitionCount = partitionCount; }
        public int getReplicationFactor() { return replicationFactor; }
        public void setReplicationFactor(int replicationFactor) { this.replicationFactor = replicationFactor; }
        public boolean isInternal() { return internal; }
        public void setInternal(boolean internal) { this.internal = internal; }
        public boolean isHealthy() { return healthy; }
        public void setHealthy(boolean healthy) { this.healthy = healthy; }
        public int getLeaderNodeCount() { return leaderNodeCount; }
        public void setLeaderNodeCount(int leaderNodeCount) { this.leaderNodeCount = leaderNodeCount; }
        public int getReplicaNodeCount() { return replicaNodeCount; }
        public void setReplicaNodeCount(int replicaNodeCount) { this.replicaNodeCount = replicaNodeCount; }
        public int getUnderReplicatedPartitions() { return underReplicatedPartitions; }
        public void setUnderReplicatedPartitions(int underReplicatedPartitions) { this.underReplicatedPartitions = underReplicatedPartitions; }
        public int getOfflinePartitions() { return offlinePartitions; }
        public void setOfflinePartitions(int offlinePartitions) { this.offlinePartitions = offlinePartitions; }
        public List<String> getIssues() { return issues; }
        public void setIssues(List<String> issues) { this.issues = issues; }
        public String getError() { return error; }
        public void setError(String error) { this.error = error; }
    }

    public static class TopicConfigInfo {
        private String topicName;
        private Map<String, String> configs;
        private List<String> recommendations;
        private String error;

        // Getters and setters
        public String getTopicName() { return topicName; }
        public void setTopicName(String topicName) { this.topicName = topicName; }
        public Map<String, String> getConfigs() { return configs; }
        public void setConfigs(Map<String, String> configs) { this.configs = configs; }
        public List<String> getRecommendations() { return recommendations; }
        public void setRecommendations(List<String> recommendations) { this.recommendations = recommendations; }
        public String getError() { return error; }
        public void setError(String error) { this.error = error; }
    }
}
```

---

## 最佳实践与优化

### 主题设计最佳实践

#### 1. 分区数设计原则
```java
/**
 * 分区数计算工具类
 */
public class PartitionSizingCalculator {
    
    /**
     * 基于吞吐量计算分区数
     */
    public static int calculatePartitionCountByThroughput(int targetThroughputMbps,
                                                         int averageMessageSizeBytes,
                                                         int consumerThroughputMbps) {
        // 计算每个分区的最大吞吐
        int bytesPerSecond = targetThroughputMbps * 1024 * 1024 / 8;
        int messageSize = averageMessageSizeBytes;
        
        // 考虑副本开销
        double replicationFactor = 3.0;
        double effectiveThroughputPerPartition = bytesPerSecond / replicationFactor;
        
        // 计算所需分区数
        int consumerCapacity = (consumerThroughputMbps * 1024 * 1024 / 8) / messageSize;
        int partitionsByConsumers = (int) (effectiveThroughputPerPartition / messageSize / consumerCapacity);
        
        // 确保至少3个分区以支持高可用
        return Math.max(3, partitionsByConsumers);
    }
    
    /**
     * 基于业务需求计算分区数
     */
    public static int calculatePartitionCountByBusinessRequirements(int dailyMessageVolume,
                                                                   double peakToAverageRatio,
                                                                   int retentionDays,
                                                                   int maxConcurrentConsumers) {
        // 计算每秒平均消息数
        int avgMessagesPerSecond = (int) (dailyMessageVolume / (24 * 3600));
        
        // 计算峰值消息数
        int peakMessagesPerSecond = (int) (avgMessagesPerSecond * peakToAverageRatio);
        
        // 考虑消费者处理能力（每个消费者每秒处理1000条消息）
        int consumerCapacityPerSecond = 1000;
        
        // 计算理论最小分区数
        int minPartitionsByConsumers = (int) Math.ceil(peakMessagesPerSecond * 1.0 / consumerCapacityPerSecond);
        
        // 考虑保留期间的处理需求
        int minPartitionsByRetention = maxConcurrentConsumers;
        
        // 取最大值并确保合理范围
        int recommendedPartitions = Math.max(minPartitionsByConsumers, minPartitionsByRetention);
        return Math.min(recommendedPartitions, 1000); // 限制最大分区数
    }
    
    /**
     * 评估现有分区数的合理性
     */
    public static PartitionHealthAssessment assessPartitionHealth(int currentPartitions,
                                                                 Map<String, Object> metrics) {
        PartitionHealthAssessment assessment = new PartitionHealthAssessment();
        assessment.setCurrentPartitions(currentPartitions);
        
        List<String> recommendations = new ArrayList<>();
        
        // 检查分区数是否过少
        if (currentPartitions < 3) {
            recommendations.add("分区数过少，建议至少3个分区以支持高可用性");
        }
        
        // 检查分区数是否过多
        if (currentPartitions > 1000) {
            recommendations.add("分区数过多，可能导致元数据开销过大");
        }
        
        // 基于消息吞吐量评估
        Object avgMessageSizeObj = metrics.get("avgMessageSize");
        Object dailyMessagesObj = metrics.get("dailyMessages");
        
        if (avgMessageSizeObj != null && dailyMessagesObj != null) {
            int avgMessageSize = (Integer) avgMessageSizeObj;
            long dailyMessages = (Long) dailyMessagesObj;
            
            int calculatedPartitions = calculatePartitionCountByBusinessRequirements(
                (int) dailyMessages, 2.0, 7, 10);
            
            if (Math.abs(calculatedPartitions - currentPartitions) > currentPartitions * 0.3) {
                recommendations.add("建议调整分区数到" + calculatedPartitions + "以更好地匹配业务需求");
            }
        }
        
        assessment.setRecommendations(recommendations);
        assessment.setHealthy(recommendations.isEmpty());
        
        return assessment;
    }
    
    public static class PartitionHealthAssessment {
        private int currentPartitions;
        private List<String> recommendations;
        private boolean healthy;
        
        // Getters and setters
        public int getCurrentPartitions() { return currentPartitions; }
        public void setCurrentPartitions(int currentPartitions) { this.currentPartitions = currentPartitions; }
        public List<String> getRecommendations() { return recommendations; }
        public void setRecommendations(List<String> recommendations) { this.recommendations = recommendations; }
        public boolean isHealthy() { return healthy; }
        public void setHealthy(boolean healthy) { this.healthy = healthy; }
    }
}
```

#### 2. 命名约定
```java
/**
 * 主题命名约定验证器
 */
public class TopicNamingValidator {
    
    private static final Pattern TOPIC_PATTERN = Pattern.compile("^[a-zA-Z0-9._-]{1,249}$");
    private static final Set<String> RESERVED_TOPICS = Set.of(
        "__consumer_offsets", "__transaction_state"
    );
    
    /**
     * 验证主题名称是否合法
     */
    public static TopicNameValidationResult validateTopicName(String topicName) {
        TopicNameValidationResult result = new TopicNameValidationResult();
        result.setTopicName(topicName);
        
        List<String> issues = new ArrayList<>();
        
        if (topicName == null || topicName.trim().isEmpty()) {
            issues.add("主题名称不能为空");
        } else {
            // 基本格式验证
            if (!TOPIC_PATTERN.matcher(topicName).matches()) {
                issues.add("主题名称包含非法字符，只允许字母、数字、点号、下划线和连字符");
            }
            
            // 长度验证
            if (topicName.length() > 249) {
                issues.add("主题名称过长，最大长度为249个字符");
            }
            
            if (topicName.length() < 1) {
                issues.add("主题名称过短，最小长度为1个字符");
            }
            
           字符验证
            if ( // 首尾topicName.startsWith(".") || topicName.startsWith("-")) {
                issues.add("主题名称不能以点号或连字符开头");
            }
            
            if (topicName.endsWith(".") || topicName.endsWith("-")) {
                issues.add("主题名称不能以点号或连字符结尾");
            }
            
            // 保留主题检查
            if (RESERVED_TOPICS.contains(topicName)) {
                issues.add("该主题名称是系统保留名称");
            }
            
            // 特定模式检查
            if (topicName.contains("..") || topicName.contains("__")) {
                issues.add("主题名称不应包含连续的点号或下划线");
            }
        }
        
        result.setValid(issues.isEmpty());
        result.setIssues(issues);
        
        return result;
    }
    
    /**
     * 生成主题名称建议
     */
    public static String generateTopicName(String category, String entity, String operation) {
        StringBuilder name = new StringBuilder();
        
        // 添加类别前缀
        if (category != null && !category.trim().isEmpty()) {
            name.append(category.toLowerCase().replace("_", "-")).append("_");
        }
        
        // 添加实体名称
        if (entity != null && !entity.trim().isEmpty()) {
            name.append(entity.toLowerCase().replace("_", "-")).append("_");
        }
        
        // 添加操作类型
        if (operation != null && !operation.trim().isEmpty()) {
            name.append(operation.toLowerCase().replace("_", "-"));
        }
        
        // 清理并返回
        String result = name.toString().replaceAll("_{2,}", "_")
                          .replaceAll("__", "_")
                          .replaceAll("-{2,}", "-")
                          .replaceAll("--", "-");
        
        // 移除尾部的分隔符
        while (result.endsWith("_") || result.endsWith("-")) {
            result = result.substring(0, result.length() - 1);
        }
        
        return result;
    }
    
    /**
     * 验证并生成多个主题名称
     */
    public static List<String> generateTopicNames(TopicNamingScheme scheme) {
        List<String> topicNames = new ArrayList<>();
        
        // 基于业务域生成主题
        for (String domain : scheme.getDomains()) {
            for (String entity : scheme.getEntities()) {
                for (String operation : scheme.getOperations()) {
                    String topicName = generateTopicName(domain, entity, operation);
                    
                    TopicNameValidationResult validation = validateTopicName(topicName);
                    if (validation.isValid()) {
                        topicNames.add(topicName);
                    } else {
                        System.err.println("生成的主题名称不合法: " + topicName);
                        validation.getIssues().forEach(System.err::println);
                    }
                }
            }
        }
        
        return topicNames;
    }
    
    public static class TopicNameValidationResult {
        private String topicName;
        private boolean valid;
        private List<String> issues;
        
        public TopicNameValidationResult() {
            this.issues = new ArrayList<>();
        }
        
        // Getters and setters
        public String getTopicName() { return topicName; }
        public void setTopicName(String topicName) { this.topicName = topicName; }
        public boolean isValid() { return valid; }
        public void setValid(boolean valid) { this.valid = valid; }
        public List<String> getIssues() { return issues; }
        public void setIssues(List<String> issues) { this.issues = issues; }
    }
    
    public static class TopicNamingScheme {
        private List<String> domains;
        private List<String> entities;
        private List<String> operations;
        
        // Getters and setters
        public List<String> getDomains() { return domains; }
        public void setDomains(List<String> domains) { this.domains = domains; }
        public List<String> getEntities() { return entities; }
        public void setEntities(List<String> entities) { this.entities = entities; }
        public List<String> getOperations() { return operations; }
        public void setOperations(List<String> operations) { this.operations = operations; }
    }
}
```

#### 3. 保留策略优化
```java
/**
 * 主题保留策略优化器
 */
public class RetentionPolicyOptimizer {
    
    /**
     * 根据数据特征推荐保留策略
     */
    public static RetentionRecommendation recommendRetentionPolicy(TopicDataProfile profile) {
        RetentionRecommendation recommendation = new RetentionRecommendation();
        
        // 分析数据特征
        DataAnalysis analysis = analyzeDataCharacteristics(profile);
        
        // 推荐保留时间
        long retentionMs;
        String cleanupPolicy;
        
        if (analysis.isTimeSensitive()) {
            // 时间敏感数据
            retentionMs = calculateTimeSensitiveRetention(profile);
            cleanupPolicy = "delete";
        } else if (analysis.isSizeSensitive()) {
            // 大小敏感数据
            retentionMs = calculateSizeSensitiveRetention(profile);
            cleanupPolicy = "delete";
        } else {
            // 配置化数据，使用compact
            retentionMs = -1; // 永久保留
            cleanupPolicy = "compact";
        }
        
        // 推荐其他配置
        Map<String, String> configs = new HashMap<>();
        configs.put("retention.ms", String.valueOf(retentionMs));
        configs.put("cleanup.policy", cleanupPolicy);
        
        // 压缩策略
        if (analysis.recommendCompression()) {
            configs.put("compression.type", analysis.getRecommendedCompression());
        }
        
        // 段大小优化
        if (analysis.recommendSegmentSizing()) {
            configs.put("log.segment.bytes", String.valueOf(analysis.getRecommendedSegmentBytes()));
        }
        
        recommendation.setRecommendedConfigs(configs);
        recommendation.setReasoning(analysis.getReasoning());
        
        return recommendation;
    }
    
    private static DataAnalysis analyzeDataCharacteristics(TopicDataProfile profile) {
        DataAnalysis analysis = new DataAnalysis();
        List<String> reasoning = new ArrayList<>();
        
        // 检查是否为时间敏感数据
        if (profile.getAverageMessageAge() != null) {
            long avgAgeHours = profile.getAverageMessageAge();
            if (avgAgeHours < 24) {
                analysis.setTimeSensitive(true);
                reasoning.add("数据平均年龄小于24小时，适合短期保留策略");
            }
        }
        
        // 检查消息大小
        if (profile.getAverageMessageSize() != null) {
            long avgSize = profile.getAverageMessageSize();
            if (avgSize > 10240) { // 10KB
                analysis.setSizeSensitive(true);
                analysis.setRecommendCompression(true);
                analysis.setRecommendedCompression("gzip");
                reasoning.add("消息平均大小超过10KB，建议启用压缩");
            }
        }
        
        // 检查数据更新模式
        if ("upsert".equals(profile.getDataUpdatePattern()) || "compact".equals(profile.getDataUpdatePattern())) {
            analysis.setCompactionWorthy(true);
            reasoning.add("数据采用upsert模式，适合使用compaction策略");
        }
        
        // 计算推荐分段大小
        if (profile.getDailyMessageVolume() != null) {
            long dailyMessages = profile.getDailyMessageVolume();
            long recommendedSegmentBytes = calculateOptimalSegmentSize(dailyMessages);
            analysis.setRecommendedSegmentBytes(recommendedSegmentBytes);
            analysis.setRecommendSegmentSizing(true);
            reasoning.add("基于每日消息量(" + dailyMessages + ")推荐分段大小: " + recommendedSegmentBytes + " bytes");
        }
        
        analysis.setReasoning(reasoning);
        return analysis;
    }
    
    private static long calculateTimeSensitiveRetention(TopicDataProfile profile) {
        // 基于数据新鲜度需求
        if (profile.getBusinessCriticality() != null) {
            switch (profile.getBusinessCriticality()) {
                case "HIGH":
                    return 24 * 3600 * 1000L; // 1天
                case "MEDIUM":
                    return 7 * 24 * 3600 * 1000L; // 7天
                case "LOW":
                    return 30 * 24 * 3600 * 1000L; // 30天
                default:
                    return 7 * 24 * 3600 * 1000L; // 默认7天
            }
        }
        
        return 7 * 24 * 3600 * 1000L; // 默认7天
    }
    
    private static long calculateSizeSensitiveRetention(TopicDataProfile profile) {
        if (profile.getMaxStorageBudgetGB() != null) {
            long maxStorageGB = profile.getMaxStorageBudgetGB();
            
            // 估算所需保留时间
            if (profile.getDailyStorageGrowthMB() != null) {
                long dailyGrowthMB = profile.getDailyStorageGrowthMB();
                long retentionDays = (maxStorageGB * 1024) / dailyGrowthMB;
                
                return retentionDays * 24 * 3600 * 1000L;
            }
        }
        
        return 7 * 24 * 3600 * 1000L; // 默认7天
    }
    
    private static long calculateOptimalSegmentSize(long dailyMessages) {
        // 根据消息量计算最佳分段大小
        if (dailyMessages < 10000) {
            return 256 * 1024 * 1024L; // 256MB
        } else if (dailyMessages < 100000) {
            return 512 * 1024 * 1024L; // 512MB
        } else {
            return 1024 * 1024 * 1024L; // 1GB
        }
    }
    
    // 数据类定义
    public static class TopicDataProfile {
        private String topicName;
        private Long averageMessageAge; // 小时
        private Long averageMessageSize; // 字节
        private Long dailyMessageVolume;
        private Long dailyStorageGrowthMB;
        private Long maxStorageBudgetGB;
        private String dataUpdatePattern; // insert, upsert, append
        private String businessCriticality; // HIGH, MEDIUM, LOW
        
        // Getters and setters
        public String getTopicName() { return topicName; }
        public void setTopicName(String topicName) { this.topicName = topicName; }
        public Long getAverageMessageAge() { return averageMessageAge; }
        public void setAverageMessageAge(Long averageMessageAge) { this.averageMessageAge = averageMessageAge; }
        public Long getAverageMessageSize() { return averageMessageSize; }
        public void setAverageMessageSize(Long averageMessageSize) { this.averageMessageSize = averageMessageSize; }
        public Long getDailyMessageVolume() { return dailyMessageVolume; }
        public void setDailyMessageVolume(Long dailyMessageVolume) { this.dailyMessageVolume = dailyMessageVolume; }
        public Long getDailyStorageGrowthMB() { return dailyStorageGrowthMB; }
        public void setDailyStorageGrowthMB(Long dailyStorageGrowthMB) { this.dailyStorageGrowthMB = dailyStorageGrowthMB; }
        public Long getMaxStorageBudgetGB() { return maxStorageBudgetGB; }
        public void setMaxStorageBudgetGB(Long maxStorageBudgetGB) { this.maxStorageBudgetGB = maxStorageBudgetGB; }
        public String getDataUpdatePattern() { return dataUpdatePattern; }
        public void setDataUpdatePattern(String dataUpdatePattern) { this.dataUpdatePattern = dataUpdatePattern; }
        public String getBusinessCriticality() { return businessCriticality; }
        public void setBusinessCriticality(String businessCriticality) { this.businessCriticality = businessCriticality; }
    }
    
    public static class DataAnalysis {
        private boolean timeSensitive;
        private boolean sizeSensitive;
        private boolean compactionWorthy;
        private boolean recommendCompression;
        private boolean recommendSegmentSizing;
        private String recommendedCompression;
        private long recommendedSegmentBytes;
        private List<String> reasoning;
        
        // Getters and setters
        public boolean isTimeSensitive() { return timeSensitive; }
        public void setTimeSensitive(boolean timeSensitive) { this.timeSensitive = timeSensitive; }
        public boolean isSizeSensitive() { return sizeSensitive; }
        public void setSizeSensitive(boolean sizeSensitive) { this.sizeSensitive = sizeSensitive; }
        public boolean isCompactionWorthy() { return compactionWorthy; }
        public void setCompactionWorthy(boolean compactionWorthy) { this.compactionWorthy = compactionWorthy; }
        public boolean recommendCompression() { return recommendCompression; }
        public void setRecommendCompression(boolean recommendCompression) { this.recommendCompression = recommendCompression; }
        public boolean recommendSegmentSizing() { return recommendSegmentSizing; }
        public void setRecommendSegmentSizing(boolean recommendSegmentSizing) { this.recommendSegmentSizing = recommendSegmentSizing; }
        public String getRecommendedCompression() { return recommendedCompression; }
        public void setRecommendedCompression(String recommendedCompression) { this.recommendedCompression = recommendedCompression; }
        public long getRecommendedSegmentBytes() { return recommendedSegmentBytes; }
        public void setRecommendedSegmentBytes(long recommendedSegmentBytes) { this.recommendedSegmentBytes = recommendedSegmentBytes; }
        public List<String> getReasoning() { return reasoning; }
        public void setReasoning(List<String> reasoning) { this.reasoning = reasoning; }
    }
    
    public static class RetentionRecommendation {
        private Map<String, String> recommendedConfigs;
        private List<String> reasoning;
        
        // Getters and setters
        public Map<String, String> getRecommendedConfigs() { return recommendedConfigs; }
        public void setRecommendedConfigs(Map<String, String> recommendedConfigs) { this.recommendedConfigs = recommendedConfigs; }
        public List<String> getReasoning() { return reasoning; }
        public void setReasoning(List<String> reasoning) { this.reasoning = reasoning; }
    }
}
```

### 配置优化策略

#### 生产环境配置模板
```java
/**
 * 生产环境主题配置模板
 */
public class ProductionTopicTemplates {
    
    /**
     * 高吞吐量日志主题模板
     */
    public static Map<String, String> highThroughputLogTemplate() {
        Map<String, String> configs = new HashMap<>();
        configs.put("cleanup.policy", "delete");
        configs.put("retention.ms", String.valueOf(7 * 24 * 3600 * 1000L)); // 7天
        configs.put("compression.type", "gzip");
        configs.put("log.segment.bytes", String.valueOf(1024 * 1024 * 1024L)); // 1GB
        configs.put("log.retention.check.interval.ms", String.valueOf(300000)); // 5分钟
        configs.put("min.insync.replicas", "2");
        return configs;
    }
    
    /**
     * 关键事务数据主题模板
     */
    public static Map<String, String> criticalTransactionTemplate() {
        Map<String, String> configs = new HashMap<>();
        configs.put("cleanup.policy", "compact");
        configs.put("retention.ms", "-1"); // 永久保留
        configs.put("compression.type", "lz4");
        configs.put("log.segment.bytes", String.valueOf(512 * 1024 * 1024L)); // 512MB
        configs.put("min.insync.replicas", "3");
        configs.put("unclean.leader.election.enable", "false");
        configs.put("default.replication.factor", "3");
        return configs;
    }
    
    /**
     * 实时分析主题模板
     */
    public static Map<String, String> realTimeAnalyticsTemplate() {
        Map<String, String> configs = new HashMap<>();
        configs.put("cleanup.policy", "delete");
        configs.put("retention.ms", String.valueOf(24 * 3600 * 1000L)); // 1天
        configs.put("compression.type", "snappy");
        configs.put("log.segment.bytes", String.valueOf(536870912L)); // 512MB
        configs.put("log.roll.hours", "24");
        configs.put("min.insync.replicas", "2");
        return configs;
    }
    
    /**
     * 监控指标主题模板
     */
    public static Map<String, String> metricsTemplate() {
        Map<String, String> configs = new HashMap<>();
        configs.put("cleanup.policy", "delete");
        configs.put("retention.ms", String.valueOf(30 * 24 * 3600 * 1000L)); // 30天
        configs.put("compression.type", "snappy");
        configs.put("log.segment.bytes", String.valueOf(268435456L)); // 256MB
        configs.put("min.insync.replicas", "2");
        return configs;
    }
}
```

---

## 常见问题与解决方案

### 1. 分区热点问题

#### 问题识别
```java
public class HotPartitionDetector {
    
    /**
     * 检测分区热点
     */
    public static Map<Integer, HotPartitionAlert> detectHotPartitions(String topicName,
                                                                     Map<Integer, TopicPartitionStats> partitionStats,
                                                                     double thresholdRatio) {
        Map<Integer, HotPartitionAlert> hotPartitions = new HashMap<>();
        
        // 计算平均负载
        double totalLoad = partitionStats.values().stream()
            .mapToDouble(TopicPartitionStats::getMessageCount)
            .sum();
        double avgLoad = totalLoad / partitionStats.size();
        
        // 识别热点分区
        partitionStats.forEach((partitionId, stats) -> {
            double loadRatio = stats.getMessageCount() / avgLoad;
            
            if (loadRatio > thresholdRatio) {
                HotPartitionAlert alert = new HotPartitionAlert();
                alert.setPartitionId(partitionId);
                alert.setLoadRatio(loadRatio);
                alert.setMessageCount(stats.getMessageCount());
                alert.setAvgMessageCount((long) avgLoad);
                alert.setIssue("分区负载过高，负载比率: " + String.format("%.2f", loadRatio));
                
                hotPartitions.put(partitionId, alert);
            }
        });
        
        return hotPartitions;
    }
    
    /**
     * 解决方案建议
     */
    public static List<SolutionRecommendation> generateHotPartitionSolutions(String topicName,
                                                                            Map<Integer, HotPartitionAlert> hotPartitions) {
        List<SolutionRecommendation> solutions = new ArrayList<>();
        
        // 方案1: 增加分区数
        if (hotPartitions.size() < partitionStatsSize(topicName) * 0.3) {
            SolutionRecommendation rec1 = new SolutionRecommendation();
            rec1.setSolution("增加分区数");
            rec1.setDescription("将主题分区数增加20-30%，以分散热点分区的负载");
            rec1.setImpact("可以立即缓解热点问题，但需要重新分配分区");
            rec1.setRiskLevel("MEDIUM");
            solutions.add(rec1);
        }
        
        // 方案2: 优化分区键
        SolutionRecommendation rec2 = new SolutionRecommendation();
        rec2.setSolution("优化分区键");
        rec2.setDescription("检查和优化消息的分区键，确保分布均匀");
        rec2.setImpact("根本性解决热点问题，需要修改生产者和分区策略");
        rec2.setRiskLevel("HIGH");
        solutions.add(rec2);
        
        // 方案3: 预分区
        SolutionRecommendation rec3 = new SolutionRecommendation();
        rec3.setSolution("预分区");
        rec3.setDescription("在发送大量消息前预先创建更多分区，避免动态扩容");
        rec3.setImpact("预防性措施，避免生产时的性能问题");
        rec3.setRiskLevel("LOW");
        solutions.add(rec3);
        
        return solutions;
    }
    
    private static int partitionStatsSize(String topicName) {
        // 获取主题分区数
        // 这里应该调用AdminClient来获取实际分区数
        return 6; // 示例值
    }
    
    public static class TopicPartitionStats {
        private long messageCount;
        private long bytesProcessed;
        private long processingTimeMs;
        
        // Getters and setters
        public long getMessageCount() { return messageCount; }
        public void setMessageCount(long messageCount) { this.messageCount = messageCount; }
        public long getBytesProcessed() { return bytesProcessed; }
        public void setBytesProcessed(long bytesProcessed) { this.bytesProcessed = bytesProcessed; }
        public long getProcessingTimeMs() { return processingTimeMs; }
        public void setProcessingTimeMs(long processingTimeMs) { this.processingTimeMs = processingTimeMs; }
    }
    
    public static class HotPartitionAlert {
        private int partitionId;
        private double loadRatio;
        private long messageCount;
        private long avgMessageCount;
        private String issue;
        
        // Getters and setters
        public int getPartitionId() { return partitionId; }
        public void setPartitionId(int partitionId) { this.partitionId = partitionId; }
        public double getLoadRatio() { return loadRatio; }
        public void setLoadRatio(double loadRatio) { this.loadRatio = loadRatio; }
        public long getMessageCount() { return messageCount; }
        public void setMessageCount(long messageCount) { this.messageCount = messageCount; }
        public long getAvgMessageCount() { return avgMessageCount; }
        public void setAvgMessageCount(long avgMessageCount) { this.avgMessageCount = avgMessageCount; }
        public String getIssue() { return issue; }
        public void setIssue(String issue) { this.issue = issue; }
    }
    
    public static class SolutionRecommendation {
        private String solution;
        private String description;
        private String impact;
        private String riskLevel;
        
        // Getters and setters
        public String getSolution() { return solution; }
        public void setSolution(String solution) { this.solution = solution; }
        public String getDescription() { return description; }
        public void setDescription(String description) { this.description = description; }
        public String get