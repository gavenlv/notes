# 第11章：流处理应用实战

## 目录
1. [Kafka Streams概述](#kafka-streams概述)
2. [流处理核心概念](#流处理核心概念)
3. [Streams DSL使用](#streams-dsl使用)
4. [Processor API使用](#processor-api使用)
5. [时间窗口与聚合](#时间窗口与聚合)
6. [状态存储与查询](#状态存储与查询)
7. [流处理最佳实践](#流处理最佳实践)
8. [复杂事件处理](#复杂事件处理)
9. [流处理实战案例](#流处理实战案例)

## Kafka Streams概述

Kafka Streams是一个用于构建实时流处理应用程序的客户端库，它允许应用程序在Kafka集群之上执行复杂的流处理操作。

### 流处理架构模式

```java
/**
 * Kafka Streams架构管理器
 */
public class KafkaStreamsArchitecture {
    
    /**
     * 流处理应用类型
     */
    public enum ProcessingType {
        STREAM("流处理"),
        TABLE("表处理"),
        GLOBAL_KTABLE("全局表"),
        KTABLE_KTABLE_JOIN("表间Join"),
        STREAM_STREAM_JOIN("流间Join"),
        AGGREGATION("聚合处理"),
        WINDOWING("窗口处理"),
        BRANCHING("分支处理");
        
        private final String description;
        
        ProcessingType(String description) {
            this.description = description;
        }
        
        public String getDescription() { return description; }
    }
    
    /**
     * 流处理拓扑构建器
     */
    public static class StreamTopologyBuilder {
        
        private final StreamsBuilder builder = new StreamsBuilder();
        private final Map<String, String> topicMappings = new HashMap<>();
        private final Map<String, ProcessingType> nodeTypes = new HashMap<>();
        
        public StreamTopologyBuilder addSource(String topic, String processorName) {
            KStream<String, String> stream = builder.stream(topic);
            nodeTypes.put(processorName, ProcessingType.STREAM);
            topicMappings.put(processorName, topic);
            return this;
        }
        
        public StreamTopologyBuilder addTable(String topic, String tableName) {
            KTable<String, String> table = builder.table(topic);
            nodeTypes.put(tableName, ProcessingType.TABLE);
            topicMappings.put(tableName, topic);
            return this;
        }
        
        public StreamTopologyBuilder addGlobalTable(String topic, String tableName) {
            GlobalKTable<String, String> globalTable = builder.globalTable(topic);
            nodeTypes.put(tableName, ProcessingType.GLOBAL_KTABLE);
            topicMappings.put(tableName, topic);
            return this;
        }
        
        public StreamTopologyBuilder addProcessor(String nodeName, 
                                                  Function<KStream<String, String>, KStream<String, String>> processor,
                                                  String inputNode) {
            ProcessingType inputType = nodeTypes.get(inputNode);
            if (inputType == ProcessingType.TABLE) {
                throw new IllegalArgumentException("不能对KTable应用流处理器");
            }
            
            // 这里会存储处理逻辑，实际情况需要使用具体的实现
            System.out.printf("添加处理器 %s (类型: %s)%n", nodeName, ProcessingType.STREAM);
            return this;
        }
        
        public StreamsBuilder build() {
            System.out.println("=== 流处理拓扑构建完成 ===");
            nodeTypes.forEach((name, type) -> 
                System.out.printf("节点 %s: %s (输入: %s)%n", name, type.getDescription(), topicMappings.get(name)));
            return builder;
        }
        
        /**
         * 验证拓扑结构
         */
        public boolean validateTopology() {
            Set<String> nodes = nodeTypes.keySet();
            
            for (String node : nodes) {
                ProcessingType type = nodeTypes.get(node);
                
                switch (type) {
                    case TABLE:
                        if (hasIncomingEdges(node)) {
                            System.out.println("警告: KTable节点 " + node + " 不应有输入边");
                            return false;
                        }
                        break;
                        
                    case STREAM:
                        if (!hasIncomingEdges(node)) {
                            System.out.println("警告: KStream节点 " + node + " 应有输入源");
                            return false;
                        }
                        break;
                        
                    case AGGREGATION:
                        if (!hasStreamInput(node)) {
                            System.out.println("警告: 聚合节点 " + node + " 只能处理KStream输入");
                            return false;
                        }
                        break;
                }
            }
            
            return true;
        }
        
        private boolean hasIncomingEdges(String node) {
            // 检查是否有输入边的逻辑（简化实现）
            return true;
        }
        
        private boolean hasStreamInput(String node) {
            // 检查是否有KStream输入的逻辑（简化实现）
            return true;
        }
    }
    
    /**
     * 流处理配置管理器
     */
    public static class StreamConfigManager {
        
        public static class StreamApplicationConfig {
            private final String applicationId;
            private final String bootstrapServers;
            private final Map<String, Object> customConfigs;
            private final ProcessingGuarantee processingGuarantee;
            private final int numStreamThreads;
            private final int numStandbyReplicas;
            private final boolean enableExactlyOnce;
            private final String stateStoreDirectory;
            
            public StreamApplicationConfig(String applicationId, String bootstrapServers) {
                this.applicationId = applicationId;
                this.bootstrapServers = bootstrapServers;
                this.customConfigs = new HashMap<>();
                this.processingGuarantee = ProcessingGuarantee.AT_LEAST_ONCE;
                this.numStreamThreads = 1;
                this.numStandbyReplicas = 0;
                this.enableExactlyOnce = false;
                this.stateStoreDirectory = null;
            }
            
            public StreamApplicationConfig enableExactlyOnce() {
                this.enableExactlyOnce = true;
                this.processingGuarantee = ProcessingGuarantee.EXACTLY_ONCE_V2;
                return this;
            }
            
            public StreamApplicationConfig setNumStreamThreads(int threads) {
                this.numStreamThreads = threads;
                return this;
            }
            
            public StreamApplicationConfig setNumStandbyReplicas(int replicas) {
                this.numStandbyReplicas = replicas;
                return this;
            }
            
            public StreamApplicationConfig setStateStoreDirectory(String directory) {
                this.stateStoreDirectory = directory;
                return this;
            }
            
            public StreamApplicationConfig addConfig(String key, Object value) {
                this.customConfigs.put(key, value);
                return this;
            }
            
            public Properties buildProperties() {
                Properties props = new Properties();
                
                // 基础配置
                props.put(StreamsConfig.APPLICATION_ID_CONFIG, applicationId);
                props.put(StreamsConfig.BOOTSTRAP_SERVERS_CONFIG, bootstrapServers);
                
                // 可靠性配置
                props.put(StreamsConfig.PROCESSING_GUARANTEE_CONFIG, processingGuarantee.name());
                props.put(StreamsConfig.NUM_STREAM_THREADS_CONFIG, numStreamThreads);
                props.put(StreamsConfig.NUM_STANDBY_REPLICAS_CONFIG, numStandbyReplicas);
                
                // 状态存储配置
                if (stateStoreDirectory != null) {
                    props.put(StreamsConfig.STATE_DIR_CONFIG, stateStoreDirectory);
                }
                
                // 自定义配置
                props.putAll(customConfigs);
                
                // 优化配置
                props.put(StreamsConfig.CACHE_MAX_BYTES_BUFFERING_CONFIG, 10 * 1024 * 1024L); // 10MB
                props.put(StreamsConfig.COMMIT_INTERVAL_MS_CONFIG, 100);
                props.put(StreamsConfig.REPLICATION_FACTOR_CONFIG, 3);
                props.put(ProducerConfig.ACKS_CONFIG, "all");
                props.put(ConsumerConfig.AUTO_OFFSET_RESET_CONFIG, "earliest");
                
                return props;
            }
            
            public void printConfiguration() {
                System.out.println("=== Kafka Streams配置 ===");
                System.out.println("应用ID: " + applicationId);
                System.out.println("Bootstrap服务器: " + bootstrapServers);
                System.out.println("处理保证: " + processingGuarantee);
                System.out.println("流线程数: " + numStreamThreads);
                System.out.println("备用副本数: " + numStandbyReplicas);
                System.out.println("Exactly-once: " + enableExactlyOnce);
                System.out.println("状态目录: " + stateStoreDirectory);
            }
        }
        
        public enum ProcessingGuarantee {
            AT_LEAST_ONCE("至少一次"),
            EXACTLY_ONCE("恰好一次"),
            EXACTLY_ONCE_V2("恰好一次V2");
            
            private final String description;
            
            ProcessingGuarantee(String description) {
                this.description = description;
            }
            
            public String getDescription() { return description; }
        }
    }
    
    /**
     * 流处理监控器
     */
    public static class StreamProcessorMonitor {
        
        private final KafkaStreams streams;
        private final ScheduledExecutorService scheduler = Executors.newScheduledThreadPool(1);
        private final Map<String, MetricInfo> metrics = new ConcurrentHashMap<>();
        
        public StreamProcessorMonitor(KafkaStreams streams) {
            this.streams = streams;
            startMonitoring();
        }
        
        private void startMonitoring() {
            scheduler.scheduleAtFixedRate(this::collectMetrics, 10, 30, TimeUnit.SECONDS);
        }
        
        private void collectMetrics() {
            try {
                // 收集Streams应用指标
                Metrics streamsMetrics = streams.metrics();
                
                // 记录关键指标
                recordMetric("active-tasks", getMetricValue(streamsMetrics, "active-tasks"));
                recordMetric("standby-tasks", getMetricValue(streamsMetrics, "standby-tasks"));
                recordMetric("poll-latency-avg", getMetricValue(streamsMetrics, "poll-latency-avg"));
                recordMetric("commit-latency-avg", getMetricValue(streamsMetrics, "commit-latency-avg"));
                
                // 检查应用状态
                State currentState = streams.state();
                if (currentState == State.ERROR || currentState == State.DEAD) {
                    System.err.println("Streams应用处于错误状态: " + currentState);
                }
                
            } catch (Exception e) {
                System.err.println("收集指标失败: " + e.getMessage());
            }
        }
        
        private double getMetricValue(Metrics metrics, String metricName) {
            // 简化的指标获取逻辑
            return Math.random() * 100; // 模拟指标值
        }
        
        private void recordMetric(String name, double value) {
            metrics.put(name, new MetricInfo(name, value, System.currentTimeMillis()));
        }
        
        /**
         * 打印应用状态报告
         */
        public void printStatusReport() {
            System.out.println("=== Kafka Streams状态报告 ===");
            System.out.println("应用状态: " + streams.state());
            System.out.println("实时指标:");
            metrics.forEach((name, info) -> 
                System.out.printf("  %s: %.2f%n", name, info.getValue()));
        }
        
        private static class MetricInfo {
            private final String name;
            private final double value;
            private final long timestamp;
            
            public MetricInfo(String name, double value, long timestamp) {
                this.name = name;
                this.value = value;
                this.timestamp = timestamp;
            }
            
            public double getValue() { return value; }
        }
    }
}
```

## 流处理核心概念

### 流与表的抽象

```java
/**
 * Kafka Streams核心概念演示
 */
public class StreamsCoreConcepts {
    
    /**
     * 流（Stream）概念演示
     */
    public static class StreamConceptDemo {
        
        /**
         * 创建消息流
         */
        public static void demonstrateStreamCreation() {
            StreamsBuilder builder = new StreamsBuilder();
            
            // 从主题创建流
            KStream<String, String> sourceStream = builder.stream("input-topic");
            
            System.out.println("=== 流创建演示 ===");
            System.out.println("• 流是无界的、有序的、容错的记录序列");
            System.out.println("• 每条记录(key, value, timestamp)");
            System.out.println("• 支持转换操作: map, filter, join等");
            
            // 流转换示例
            KStream<String, String> processedStream = sourceStream
                .filter((key, value) -> value != null && !value.isEmpty())
                .map((key, value) -> {
                    String processed = value.toUpperCase();
                    return KeyValue.pair(key, processed);
                })
                .selectKey((key, value) -> key.toLowerCase());
            
            System.out.println("转换后的流已创建");
        }
        
        /**
         * 流操作演示
         */
        public static void demonstrateStreamOperations() {
            StreamsBuilder builder = new StreamsBuilder();
            KStream<String, String> stream = builder.stream("orders");
            
            System.out.println("=== 流操作演示 ===");
            
            // 过滤操作
            KStream<String, String> highValueOrders = stream
                .filter((key, order) -> {
                    try {
                        double amount = Double.parseDouble(order);
                        return amount > 1000;
                    } catch (NumberFormatException e) {
                        return false;
                    }
                });
            
            // 映射操作
            KStream<String, String> upperCaseStream = stream
                .mapValues(value -> value.toUpperCase());
            
            // 分支操作
            KStream<String, String>[] branches = stream.branch(
                (key, value) -> value.contains("VIP"),
                (key, value) -> value.contains("NORMAL")
            );
            
            KStream<String, String> vipOrders = branches[0];
            KStream<String, String> normalOrders = branches[1];
            
            System.out.println("流操作完成: filter, mapValues, branch");
        }
    }
    
    /**
     * 表（Table）概念演示
     */
    public static class TableConceptDemo {
        
        /**
         * 创建KTable
         */
        public static void demonstrateTableCreation() {
            StreamsBuilder builder = new StreamsBuilder();
            
            // 从主题创建表
            KTable<String, String> sourceTable = builder.table("user-profiles");
            
            // 从流创建表
            KStream<String, String> userStream = builder.stream("user-events");
            KTable<String, String> aggregatedTable = userStream
                .groupByKey()
                .aggregate(
                    () -> "", // 初始化器
                    (key, value, aggregate) -> aggregate + value, // 聚合函数
                    Materialized.as("user-aggregates-store")
                );
            
            System.out.println("=== 表创建演示 ===");
            System.out.println("• 表是流的状态化快照");
            System.out.println("• 每条记录代表最新状态");
            System.out.println("• 支持upsert操作");
        }
        
        /**
         * 表操作演示
         */
        public static void demonstrateTableOperations() {
            StreamsBuilder builder = new StreamsBuilder();
            KTable<String, String> usersTable = builder.table("users");
            
            System.out.println("=== 表操作演示 ===");
            
            // 过滤表
            KTable<String, String> activeUsers = usersTable
                .filter((key, value) -> value.contains("ACTIVE"));
            
            // 映射表值
            KTable<String, String> normalizedUsers = usersTable
                .mapValues(value -> value.toLowerCase());
            
            // 分组聚合
            KTable<String, Long> userCounts = usersTable
                .groupBy((key, value) -> KeyValue.pair(value.split(",")[0], key))
                .count(Materialized.as("user-counts-store"));
            
            System.out.println("表操作完成: filter, mapValues, groupBy, count");
        }
    }
    
    /**
     * 时间概念演示
     */
    public static class TimeConceptDemo {
        
        /**
         * 时间戳提取器
         */
        public static class CustomTimestampExtractor implements TimestampExtractor {
            
            @Override
            public long extract(ConsumerRecord<Object, Object> record, long previousTimestamp) {
                // 自定义时间戳提取逻辑
                String value = record.value().toString();
                try {
                    // 从消息中提取时间戳
                    String[] parts = value.split(",");
                    if (parts.length > 1) {
                        return Long.parseLong(parts[1]);
                    }
                } catch (NumberFormatException e) {
                    // 使用当前时间
                }
                return System.currentTimeMillis();
            }
        }
        
        /**
         * 事件时间处理
         */
        public static void demonstrateEventTimeProcessing() {
            StreamsBuilder builder = new StreamsBuilder();
            
            // 使用事件时间
            KStream<String, String> eventTimeStream = builder
                .stream("events", Consumed.with(Serdes.String(), Serdes.String(), 
                    new CustomTimestampExtractor(), Topology.AutoOffsetReset.EARLIEST));
            
            // 基于事件时间的窗口操作
            KTable<Windowed<String>, Long> windowedCounts = eventTimeStream
                .groupByKey()
                .windowedBy(TimeWindows.of(Duration.ofMinutes(5)))
                .count(Materialized.as("event-counts-store"));
            
            System.out.println("=== 事件时间处理 ===");
            System.out.println("• 处理时间: 记录被处理的时间");
            System.out.println("• 事件时间: 事件发生的时间");
            System.out.println("• 提取时间: 记录被提取到流中的时间");
        }
        
        /**
         * 乱序事件处理
         */
        public static void demonstrateOutOfOrderEventProcessing() {
            StreamsBuilder builder = new StreamsBuilder();
            KStream<String, String> events = builder.stream("events");
            
            // 配置时间窗口
            TimeWindows tumblingWindows = TimeWindows.of(Duration.ofMinutes(5))
                .advanceBy(Duration.ofMinutes(1));
            
            // 处理乱序事件
            KTable<Windowed<String>, Long> counts = events
                .groupByKey()
                .windowedBy(tumblingWindows)
                .count(Materialized.as("counts-store"));
            
            System.out.println("=== 乱序事件处理 ===");
            System.out.println("• 窗口延迟: 等待乱序数据的时间");
            System.out.println("• 允许迟到: 允许事件迟到的时间范围");
            System.out.println("• 侧输出: 处理迟到事件的可选输出");
        }
    }
    
    /**
     * 状态管理演示
     */
    public static class StateManagementDemo {
        
        /**
         * 键值状态存储
         */
        public static void demonstrateKeyValueStateStore() {
            // 键值状态存储配置
            StoreBuilder<KeyValueStore<String, Long>> storeBuilder = 
                Stores.keyValueStoreBuilder(
                    Stores.persistentKeyValueStore("word-counts"),
                    Serdes.String(),
                    Serdes.Long()
                );
            
            StreamsBuilder builder = new StreamsBuilder();
            
            // 使用状态存储
            KStream<String, String> textStream = builder.stream("text-input");
            KTable<String, Long> wordCounts = textStream
                .flatMapValues(value -> Arrays.asList(value.toLowerCase().split(" ")))
                .groupBy((key, word) -> word)
                .count(Materialized.as("word-counts"));
            
            System.out.println("=== 键值状态存储 ===");
            System.out.println("• 支持点查询和范围查询");
            System.out.println("• 持久化到本地和远程存储");
            System.out.println("• 支持事务性更新");
        }
        
        /**
         * 窗口状态存储
         */
        public static void demonstrateWindowStateStore() {
            StoreBuilder<WindowStore<String, Long>> windowStoreBuilder = 
                Stores.windowStoreBuilder(
                    Stores.persistentWindowStore("session-store", 
                        Duration.ofHours(1), Duration.ofMinutes(5), false),
                    Serdes.String(),
                    Serdes.Long()
                );
            
            StreamsBuilder builder = new StreamsBuilder();
            KStream<String, String> sessionStream = builder.stream("user-sessions");
            
            // 会话窗口聚合
            KTable<Windowed<String>, Long> sessionCounts = sessionStream
                .groupByKey()
                .windowedBy(SessionWindows.with(Duration.ofMinutes(30)))
                .count(Materialized.as("session-counts"));
            
            System.out.println("=== 窗口状态存储 ===");
            System.out.println("• 支持滚动、滑动、会话窗口");
            System.out.println("• 窗口状态在窗口结束后可查询");
            System.out.println("• 支持窗口过期清理");
        }
        
        /**
         * 状态存储交互
         */
        public static class StateStoreInteractor {
            
            public static void interactWithStateStore(KafkaStreams streams, String storeName) {
                try {
                    // 获取状态存储
                    ReadOnlyKeyValueStore<String, Long> store = 
                        streams.store(StoreQueryParameters.fromNameAndType(
                            storeName, QueryableStoreTypes.keyValueStore()));
                    
                    // 状态存储查询
                    System.out.println("=== 状态存储交互 ===");
                    
                    // 点查询
                    Long count = store.get("example-key");
                    System.out.println("键 'example-key' 的值: " + count);
                    
                    // 范围查询
                    try (KeyValueIterator<String, Long> iterator = store.range("key1", "key9")) {
                        System.out.println("范围查询结果:");
                        while (iterator.hasNext()) {
                            KeyValueIterator<String, Long> next = iterator;
                            System.out.printf("  %s: %d%n", next.key(), next.value());
                        }
                    }
                    
                } catch (InvalidStateStoreException e) {
                    System.out.println("状态存储暂时不可用: " + e.getMessage());
                }
            }
        }
    }
}
```

## Streams DSL使用

### 转换操作

```java
/**
 * Kafka Streams DSL转换操作
 */
public class StreamsDSLTransformations {
    
    /**
     * 基础转换操作
     */
    public static class BasicTransformations {
        
        /**
         * map操作
         */
        public static void demonstrateMapOperations() {
            StreamsBuilder builder = new StreamsBuilder();
            KStream<String, String> source = builder.stream("input");
            
            System.out.println("=== Map操作演示 ===");
            
            // KeyValue.map: 转换key和value
            KStream<String, Integer> mappedStream = source
                .map((key, value) -> {
                    try {
                        int number = Integer.parseInt(value);
                        return KeyValue.pair(key, number * 2);
                    } catch (NumberFormatException e) {
                        return KeyValue.pair(key, 0);
                    }
                });
            
            // mapValues: 只转换value
            KStream<String, String> mappedValueStream = source
                .mapValues(value -> value.toUpperCase());
            
            // flatMap: 一对多转换
            KStream<String, String> flatMappedStream = source
                .flatMap((key, value) -> {
                    List<KeyValue<String, String>> results = new ArrayList<>();
                    String[] words = value.split(" ");
                    for (String word : words) {
                        if (!word.isEmpty()) {
                            results.add(KeyValue.pair(key, word));
                        }
                    }
                    return results;
                });
            
            // flatMapValues: 一对多值转换
            KStream<String, String> flatMappedValueStream = source
                .flatMapValues(value -> Arrays.asList(value.toLowerCase().split(" ")));
            
            System.out.println("Map操作类型: map, mapValues, flatMap, flatMapValues");
        }
        
        /**
         * filter操作
         */
        public static void demonstrateFilterOperations() {
            StreamsBuilder builder = new StreamsBuilder();
            KStream<String, String> orders = builder.stream("orders");
            
            System.out.println("=== Filter操作演示 ===");
            
            // 基础过滤
            KStream<String, String> highValueOrders = orders
                .filter((key, value) -> {
                    try {
                        double amount = extractAmount(value);
                        return amount > 1000;
                    } catch (Exception e) {
                        return false;
                    }
                });
            
            // 否定过滤
            KStream<String, String> nonVipOrders = orders
                .filterNot((key, value) -> value.contains("VIP"));
            
            // 分支操作
            KStream<String, String>[] orderBranches = orders.branch(
                (key, value) -> value.contains("URGENT"),
                (key, value) -> value.contains("NORMAL"),
                (key, value) -> true // 默认分支
            );
            
            KStream<String, String> urgentOrders = orderBranches[0];
            KStream<String, String> normalOrders = orderBranches[1];
            KStream<String, String> defaultOrders = orderBranches[2];
            
            System.out.println("Filter操作类型: filter, filterNot, branch");
        }
        
        /**
         * 提取订单金额的辅助方法
         */
        private static double extractAmount(String order) {
            String[] parts = order.split(",");
            return Double.parseDouble(parts[parts.length - 1]);
        }
    }
    
    /**
     * 聚合操作
     */
    public static class AggregationOperations {
        
        /**
         * count操作
         */
        public static void demonstrateCountOperations() {
            StreamsBuilder builder = new StreamsBuilder();
            KStream<String, String> events = builder.stream("user-events");
            
            System.out.println("=== Count操作演示 ===");
            
            // 按key分组计数
            KTable<String, Long> userEventCounts = events
                .groupByKey()
                .count(Materialized.as("user-event-counts"));
            
            // 分组并计数（重映射key）
            KTable<String, Long> actionCounts = events
                .groupBy((key, value) -> value.split(",")[0]) // 提取操作类型作为key
                .count(Materialized.as("action-counts"));
            
            System.out.println("Count操作完成");
        }
        
        /**
         * sum操作
         */
        public static void demonstrateSumOperations() {
            StreamsBuilder builder = new StreamsBuilder();
            KStream<String, String> transactions = builder.stream("transactions");
            
            System.out.println("=== Sum操作演示 ===");
            
            // 按用户聚合交易金额
            KTable<String, Double> userTotals = transactions
                .mapValues(value -> {
                    String[] parts = value.split(",");
                    return Double.parseDouble(parts[parts.length - 1]);
                })
                .groupByKey()
                .sum(Materialized.as("user-totals"));
            
            System.out.println("Sum操作完成");
        }
        
        /**
         * aggregate操作
         */
        public static void demonstrateAggregateOperations() {
            StreamsBuilder builder = new StreamsBuilder();
            KStream<String, String> userEvents = builder.stream("user-sessions");
            
            System.out.println("=== Aggregate操作演示 ===");
            
            // 聚合用户会话信息
            KTable<String, UserSession> userSessions = userEvents
                .groupByKey()
                .aggregate(
                    () -> new UserSession("", 0, 0), // 初始化器
                    (key, event, session) -> {
                        String[] parts = event.split(",");
                        session.update(
                            parts[0], // 用户ID
                            Integer.parseInt(parts[1]), // 事件数量
                            Long.parseLong(parts[2]) // 会话时长
                        );
                        return session;
                    },
                    (key, left, right) -> {
                        // 合并函数
                        left.merge(right);
                        return left;
                    },
                    Materialized.as("user-sessions-store")
                        .withValueSerde(new UserSessionSerde())
                );
            
            System.out.println("Aggregate操作完成");
        }
        
        /**
         * 用户会话实体类
         */
        public static class UserSession {
            private String userId;
            private int eventCount;
            private long sessionDuration;
            
            public UserSession(String userId, int eventCount, long sessionDuration) {
                this.userId = userId;
                this.eventCount = eventCount;
                this.sessionDuration = sessionDuration;
            }
            
            public void update(String userId, int eventCount, long sessionDuration) {
                this.userId = userId;
                this.eventCount += eventCount;
                this.sessionDuration += sessionDuration;
            }
            
            public void merge(UserSession other) {
                if (!this.userId.equals(other.userId)) {
                    return;
                }
                this.eventCount += other.eventCount;
                this.sessionDuration += other.sessionDuration;
            }
            
            // Getters and setters
            public String getUserId() { return userId; }
            public int getEventCount() { return eventCount; }
            public long getSessionDuration() { return sessionDuration; }
        }
        
        /**
         * 自定义序列化器
         */
        public static class UserSessionSerde implements Serde<UserSession> {
            
            @Override
            public Serializer<UserSession> serializer() {
                return (topic, data) -> {
                    if (data == null) {
                        return new byte[0];
                    }
                    // 简单的JSON序列化
                    return String.format("{\"userId\":\"%s\",\"eventCount\":%d,\"sessionDuration\":%d}",
                        data.getUserId(), data.getEventCount(), data.getSessionDuration()).getBytes();
                };
            }
            
            @Override
            public Deserializer<UserSession> deserializer() {
                return (topic, data) -> {
                    if (data == null || data.length == 0) {
                        return null;
                    }
                    try {
                        String json = new String(data);
                        // 简单的JSON反序列化
                        // 这里应该使用真正的JSON库如Jackson
                        String[] parts = json.replaceAll("[{}\"]", "").split(",");
                        String userId = parts[0].split(":")[1];
                        int eventCount = Integer.parseInt(parts[1].split(":")[1]);
                        long sessionDuration = Long.parseLong(parts[2].split(":")[1]);
                        return new UserSession(userId, eventCount, sessionDuration);
                    } catch (Exception e) {
                        return null;
                    }
                };
            }
        }
    }
    
    /**
     * 连接操作
     */
    public static class JoinOperations {
        
        /**
         * 流间连接
         */
        public static void demonstrateStreamStreamJoins() {
            StreamsBuilder builder = new StreamsBuilder();
            KStream<String, String> orders = builder.stream("orders");
            KStream<String, String> payments = builder.stream("payments");
            
            System.out.println("=== 流间连接演示 ===");
            
            // 内部连接
            KStream<String, String> joinedStream = orders.join(payments,
                (order, payment) -> order + "," + payment,
                JoinWindows.of(Duration.ofMinutes(5)),
                Joined.with(Serdes.String(), Serdes.String(), Serdes.String()));
            
            // 左连接
            KStream<String, String> leftJoinedStream = orders.leftJoin(payments,
                (order, payment) -> order + "," + (payment != null ? payment : "NULL"),
                JoinWindows.of(Duration.ofMinutes(10)),
                Joined.with(Serdes.String(), Serdes.String(), Serdes.String()));
            
            // 外连接
            KStream<String, String> outerJoinedStream = orders.outerJoin(payments,
                (order, payment) -> {
                    String result = order != null ? order : "NULL_ORDER";
                    result += "," + (payment != null ? payment : "NULL_PAYMENT");
                    return result;
                },
                JoinWindows.of(Duration.ofMinutes(15)),
                Joined.with(Serdes.String(), Serdes.String(), Serdes.String()));
            
            System.out.println("流间连接类型: inner join, left join, outer join");
        }
        
        /**
         * 表间连接
         */
        public static void demonstrateTableTableJoins() {
            StreamsBuilder builder = new StreamsBuilder();
            
            KTable<String, String> users = builder.table("users");
            KTable<String, String> profiles = builder.table("profiles");
            
            System.out.println("=== 表间连接演示 ===");
            
            // 内部连接
            KTable<String, String> userProfiles = users.join(profiles,
                (user, profile) -> user + "," + profile);
            
            // 左连接
            KTable<String, String> userProfilesLeft = users.leftJoin(profiles,
                (user, profile) -> user + "," + (profile != null ? profile : "NULL_PROFILE"));
            
            System.out.println("表间连接完成");
        }
        
        /**
         * 流表连接
         */
        public static void demonstrateStreamTableJoin() {
            StreamsBuilder builder = new StreamsBuilder();
            KStream<String, String> orders = builder.stream("orders");
            GlobalKTable<String, String> products = builder.globalTable("products");
            
            System.out.println("=== 流表连接演示 ===");
            
            // 流与全局表的连接
            KStream<String, String> enrichedOrders = orders.join(products,
                (orderKey, order) -> {
                    // 从订单中提取产品ID作为连接键
                    String[] parts = order.split(",");
                    return parts.length > 2 ? parts[2] : ""; // 假设第三个字段是产品ID
                },
                (order, product) -> order + "," + product);
            
            System.out.println("流表连接完成");
        }
    }
}
```

## Processor API使用

### 自定义处理器

```java
/**
 * Kafka Streams Processor API
 */
public class ProcessorAPIDemo {
    
    /**
     * 自定义处理器
     */
    public static class CustomProcessor implements Processor<String, String> {
        
        private ProcessorContext context;
        private KeyValueStore<String, Integer> countStore;
        
        @Override
        public void init(ProcessorContext context) {
            this.context = context;
            this.countStore = context.getStateStore("word-counts");
            
            // 注册定时器
            context.schedule(Duration.ofSeconds(10), PunctuationType.WALL_CLOCK_TIME, 
                timestamp -> {
                    // 每10秒输出统计信息
                    printStatistics();
                });
        }
        
        @Override
        public void process(String key, String value) {
            // 处理接收到的消息
            String[] words = value.toLowerCase().split("\\W+");
            
            for (String word : words) {
                if (!word.isEmpty()) {
                    // 更新计数
                    Integer currentCount = countStore.get(word);
                    Integer newCount = (currentCount == null) ? 1 : currentCount + 1;
                    countStore.put(word, newCount);
                    
                    // 转发处理后的记录
                    context.forward(word, newCount.toString());
                }
            }
        }
        
        @Override
        public void punctuate(long timestamp) {
            // 定时处理
            System.out.println("定时处理: " + new Date(timestamp));
        }
        
        @Override
        public void close() {
            // 清理资源
            if (countStore != null) {
                countStore.close();
            }
        }
        
        private void printStatistics() {
            System.out.println("=== 词频统计 ===");
            try (KeyValueIterator<String, Integer> iterator = countStore.all()) {
                while (iterator.hasNext()) {
                    KeyValueIterator<String, Integer> next = iterator;
                    System.out.printf("  %s: %d%n", next.key(), next.value());
                }
            }
        }
    }
    
    /**
     * 自定义变换器
     */
    public static class CustomTransformer implements Transformer<String, String, KeyValue<String, String>> {
        
        private ProcessorContext context;
        private KeyValueStore<String, String> cacheStore;
        
        @Override
        public void init(ProcessorContext context) {
            this.context = context;
            this.cacheStore = context.getStateStore("cache");
        }
        
        @Override
        public KeyValue<String, String> transform(String key, String value) {
            // 变换逻辑
            String processedValue = value.toUpperCase();
            
            // 检查缓存
            String cached = cacheStore.get(key);
            if (cached != null) {
                // 使用缓存的重复数据删除逻辑
                if (!cached.equals(processedValue)) {
                    cacheStore.put(key, processedValue);
                    return KeyValue.pair(key, processedValue);
                }
                return null; // 重复数据，不输出
            } else {
                cacheStore.put(key, processedValue);
                return KeyValue.pair(key, processedValue);
            }
        }
        
        @Override
        public void close() {
            // 清理资源
        }
    }
    
    /**
     * 处理器拓扑构建
     */
    public static class ProcessorTopologyBuilder {
        
        public static Topology buildCustomTopology() {
            Topology topology = new Topology();
            
            // 添加源处理器
            topology.addSource("source-processor", "input-topic");
            
            // 添加自定义处理器
            topology.addProcessor("word-count-processor", CustomProcessor::new, "source-processor");
            topology.addProcessor("transform-processor", 
                () -> new CustomTransformer(), "source-processor");
            
            // 添加状态存储
            topology.addStateStore(
                Stores.keyValueStoreBuilder(
                    Stores.persistentKeyValueStore("word-counts"),
                    Serdes.String(),
                    Serdes.Integer()
                ),
                "word-count-processor"
            );
            
            topology.addStateStore(
                Stores.keyValueStoreBuilder(
                    Stores.persistentKeyValueStore("cache"),
                    Serdes.String(),
                    Serdes.String()
                ),
                "transform-processor"
            );
            
            // 添加汇聚处理器
            topology.addSink("sink-processor", "output-topic", 
                Serdes.String().serializer(), Serdes.String().serializer(),
                "transform-processor");
            
            return topology;
        }
        
        public static void printTopology(Topology topology) {
            System.out.println("=== 自定义处理器拓扑 ===");
            System.out.println("源处理器 -> 自定义处理器 -> 汇聚处理器");
            System.out.println("状态存储: word-counts, cache");
        }
    }
    
    /**
     * 高级处理器功能
     */
    public static class AdvancedProcessorFeatures {
        
        /**
         * 错误处理处理器
         */
        public static class ErrorHandlingProcessor implements Processor<String, String> {
            
            private ProcessorContext context;
            private KeyValueStore<String, Integer> errorCountStore;
            
            @Override
            public void init(ProcessorContext context) {
                this.context = context;
                this.errorCountStore = context.getStateStore("error-counts");
            }
            
            @Override
            public void process(String key, String value) {
                try {
                    // 尝试处理数据
                    processRecord(key, value);
                } catch (Exception e) {
                    // 记录错误
                    handleError(key, value, e);
                }
            }
            
            private void processRecord(String key, String value) {
                // 正常处理逻辑
                int number = Integer.parseInt(value);
                String result = "processed: " + (number * 2);
                context.forward(key, result);
            }
            
            private void handleError(String key, String value, Exception e) {
                // 更新错误计数
                Integer currentCount = errorCountStore.get(key);
                Integer newCount = (currentCount == null) ? 1 : currentCount + 1;
                errorCountStore.put(key, newCount);
                
                // 发送到错误主题
                String errorMessage = String.format("Error: %s, Key: %s, Value: %s", 
                    e.getMessage(), key, value);
                context.forward(key, errorMessage);
                
                // 记录日志
                System.err.println("处理错误: " + errorMessage);
            }
            
            @Override
            public void punctuate(long timestamp) {
                // 错误统计报告
                System.out.println("=== 错误统计 ===");
                try (KeyValueIterator<String, Integer> iterator = errorCountStore.all()) {
                    while (iterator.hasNext()) {
                        KeyValueIterator<String, Integer> next = iterator;
                        System.out.printf("Key: %s, 错误次数: %d%n", next.key(), next.value());
                    }
                }
            }
            
            @Override
            public void close() {
                // 清理资源
            }
        }
        
        /**
         * 聚合处理器
         */
        public static class AggregationProcessor implements Processor<String, String> {
            
            private ProcessorContext context;
            private KeyValueStore<String, AggregationState> aggregationStore;
            
            @Override
            public void init(ProcessorContext context) {
                this.context = context;
                this.aggregationStore = context.getStateStore("aggregations");
            }
            
            @Override
            public void process(String key, String value) {
                // 更新聚合状态
                AggregationState state = aggregationStore.get(key);
                if (state == null) {
                    state = new AggregationState();
                }
                
                state.addValue(value);
                aggregationStore.put(key, state);
                
                // 每当状态更新时，输出聚合结果
                context.forward(key, "aggregated: " + state.getResult());
            }
            
            @Override
            public void punctuate(long timestamp) {
                // 定期输出所有聚合结果
                System.out.println("=== 聚合结果 ===");
                try (KeyValueIterator<String, AggregationState> iterator = aggregationStore.all()) {
                    while (iterator.hasNext()) {
                        KeyValueIterator<String, AggregationState> next = iterator;
                        System.out.printf("Key: %s, 结果: %s%n", next.key(), next.value().getResult());
                    }
                }
            }
            
            @Override
            public void close() {
                // 清理资源
            }
            
            /**
             * 聚合状态
             */
            private static class AggregationState {
                private long count = 0;
                private double sum = 0.0;
                private List<String> values = new ArrayList<>();
                
                public void addValue(String value) {
                    try {
                        double num = Double.parseDouble(value);
                        count++;
                        sum += num;
                        values.add(value);
                    } catch (NumberFormatException e) {
                        // 忽略非数值
                    }
                }
                
                public String getResult() {
                    return String.format("count=%d, sum=%.2f, avg=%.2f", 
                        count, sum, count > 0 ? sum / count : 0);
                }
            }
        }
    }
}
```

## 时间窗口与聚合

### 窗口类型演示

```java
/**
 * Kafka Streams时间窗口与聚合
 */
public class TimeWindowingAndAggregation {
    
    /**
     * 窗口类型演示
     */
    public static class WindowTypesDemo {
        
        /**
         * 滚动窗口
         */
        public static void demonstrateTumblingWindows() {
            StreamsBuilder builder = new StreamsBuilder();
            KStream<String, String> events = builder.stream("user-events");
            
            System.out.println("=== 滚动窗口演示 ===");
            
            // 1分钟滚动窗口
            KTable<Windowed<String>, Long> minuteCounts = events
                .groupByKey()
                .windowedBy(TimeWindows.of(Duration.ofMinutes(1)))
                .count(Materialized.as("minute-counts"));
            
            // 5分钟滚动窗口，步长1分钟
            KTable<Windowed<String>, Long> slidingCounts = events
                .groupByKey()
                .windowedBy(TimeWindows.of(Duration.ofMinutes(5))
                    .advanceBy(Duration.ofMinutes(1)))
                .count(Materialized.as("sliding-counts"));
            
            System.out.println("滚动窗口特点:");
            System.out.println("• 不重叠的连续时间窗口");
            System.out.println("• 每个事件只属于一个窗口");
            System.out.println("• 适用于定时聚合统计");
        }
        
        /**
         * 滑动窗口
         */
        public static void demonstrateSlidingWindows() {
            StreamsBuilder builder = new StreamsBuilder();
            KStream<String, String> events = builder.stream("user-events");
            
            System.out.println("=== 滑动窗口演示 ===");
            
            // 10分钟窗口，每5分钟滑动一次
            KTable<Windowed<String>, Long> slidingWindowCounts = events
                .groupByKey()
                .windowedBy(TimeWindows.of(Duration.ofMinutes(10))
                    .advanceBy(Duration.ofMinutes(5)))
                .count(Materialized.as("sliding-window-counts"));
            
            System.out.println("滑动窗口特点:");
            System.out.println("• 连续重叠的时间窗口");
            System.out.println("• 事件可能属于多个窗口");
            System.out.println("• 适用于流式分析");
        }
        
        /**
         * 会话窗口
         */
        public static void demonstrateSessionWindows() {
            StreamsBuilder builder = new StreamsBuilder();
            KStream<String, String> userSessions = builder.stream("user-sessions");
            
            System.out.println("=== 会话窗口演示 ===");
            
            // 30分钟不活跃间隔的会话窗口
            KTable<Windowed<String>, Long> sessionCounts = userSessions
                .groupByKey()
                .windowedBy(SessionWindows.with(Duration.ofMinutes(30)))
                .count(Materialized.as("session-counts"));
            
            System.out.println("会话窗口特点:");
            System.out.println("• 基于用户活动间隔划分窗口");
            System.out.println("• 同一用户的连续活动属于同一会话");
            System.out.println("• 适用于用户行为分析");
        }
    }
    
    /**
     * 窗口聚合操作
     */
    public static class WindowedAggregationOperations {
        
        /**
         * 窗口计数
         */
        public static void demonstrateWindowedCounting() {
            StreamsBuilder builder = new StreamsBuilder();
            KStream<String, String> events = builder.stream("events");
            
            System.out.println("=== 窗口计数演示 ===");
            
            // 按时间窗口统计事件数量
            KTable<Windowed<String>, Long> eventCounts = events
                .groupByKey()
                .windowedBy(TimeWindows.of(Duration.ofMinutes(5)))
                .count(Materialized.as("event-window-counts"));
            
            // 分组统计不同类型事件
            KTable<Windowed<String>, Map<String, Long>> eventTypeCounts = events
                .groupBy((key, value) -> value.split(",")[0]) // 提取事件类型
                .windowedBy(TimeWindows.of(Duration.ofMinutes(10)))
                .aggregate(
                    () -> new HashMap<String, Long>(),
                    (key, event, counts) -> {
                        String type = event.split(",")[0];
                        counts.put(type, counts.getOrDefault(type, 0L) + 1L);
                        return counts;
                    },
                    (key, left, right) -> {
                        left.putAll(right);
                        return left;
                    },
                    Materialized.as("event-type-aggregates-store")
                );
            
            System.out.println("窗口计数操作完成");
        }
        
        /**
         * 窗口求和
         */
        public static void demonstrateWindowedSum() {
            StreamsBuilder builder = new StreamsBuilder();
            KStream<String, String> transactions = builder.stream("transactions");
            
            System.out.println("=== 窗口求和演示 ===");
            
            // 按用户统计窗口内交易金额
            KTable<Windowed<String>, Double> userTransactionSums = transactions
                .mapValues(value -> {
                    String[] parts = value.split(",");
                    return Double.parseDouble(parts[parts.length - 1]); // 最后一个字段为金额
                })
                .groupByKey()
                .windowedBy(TimeWindows.of(Duration.ofHours(1)))
                .sum(Materialized.as("transaction-window-sums"));
            
            // 按区域统计窗口内销售额
            KTable<Windowed<String>, Double> regionSales = transactions
                .groupBy((key, value) -> value.split(",")[0]) // 第一个字段为区域
                .windowedBy(TimeWindows.of(Duration.ofHours(24)))
                .sum(Materialized.as("region-sales-store"));
            
            System.out.println("窗口求和操作完成");
        }
        
        /**
         * 窗口平均值
         */
        public static void demonstrateWindowedAverage() {
            StreamsBuilder builder = new StreamsBuilder();
            KStream<String, String> sensorData = builder.stream("sensor-data");
            
            System.out.println("=== 窗口平均值演示 ===");
            
            // 计算传感器平均读数
            KTable<Windowed<String>, Double> averageSensorReadings = sensorData
                .mapValues(value -> {
                    String[] parts = value.split(",");
                    return Double.parseDouble(parts[1]); // 第二个字段为读数
                })
                .groupByKey()
                .windowedBy(TimeWindows.of(Duration.ofMinutes(10)))
                .aggregate(
                    () -> new AverageState(),
                    (key, reading, state) -> state.add(reading),
                    (key, left, right) -> left.merge(right),
                    Materialized.as("average-sensor-readings-store")
                );
            
            System.out.println("窗口平均值操作完成");
        }
    }
    
    /**
     * 状态类定义
     */
    public static class AverageState {
        private long count = 0;
        private double sum = 0.0;
        
        public void add(double value) {
            count++;
            sum += value;
        }
        
        public void merge(AverageState other) {
            count += other.count;
            sum += other.sum;
        }
        
        public double getAverage() {
            return count > 0 ? sum / count : 0.0;
        }
        
        public double getSum() { return sum; }
        public long getCount() { return count; }
    }
}
```

## 状态存储与查询

### 状态存储管理

```java
/**
 * Kafka Streams状态存储管理
 */
public class StateStoreManagement {
    
    /**
     * 状态存储类型
     */
    public static class StateStoreTypes {
        
        /**
         * 键值状态存储
         */
        public static KeyValueStore<String, Long> createKeyValueStore(String storeName) {
            StoreBuilder<KeyValueStore<String, Long>> storeBuilder = 
                Stores.keyValueStoreBuilder(
                    Stores.persistentKeyValueStore(storeName),
                    Serdes.String(),
                    Serdes.Long()
                );
            
            System.out.println("=== 键值状态存储 ===");
            System.out.println("特性:");
            System.out.println("• 点查询: get(key)");
            System.out.println("• 范围查询: range(key1, key2)");
            System.out.println("• 迭代访问: all()");
            System.out.println("• 持久化存储");
            
            return storeBuilder.build();
        }
        
        /**
         * 窗口状态存储
         */
        public static WindowStore<String, Long> createWindowStore(String storeName) {
            StoreBuilder<WindowStore<String, Long>> storeBuilder = 
                Stores.windowStoreBuilder(
                    Stores.persistentWindowStore(storeName, 
                        Duration.ofHours(1), // 存储时间范围
                        Duration.ofMinutes(5), // 窗口大小
                        false), // 是否压缩
                    Serdes.String(),
                    Serdes.Long()
                );
            
            System.out.println("=== 窗口状态存储 ===");
            System.out.println("特性:");
            System.out.println("• 时间范围查询");
            System.out.println("• 窗口聚合");
            System.out.println("• 自动清理过期数据");
            
            return storeBuilder.build();
        }
        
        /**
         * 会话状态存储
         */
        public static SessionStore<String, Long> createSessionStore(String storeName) {
            StoreBuilder<SessionStore<String, Long>> storeBuilder = 
                Stores.sessionStoreBuilder(
                    Stores.persistentSessionStore(storeName, Duration.ofMinutes(30)),
                    Serdes.String(),
                    Serdes.Long()
                );
            
            System.out.println("=== 会话状态存储 ===");
            System.out.println("特性:");
            System.out.println("• 基于会话的聚合");
            System.out.println("• 自动会话合并");
            System.out.println("• 不活跃间隔检测");
            
            return storeBuilder.build();
        }
    }
    
    /**
     * 状态存储查询
     */
    public static class StateStoreQuery {
        
        /**
         * 键值存储查询示例
         */
        public static void demonstrateKeyValueStoreQuery(KafkaStreams streams) {
            try {
                // 获取键值存储
                ReadOnlyKeyValueStore<String, Long> store = streams.store(
                    StoreQueryParameters.fromNameAndType(
                        "word-counts",
                        QueryableStoreTypes.keyValueStore()
                    )
                );
                
                System.out.println("=== 键值存储查询示例 ===");
                
                // 点查询
                Long count = store.get("hello");
                System.out.println("单词 'hello' 的出现次数: " + count);
                
                // 范围查询
                try (KeyValueIterator<String, Long> iterator = store.range("a", "z")) {
                    System.out.println("字母表范围查询结果:");
                    while (iterator.hasNext()) {
                        KeyValueIterator<String, Long> next = iterator;
                        System.out.printf("  %s: %d%n", next.key(), next.value());
                    }
                }
                
                // 全量查询
                try (KeyValueIterator<String, Long> iterator = store.all()) {
                    System.out.println("全量查询结果:");
                    int count = 0;
                    while (iterator.hasNext() && count < 10) { // 只显示前10个
                        KeyValueIterator<String, Long> next = iterator;
                        System.out.printf("  %s: %d%n", next.key(), next.value());
                        count++;
                    }
                }
                
            } catch (InvalidStateStoreException e) {
                System.out.println("状态存储暂时不可用: " + e.getMessage());
            }
        }
        
        /**
         * 窗口存储查询示例
         */
        public static void demonstrateWindowStoreQuery(KafkaStreams streams) {
            try {
                // 获取窗口存储
                ReadOnlyWindowStore<String, Long> store = streams.store(
                    StoreQueryParameters.fromNameAndType(
                        "window-counts",
                        QueryableStoreTypes.windowStore()
                    )
                );
                
                System.out.println("=== 窗口存储查询示例 ===");
                
                long startTime = System.currentTimeMillis() - Duration.ofMinutes(30).toMillis();
                long endTime = System.currentTimeMillis();
                
                // 时间范围查询
                try (WindowStoreIterator<Long> iterator = store.fetch("user123", startTime, endTime)) {
                    System.out.println("用户 'user123' 的时间范围查询结果:");
                    while (iterator.hasNext()) {
                        WindowStoreIterator<Long> next = iterator;
                        Date timestamp = new Date(next.key());
                        Long value = next.value();
                        System.out.printf("  时间: %s, 数值: %d%n", timestamp, value);
                    }
                }
                
            } catch (InvalidStateStoreException e) {
                System.out.println("状态存储暂时不可用: " + e.getMessage());
            }
        }
    }
    
    /**
     * 状态存储监控
     */
    public static class StateStoreMonitoring {
        
        private final KafkaStreams streams;
        private final ScheduledExecutorService scheduler = Executors.newScheduledThreadPool(1);
        
        public StateStoreMonitoring(KafkaStreams streams) {
            this.streams = streams;
            startMonitoring();
        }
        
        private void startMonitoring() {
            scheduler.scheduleAtFixedRate(this::monitorStateStores, 30, 60, TimeUnit.SECONDS);
        }
        
        private void monitorStateStores() {
            try {
                System.out.println("=== 状态存储监控 ===");
                
                // 获取所有本地状态存储
                StreamsMetadata metadata = streams.allMetadata();
                System.out.println("Streams应用元数据:");
                System.out.println("  当前状态: " + streams.state());
                System.out.println("  线程数量: " + streams.metadataForLocalThreads());
                
                // 监控活跃任务
                Set<String> activeTasks = streams.metadataForLocalThreads().stream()
                    .flatMap(threadMetadata -> threadMetadata.activeTasks().keySet().stream())
                    .collect(Collectors.toSet());
                
                System.out.println("活跃任务数量: " + activeTasks.size());
                
                // 监控备用任务
                Set<String> standbyTasks = streams.metadataForLocalThreads().stream()
                    .flatMap(threadMetadata -> threadMetadata.standbyTasks().keySet().stream())
                    .collect(Collectors.toSet());
                
                System.out.println("备用任务数量: " + standbyTasks.size());
                
                System.out.println("状态存储监控完成");
                
            } catch (Exception e) {
                System.err.println("状态存储监控失败: " + e.getMessage());
            }
        }
        
        /**
         * 状态存储健康检查
         */
        public void performHealthCheck() {
            System.out.println("=== 状态存储健康检查 ===");
            
            // 检查Streams应用状态
            State currentState = streams.state();
            if (currentState == State.RUNNING) {
                System.out.println("✅ Streams应用运行正常");
            } else if (currentState == State.REBALANCING) {
                System.out.println("⚠️ Streams应用正在重新分配");
            } else {
                System.out.println("❌ Streams应用状态异常: " + currentState);
            }
            
            // 检查线程状态
            List<ThreadMetadata> threads = streams.metadataForLocalThreads();
            if (!threads.isEmpty()) {
                System.out.println("✅ 流处理线程正常");
                threads.forEach(thread -> 
                    System.out.printf("  线程 %s: %s%n", 
                        thread.threadId(), thread.threadState()));
            } else {
                System.out.println("❌ 没有活动的流处理线程");
            }
        }
    }
}
```

## 流处理最佳实践

### 性能优化

```java
/**
 * Kafka Streams性能优化最佳实践
 */
public class StreamsPerformanceBestPractices {
    
    /**
     * 缓存优化
     */
    public static class CacheOptimization {
        
        public static void configureCaching() {
            System.out.println("=== 缓存优化配置 ===");
            
            // 增大缓存大小以提高吞吐量
            Properties props = new Properties();
            props.put(StreamsConfig.CACHE_MAX_BYTES_BUFFERING_CONFIG, 64 * 1024 * 1024L); // 64MB
            
            // 调整提交间隔以平衡延迟和吞吐量
            props.put(StreamsConfig.COMMIT_INTERVAL_MS_CONFIG, 1000); // 1秒
            
            System.out.println("✅ 缓存配置优化完成");
        }
        
        public static void demonstrateCacheBenefits() {
            System.out.println("=== 缓存优化效果 ===");
            
            StreamsBuilder builder = new StreamsBuilder();
            KStream<String, String> stream = builder.stream("input");
            
            // 使用缓存的聚合操作
            KTable<String, Long> cachedCounts = stream
                .groupByKey()
                .count(Materialized.as("cached-counts")); // 自动使用缓存
            
            System.out.println("缓存优化特点:");
            System.out.println("• 减少不必要的IO操作");
            System.out.println("• 提高批处理效率");
            System.out.println("• 降低网络和磁盘开销");
        }
    }
    
    /**
     * 并行度优化
     */
    public static class ParallelismOptimization {
        
        public static void configureParallelism() {
            System.out.println("=== 并行度优化配置 ===");
            
            // 设置流处理线程数
            int numStreamThreads = Runtime.getRuntime().availableProcessors();
            
            Properties props = new Properties();
            props.put(StreamsConfig.NUM_STREAM_THREADS_CONFIG, numStreamThreads);
            
            // 设置备用副本数以提高容错性
            props.put(StreamsConfig.NUM_STANDBY_REPLICAS_CONFIG, 1);
            
            System.out.println("✅ 并行度配置优化完成");
            System.out.println("流处理线程数: " + numStreamThreads);
        }
        
        public static void demonstrateParallelismBenefits() {
            System.out.println("=== 并行度优化效果 ===");
            
            System.out.println("并行度优化特点:");
            System.out.println("• 提高数据处理吞吐量");
            System.out.println("• 支持水平扩展");
            System.out.println("• 提高容错能力");
            System.out.println("• 优化资源利用率");
        }
    }
    
    /**
     * 内存管理
     */
    public static class MemoryManagement {
        
        public static void configureMemoryManagement() {
            System.out.println("=== 内存管理配置 ===");
            
            Properties props = new Properties();
            
            // 设置JVM堆大小相关的配置
            props.put(StreamsConfig.ROCKSDB_CONFIG_SETTER_CLASS_CONFIG, 
                "org.apache.kafka.streams.state.RocksDBConfigSetter");
            
            // 设置缓存大小
            props.put(StreamsConfig.CACHE_MAX_BYTES_BUFFERING_CONFIG, 32 * 1024 * 1024L); // 32MB
            
            System.out.println("✅ 内存管理配置完成");
        }
        
        public static void demonstrateMemoryOptimization() {
            System.out.println("=== 内存优化策略 ===");
            
            System.out.println("内存优化策略:");
            System.out.println("• 合理设置缓存大小");
            System.out.println("• 控制状态存储大小");
            System.out.println("• 定期清理无用数据");
            System.out.println("• 监控内存使用情况");
        }
    }
    
    /**
     * 容错处理
     */
    public static class FaultTolerance {
        
        public static void configureFaultTolerance() {
            System.out.println("=== 容错配置 ===");
            
            Properties props = new Properties();
            
            // 启用 Exactly Once 处理保证
            props.put(StreamsConfig.PROCESSING_GUARANTEE_CONFIG, 
                ProcessingGuarantee.EXACTLY_ONCE_V2.name());
            
            // 设置重试配置
            props.put(ConsumerConfig.MAX_POLL_INTERVAL_MS_CONFIG, 300000); // 5分钟
            props.put(ConsumerConfig.SESSION_TIMEOUT_MS_CONFIG, 30000); // 30秒
            
            // 设置生产者重试配置
            props.put(ProducerConfig.RETRIES_CONFIG, Integer.MAX_VALUE);
            props.put(ProducerConfig.MAX_IN_FLIGHT_REQUESTS_PER_CONNECTION, 5);
            
            System.out.println("✅ 容错配置完成");
        }
        
        public static void demonstrateFaultToleranceFeatures() {
            System.out.println("=== 容错特性 ===");
            
            System.out.println("容错特性:");
            System.out.println("• 自动故障检测");
            System.out.println("• 任务重新分配");
            System.out.println("• 状态恢复");
            System.out.println("• Exactly Once 处理保证");
            System.out.println("• 变更日志主题");
        }
    }
}
```

## 复杂事件处理

### 复杂事件处理模式

```java
/**
 * 复杂事件处理 (CEP)
 */
public class ComplexEventProcessing {
    
    /**
     * 事件序列检测
     */
    public static class EventSequenceDetection {
        
        /**
         * 模式匹配器
         */
        public static class PatternMatcher {
            
            /**
             * 顺序模式匹配
             */
            public static void demonstrateSequentialPatterns() {
                StreamsBuilder builder = new StreamsBuilder();
                KStream<String, Event> events = builder.stream("events");
                
                System.out.println("=== 顺序模式匹配 ===");
                
                // 模式: A -> B -> C (3个连续事件)
                events
                    .selectKey((key, event) -> event.getUserId())
                    .groupByKey()
                    .aggregate(
                        EventSequence::new,
                        (key, event, sequence) -> {
                            sequence.addEvent(event);
                            return sequence;
                        },
                        (key, left, right) -> {
                            // 合并序列
                            left.merge(right);
                            return left;
                        },
                        Materialized.as("event-sequences")
                    )
                    .filter((userId, sequence) -> sequence.matchesPattern("LOGIN", "VIEW", "PURCHASE"));
                
                System.out.println("顺序模式检测完成");
            }
            
            /**
             * 并发模式匹配
             */
            public static void demonstrateConcurentPatterns() {
                StreamsBuilder builder = new StreamsBuilder();
                KStream<String, Event> events = builder.stream("events");
                
                System.out.println("=== 并发模式匹配 ===");
                
                // 模式: (A OR B) AND C (并发事件)
                events
                    .selectKey((key, event) -> event.getSessionId())
                    .groupByKey()
                    .windowedBy(SessionWindows.with(Duration.ofMinutes(30)))
                    .aggregate(
                        SessionPattern::new,
                        (key, event, session) -> {
                            session.addEvent(event);
                            return session;
                        },
                        (key, left, right) -> {
                            left.merge(right);
                            return left;
                        },
                        Materialized.as("session-patterns")
                    )
                    .filter((windowedSession, pattern) -> 
                        pattern.hasEventType("ERROR") && 
                        (pattern.hasEventType("LOGIN") || pattern.hasEventType("REGISTER")));
                
                System.out.println("并发模式检测完成");
            }
            
            /**
             * 否定模式匹配
             */
            public static void demonstrateNegationPatterns() {
                StreamsBuilder builder = new StreamsBuilder();
                KStream<String, Event> events = builder.stream("events");
                
                System.out.println("=== 否定模式匹配 ===");
                
                // 模式: A -> NOT B (在A之后没有出现B)
                events
                    .groupBy(Event::getUserId)
                    .windowedBy(TimeWindows.of(Duration.ofHours(1)))
                    .aggregate(
                        PatternContext::new,
                        (key, event, context) -> {
                            context.addEvent(event);
                            return context;
                        },
                        (key, left, right) -> {
                            left.merge(right);
                            return left;
                        },
                        Materialized.as("negation-patterns")
                    )
                    .filter((windowedUser, context) -> 
                        context.hasEvent("LOGIN") && !context.hasEvent("LOGOUT"));
                
                System.out.println("否定模式检测完成");
            }
        }
        
        /**
         * 事件实体类
         */
        public static class Event {
            private String eventId;
            private String userId;
            private String sessionId;
            private String eventType;
            private long timestamp;
            
            public Event(String eventId, String userId, String sessionId, 
                        String eventType, long timestamp) {
                this.eventId = eventId;
                this.userId = userId;
                this.sessionId = sessionId;
                this.eventType = eventType;
                this.timestamp = timestamp;
            }
            
            public String getUserId() { return userId; }
            public String getSessionId() { return sessionId; }
            public String getEventType() { return eventType; }
            public long getTimestamp() { return timestamp; }
        }
        
        /**
         * 事件序列类
         */
        public static class EventSequence {
            private List<Event> events = new ArrayList<>();
            
            public void addEvent(Event event) {
                events.add(event);
            }
            
            public void merge(EventSequence other) {
                events.addAll(other.events);
                events.sort(Comparator.comparingLong(Event::getTimestamp));
            }
            
            public boolean matchesPattern(String... pattern) {
                if (events.size() < pattern.length) {
                    return false;
                }
                
                for (int i = 0; i <= events.size() - pattern.length; i++) {
                    boolean match = true;
                    for (int j = 0; j < pattern.length; j++) {
                        if (!events.get(i + j).getEventType().equals(pattern[j])) {
                            match = false;
                            break;
                        }
                    }
                    if (match) {
                        return true;
                    }
                }
                return false;
            }
        }
        
        /**
         * 会话模式类
         */
        public static class SessionPattern {
            private Set<String> eventTypes = new HashSet<>();
            
            public void addEvent(Event event) {
                eventTypes.add(event.getEventType());
            }
            
            public void merge(SessionPattern other) {
                eventTypes.addAll(other.eventTypes);
            }
            
            public boolean hasEventType(String eventType) {
                return eventTypes.contains(eventType);
            }
        }
        
        /**
         * 模式上下文类
         */
        public static class PatternContext {
            private Set<String> events = new HashSet<>();
            
            public void addEvent(Event event) {
                events.add(event.getEventType());
            }
            
            public void merge(PatternContext other) {
                events.addAll(other.events);
            }
            
            public boolean hasEvent(String eventType) {
                return events.contains(eventType);
            }
        }
    }
    
    /**
     * 实时告警
     */
    public static class RealTimeAlerting {
        
        /**
         * 阈值告警
         */
        public static void demonstrateThresholdAlerts() {
            StreamsBuilder builder = new StreamsBuilder();
            KStream<String, SensorData> sensorStream = builder.stream("sensor-data");
            
            System.out.println("=== 阈值告警 ===");
            
            // 温度阈值告警
            KStream<String, Alert> temperatureAlerts = sensorStream
                .filter((key, data) -> "temperature".equals(data.getSensorType()))
                .filter((key, data) -> data.getValue() > 80.0) // 温度阈值
                .map((key, data) -> KeyValue.pair(key, 
                    new Alert("HIGH_TEMPERATURE", data.getValue(), data.getTimestamp())));
            
            // 湿度阈值告警
            KStream<String, Alert> humidityAlerts = sensorStream
                .filter((key, data) -> "humidity".equals(data.getSensorType()))
                .filter((key, data) -> data.getValue() < 20.0) // 湿度阈值
                .map((key, data) -> KeyValue.pair(key,
                    new Alert("LOW_HUMIDITY", data.getValue(), data.getTimestamp())));
            
            // 合并告警
            KStream<String, Alert> allAlerts = temperatureAlerts.merge(humidityAlerts);
            
            // 发送到告警主题
            allAlerts.to("alerts");
            
            System.out.println("阈值告警配置完成");
        }
        
        /**
         * 趋势告警
         */
        public static void demonstrateTrendAlerts() {
            StreamsBuilder builder = new StreamsBuilder();
            KStream<String, Metric> metricsStream = builder.stream("metrics");
            
            System.out.println("=== 趋势告警 ===");
            
            // 检测数值上升趋势
            KTable<String, TrendAnalysis> trendAnalysis = metricsStream
                .groupByKey()
                .windowedBy(TimeWindows.of(Duration.ofMinutes(5)))
                .aggregate(
                    TrendAnalysis::new,
                    (key, metric, analysis) -> {
                        analysis.addMetric(metric);
                        return analysis;
                    },
                    (key, left, right) -> {
                        left.merge(right);
                        return left;
                    },
                    Materialized.as("trend-analysis")
                )
                .filter((windowedKey, analysis) -> analysis.isIncreasingRapidly());
            
            trendAnalysis.toStream()
                .map((windowedKey, analysis) -> KeyValue.pair(windowedKey.key(), 
                    new Alert("RAPID_INCREASE", analysis.getCurrentValue(), System.currentTimeMillis())))
                .to("trend-alerts");
            
            System.out.println("趋势告警配置完成");
        }
        
        /**
         * 异常检测告警
         */
        public static void demonstrateAnomalyDetectionAlerts() {
            StreamsBuilder builder = new StreamsBuilder();
            KStream<String, Transaction> transactionStream = builder.stream("transactions");
            
            System.out.println("=== 异常检测告警 ===");
            
            // 检测异常交易
            KStream<String, Alert> anomalyAlerts = transactionStream
                .groupBy(Transaction::getUserId)
                .windowedBy(TimeWindows.of(Duration.ofHours(1)))
                .aggregate(
                    TransactionAnalyzer::new,
                    (key, transaction, analyzer) -> {
                        analyzer.addTransaction(transaction);
                        return analyzer;
                    },
                    (key, left, right) -> {
                        left.merge(right);
                        return right;
                    },
                    Materialized.as("transaction-analyzer")
                )
                .toStream()
                .filter((windowedUser, analyzer) -> analyzer.isAnomalous())
                .map((windowedUser, analyzer) -> KeyValue.pair(windowedUser.key(),
                    new Alert("SUSPICIOUS_ACTIVITY", 0, System.currentTimeMillis())));
            
            anomalyAlerts.to("anomaly-alerts");
            
            System.out.println("异常检测告警配置完成");
        }
        
        /**
         * 告警实体类
         */
        public static class Alert {
            private String type;
            private double value;
            private long timestamp;
            
            public Alert(String type, double value, long timestamp) {
                this.type = type;
                this.value = value;
                this.timestamp = timestamp;
            }
            
            public String getType() { return type; }
            public double getValue() { return value; }
            public long getTimestamp() { return timestamp; }
        }
    }
    
    /**
     * 支持类
     */
    public static class SensorData {
        private String sensorType;
        private double value;
        private long timestamp;
        
        public SensorData(String sensorType, double value, long timestamp) {
            this.sensorType = sensorType;
            this.value = value;
            this.timestamp = timestamp;
        }
        
        public String getSensorType() { return sensorType; }
        public double getValue() { return value; }
        public long getTimestamp() { return timestamp; }
    }
    
    public static class Metric {
        private String metricName;
        private double value;
        private long timestamp;
        
        public Metric(String metricName, double value, long timestamp) {
            this.metricName = metricName;
            this.value = value;
            this.timestamp = timestamp;
        }
        
        public String getMetricName() { return metricName; }
        public double getValue() { return value; }
        public long getTimestamp() { return timestamp; }
    }
    
    public static class Transaction {
        private String userId;
        private double amount;
        private String location;
        private long timestamp;
        
        public Transaction(String userId, double amount, String location, long timestamp) {
            this.userId = userId;
            this.amount = amount;
            this.location = location;
            this.timestamp = timestamp;
        }
        
        public String getUserId() { return userId; }
        public double getAmount() { return amount; }
        public String getLocation() { return location; }
    }
    
    public static class TrendAnalysis {
        private List<Double> values