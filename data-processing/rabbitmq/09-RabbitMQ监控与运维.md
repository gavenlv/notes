# ç¬¬9ç« ï¼šRabbitMQç›‘æ§ä¸è¿ç»´

## ğŸ“– æ¦‚è¿°

æœ¬ç« èŠ‚æ·±å…¥æ¢è®¨RabbitMQçš„ç›‘æ§ä¸è¿ç»´æœ€ä½³å®è·µï¼ŒåŒ…æ‹¬æ€§èƒ½ç›‘æ§ã€æ—¥å¿—ç®¡ç†ã€å‘Šè­¦é…ç½®ã€å¥åº·æ£€æŸ¥ã€å®¹é‡è§„åˆ’å’Œæ•…éšœè¯Šæ–­ç­‰å…³é”®è¿ç»´ä»»åŠ¡ã€‚é€šè¿‡å®Œå–„çš„ç›‘æ§ä½“ç³»å’Œè‡ªåŠ¨åŒ–è¿ç»´å·¥å…·ï¼Œç¡®ä¿RabbitMQé›†ç¾¤çš„ç¨³å®šè¿è¡Œã€‚

### ğŸ¯ å­¦ä¹ ç›®æ ‡

- æŒæ¡RabbitMQç›‘æ§æŒ‡æ ‡ä½“ç³»å’Œå…³é”®æ€§èƒ½æŒ‡æ ‡
- å­¦ä¼šé…ç½®Prometheusã€Grafanaç­‰ç›‘æ§å·¥å…·
- ç†è§£æ—¥å¿—ç®¡ç†å’Œåˆ†æçš„æœ€ä½³å®è·µ
- æŒæ¡å‘Šè­¦é…ç½®å’Œæ•…éšœè¯Šæ–­æŠ€å·§
- å­¦ä¼šå®¹é‡è§„åˆ’å’Œæ€§èƒ½è°ƒä¼˜æ–¹æ³•
- äº†è§£è‡ªåŠ¨åŒ–è¿ç»´å’ŒDevOpsé›†æˆ

## ğŸ” æ ¸å¿ƒæ¦‚å¿µ

### 1. ç›‘æ§å±‚æ¬¡ç»“æ„

RabbitMQç›‘æ§é€šå¸¸åˆ†ä¸ºå››ä¸ªå±‚æ¬¡ï¼š

- **ç³»ç»Ÿå±‚ç›‘æ§**: CPUã€å†…å­˜ã€ç£ç›˜ã€ç½‘ç»œç­‰åŸºç¡€èµ„æº
- **å®¹å™¨å±‚ç›‘æ§**: Docker/Kuberneteså®¹å™¨èµ„æºä½¿ç”¨
- **åº”ç”¨å±‚ç›‘æ§**: RabbitMQè¿›ç¨‹ã€é˜Ÿåˆ—ã€è¿æ¥ç­‰åº”ç”¨æŒ‡æ ‡
- **ä¸šåŠ¡å±‚ç›‘æ§**: æ¶ˆæ¯å»¶è¿Ÿã€ååé‡ã€é”™è¯¯ç‡ç­‰ä¸šåŠ¡æŒ‡æ ‡

### 2. å…³é”®ç›‘æ§æŒ‡æ ‡

#### ç³»ç»ŸæŒ‡æ ‡
- CPUä½¿ç”¨ç‡ã€è´Ÿè½½å¹³å‡å€¼
- å†…å­˜ä½¿ç”¨ç‡ã€äº¤æ¢åˆ†åŒºä½¿ç”¨
- ç£ç›˜ç©ºé—´ä½¿ç”¨ç‡ã€I/Oæ€§èƒ½
- ç½‘ç»œå¸¦å®½ä½¿ç”¨ç‡ã€è¿æ¥æ•°

#### åº”ç”¨æŒ‡æ ‡
- è¿æ¥æ•°ã€é€šé“æ•°ã€é˜Ÿåˆ—æ•°
- æ¶ˆæ¯å‘å¸ƒ/æ¶ˆè´¹é€Ÿç‡
- é˜Ÿåˆ—æ·±åº¦ã€æ¶ˆæ¯å †ç§¯
- æ¶ˆæ¯ç¡®è®¤ç‡ã€é‡è¯•ç‡
- é”™è¯¯ç‡ã€è¶…æ—¶ç‡

#### æ€§èƒ½æŒ‡æ ‡
- æ¶ˆæ¯å»¶è¿Ÿï¼ˆç«¯åˆ°ç«¯å»¶è¿Ÿï¼‰
- ååé‡ï¼ˆæ¶ˆæ¯/ç§’ï¼‰
- ç³»ç»Ÿå“åº”æ—¶é—´
- èµ„æºåˆ©ç”¨ç‡

### 3. ç›‘æ§å·¥å…·ç”Ÿæ€

```
æ•°æ®é‡‡é›†å±‚: RabbitMQæ’ä»¶ã€Node Exporterã€Prometheus Client
æ•°æ®å­˜å‚¨å±‚: Prometheusã€InfluxDBã€Elasticsearch
æ•°æ®å±•ç¤ºå±‚: Grafanaã€Kibanaã€è‡ªå®šä¹‰ä»ªè¡¨æ¿
å‘Šè­¦å±‚: Alertmanagerã€PagerDutyã€Slack
æ—¥å¿—å±‚: ELK Stackã€Fluentdã€Loki
```

## ğŸ›  ç›‘æ§å·¥å…·é…ç½®

### 1. Prometheusç›‘æ§é›†æˆ

#### å®‰è£…Prometheusæ’ä»¶

```bash
# å¯ç”¨Prometheusæ’ä»¶
rabbitmq-plugins enable rabbitmq_prometheus

# æ£€æŸ¥æ’ä»¶çŠ¶æ€
rabbitmq-plugins list | grep prometheus
```

#### é…ç½®Prometheus

```yaml
# prometheus.yml
global:
  scrape_interval: 15s
  evaluation_interval: 15s

rule_files:
  - "rabbitmq_rules.yml"

scrape_configs:
  - job_name: 'rabbitmq'
    static_configs:
      - targets: ['localhost:15692']
    metrics_path: '/metrics'
    scrape_interval: 30s
    scrape_timeout: 10s
    params:
      format: ['prometheus']
```

#### RabbitMQå‘Šè­¦è§„åˆ™

```yaml
# rabbitmq_rules.yml
groups:
- name: rabbitmq.rules
  rules:
  - alert: RabbitMQDown
    expr: up{job="rabbitmq"} == 0
    for: 1m
    labels:
      severity: critical
    annotations:
      summary: "RabbitMQ instance is down"
      description: "RabbitMQ instance {{ $labels.instance }} is down for more than 1 minute."
      
  - alert: RabbitMQHighMemoryUsage
    expr: rabbitmq_process_resident_memory_bytes / rabbitmq_process_max_memory_bytes > 0.8
    for: 5m
    labels:
      severity: warning
    annotations:
      summary: "RabbitMQ high memory usage"
      description: "RabbitMQ instance {{ $labels.instance }} is using {{ $value | humanizePercentage }} of available memory."
      
  - alert: RabbitMQDiskSpaceLow
    expr: rabbitmq_disk_space_available_bytes / rabbitmq_disk_space_available_limit_bytes < 0.2
    for: 5m
    labels:
      severity: warning
    annotations:
      summary: "RabbitMQ low disk space"
      description: "RabbitMQ instance {{ $labels.instance }} has less than 20% disk space available."
      
  - alert: RabbitMQQueueMessagesHigh
    expr: rabbitmq_queue_messages > 10000
    for: 5m
    labels:
      severity: warning
    annotations:
      summary: "RabbitMQ queue high message count"
      description: "Queue {{ $labels.queue }} on {{ $labels.instance }} has {{ $value }} messages."
      
  - alert: RabbitMQConnectionCountHigh
    expr: rabbitmq_connections > 1000
    for: 5m
    labels:
      severity: warning
    annotations:
      summary: "RabbitMQ high connection count"
      description: "RabbitMQ instance {{ $labels.instance }} has {{ $value }} connections."
      
  - alert: RabbitMQChannelCountHigh
    expr: rabbitmq_channels > 2000
    for: 5m
    labels:
      severity: warning
    annotations:
      summary: "RabbitMQ high channel count"
      description: "RabbitMQ instance {{ $labels.instance }} has {{ $value }} channels."
```

### 2. Grafanaä»ªè¡¨æ¿é…ç½®

#### åˆ›å»ºæ•°æ®æº

```json
{
  "name": "Prometheus",
  "type": "prometheus",
  "url": "http://localhost:9090",
  "access": "proxy",
  "basicAuth": false,
  "jsonData": {
    "httpMethod": "POST"
  }
}
```

#### RabbitMQæ¦‚è§ˆä»ªè¡¨æ¿

```json
{
  "dashboard": {
    "id": null,
    "title": "RabbitMQ Overview",
    "tags": ["rabbitmq", "messaging"],
    "timezone": "browser",
    "refresh": "30s",
    "panels": [
      {
        "id": 1,
        "title": "Connection Count",
        "type": "stat",
        "targets": [
          {
            "expr": "rabbitmq_connections",
            "legendFormat": "Total Connections"
          }
        ],
        "fieldConfig": {
          "defaults": {
            "color": {
              "mode": "thresholds"
            },
            "thresholds": {
              "steps": [
                {"color": "green", "value": null},
                {"color": "yellow", "value": 500},
                {"color": "red", "value": 1000}
              ]
            }
          }
        }
      },
      {
        "id": 2,
        "title": "Message Rate",
        "type": "graph",
        "targets": [
          {
            "expr": "rate(rabbitmq_channel_messages_published_total[5m])",
            "legendFormat": "Publish Rate"
          },
          {
            "expr": "rate(rabbitmq_channel_messages_delivered_total[5m])",
            "legendFormat": "Deliver Rate"
          }
        ]
      },
      {
        "id": 3,
        "title": "Memory Usage",
        "type": "graph",
        "targets": [
          {
            "expr": "rabbitmq_process_resident_memory_bytes",
            "legendFormat": "Memory Usage"
          },
          {
            "expr": "rabbitmq_process_max_memory_bytes",
            "legendFormat": "Memory Limit"
          }
        ]
      },
      {
        "id": 4,
        "title": "Disk Usage",
        "type": "graph",
        "targets": [
          {
            "expr": "rabbitmq_disk_space_available_bytes",
            "legendFormat": "Available Disk Space"
          }
        ]
      },
      {
        "id": 5,
        "title": "Queue Depth",
        "type": "table",
        "targets": [
          {
            "expr": "rabbitmq_queue_messages",
            "legendFormat": "{{queue}}"
          }
        ],
        "transformations": [
          {
            "id": "organize",
            "options": {
              "excludeByName": {},
              "indexByName": {},
              "renameByName": {
                "queue": "Queue Name",
                "Value": "Message Count"
              }
            }
          }
        ]
      }
    ],
    "time": {
      "from": "now-1h",
      "to": "now"
    }
  }
}
```

### 3. æ—¥å¿—ç®¡ç†é…ç½®

#### é…ç½®RabbitMQæ—¥å¿—

```conf
# rabbitmq.conf
log.file.level = info
log.file.path = /var/log/rabbitmq/rabbit.log
log.file.rotation.date = $D0
log.file.rotation.count = 7

log.console = true
log.console.level = info

log.exchange = true
log.exchange.level = info

# å¯ç”¨å®¡è®¡æ—¥å¿—
log_levels.connection = debug
log_levels.authentication_failure_detailed = true
```

#### Filebeaté…ç½®

```yaml
# filebeat.yml
filebeat.inputs:
- type: log
  enabled: true
  paths:
    - /var/log/rabbitmq/*.log
  fields:
    service: rabbitmq
    environment: production
  multiline.pattern: '^[[:space:]]'
  multiline.negate: false
  multiline.match: after

output.elasticsearch:
  hosts: ["localhost:9200"]
  index: "rabbitmq-logs-%{+yyyy.MM.dd}"

processors:
  - add_host_metadata:
      when.not.contains.tags: forwarded
  - add_cloud_metadata: ~
  - add_docker_metadata: ~
```

## ğŸ“ˆ æ€§èƒ½ç›‘æ§ä¸åˆ†æ

### 1. æ¶ˆæ¯å»¶è¿Ÿç›‘æ§

#### å»¶è¿Ÿç›‘æ§å®ç°

```python
import time
import pika
import json
from datetime import datetime
from typing import Dict, List
import logging

class MessageLatencyMonitor:
    """æ¶ˆæ¯å»¶è¿Ÿç›‘æ§å™¨"""
    
    def __init__(self, rabbitmq_url: str, queue_name: str = "latency_test"):
        self.rabbitmq_url = rabbitmq_url
        self.queue_name = queue_name
        self.latencies = []
        self.logger = logging.getLogger(__name__)
        
    def setup_test_queue(self):
        """è®¾ç½®æµ‹è¯•é˜Ÿåˆ—"""
        connection = pika.BlockingConnection(pika.URLParameters(self.rabbitmq_url))
        channel = connection.channel()
        
        # å£°æ˜æµ‹è¯•é˜Ÿåˆ—
        channel.queue_declare(
            queue=self.queue_name,
            durable=True,
            arguments={
                'x-message-ttl': 600000,  # 10åˆ†é’ŸTTL
                'x-dead-letter-exchange': 'dlx.latency_test'
            }
        )
        
        # è®¾ç½®æ¶ˆè´¹è€…
        channel.basic_consume(
            queue=self.queue_name,
            on_message_callback=self._on_message_received,
            auto_ack=False
        )
        
        connection.close()
        
    def _on_message_received(self, ch, method, properties, body):
        """æ¶ˆæ¯æ¥æ”¶å›è°ƒ"""
        try:
            # è§£ææ¶ˆæ¯æ—¶é—´æˆ³
            message_data = json.loads(body.decode())
            sent_time = datetime.fromisoformat(message_data['timestamp'])
            received_time = datetime.now()
            
            # è®¡ç®—å»¶è¿Ÿï¼ˆæ¯«ç§’ï¼‰
            latency_ms = (received_time - sent_time).total_seconds() * 1000
            
            # è®°å½•å»¶è¿Ÿ
            self.latencies.append({
                'timestamp': received_time.isoformat(),
                'latency_ms': latency_ms,
                'message_id': message_data['message_id']
            })
            
            self.logger.info(f"æ¶ˆæ¯å»¶è¿Ÿ: {latency_ms:.2f}ms")
            
            # ç¡®è®¤æ¶ˆæ¯
            ch.basic_ack(delivery_tag=method.delivery_tag)
            
        except Exception as e:
            self.logger.error(f"å¤„ç†æ¶ˆæ¯æ—¶å‡ºé”™: {e}")
            ch.basic_nack(delivery_tag=method.delivery_tag, requeue=False)
            
    def send_test_message(self, message_id: str):
        """å‘é€æµ‹è¯•æ¶ˆæ¯"""
        connection = pika.BlockingConnection(pika.URLParameters(self.rabbitmq_url))
        channel = connection.channel()
        
        message_data = {
            'message_id': message_id,
            'timestamp': datetime.now().isoformat(),
            'test_type': 'latency'
        }
        
        channel.basic_publish(
            exchange='',
            routing_key=self.queue_name,
            body=json.dumps(message_data).encode(),
            properties=pika.BasicProperties(
                delivery_mode=2,  # æŒä¹…åŒ–
                timestamp=int(time.time() * 1000)
            )
        )
        
        connection.close()
        self.logger.info(f"å‘é€æµ‹è¯•æ¶ˆæ¯: {message_id}")
        
    def get_latency_stats(self) -> Dict:
        """è·å–å»¶è¿Ÿç»Ÿè®¡ä¿¡æ¯"""
        if not self.latencies:
            return {'error': 'No latency data available'}
            
        latencies = [l['latency_ms'] for l in self.latencies]
        
        return {
            'count': len(latencies),
            'min_latency_ms': min(latencies),
            'max_latency_ms': max(latencies),
            'avg_latency_ms': sum(latencies) / len(latencies),
            'p50_latency_ms': self._percentile(latencies, 50),
            'p95_latency_ms': self._percentile(latencies, 95),
            'p99_latency_ms': self._percentile(latencies, 99)
        }
        
    def _percentile(self, data: List[float], percentile: int) -> float:
        """è®¡ç®—ç™¾åˆ†ä½æ•°"""
        if not data:
            return 0.0
        
        sorted_data = sorted(data)
        index = int(len(sorted_data) * percentile / 100)
        return sorted_data[index]
        
    def export_metrics(self) -> Dict:
        """å¯¼å‡ºç›‘æ§æŒ‡æ ‡"""
        stats = self.get_latency_stats()
        
        return {
            'timestamp': datetime.now().isoformat(),
            'queue_name': self.queue_name,
            'latency_stats': stats,
            'recent_latencies': self.latencies[-100:]  # æœ€è¿‘100æ¡
        }
```

### 2. ååé‡ç›‘æ§

```python
import threading
import time
from collections import defaultdict
from datetime import datetime, timedelta

class ThroughputMonitor:
    """ååé‡ç›‘æ§å™¨"""
    
    def __init__(self, rabbitmq_url: str):
        self.rabbitmq_url = rabbitmq_url
        self.publish_counts = defaultdict(int)
        self.consume_counts = defaultdict(int)
        self.start_time = datetime.now()
        self.is_monitoring = False
        self.monitor_thread = None
        
    def start_monitoring(self, interval: int = 60):
        """å¼€å§‹ç›‘æ§"""
        self.is_monitoring = True
        self.monitor_thread = threading.Thread(target=self._monitor_loop, args=(interval,))
        self.monitor_thread.daemon = True
        self.monitor_thread.start()
        
    def stop_monitoring(self):
        """åœæ­¢ç›‘æ§"""
        self.is_monitoring = False
        if self.monitor_thread:
            self.monitor_thread.join()
            
    def _monitor_loop(self, interval: int):
        """ç›‘æ§å¾ªç¯"""
        while self.is_monitoring:
            try:
                # è·å–å½“å‰ç»Ÿè®¡
                current_stats = self._get_current_stats()
                
                # è®¡ç®—ååé‡
                throughput = self._calculate_throughput(current_stats, interval)
                
                # è®°å½•æ•°æ®
                self._log_throughput(throughput)
                
                # ç­‰å¾…ä¸‹ä¸€ä¸ªå‘¨æœŸ
                time.sleep(interval)
                
            except Exception as e:
                logging.error(f"ç›‘æ§å¾ªç¯é”™è¯¯: {e}")
                time.sleep(interval)
                
    def _get_current_stats(self) -> Dict:
        """è·å–å½“å‰ç»Ÿè®¡"""
        import pika
        
        connection = pika.BlockingConnection(pika.URLParameters(self.rabbitmq_url))
        channel = connection.channel()
        
        # è·å–é˜Ÿåˆ—ç»Ÿè®¡
        queue_stats = {}
        try:
            result = channel.queue_declare(queue='', passive=True)
            queue_stats['messages'] = result.method.message_count
            queue_stats['consumers'] = result.method.consumer_count
        except:
            pass
            
        connection.close()
        
        return queue_stats
        
    def _calculate_throughput(self, current_stats: Dict, interval: int) -> Dict:
        """è®¡ç®—ååé‡"""
        current_time = datetime.now()
        
        # è®¡ç®—æ¶ˆæ¯é€Ÿç‡
        publish_rate = self.publish_counts[current_time] / interval
        consume_rate = self.consume_counts[current_time] / interval
        
        return {
            'timestamp': current_time.isoformat(),
            'publish_rate': publish_rate,
            'consume_rate': consume_rate,
            'total_messages': current_stats.get('messages', 0),
            'interval_seconds': interval
        }
        
    def _log_throughput(self, throughput: Dict):
        """è®°å½•ååé‡"""
        logging.info(f"ååé‡ - å‘å¸ƒ: {throughput['publish_rate']:.2f} msg/s, "
                    f"æ¶ˆè´¹: {throughput['consume_rate']:.2f} msg/s")
```

### 3. é”™è¯¯ç‡ç›‘æ§

```python
import requests
from datetime import datetime
from typing import Dict, List

class ErrorRateMonitor:
    """é”™è¯¯ç‡ç›‘æ§å™¨"""
    
    def __init__(self, rabbitmq_api_url: str, username: str, password: str):
        self.api_url = rabbitmq_api_url
        self.auth = (username, password)
        self.error_counts = defaultdict(int)
        self.total_counts = defaultdict(int)
        
    def get_cluster_status(self) -> Dict:
        """è·å–é›†ç¾¤çŠ¶æ€"""
        try:
            response = requests.get(f"{self.api_url}/api/overview", auth=self.auth)
            response.raise_for_status()
            return response.json()
        except Exception as e:
            logging.error(f"è·å–é›†ç¾¤çŠ¶æ€å¤±è´¥: {e}")
            return {}
            
    def get_node_status(self, node_name: str) -> Dict:
        """è·å–èŠ‚ç‚¹çŠ¶æ€"""
        try:
            response = requests.get(f"{self.api_url}/api/nodes/{node_name}", auth=self.auth)
            response.raise_for_status()
            return response.json()
        except Exception as e:
            logging.error(f"è·å–èŠ‚ç‚¹çŠ¶æ€å¤±è´¥: {e}")
            return {}
            
    def get_queue_status(self, queue_name: str) -> Dict:
        """è·å–é˜Ÿåˆ—çŠ¶æ€"""
        try:
            response = requests.get(f"{self.api_url}/api/queues/%2F/{queue_name}", auth=self.auth)
            response.raise_for_status()
            return response.json()
        except Exception as e:
            logging.error(f"è·å–é˜Ÿåˆ—çŠ¶æ€å¤±è´¥: {e}")
            return {}
            
    def calculate_error_rate(self, time_window: int = 300) -> Dict:
        """è®¡ç®—é”™è¯¯ç‡"""
        current_time = datetime.now()
        
        # è·å–é”™è¯¯ç»Ÿè®¡
        cluster_stats = self.get_cluster_status()
        
        # è®¡ç®—å„ç§é”™è¯¯ç‡
        error_rates = {
            'timestamp': current_time.isoformat(),
            'connection_error_rate': self._calculate_connection_error_rate(cluster_stats),
            'channel_error_rate': self._calculate_channel_error_rate(cluster_stats),
            'message_error_rate': self._calculate_message_error_rate(cluster_stats),
            'queue_error_rate': self._calculate_queue_error_rate(cluster_stats)
        }
        
        return error_rates
        
    def _calculate_connection_error_rate(self, stats: Dict) -> float:
        """è®¡ç®—è¿æ¥é”™è¯¯ç‡"""
        total_connections = stats.get('object_totals', {}).get('connections', 0)
        failed_connections = stats.get('connection_churn_rates', {}).get('connection_closed_details', {}).get('rate', 0)
        
        if total_connections > 0:
            return (failed_connections / total_connections) * 100
        return 0.0
        
    def _calculate_message_error_rate(self, stats: Dict) -> float:
        """è®¡ç®—æ¶ˆæ¯é”™è¯¯ç‡"""
        total_messages = stats.get('queue_totals', {}).get('messages', 0)
        failed_messages = stats.get('message_stats', {}).get('redeliver', 0)
        
        if total_messages > 0:
            return (failed_messages / total_messages) * 100
        return 0.0
```

## ğŸš¨ å‘Šè­¦ç®¡ç†

### 1. å‘Šè­¦é…ç½®

```yaml
# alertmanager.yml
global:
  smtp_smarthost: 'smtp.gmail.com:587'
  smtp_from: 'rabbitmq-alerts@example.com'
  smtp_auth_username: 'your-email@gmail.com'
  smtp_auth_password: 'your-app-password'

route:
  group_by: ['alertname']
  group_wait: 10s
  group_interval: 10s
  repeat_interval: 1h
  receiver: 'web.hook'
  routes:
  - match:
      severity: critical
    receiver: 'critical-alerts'
  - match:
      severity: warning
    receiver: 'warning-alerts'

receivers:
- name: 'critical-alerts'
  email_configs:
  - to: 'ops-team@example.com'
    subject: '[CRITICAL] RabbitMQ Alert - {{ .GroupLabels.alertname }}'
    body: |
      {{ range .Alerts }}
      Alert: {{ .Annotations.summary }}
      Description: {{ .Annotations.description }}
      Instance: {{ .Labels.instance }}
      Severity: {{ .Labels.severity }}
      {{ end }}
  
  slack_configs:
  - api_url: 'https://hooks.slack.com/services/YOUR/SLACK/WEBHOOK'
    channel: '#ops-alerts'
    title: 'RabbitMQ Critical Alert'
    text: |
      {{ range .Alerts }}
      ğŸš¨ *{{ .Labels.alertname }}*
      *Instance:* {{ .Labels.instance }}
      *Severity:* {{ .Labels.severity }}
      *Description:* {{ .Annotations.description }}
      {{ end }}

- name: 'warning-alerts'
  email_configs:
  - to: 'dev-team@example.com'
    subject: '[WARNING] RabbitMQ Alert - {{ .GroupLabels.alertname }}'
    body: |
      {{ range .Alerts }}
      Alert: {{ .Annotations.summary }}
      Description: {{ .Annotations.description }}
      Instance: {{ .Labels.instance }}
      Severity: {{ .Labels.severity }}
      {{ end }}
```

### 2. æ™ºèƒ½å‘Šè­¦

```python
import time
from datetime import datetime, timedelta
from typing import Dict, List

class SmartAlertManager:
    """æ™ºèƒ½å‘Šè­¦ç®¡ç†å™¨"""
    
    def __init__(self):
        self.alert_history = defaultdict(list)
        self.suppression_windows = defaultdict(lambda: timedelta(minutes=5))
        
    def should_send_alert(self, alert_name: str, instance: str, 
                         severity: str, value: float) -> bool:
        """åˆ¤æ–­æ˜¯å¦åº”è¯¥å‘é€å‘Šè­¦"""
        key = f"{alert_name}:{instance}"
        current_time = datetime.now()
        
        # è·å–å†å²è®°å½•
        history = self.alert_history[key]
        
        # æ£€æŸ¥æ˜¯å¦åœ¨æœ€è¿‘å‘é€è¿‡å‘Šè­¦
        if history:
            last_alert = history[-1]
            time_since_last = current_time - last_alert['timestamp']
            
            # å¦‚æœåœ¨æŠ‘åˆ¶çª—å£å†…ï¼Œä¸å‘é€å‘Šè­¦
            if time_since_last < self.suppression_windows[key]:
                return False
                
            # æ£€æŸ¥æ˜¯å¦æ˜¯é‡å¤å‘Šè­¦
            if self._is_duplicate_alert(last_alert, value):
                return False
                
            # æ£€æŸ¥æ˜¯å¦æ˜¯è¯¯æŠ¥
            if self._is_false_positive(alert_name, instance, value):
                return False
                
        return True
        
    def _is_duplicate_alert(self, last_alert: Dict, current_value: float) -> bool:
        """æ£€æŸ¥æ˜¯å¦æ˜¯é‡å¤å‘Šè­¦"""
        # å¦‚æœå€¼å˜åŒ–ä¸å¤§ï¼Œè®¤ä¸ºæ˜¯é‡å¤å‘Šè­¦
        threshold = 0.1  # 10%çš„å˜åŒ–é˜ˆå€¼
        value_diff = abs(current_value - last_alert['value']) / last_alert['value']
        return value_diff < threshold
        
    def _is_false_positive(self, alert_name: str, instance: str, value: float) -> bool:
        """æ£€æŸ¥æ˜¯å¦æ˜¯è¯¯æŠ¥"""
        # æ£€æŸ¥å†å²è¶‹åŠ¿
        key = f"{alert_name}:{instance}"
        history = self.alert_history[key]
        
        if len(history) < 3:
            return False  # æ•°æ®ä¸è¶³ï¼Œä¸è®¤ä¸ºæ˜¯è¯¯æŠ¥
            
        # æ£€æŸ¥æ˜¯å¦æ˜¯ç¬æ—¶å³°å€¼
        recent_values = [h['value'] for h in history[-3:]]
        avg_value = sum(recent_values) / len(recent_values)
        
        # å¦‚æœå½“å‰å€¼è¿œé«˜äºå¹³å‡å€¼ï¼Œå¯èƒ½æ˜¯ç¬æ—¶å³°å€¼
        if value > avg_value * 2:
            return True
            
        return False
        
    def record_alert(self, alert_name: str, instance: str, 
                    severity: str, value: float):
        """è®°å½•å‘Šè­¦"""
        key = f"{alert_name}:{instance}"
        self.alert_history[key].append({
            'timestamp': datetime.now(),
            'severity': severity,
            'value': value
        })
        
        # ä¿æŒå†å²è®°å½•é•¿åº¦
        if len(self.alert_history[key]) > 100:
            self.alert_history[key] = self.alert_history[key][-100:]
```

## ğŸ”§ è¿ç»´è‡ªåŠ¨åŒ–

### 1. è‡ªåŠ¨æ‰©ç¼©å®¹

```python
import docker
import kubernetes
from kubernetes import client, config

class AutoScaler:
    """è‡ªåŠ¨æ‰©ç¼©å®¹ç®¡ç†å™¨"""
    
    def __init__(self, rabbitmq_api_url: str, k8s_config: str = None):
        self.rabbitmq_api_url = rabbitmq_api_url
        
        if k8s_config:
            config.load_kube_config(config_file=k8s_config)
        else:
            config.load_incluster_config()
            
        self.v1 = client.AppsV1Api()
        self.metrics_client = client.CustomObjectsApi()
        
    def check_scaling_conditions(self, deployment_name: str, namespace: str) -> Dict:
        """æ£€æŸ¥æ‰©ç¼©å®¹æ¡ä»¶"""
        conditions = {
            'should_scale_up': False,
            'should_scale_down': False,
            'reason': '',
            'metrics': {}
        }
        
        # è·å–RabbitMQæŒ‡æ ‡
        metrics = self._get_rabbitmq_metrics()
        conditions['metrics'] = metrics
        
        # è·å–å½“å‰å‰¯æœ¬æ•°
        current_replicas = self._get_current_replicas(deployment_name, namespace)
        
        # æ£€æŸ¥æ‰©å®¹æ¡ä»¶
        if self._should_scale_up(metrics, current_replicas):
            conditions['should_scale_up'] = True
            conditions['reason'] = f"High load detected: {metrics['queue_depth']} messages, {metrics['connection_count']} connections"
            
        # æ£€æŸ¥ç¼©å®¹æ¡ä»¶
        elif self._should_scale_down(metrics, current_replicas):
            conditions['should_scale_down'] = True
            conditions['reason'] = f"Low load detected: {metrics['queue_depth']} messages, {metrics['connection_count']} connections"
            
        return conditions
        
    def _get_rabbitmq_metrics(self) -> Dict:
        """è·å–RabbitMQæŒ‡æ ‡"""
        try:
            # è¿™é‡Œåº”è¯¥è°ƒç”¨RabbitMQ APIè·å–æŒ‡æ ‡
            # ç®€åŒ–å®ç°ï¼Œè¿”å›æ¨¡æ‹Ÿæ•°æ®
            return {
                'queue_depth': 15000,  # é˜Ÿåˆ—æ·±åº¦
                'connection_count': 500,  # è¿æ¥æ•°
                'message_rate': 1000,  # æ¶ˆæ¯é€Ÿç‡
                'error_rate': 0.01  # é”™è¯¯ç‡
            }
        except Exception as e:
            logging.error(f"è·å–RabbitMQæŒ‡æ ‡å¤±è´¥: {e}")
            return {}
            
    def _should_scale_up(self, metrics: Dict, current_replicas: int) -> bool:
        """åˆ¤æ–­æ˜¯å¦åº”è¯¥æ‰©å®¹"""
        # æ‰©å®¹æ¡ä»¶
        scale_up_conditions = [
            metrics.get('queue_depth', 0) > 10000,  # é˜Ÿåˆ—æ·±åº¦è¶…è¿‡10000
            metrics.get('connection_count', 0) > 1000,  # è¿æ¥æ•°è¶…è¿‡1000
            metrics.get('message_rate', 0) > 2000,  # æ¶ˆæ¯é€Ÿç‡è¶…è¿‡2000/s
            metrics.get('error_rate', 0) > 0.05  # é”™è¯¯ç‡è¶…è¿‡5%
        ]
        
        # è‡³å°‘æ»¡è¶³2ä¸ªæ¡ä»¶æ‰æ‰©å®¹
        satisfied_conditions = sum(scale_up_conditions)
        max_replicas = 10
        
        return satisfied_conditions >= 2 and current_replicas < max_replicas
        
    def _should_scale_down(self, metrics: Dict, current_replicas: int) -> bool:
        """åˆ¤æ–­æ˜¯å¦åº”è¯¥ç¼©å®¹"""
        # ç¼©å®¹æ¡ä»¶
        scale_down_conditions = [
            metrics.get('queue_depth', 0) < 100,  # é˜Ÿåˆ—æ·±åº¦å°äº100
            metrics.get('connection_count', 0) < 10,  # è¿æ¥æ•°å°äº10
            metrics.get('message_rate', 0) < 100,  # æ¶ˆæ¯é€Ÿç‡å°äº100/s
            metrics.get('error_rate', 0) < 0.001  # é”™è¯¯ç‡å°äº0.1%
        ]
        
        # æ‰€æœ‰æ¡ä»¶éƒ½æ»¡è¶³æ‰ç¼©å®¹
        satisfied_conditions = sum(scale_down_conditions)
        min_replicas = 2
        
        return satisfied_conditions >= 3 and current_replicas > min_replicas
        
    def scale_deployment(self, deployment_name: str, namespace: str, replicas: int) -> bool:
        """æ‰©ç¼©å®¹éƒ¨ç½²"""
        try:
            # è·å–å½“å‰éƒ¨ç½²
            deployment = self.v1.read_namespaced_deployment(deployment_name, namespace)
            
            # æ›´æ–°å‰¯æœ¬æ•°
            deployment.spec.replicas = replicas
            
            # åº”ç”¨æ›´æ–°
            self.v1.patch_namespaced_deployment(
                deployment_name, 
                namespace, 
                deployment
            )
            
            logging.info(f"æˆåŠŸå°† {deployment_name} æ‰©ç¼©å®¹åˆ° {replicas} å‰¯æœ¬")
            return True
            
        except Exception as e:
            logging.error(f"æ‰©ç¼©å®¹å¤±è´¥: {e}")
            return False
```

### 2. è‡ªåŠ¨æ•…éšœæ¢å¤

```python
import time
import docker
from datetime import datetime, timedelta

class AutoRecovery:
    """è‡ªåŠ¨æ•…éšœæ¢å¤ç®¡ç†å™¨"""
    
    def __init__(self, rabbitmq_api_url: str, max_restart_attempts: int = 3):
        self.rabbitmq_api_url = rabbitmq_api_url
        self.max_restart_attempts = max_restart_attempts
        self.restart_history = defaultdict(list)
        self.docker_client = docker.from_env()
        
    def check_health_and_recover(self, container_name: str) -> bool:
        """æ£€æŸ¥å¥åº·çŠ¶æ€å¹¶æ‰§è¡Œæ¢å¤"""
        try:
            # è·å–å®¹å™¨çŠ¶æ€
            container = self.docker_client.containers.get(container_name)
            
            # æ£€æŸ¥å®¹å™¨çŠ¶æ€
            if container.status != 'running':
                return self._restart_container(container_name)
                
            # æ£€æŸ¥RabbitMQå¥åº·çŠ¶æ€
            if not self._check_rabbitmq_health():
                return self._restart_container(container_name)
                
            # æ£€æŸ¥é›†ç¾¤çŠ¶æ€
            if not self._check_cluster_health():
                return self._repair_cluster()
                
            return True
            
        except Exception as e:
            logging.error(f"å¥åº·æ£€æŸ¥å’Œæ¢å¤å¤±è´¥: {e}")
            return False
            
    def _check_rabbitmq_health(self) -> bool:
        """æ£€æŸ¥RabbitMQå¥åº·çŠ¶æ€"""
        try:
            # è¿™é‡Œåº”è¯¥è°ƒç”¨RabbitMQå¥åº·æ£€æŸ¥API
            # ç®€åŒ–å®ç°
            return True
        except Exception as e:
            logging.error(f"RabbitMQå¥åº·æ£€æŸ¥å¤±è´¥: {e}")
            return False
            
    def _check_cluster_health(self) -> bool:
        """æ£€æŸ¥é›†ç¾¤å¥åº·çŠ¶æ€"""
        try:
            # è¿™é‡Œåº”è¯¥æ£€æŸ¥é›†ç¾¤çŠ¶æ€
            # ç®€åŒ–å®ç°
            return True
        except Exception as e:
            logging.error(f"é›†ç¾¤å¥åº·æ£€æŸ¥å¤±è´¥: {e}")
            return False
            
    def _restart_container(self, container_name: str) -> bool:
        """é‡å¯å®¹å™¨"""
        try:
            # æ£€æŸ¥é‡å¯æ¬¡æ•°
            key = container_name
            current_time = datetime.now()
            
            # æ¸…ç†è¿‡æœŸè®°å½•ï¼ˆ24å°æ—¶å‰ï¼‰
            self.restart_history[key] = [
                restart_time for restart_time in self.restart_history[key]
                if current_time - restart_time < timedelta(hours=24)
            ]
            
            # æ£€æŸ¥é‡å¯æ¬¡æ•°é™åˆ¶
            if len(self.restart_history[key]) >= self.max_restart_attempts:
                logging.warning(f"å®¹å™¨ {container_name} é‡å¯æ¬¡æ•°è¿‡å¤šï¼Œè·³è¿‡é‡å¯")
                return False
                
            # é‡å¯å®¹å™¨
            container = self.docker_client.containers.get(container_name)
            container.restart()
            
            # è®°å½•é‡å¯æ—¶é—´
            self.restart_history[key].append(current_time)
            
            logging.info(f"æˆåŠŸé‡å¯å®¹å™¨: {container_name}")
            return True
            
        except Exception as e:
            logging.error(f"é‡å¯å®¹å™¨å¤±è´¥: {e}")
            return False
            
    def _repair_cluster(self) -> bool:
        """ä¿®å¤é›†ç¾¤"""
        try:
            # è¿™é‡Œåº”è¯¥å®ç°é›†ç¾¤ä¿®å¤é€»è¾‘
            # ç®€åŒ–å®ç°
            logging.info("é›†ç¾¤ä¿®å¤å®Œæˆ")
            return True
        except Exception as e:
            logging.error(f"é›†ç¾¤ä¿®å¤å¤±è´¥: {e}")
            return False
```

## ğŸ“Š å®¹é‡è§„åˆ’

### 1. å®¹é‡è¯„ä¼°

```python
import pandas as pd
from datetime import datetime, timedelta
from typing import Dict, List, Tuple
import numpy as np

class CapacityPlanner:
    """å®¹é‡è§„åˆ’å™¨"""
    
    def __init__(self, rabbitmq_api_url: str, historical_data_days: int = 30):
        self.rabbitmq_api_url = rabbitmq_api_url
        self.historical_data_days = historical_data_days
        self.capacity_data = []
        
    def analyze_current_capacity(self) -> Dict:
        """åˆ†æå½“å‰å®¹é‡"""
        current_metrics = self._get_current_metrics()
        
        capacity_analysis = {
            'timestamp': datetime.now().isoformat(),
            'current_utilization': self._calculate_utilization(current_metrics),
            'capacity_remaining': self._calculate_remaining_capacity(current_metrics),
            'bottlenecks': self._identify_bottlenecks(current_metrics),
            'recommendations': self._generate_recommendations(current_metrics)
        }
        
        return capacity_analysis
        
    def predict_future_capacity(self, forecast_days: int = 30) -> Dict:
        """é¢„æµ‹æœªæ¥å®¹é‡éœ€æ±‚"""
        # è·å–å†å²æ•°æ®
        historical_data = self._get_historical_metrics(self.historical_data_days)
        
        # ä½¿ç”¨ç®€å•çº¿æ€§å›å½’é¢„æµ‹
        predictions = self._linear_forecast(historical_data, forecast_days)
        
        return {
            'forecast_period_days': forecast_days,
            'predicted_metrics': predictions,
            'capacity_warnings': self._generate_capacity_warnings(predictions),
            'scaling_recommendations': self._generate_scaling_recommendations(predictions)
        }
        
    def _get_current_metrics(self) -> Dict:
        """è·å–å½“å‰æŒ‡æ ‡"""
        # è¿™é‡Œåº”è¯¥è°ƒç”¨RabbitMQ APIè·å–æŒ‡æ ‡
        # ç®€åŒ–å®ç°
        return {
            'connection_count': 500,
            'queue_count': 100,
            'message_rate': 1000,
            'memory_usage': 2.5,  # GB
            'disk_usage': 10.0,  # GB
            'cpu_usage': 45.0  # percentage
        }
        
    def _calculate_utilization(self, metrics: Dict) -> Dict:
        """è®¡ç®—åˆ©ç”¨ç‡"""
        # åŸºäºç»éªŒé˜ˆå€¼è®¡ç®—åˆ©ç”¨ç‡
        utilization = {
            'connection_utilization': min(metrics['connection_count'] / 1000 * 100, 100),
            'queue_utilization': min(metrics['queue_count'] / 500 * 100, 100),
            'message_rate_utilization': min(metrics['message_rate'] / 5000 * 100, 100),
            'memory_utilization': min(metrics['memory_usage'] / 4.0 * 100, 100),
            'disk_utilization': min(metrics['disk_usage'] / 50.0 * 100, 100),
            'cpu_utilization': metrics['cpu_usage']
        }
        
        # è®¡ç®—æ€»ä½“åˆ©ç”¨ç‡
        utilization['overall_utilization'] = np.mean([
            utilization['connection_utilization'],
            utilization['queue_utilization'],
            utilization['message_rate_utilization'],
            utilization['memory_utilization'],
            utilization['disk_utilization'],
            utilization['cpu_utilization']
        ])
        
        return utilization
        
    def _identify_bottlenecks(self, metrics: Dict) -> List[str]:
        """è¯†åˆ«ç“¶é¢ˆ"""
        bottlenecks = []
        
        # æ£€æŸ¥å„ç§ç“¶é¢ˆæ¡ä»¶
        if metrics['connection_count'] > 800:
            bottlenecks.append('High connection count')
        if metrics['queue_count'] > 400:
            bottlenecks.append('High queue count')
        if metrics['message_rate'] > 4000:
            bottlenecks.append('High message rate')
        if metrics['memory_usage'] > 3.2:
            bottlenecks.append('High memory usage')
        if metrics['disk_usage'] > 40:
            bottlenecks.append('High disk usage')
        if metrics['cpu_usage'] > 80:
            bottlenecks.append('High CPU usage')
            
        return bottlenecks
        
    def _generate_recommendations(self, metrics: Dict) -> List[str]:
        """ç”Ÿæˆä¼˜åŒ–å»ºè®®"""
        recommendations = []
        
        # åŸºäºæŒ‡æ ‡ç”Ÿæˆå»ºè®®
        if metrics['connection_count'] > 800:
            recommendations.append('Consider connection pooling or load balancing')
        if metrics['queue_count'] > 400:
            recommendations.append('Consider queue consolidation or partitioning')
        if metrics['message_rate'] > 4000:
            recommendations.append('Consider message batching or rate limiting')
        if metrics['memory_usage'] > 3.2:
            recommendations.append('Consider memory optimization or scaling')
        if metrics['disk_usage'] > 40:
            recommendations.append('Consider disk cleanup or scaling')
        if metrics['cpu_usage'] > 80:
            recommendations.append('Consider CPU optimization or scaling')
            
        return recommendations
```

### 2. èµ„æºä¼˜åŒ–å»ºè®®

```python
class ResourceOptimizer:
    """èµ„æºä¼˜åŒ–å™¨"""
    
    def __init__(self, rabbitmq_api_url: str):
        self.rabbitmq_api_url = rabbitmq_api_url
        
    def analyze_memory_usage(self) -> Dict:
        """åˆ†æå†…å­˜ä½¿ç”¨"""
        # è·å–å†…å­˜ä½¿ç”¨ç»Ÿè®¡
        memory_stats = self._get_memory_stats()
        
        optimization_suggestions = {
            'current_memory_usage': memory_stats,
            'memory_optimization_suggestions': self._generate_memory_suggestions(memory_stats),
            'queue_memory_analysis': self._analyze_queue_memory(memory_stats),
            'connection_memory_analysis': self._analyze_connection_memory(memory_stats)
        }
        
        return optimization_suggestions
        
    def analyze_disk_usage(self) -> Dict:
        """åˆ†æç£ç›˜ä½¿ç”¨"""
        disk_stats = self._get_disk_stats()
        
        optimization_suggestions = {
            'current_disk_usage': disk_stats,
            'disk_optimization_suggestions': self._generate_disk_suggestions(disk_stats),
            'log_file_analysis': self._analyze_log_files(disk_stats),
            'message_persistence_analysis': self._analyze_message_persistence(disk_stats)
        }
        
        return optimization_suggestions
        
    def _generate_memory_suggestions(self, stats: Dict) -> List[str]:
        """ç”Ÿæˆå†…å­˜ä¼˜åŒ–å»ºè®®"""
        suggestions = []
        
        # åŸºäºå†…å­˜ç»Ÿè®¡ç”Ÿæˆå»ºè®®
        if stats.get('queue_memory', 0) > 1024 * 1024 * 1024:  # 1GB
            suggestions.append('Consider reducing queue memory usage')
        if stats.get('connection_memory', 0) > 512 * 1024 * 1024:  # 512MB
            suggestions.append('Consider reducing connection memory usage')
        if stats.get('message_memory', 0) > 2 * 1024 * 1024 * 1024:  # 2GB
            suggestions.append('Consider reducing message memory usage')
            
        return suggestions
        
    def _generate_disk_suggestions(self, stats: Dict) -> List[str]:
        """ç”Ÿæˆç£ç›˜ä¼˜åŒ–å»ºè®®"""
        suggestions = []
        
        # åŸºäºç£ç›˜ç»Ÿè®¡ç”Ÿæˆå»ºè®®
        if stats.get('log_size', 0) > 10 * 1024 * 1024 * 1024:  # 10GB
            suggestions.append('Consider log rotation and cleanup')
        if stats.get('message_store_size', 0) > 50 * 1024 * 1024 * 1024:  # 50GB
            suggestions.append('Consider message store cleanup')
        if stats.get('queue_index_size', 0) > 5 * 1024 * 1024 * 1024:  # 5GB
            suggestions.append('Consider queue index optimization')
            
        return suggestions
```

## ğŸ¯ æœ€ä½³å®è·µæ€»ç»“

### 1. ç›‘æ§ç­–ç•¥

1. **åˆ†å±‚ç›‘æ§**: ç³»ç»Ÿå±‚ã€å®¹å™¨å±‚ã€åº”ç”¨å±‚ã€ä¸šåŠ¡å±‚
2. **å…³é”®æŒ‡æ ‡**: è¿æ¥æ•°ã€é˜Ÿåˆ—æ·±åº¦ã€æ¶ˆæ¯é€Ÿç‡ã€èµ„æºä½¿ç”¨ç‡
3. **å®æ—¶ç›‘æ§**: å®æ—¶æ•°æ®é‡‡é›†å’Œå‘Šè­¦
4. **å†å²åˆ†æ**: è¶‹åŠ¿åˆ†æå’Œå®¹é‡è§„åˆ’

### 2. å‘Šè­¦ç­–ç•¥

1. **åˆ†çº§å‘Šè­¦**: æ ¹æ®ä¸¥é‡ç¨‹åº¦åˆ†çº§å¤„ç†
2. **æ™ºèƒ½æŠ‘åˆ¶**: é¿å…é‡å¤å’Œè¯¯æŠ¥å‘Šè­¦
3. **å¤šæ¸ é“é€šçŸ¥**: é‚®ä»¶ã€çŸ­ä¿¡ã€Slackç­‰
4. **è‡ªåŠ¨å“åº”**: è‡ªåŠ¨æ•…éšœæ¢å¤å’Œæ‰©ç¼©å®¹

### 3. è¿ç»´è‡ªåŠ¨åŒ–

1. **è‡ªåŠ¨åŒ–éƒ¨ç½²**: CI/CDé›†æˆ
2. **è‡ªåŠ¨ç›‘æ§**: ç›‘æ§å·¥å…·è‡ªåŠ¨é…ç½®
3. **è‡ªåŠ¨æ¢å¤**: æ•…éšœè‡ªåŠ¨æ£€æµ‹å’Œæ¢å¤
4. **è‡ªåŠ¨ä¼˜åŒ–**: æ€§èƒ½è‡ªåŠ¨è°ƒä¼˜

### 4. å®¹é‡è§„åˆ’

1. **å®šæœŸè¯„ä¼°**: å®šæœŸå®¹é‡è¯„ä¼°å’Œé¢„æµ‹
2. **å¼¹æ€§æ‰©å±•**: åŸºäºè´Ÿè½½çš„è‡ªåŠ¨æ‰©ç¼©å®¹
3. **èµ„æºä¼˜åŒ–**: æŒç»­çš„èµ„æºä¼˜åŒ–å»ºè®®
4. **æˆæœ¬æ§åˆ¶**: å¹³è¡¡æ€§èƒ½å’Œæˆæœ¬çš„ä¼˜åŒ–

é€šè¿‡å®Œå–„çš„ç›‘æ§ä¸è¿ç»´ä½“ç³»ï¼Œå¯ä»¥ç¡®ä¿RabbitMQé›†ç¾¤çš„ç¨³å®šè¿è¡Œï¼ŒåŠæ—¶å‘ç°å’Œè§£å†³é—®é¢˜ï¼Œæé«˜ç³»ç»Ÿçš„å¯é æ€§å’Œå¯ç”¨æ€§ã€‚