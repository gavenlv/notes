# Day 10: ç›‘æ§ä¸æ—¥å¿— - å­¦ä¹ èµ„æ–™

## ğŸ“˜ æ ¸å¿ƒæ¦‚å¿µ

### Airflowæ—¥å¿—ç³»ç»Ÿæ¶æ„

Airflowçš„æ—¥å¿—ç³»ç»Ÿè®¾è®¡ç”¨äºè·Ÿè¸ªDAGæ‰§è¡Œã€ä»»åŠ¡è¿è¡Œå’Œç³»ç»Ÿäº‹ä»¶ã€‚ç†è§£å…¶æ¶æ„å¯¹äºæœ‰æ•ˆç›‘æ§å’Œæ•…éšœæ’æŸ¥è‡³å…³é‡è¦ã€‚

#### æ—¥å¿—ç±»å‹
1. **ä»»åŠ¡æ—¥å¿— (Task Logs)**: æ¯ä¸ªä»»åŠ¡å®ä¾‹æ‰§è¡Œæ—¶ç”Ÿæˆçš„æ—¥å¿—
2. **è°ƒåº¦å™¨æ—¥å¿— (Scheduler Logs)**: è°ƒåº¦å™¨è¿è¡Œæ—¶çš„æ—¥å¿—
3. **WebæœåŠ¡å™¨æ—¥å¿— (Webserver Logs)**: Webç•Œé¢è®¿é—®å’Œæ“ä½œæ—¥å¿—
4. **å·¥ä½œè€…æ—¥å¿— (Worker Logs)**: åœ¨åˆ†å¸ƒå¼ç¯å¢ƒä¸­å·¥ä½œè€…èŠ‚ç‚¹çš„æ—¥å¿—

#### æ—¥å¿—å­˜å‚¨
- **æœ¬åœ°å­˜å‚¨**: é»˜è®¤æƒ…å†µä¸‹ï¼Œæ—¥å¿—å­˜å‚¨åœ¨æœ¬åœ°æ–‡ä»¶ç³»ç»Ÿä¸­
- **è¿œç¨‹å­˜å‚¨**: å¯é…ç½®ä¸ºå­˜å‚¨åœ¨S3ã€GCSã€Azure Blobç­‰äº‘å­˜å‚¨ä¸­
- **æ•°æ®åº“å­˜å‚¨**: æŸäº›æƒ…å†µä¸‹æ—¥å¿—å¯ä»¥å­˜å‚¨åœ¨æ•°æ®åº“ä¸­

### ç›‘æ§æŒ‡æ ‡ä½“ç³»

Airflowæä¾›äº†ä¸°å¯Œçš„ç›‘æ§æŒ‡æ ‡ï¼Œå¯ä»¥å¸®åŠ©ä½ äº†è§£ç³»ç»Ÿå¥åº·çŠ¶å†µå’Œæ€§èƒ½è¡¨ç°ã€‚

#### æ ¸å¿ƒæŒ‡æ ‡ç±»åˆ«
1. **DAGæŒ‡æ ‡**: DAGæ‰§è¡ŒæˆåŠŸç‡ã€æ‰§è¡Œæ—¶é—´ç­‰
2. **ä»»åŠ¡æŒ‡æ ‡**: ä»»åŠ¡æ‰§è¡ŒçŠ¶æ€ã€é‡è¯•æ¬¡æ•°ç­‰
3. **ç³»ç»ŸæŒ‡æ ‡**: å†…å­˜ä½¿ç”¨ã€CPUä½¿ç”¨ç‡ç­‰
4. **è°ƒåº¦å™¨æŒ‡æ ‡**: è°ƒåº¦å»¶è¿Ÿã€DAGå¤„ç†é€Ÿç‡ç­‰

#### æŒ‡æ ‡æ”¶é›†æ–¹å¼
- **StatsD**: é€šè¿‡StatsDåè®®æ”¶é›†æŒ‡æ ‡
- **Prometheus**: é€šè¿‡Prometheusç«¯ç‚¹æš´éœ²æŒ‡æ ‡
- **è‡ªå®šä¹‰æ”¶é›†å™¨**: ç¼–å†™è‡ªå®šä¹‰ä»£ç æ”¶é›†ç‰¹å®šæŒ‡æ ‡

## ğŸ› ï¸ é…ç½®æŒ‡å—

### æ—¥å¿—é…ç½®

Airflowçš„æ—¥å¿—é…ç½®ä¸»è¦åœ¨`airflow.cfg`æ–‡ä»¶ä¸­è¿›è¡Œï¼š

```ini
[logging]
# æ—¥å¿—çº§åˆ«
logging_level = INFO

# æ—¥å¿—æ ¼å¼
log_format = [%%(asctime)s] {%%(filename)s:%%(lineno)d} %%(levelname)s - %%(message)s
simple_log_format = %%(asctime)s %%(levelname)s - %%(message)s

# æ—¥å¿—æ–‡ä»¶ä½ç½®
base_log_folder = /usr/local/airflow/logs
dag_processor_manager_log_location = /usr/local/airflow/logs/dag_processor_manager/dag_processor_manager.log

# ä»»åŠ¡æ—¥å¿—ç›¸å…³é…ç½®
task_log_reader = task
# æ˜¯å¦åœ¨Web UIä¸­æ˜¾ç¤ºæ—¥å¿—
show_log_confidence = True

# è¿œç¨‹æ—¥å¿—é…ç½®
remote_logging = False
remote_base_log_folder =
remote_log_conn_id =
delete_local_logs = False
```

### ç›‘æ§é…ç½®

ç›‘æ§é…ç½®é€šå¸¸æ¶‰åŠæŒ‡æ ‡æ”¶é›†å’Œå‘Šè­¦è®¾ç½®ï¼š

```ini
[scheduler]
# è°ƒåº¦å™¨ç›¸å…³æŒ‡æ ‡
scheduler_heartbeat_sec = 5
scheduler_health_check_threshold = 30

[metrics]
# æŒ‡æ ‡æ”¶é›†é…ç½®
metrics_allow_list = 
metrics_block_list = 
statsd_on = False
statsd_host = localhost
statsd_port = 8125
statsd_prefix = airflow

[smtp]
# SMTPé…ç½®ç”¨äºå‘é€å‘Šè­¦é‚®ä»¶
smtp_host = localhost
smtp_starttls = True
smtp_ssl = False
smtp_port = 25
smtp_mail_from = airflow@example.com
```

## ğŸ“Š å®è·µç¤ºä¾‹

### æ—¥å¿—åˆ†æè„šæœ¬

```python
import os
import re
from datetime import datetime, timedelta
from collections import defaultdict

class AirflowLogAnalyzer:
    def __init__(self, log_directory):
        self.log_directory = log_directory
        self.error_patterns = [
            r'ERROR',
            r'FAILED',
            r'Exception',
            r'Traceback'
        ]
    
    def analyze_task_logs(self, dag_id, days=7):
        """åˆ†ææŒ‡å®šDAGçš„ä»»åŠ¡æ—¥å¿—"""
        cutoff_date = datetime.now() - timedelta(days=days)
        error_summary = defaultdict(int)
        
        dag_log_path = os.path.join(self.log_directory, dag_id)
        if not os.path.exists(dag_log_path):
            print(f"No logs found for DAG: {dag_id}")
            return {}
        
        for root, dirs, files in os.walk(dag_log_path):
            for file in files:
                if file.endswith('.log'):
                    file_path = os.path.join(root, file)
                    file_date = datetime.fromtimestamp(os.path.getmtime(file_path))
                    
                    if file_date >= cutoff_date:
                        self._analyze_log_file(file_path, error_summary)
        
        return dict(error_summary)
    
    def _analyze_log_file(self, file_path, error_summary):
        """åˆ†æå•ä¸ªæ—¥å¿—æ–‡ä»¶"""
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                for line in f:
                    for pattern in self.error_patterns:
                        if re.search(pattern, line):
                            error_summary[pattern] += 1
        except Exception as e:
            print(f"Error reading {file_path}: {e}")

# ä½¿ç”¨ç¤ºä¾‹
analyzer = AirflowLogAnalyzer('/usr/local/airflow/logs')
errors = analyzer.analyze_task_logs('example_dag', days=7)
print("Error summary:", errors)
```

### æŒ‡æ ‡æ”¶é›†å™¨

```python
from airflow.models import DagRun, TaskInstance
from airflow.utils.state import State
from datetime import datetime, timedelta
import statsd

class AirflowMetricsCollector:
    def __init__(self, statsd_host='localhost', statsd_port=8125, prefix='airflow'):
        self.statsd_client = statsd.StatsClient(statsd_host, statsd_port, prefix=prefix)
    
    def collect_dag_metrics(self):
        """æ”¶é›†DAGç›¸å…³æŒ‡æ ‡"""
        # è·å–æœ€è¿‘24å°æ—¶çš„DAGè¿è¡Œæƒ…å†µ
        cutoff_time = datetime.now() - timedelta(hours=24)
        
        # æˆåŠŸçš„DAGè¿è¡Œ
        successful_runs = DagRun.query.filter(
            DagRun.state == State.SUCCESS,
            DagRun.start_date >= cutoff_time
        ).count()
        
        # å¤±è´¥çš„DAGè¿è¡Œ
        failed_runs = DagRun.query.filter(
            DagRun.state == State.FAILED,
            DagRun.start_date >= cutoff_time
        ).count()
        
        # å‘é€æŒ‡æ ‡åˆ°StatsD
        self.statsd_client.gauge('dag.runs.successful', successful_runs)
        self.statsd_client.gauge('dag.runs.failed', failed_runs)
        
        if successful_runs + failed_runs > 0:
            success_rate = successful_runs / (successful_runs + failed_runs) * 100
            self.statsd_client.gauge('dag.runs.success_rate', success_rate)
    
    def collect_task_metrics(self):
        """æ”¶é›†ä»»åŠ¡ç›¸å…³æŒ‡æ ‡"""
        cutoff_time = datetime.now() - timedelta(hours=24)
        
        # è·å–ä»»åŠ¡å®ä¾‹çŠ¶æ€ç»Ÿè®¡
        task_states = TaskInstance.query.with_entities(
            TaskInstance.state,
            db.func.count(TaskInstance.state)
        ).filter(
            TaskInstance.start_date >= cutoff_time
        ).group_by(TaskInstance.state).all()
        
        for state, count in task_states:
            if state:
                self.statsd_client.gauge(f'task.instances.{state.lower()}', count)

# ä½¿ç”¨ç¤ºä¾‹
collector = AirflowMetricsCollector()
collector.collect_dag_metrics()
collector.collect_task_metrics()
```

### å‘Šè­¦ç³»ç»Ÿ

```python
from airflow.models import DagRun
from airflow.utils.state import State
from airflow.utils.email import send_email
import logging

class AirflowAlertingSystem:
    def __init__(self, smtp_config):
        self.smtp_config = smtp_config
        self.logger = logging.getLogger(__name__)
    
    def check_dag_failures(self, dag_id, threshold=3):
        """æ£€æŸ¥DAGå¤±è´¥æ¬¡æ•°æ˜¯å¦è¶…è¿‡é˜ˆå€¼"""
        # è·å–æœ€è¿‘çš„DAGè¿è¡Œ
        recent_runs = DagRun.query.filter(
            DagRun.dag_id == dag_id
        ).order_by(DagRun.execution_date.desc()).limit(threshold).all()
        
        # ç»Ÿè®¡å¤±è´¥æ¬¡æ•°
        failed_count = sum(1 for run in recent_runs if run.state == State.FAILED)
        
        if failed_count >= threshold:
            self._send_failure_alert(dag_id, failed_count, recent_runs)
            return True
        return False
    
    def _send_failure_alert(self, dag_id, failed_count, recent_runs):
        """å‘é€å¤±è´¥å‘Šè­¦é‚®ä»¶"""
        subject = f"Airflow DAG Alert: {dag_id} has failed {failed_count} times"
        
        # æ„å»ºé‚®ä»¶å†…å®¹
        html_content = f"""
        <h2>Airflow DAG Failure Alert</h2>
        <p><strong>DAG ID:</strong> {dag_id}</p>
        <p><strong>Consecutive Failures:</strong> {failed_count}</p>
        <p><strong>Recent Runs:</strong></p>
        <ul>
        """
        
        for run in recent_runs:
            html_content += f"<li>{run.execution_date} - {run.state}</li>"
        
        html_content += """
        </ul>
        <p>Please check the Airflow UI and logs for more details.</p>
        """
        
        try:
            send_email(
                to=['admin@example.com'],
                subject=subject,
                html_content=html_content,
                smtp_host=self.smtp_config.get('host', 'localhost'),
                smtp_port=self.smtp_config.get('port', 25),
                smtp_user=self.smtp_config.get('user'),
                smtp_password=self.smtp_config.get('password')
            )
            self.logger.info(f"Alert email sent for DAG {dag_id}")
        except Exception as e:
            self.logger.error(f"Failed to send alert email: {e}")

# ä½¿ç”¨ç¤ºä¾‹
smtp_config = {
    'host': 'smtp.example.com',
    'port': 587,
    'user': 'airflow@example.com',
    'password': 'password'
}

alerting_system = AirflowAlertingSystem(smtp_config)
alerting_system.check_dag_failures('example_dag', threshold=3)
```

## ğŸ”§ æ•…éšœæ’æŸ¥æŠ€å·§

### å¸¸è§é—®é¢˜åŠè§£å†³æ–¹æ¡ˆ

1. **æ—¥å¿—ä¸æ˜¾ç¤ºåœ¨Web UIä¸­**
   - æ£€æŸ¥`airflow.cfg`ä¸­çš„æ—¥å¿—é…ç½®
   - ç¡®è®¤æ—¥å¿—æ–‡ä»¶æƒé™
   - éªŒè¯è¿œç¨‹æ—¥å¿—å­˜å‚¨é…ç½®

2. **ç›‘æ§æŒ‡æ ‡ä¸æ›´æ–°**
   - æ£€æŸ¥StatsDé…ç½®æ˜¯å¦æ­£ç¡®
   - ç¡®è®¤ç½‘ç»œè¿æ¥æ˜¯å¦æ­£å¸¸
   - éªŒè¯æŒ‡æ ‡æ”¶é›†ä»£ç æ˜¯å¦æ‰§è¡Œ

3. **å‘Šè­¦é‚®ä»¶å‘é€å¤±è´¥**
   - æ£€æŸ¥SMTPé…ç½®
   - ç¡®è®¤ç½‘ç»œè¿æ¥å’Œé˜²ç«å¢™è®¾ç½®
   - éªŒè¯é‚®ä»¶æœåŠ¡å™¨çŠ¶æ€

### è°ƒè¯•å·¥å…·

1. **Airflow CLIå‘½ä»¤**
   ```bash
   # æŸ¥çœ‹ä»»åŠ¡æ—¥å¿—
   airflow tasks test <dag_id> <task_id> <execution_date>
   
   # æŸ¥çœ‹DAGè¿è¡ŒçŠ¶æ€
   airflow dags list-runs -d <dag_id>
   
   # æŸ¥çœ‹ä»»åŠ¡å®ä¾‹çŠ¶æ€
   airflow tasks list <dag_id>
   ```

2. **æ—¥å¿—çº§åˆ«è°ƒæ•´**
   ```python
   import logging
   
   # ä¸´æ—¶æé«˜æ—¥å¿—çº§åˆ«ç”¨äºè°ƒè¯•
   logging.getLogger('airflow').setLevel(logging.DEBUG)
   ```

## ğŸ“š æ‰©å±•é˜…è¯»

### å®˜æ–¹èµ„æº
- [Airflow Logging Documentation](https://airflow.apache.org/docs/apache-airflow/stable/logging.html)
- [Airflow Metrics Documentation](https://airflow.apache.org/docs/apache-airflow/stable/logging-monitoring/metrics.html)
- [Airflow Alerts Documentation](https://airflow.apache.org/docs/apache-airflow/stable/logging-monitoring/alerts.html)

### ç¤¾åŒºèµ„æº
- [Airflow GitHub Issues](https://github.com/apache/airflow/issues)
- [Airflow Slack Community](https://apache-airflow.slack.com)
- [Stack Overflow Airflow Tag](https://stackoverflow.com/questions/tagged/apache-airflow)

### ç¬¬ä¸‰æ–¹å·¥å…·é›†æˆ
- **Prometheus**: ç”¨äºæŒ‡æ ‡æ”¶é›†å’Œå¯è§†åŒ–
- **Grafana**: ç”¨äºåˆ›å»ºç›‘æ§ä»ªè¡¨æ¿
- **ELK Stack**: ç”¨äºæ—¥å¿—æ”¶é›†å’Œåˆ†æ
- **Datadog**: ç”¨äºå…¨é¢çš„ç›‘æ§å’Œå‘Šè­¦