# ç¬¬5ç« ï¼šæ•°æ®ä¸æœºå™¨å­¦ä¹ æœåŠ¡

## ğŸ“š æœ¬ç« å¯¼å­¦

æ•°æ®åˆ†æå’Œæœºå™¨å­¦ä¹ æ˜¯Google Cloud Platformçš„æ ¸å¿ƒä¼˜åŠ¿é¢†åŸŸã€‚GCPæä¾›äº†å…¨é¢çš„æ•°æ®å¤„ç†ã€åˆ†æå’Œæœºå™¨å­¦ä¹ å¹³å°ï¼Œå¸®åŠ©ä¼ä¸šä»æ•°æ®ä¸­è·å–æ´å¯Ÿï¼Œæ„å»ºæ™ºèƒ½åº”ç”¨ã€‚æœ¬ç« å°†è¯¦ç»†ä»‹ç»GCPçš„æ•°æ®åˆ†æå’Œæœºå™¨å­¦ä¹ æœåŠ¡ï¼Œå¸®åŠ©æ‚¨æ„å»ºç«¯åˆ°ç«¯çš„æ•°æ®é©±åŠ¨è§£å†³æ–¹æ¡ˆã€‚

### ğŸ¯ å­¦ä¹ ç›®æ ‡

å®Œæˆæœ¬ç« å­¦ä¹ åï¼Œæ‚¨å°†èƒ½å¤Ÿï¼š

- ç†è§£GCPæ•°æ®åˆ†æå¹³å°çš„æ¶æ„å’Œç»„ä»¶
- æŒæ¡BigQueryæ•°æ®ä»“åº“çš„ä½¿ç”¨æ–¹æ³•å’Œä¼˜åŒ–æŠ€å·§
- å­¦ä¼šä½¿ç”¨æ•°æ®æµå¤„ç†æœåŠ¡ï¼ˆDataflow, Pub/Subï¼‰
- äº†è§£GCPæœºå™¨å­¦ä¹ å¹³å°çš„æ ¸å¿ƒåŠŸèƒ½å’Œå·¥å…·
- æŒæ¡æ¨¡å‹è®­ç»ƒã€éƒ¨ç½²å’Œç®¡ç†çš„å®Œæ•´æµç¨‹
- èƒ½å¤Ÿè®¾è®¡å’Œå®ç°æ•°æ®é©±åŠ¨çš„AIè§£å†³æ–¹æ¡ˆ

### ğŸ“– æœ¬ç« å†…å®¹æ¦‚è§ˆ

1. [GCPæ•°æ®åˆ†æå¹³å°æ¦‚è§ˆ](#1-gcpæ•°æ®åˆ†æå¹³å°æ¦‚è§ˆ)
2. [BigQueryæ•°æ®ä»“åº“](#2-bigqueryæ•°æ®ä»“åº“)
3. [å®æ—¶æ•°æ®å¤„ç†](#3-å®æ—¶æ•°æ®å¤„ç†)
4. [GCPæœºå™¨å­¦ä¹ å¹³å°](#4-gcpæœºå™¨å­¦ä¹ å¹³å°)
5. [æ¨¡å‹éƒ¨ç½²ä¸ç®¡ç†](#5-æ¨¡å‹éƒ¨ç½²ä¸ç®¡ç†)
6. [å®éªŒï¼šæ„å»ºæ•°æ®é©±åŠ¨AIåº”ç”¨](#6-å®éªŒæ„å»ºæ•°æ®é©±åŠ¨aiåº”ç”¨)

---

## 1. GCPæ•°æ®åˆ†æå¹³å°æ¦‚è§ˆ

### 1.1 æ•°æ®å¤„ç†ç”Ÿå‘½å‘¨æœŸ

GCPæä¾›äº†å®Œæ•´çš„æ•°æ®å¤„ç†ç”Ÿå‘½å‘¨æœŸæ”¯æŒï¼Œä»æ•°æ®é‡‡é›†åˆ°æ´å¯Ÿç”Ÿæˆï¼š

```mermaid
graph LR
    A[æ•°æ®æº] --> B[æ•°æ®é‡‡é›†ä¸ä¼ è¾“]
    B --> C[æ•°æ®å­˜å‚¨ä¸å¤„ç†]
    C --> D[æ•°æ®åˆ†æä¸æ¢ç´¢]
    D --> E[å¯è§†åŒ–ä¸æŠ¥å‘Š]
    E --> F[æœºå™¨å­¦ä¹ ä¸AI]
    
    G[æ‰¹å¤„ç†] --> C
    H[æµå¤„ç†] --> C
```

### 1.2 æ ¸å¿ƒæ•°æ®åˆ†ææœåŠ¡

GCPæ•°æ®åˆ†æå¹³å°åŒ…å«å¤šç§æ ¸å¿ƒæœåŠ¡ï¼š

| æœåŠ¡ç±»åˆ« | æ ¸å¿ƒæœåŠ¡ | ä¸»è¦åŠŸèƒ½ | é€‚ç”¨åœºæ™¯ |
|----------|----------|----------|----------|
| **æ•°æ®ä»“åº“** | BigQuery | PBçº§æ•°æ®ä»“åº“ï¼ŒSQLæŸ¥è¯¢ | å¤§æ•°æ®åˆ†æï¼Œå•†ä¸šæ™ºèƒ½ |
| **æ•°æ®æ¹–** | Cloud Storage | å¤§è§„æ¨¡å¯¹è±¡å­˜å‚¨ | åŸå§‹æ•°æ®ï¼Œéç»“æ„åŒ–æ•°æ® |
| **æ‰¹å¤„ç†** | Dataflow, Dataproc | å¤§è§„æ¨¡æ•°æ®å¤„ç† | ETLï¼Œå¤æ‚åˆ†æ |
| **æµå¤„ç†** | Dataflow, Pub/Sub | å®æ—¶æ•°æ®å¤„ç† | å®æ—¶åˆ†æï¼Œäº‹ä»¶é©±åŠ¨ |
| **æ•°æ®é›†æˆ** | Data Fusion | ETL/ELTå·¥å…· | æ•°æ®ç®¡é“æ„å»º |
| **BIä¸å¯è§†åŒ–** | Looker, Data Studio | æ•°æ®å¯è§†åŒ– | ä»ªè¡¨æ¿ï¼ŒæŠ¥å‘Š |

### 1.3 æ•°æ®åˆ†ææ¶æ„æ¨¡å¼

å¸¸è§çš„æ•°æ®åˆ†ææ¶æ„æ¨¡å¼ï¼š

#### ä¼ ç»Ÿæ•°æ®ä»“åº“æ¶æ„

```mermaid
graph TB
    subgraph "æ•°æ®æº"
        A[ä¸šåŠ¡æ•°æ®åº“]
        B[æ—¥å¿—æ–‡ä»¶]
        C[ç¬¬ä¸‰æ–¹æ•°æ®]
    end
    
    subgraph "æ•°æ®ä¼ è¾“"
        D[Data Transfer]
        E[Cloud Storage Upload]
        F[Streaming]
    end
    
    subgraph "æ•°æ®å¤„ç†"
        G[BigQuery]
        H[Dataflow]
    end
    
    subgraph "åˆ†æåº”ç”¨"
        I[BIå·¥å…·]
        J[æœºå™¨å­¦ä¹ ]
    end
    
    A --> D
    B --> E
    C --> F
    
    D --> G
    E --> H
    F --> H
    
    G --> I
    H --> G
    G --> J
```

#### ç°ä»£æ•°æ®æ¹–å±‹æ¶æ„

```mermaid
graph TB
    subgraph "æ•°æ®æ¹–"
        A[Cloud Storage]
    end
    
    subgraph "æ•°æ®å¤„ç†"
        B[Spark/Dataproc]
        C[BigQuery]
        D[BigLake]
    end
    
    subgraph "æ•°æ®ç›®å½•"
        E[Dataplex]
    end
    
    subgraph "åˆ†æåº”ç”¨"
        F[BIå·¥å…·]
        G[MLå¹³å°]
    end
    
    A --> B
    A --> C
    A --> D
    
    B --> C
    D --> C
    
    E --> A
    E --> C
    E --> D
    
    C --> F
    C --> G
```

---

## 2. BigQueryæ•°æ®ä»“åº“

### 2.1 BigQueryåŸºç¡€

BigQueryæ˜¯GCPçš„å…¨æ‰˜ç®¡ã€æ— æœåŠ¡å™¨æ•°æ®ä»“åº“ï¼Œä¸“ä¸ºå¤§è§„æ¨¡æ•°æ®åˆ†æè®¾è®¡ã€‚

#### æ ¸å¿ƒç‰¹æ€§

- **æ— æœåŠ¡å™¨æ¶æ„**ï¼šæ— éœ€ç®¡ç†åŸºç¡€è®¾æ–½
- **é«˜æ€§èƒ½æŸ¥è¯¢**ï¼šä½¿ç”¨åˆ—å¼å­˜å‚¨å’Œåˆ†å¸ƒå¼å¤„ç†
- **å¯æ‰©å±•æ€§**ï¼šå¯å¤„ç†PBçº§æ•°æ®ï¼Œè‡ªåŠ¨æ‰©å±•
- **SQLæ”¯æŒ**ï¼šæ ‡å‡†SQLå’Œæ—§ç‰ˆSQL
- **æˆæœ¬çµæ´»**ï¼šæŒ‰å­˜å‚¨é‡å’ŒæŸ¥è¯¢æ‰«æé‡è®¡è´¹
- **é›†æˆæ€§**ï¼šä¸GCPå…¶ä»–æœåŠ¡æ·±åº¦é›†æˆ

#### æ•°æ®æ¨¡å‹

BigQueryä½¿ç”¨åŸºäºè¡¨çš„æ•°æ®æ¨¡å‹ï¼š

```mermaid
graph TB
    A[é¡¹ç›®] --> B[æ•°æ®é›†]
    B --> C[è¡¨1]
    B --> D[è¡¨2]
    B --> E[è¡¨N]
    
    C --> F[åˆ†åŒºè¡¨]
    G[åˆ†ç°‡è¡¨]
    C --> G
```

- **é¡¹ç›®**ï¼šé¡¶çº§å®¹å™¨ï¼ŒåŒ…å«æ•°æ®é›†
- **æ•°æ®é›†**ï¼šè¡¨çš„é›†åˆï¼Œå¯è®¾ç½®è®¿é—®æ§åˆ¶
- **è¡¨**ï¼šæ•°æ®çš„ç»“æ„åŒ–é›†åˆï¼Œæ”¯æŒåˆ†åŒºå’Œåˆ†ç°‡

### 2.2 æ•°æ®æ“ä½œ

#### åˆ›å»ºæ•°æ®é›†å’Œè¡¨

```sql
-- åˆ›å»ºæ•°æ®é›†
CREATE SCHEMA my_project.analytics_dataset
OPTIONS(
  location="US",
  description="Analytics dataset for web traffic"
);

-- åˆ›å»ºè¡¨
CREATE TABLE my_project.analytics_dataset.web_traffic (
  user_id STRING,
  page_url STRING,
  timestamp TIMESTAMP,
  session_id STRING,
  referrer STRING,
  user_agent STRUCT<
    browser STRING,
    os STRING,
    device STRING
  >
)
OPTIONS(
  description="Web traffic data"
);

-- åˆ›å»ºåˆ†åŒºè¡¨ï¼ˆæŒ‰æ—¶é—´åˆ†åŒºï¼‰
CREATE TABLE my_project.analytics_dataset.web_traffic_partitioned (
  user_id STRING,
  page_url STRING,
  session_id STRING,
  referrer STRING,
  user_agent STRUCT<
    browser STRING,
    os STRING,
    device STRING
  >
)
PARTITION BY DATE(timestamp)
OPTIONS(
  description="Partitioned web traffic data by date"
);

-- åˆ›å»ºåˆ†ç°‡è¡¨
CREATE TABLE my_project.analytics_dataset.web_traffic_clustered (
  user_id STRING,
  page_url STRING,
  timestamp TIMESTAMP,
  session_id STRING,
  referrer STRING
)
PARTITION BY DATE(timestamp)
CLUSTER BY user_id, session_id
OPTIONS(
  description="Clustered web traffic data"
);
```

#### æ•°æ®å¯¼å…¥å’Œå¯¼å‡º

```bash
# ä»æœ¬åœ°æ–‡ä»¶å¯¼å…¥
bq load \
  --source_format=CSV \
  my_project.analytics_dataset.web_traffic \
  ./data/web_traffic.csv \
  ./schema/web_traffic_schema.json

# ä»Cloud Storageå¯¼å…¥
bq load \
  --source_format=PARQUET \
  my_project.analytics_dataset.web_traffic \
  gs://my-bucket/data/web_traffic.parquet

# å¯¼å‡ºæ•°æ®åˆ°Cloud Storage
bq extract \
  --destination_format=CSV \
  my_project.analytics_dataset.web_traffic \
  gs://my-bucket/exports/web_traffic_*.csv
```

#### æŸ¥è¯¢ä¼˜åŒ–æŠ€å·§

```sql
-- ä½¿ç”¨åˆ†åŒºè¡¨è¿‡æ»¤æ•°æ®
SELECT
  user_id,
  page_url,
  timestamp
FROM
  my_project.analytics_dataset.web_traffic_partitioned
WHERE
  DATE(timestamp) BETWEEN "2023-01-01" AND "2023-01-31";

-- ä½¿ç”¨åˆ†ç°‡ä¼˜åŒ–æŸ¥è¯¢
SELECT
  user_id,
  COUNT(*) AS page_views
FROM
  my_project.analytics_dataset.web_traffic_clustered
WHERE
  DATE(timestamp) = "2023-01-15"
  AND user_id = "user123"
GROUP BY
  user_id;

-- ä½¿ç”¨åµŒå¥—å’Œé‡å¤å­—æ®µ
SELECT
  page_url,
  user_agent.browser,
  user_agent.os,
  user_agent.device
FROM
  my_project.analytics_dataset.web_traffic;

-- ä½¿ç”¨çª—å£å‡½æ•°
SELECT
  user_id,
  page_url,
  timestamp,
  ROW_NUMBER() OVER (PARTITION BY user_id ORDER BY timestamp DESC) as rank
FROM
  my_project.analytics_dataset.web_traffic;

-- ä½¿ç”¨è¿‘ä¼¼èšåˆå‡½æ•°æé«˜æ€§èƒ½
SELECT
  APPROX_COUNT_DISTINCT(user_id) as distinct_users,
  APPROX_QUANTILES(timestamp, 10) as percentile_timestamps
FROM
  my_project.analytics_dataset.web_traffic;
```

### 2.3 é«˜çº§åŠŸèƒ½

#### å­˜å‚¨è¿‡ç¨‹å’Œç”¨æˆ·å®šä¹‰å‡½æ•°(UDF)

```sql
-- åˆ›å»ºå­˜å‚¨è¿‡ç¨‹
CREATE PROCEDURE my_project.analytics_dataset.sp_populate_sessions()
BEGIN
  -- æ’å…¥æˆ–æ›´æ–°ä¼šè¯æ•°æ®
  MERGE my_project.analytics_dataset.web_sessions T
  USING (
    SELECT
      session_id,
      MIN(timestamp) as start_time,
      MAX(timestamp) as end_time,
      COUNT(*) as page_views
    FROM my_project.analytics_dataset.web_traffic
    WHERE DATE(timestamp) = CURRENT_DATE()
    GROUP BY session_id
  ) S
  ON T.session_id = S.session_id
  WHEN MATCHED THEN
    UPDATE SET
      T.start_time = S.start_time,
      T.end_time = S.end_time,
      T.page_views = S.page_views
  WHEN NOT MATCHED THEN
    INSERT (session_id, start_time, end_time, page_views)
    VALUES (S.session_id, S.start_time, S.end_time, S.page_views);
END;

-- æ‰§è¡Œå­˜å‚¨è¿‡ç¨‹
CALL my_project.analytics_dataset.sp_populate_sessions();

-- åˆ›å»ºUDF
CREATE TEMP FUNCTION classify_session(page_views INT64)
RETURNS STRING
LANGUAGE js AS """
  if (page_views < 5) {
    return 'short';
  } else if (page_views < 20) {
    return 'medium';
  } else {
    return 'long';
  }
""";

-- ä½¿ç”¨UDF
SELECT
  session_id,
  page_views,
  classify_session(page_views) as session_type
FROM
  my_project.analytics_dataset.web_sessions;
```

#### å¤–éƒ¨æ•°æ®æº

```sql
-- åˆ›å»ºå¤–éƒ¨è¡¨æŒ‡å‘Cloud Storageä¸­çš„æ•°æ®
CREATE EXTERNAL TABLE my_project.analytics_dataset.external_logs (
  timestamp TIMESTAMP,
  log_level STRING,
  message STRING
)
OPTIONS (
  format = "JSON",
  uris = ["gs://my-bucket/logs/*.json"]
);

-- æŸ¥è¯¢å¤–éƒ¨è¡¨
SELECT
  DATE(timestamp) as log_date,
  log_level,
  COUNT(*) as log_count
FROM
  my_project.analytics_dataset.external_logs
GROUP BY
  log_date,
  log_level
ORDER BY
  log_date,
  log_level;
```

---

## 3. å®æ—¶æ•°æ®å¤„ç†

### 3.1 å®æ—¶æ•°æ®æ¶æ„

GCPæä¾›äº†å®Œæ•´çš„å®æ—¶æ•°æ®å¤„ç†è§£å†³æ–¹æ¡ˆï¼š

```mermaid
graph TB
    subgraph "æ•°æ®æº"
        A[IoTè®¾å¤‡]
        B[Webåº”ç”¨]
        C[ç§»åŠ¨åº”ç”¨]
    end
    
    subgraph "æ•°æ®é‡‡é›†"
        D[Pub/Sub]
        E[Cloud Logging]
    end
    
    subgraph "æ•°æ®å¤„ç†"
        F[Dataflow]
        G[BigQuery]
        H[Cloud Functions]
    end
    
    subgraph "æ•°æ®æ¶ˆè´¹"
        I[å®æ—¶ä»ªè¡¨æ¿]
        J[å®æ—¶æ¨è]
        K[å¼‚å¸¸æ£€æµ‹]
    end
    
    A --> D
    B --> D
    C --> D
    
    D --> F
    D --> H
    E --> F
    
    F --> G
    H --> G
    
    G --> I
    F --> J
    F --> K
```

### 3.2 Pub/Subæ¶ˆæ¯æœåŠ¡

Pub/Subæ˜¯GCPçš„æ‰˜ç®¡æ¶ˆæ¯æœåŠ¡ï¼Œç”¨äºå®æ—¶æ•°æ®æµå¤„ç†ã€‚

#### æ ¸å¿ƒç‰¹æ€§

- **æ¶ˆæ¯ä¼ é€’**ï¼šå¯é çš„æ¶ˆæ¯ä¼ é€’å’ŒæŒä¹…åŒ–
- **æ‹‰å–å’Œæ¨é€**ï¼šæ”¯æŒä¸¤ç§æ¶ˆæ¯ä¼ é€’æ¨¡å¼
- **è¿‡æ»¤**ï¼šæœåŠ¡å™¨ç«¯æ¶ˆæ¯è¿‡æ»¤
- **é‡æ”¾**ï¼šæ¶ˆæ¯é‡æ”¾èƒ½åŠ›
- **å…¨çƒåˆ†å¸ƒ**ï¼šå…¨çƒæ¶ˆæ¯åˆ†å‘

#### åˆ›å»ºå’Œä½¿ç”¨Pub/Sub

```bash
# åˆ›å»ºä¸»é¢˜
gcloud pubsub topics create user-events

# åˆ›å»ºè®¢é˜…
gcloud pubsub subscriptions create user-events-sub \
  --topic=user-events \
  --ack-deadline=60

# å‘å¸ƒæ¶ˆæ¯
gcloud pubsub topics publish user-events \
  --message='{"user_id": "12345", "action": "page_view", "timestamp": "2023-01-01T12:00:00Z"}'

# æ‹‰å–æ¶ˆæ¯
gcloud pubsub subscriptions pull user-events-sub --auto-ack --limit=10
```

#### ä½¿ç”¨å®¢æˆ·ç«¯åº“

```python
# publisher.py - æ¶ˆæ¯å‘å¸ƒè€…
import json
from google.cloud import pubsub_v1

publisher = pubsub_v1.PublisherClient()
topic_name = "projects/your-project-id/topics/user-events"

def publish_user_event(user_id, action):
    # åˆ›å»ºæ¶ˆæ¯
    message = {
        "user_id": user_id,
        "action": action,
        "timestamp": datetime.utcnow().isoformat() + "Z"
    }
    
    # ç¼–ç å¹¶å‘å¸ƒæ¶ˆæ¯
    data = json.dumps(message).encode("utf-8")
    future = publisher.publish(topic_name, data=data)
    print(f"Published message ID: {future.result()}")

# å‘å¸ƒç”¨æˆ·äº‹ä»¶
publish_user_event("user123", "page_view")
publish_user_event("user123", "add_to_cart")
```

```python
# subscriber.py - æ¶ˆæ¯è®¢é˜…è€…
from concurrent.futures import TimeoutError
from google.cloud import pubsub_v1

subscriber = pubsub_v1.SubscriberClient()
subscription_name = "projects/your-project-id/subscriptions/user-events-sub"

def callback(message):
    print(f"Received message: {message.data}")
    # å¤„ç†æ¶ˆæ¯
    user_event = json.loads(message.data.decode("utf-8"))
    print(f"User {user_event['user_id']} performed {user_event['action']}")
    
    # ç¡®è®¤æ¶ˆæ¯å¤„ç†å®Œæˆ
    message.ack()

# è®¢é˜…æ¶ˆæ¯
streaming_pull_future = subscriber.subscribe(subscription_name, callback=callback)
print(f"Listening for messages on {subscription_name}..")

try:
    streaming_pull_future.result(timeout=timeout)
except TimeoutError:
    streaming_pull_future.cancel()
```

### 3.3 Dataflowæ•°æ®æµå¤„ç†

Dataflowæ˜¯GCPçš„æ‰˜ç®¡æ•°æ®æµå¤„ç†æœåŠ¡ï¼ŒåŸºäºApache Beamæ„å»ºã€‚

#### æ ¸å¿ƒç‰¹æ€§

- **ç»Ÿä¸€æ¨¡å‹**ï¼šæ‰¹å¤„ç†å’Œæµå¤„ç†ç»Ÿä¸€API
- **è‡ªåŠ¨æ‰©å±•**ï¼šæ ¹æ®è´Ÿè½½è‡ªåŠ¨è°ƒæ•´èµ„æº
- **æ‰˜ç®¡æœåŠ¡**ï¼šæ— éœ€ç®¡ç†åŸºç¡€è®¾æ–½
- **ç²¾ç¡®ä¸€æ¬¡**ï¼šç¡®ä¿æ•°æ®å¤„ç†çš„ç²¾ç¡®æ€§
- **ç›‘æ§é›†æˆ**ï¼šä¸Cloud Monitoringé›†æˆ

#### Dataflowæµæ°´çº¿ç¤ºä¾‹

```python
# dataflow_pipeline.py
import apache_beam as beam
from apache_beam.options.pipeline_options import PipelineOptions
from apache_beam.options.pipeline_options import StandardOptions
import json

# å®šä¹‰å¤„ç†å‡½æ•°
class ParseEventFn(beam.DoFn):
    def process(self, element):
        event = json.loads(element.decode('utf-8'))
        return [(event['user_id'], 1)]

class ExtractAndCountUserEvents(beam.PTransform):
    def expand(self, pcoll):
        return (
            pcoll
            | 'ParseEvent' >> beam.ParDo(ParseEventFn())
            | 'CountPerUser' >> beam.CombinePerKey(sum)
        )

# è¿è¡Œæµæ°´çº¿
def run_pipeline():
    pipeline_options = PipelineOptions([
        '--project=your-project-id',
        '--region=us-central1',
        '--temp_location=gs://your-bucket/temp/',
        '--staging_location=gs://your-bucket/staging/',
        '--runner=DataflowRunner',
        '--job_name=user-events-analysis'
    ])
    
    pipeline_options.view_as(StandardOptions).streaming = True
    
    with beam.Pipeline(options=pipeline_options) as p:
        # ä»Pub/Subè¯»å–æ¶ˆæ¯
        events = (
            p
            | 'ReadFromPubSub' >> beam.io.ReadFromPubSub(
                subscription='projects/your-project-id/subscriptions/user-events-sub'
            )
            | 'ExtractAndCount' >> ExtractAndCountUserEvents()
            | 'WriteToBigQuery' >> beam.io.WriteToBigQuery(
                table='your-project:analytics.user_events_count',
                schema='user_id:STRING, event_count:INTEGER',
                write_disposition=beam.io.BigQueryDisposition.WRITE_APPEND,
                create_disposition=beam.io.BigQueryDisposition.CREATE_IF_NEEDED
            )
        )

if __name__ == '__main__':
    run_pipeline()
```

#### è¿è¡ŒDataflowä½œä¸š

```bash
# æœ¬åœ°è¿è¡Œï¼ˆç”¨äºæµ‹è¯•ï¼‰
python dataflow_pipeline.py --runner=DirectRunner

# æäº¤åˆ°Dataflowè¿è¡Œ
python dataflow_pipeline.py --runner=DataflowRunner --project=your-project-id

# ä½¿ç”¨æ¨¡æ¿åˆ›å»ºä½œä¸š
gcloud dataflow jobs run user-events-analysis \
  --gcs-location gs://your-bucket/templates/dataflow_pipeline \
  --region us-central1 \
  --parameters input_subscription=projects/your-project-id/subscriptions/user-events-sub
```

---

## 4. GCPæœºå™¨å­¦ä¹ å¹³å°

### 4.1 æœºå™¨å­¦ä¹ æœåŠ¡æ¦‚è§ˆ

GCPæä¾›äº†å…¨é¢çš„æœºå™¨å­¦ä¹ å¹³å°ï¼Œè¦†ç›–æ¨¡å‹å¼€å‘ç”Ÿå‘½å‘¨æœŸçš„å„ä¸ªé˜¶æ®µï¼š

```mermaid
graph TB
    subgraph "æ•°æ®å‡†å¤‡"
        A[BigQuery]
        B[Cloud Storage]
        C[Dataflow]
    end
    
    subgraph "æ¨¡å‹å¼€å‘"
        D[Vertex AI Workbench]
        E[Vertex AI Training]
        F[AutoML]
    end
    
    subgraph "æ¨¡å‹éƒ¨ç½²"
        G[Vertex AI Endpoints]
        H[AI Platform Prediction]
    end
    
    subgraph "MLOps"
        I[Vertex AI Pipelines]
        J[Feature Store]
        K[Model Registry]
    end
    
    A --> D
    B --> D
    C --> D
    
    D --> E
    E --> F
    
    F --> G
    G --> H
    
    I --> D
    I --> E
    I --> F
    I --> G
    
    J --> D
    J --> E
    
    K --> G
```

### 4.2 Vertex AIå¹³å°

Vertex AIæ˜¯GCPçš„ç»Ÿä¸€æœºå™¨å­¦ä¹ å¹³å°ï¼Œé›†æˆäº†å„ç§MLå·¥å…·å’ŒæœåŠ¡ã€‚

#### æ ¸å¿ƒç»„ä»¶

- **Vertex AI Workbench**ï¼šæ‰˜ç®¡çš„Jupyterç¬”è®°æœ¬ç¯å¢ƒ
- **Vertex AI Training**ï¼šè‡ªå®šä¹‰æ¨¡å‹è®­ç»ƒæœåŠ¡
- **Vertex AI Prediction**ï¼šæ¨¡å‹é¢„æµ‹å’Œæ¨ç†æœåŠ¡
- **AutoML**ï¼šè‡ªåŠ¨æ¨¡å‹è®­ç»ƒæœåŠ¡
- **Vertex AI Pipelines**ï¼šMLå·¥ä½œæµç¼–æ’
- **Feature Store**ï¼šç‰¹å¾å­˜å‚¨å’Œç®¡ç†
- **Model Registry**ï¼šæ¨¡å‹ç‰ˆæœ¬å’Œç”Ÿå‘½å‘¨æœŸç®¡ç†

#### åˆ›å»ºè‡ªå®šä¹‰æ¨¡å‹è®­ç»ƒä½œä¸š

```python
# train_model.py - è‡ªå®šä¹‰æ¨¡å‹è®­ç»ƒä»£ç 
import argparse
import os
import tensorflow as tf
from google.cloud import storage

def parse_args():
    parser = argparse.ArgumentParser()
    parser.add_argument('--model-dir', required=True)
    parser.add_argument('--epochs', type=int, default=10)
    parser.add_argument('--batch-size', type=int, default=32)
    return parser.parse_args()

def download_data(args):
    """ä»Cloud Storageä¸‹è½½æ•°æ®é›†"""
    client = storage.Client()
    bucket = client.get_bucket('your-bucket')
    
    # ä¸‹è½½è®­ç»ƒæ•°æ®
    blob = bucket.blob('data/train.csv')
    blob.download_to_filename('/tmp/train.csv')
    
    # ä¸‹è½½æµ‹è¯•æ•°æ®
    blob = bucket.blob('data/test.csv')
    blob.download_to_filename('/tmp/test.csv')

def build_model():
    """æ„å»ºKerasæ¨¡å‹"""
    model = tf.keras.Sequential([
        tf.keras.layers.Dense(128, activation='relu', input_shape=(784,)),
        tf.keras.layers.Dropout(0.2),
        tf.keras.layers.Dense(10, activation='softmax')
    ])
    
    model.compile(
        optimizer='adam',
        loss='sparse_categorical_crossentropy',
        metrics=['accuracy']
    )
    
    return model

def train_model(model, args):
    """è®­ç»ƒæ¨¡å‹"""
    # åŠ è½½æ•°æ®ï¼ˆç¤ºä¾‹ï¼‰
    # è¿™é‡Œåº”è¯¥å®ç°å®é™…çš„æ•°æ®åŠ è½½å’Œé¢„å¤„ç†é€»è¾‘
    
    # è®­ç»ƒæ¨¡å‹
    history = model.fit(
        # train_data, train_labels,
        validation_data=(# val_data, val_labels),
        epochs=args.epochs,
        batch_size=args.batch_size
    )
    
    return history

def save_model(model, model_dir):
    """ä¿å­˜æ¨¡å‹"""
    # åˆ›å»ºæœ¬åœ°æ¨¡å‹ç›®å½•
    os.makedirs(model_dir, exist_ok=True)
    
    # ä¿å­˜ä¸ºTensorFlow SavedModelæ ¼å¼
    tf.saved_model.save(model, model_dir)

def main():
    args = parse_args()
    
    # ä¸‹è½½æ•°æ®
    download_data(args)
    
    # æ„å»ºæ¨¡å‹
    model = build_model()
    
    # è®­ç»ƒæ¨¡å‹
    history = train_model(model, args)
    
    # ä¿å­˜æ¨¡å‹
    save_model(model, args.model_dir)
    
    # å°†æ¨¡å‹ä¸Šä¼ åˆ°GCS
    model_gcs_path = args.model_dir.replace('gs://', '')
    bucket_name, object_prefix = model_gcs_path.split('/', 1)
    storage_client = storage.Client()
    bucket = storage_client.bucket(bucket_name)
    
    for local_file in tf.io.gfile.glob(f'{args.model_dir}/**/*'):
        if tf.io.gfile.isdir(local_file):
            continue
        
        relative_path = local_file.replace(args.model_dir + '/', '')
        blob = bucket.blob(f'{object_prefix}/{relative_path}')
        blob.upload_from_filename(local_file)

if __name__ == '__main__':
    main()
```

#### æäº¤è®­ç»ƒä½œä¸š

```python
# submit_training_job.py - æäº¤Vertex AIè®­ç»ƒä½œä¸š
from google.cloud import aiplatform

def submit_custom_training_job():
    aiplatform.init(
        project='your-project-id',
        location='us-central1',
        staging_bucket='gs://your-bucket/staging'
    )
    
    job = aiplatform.CustomTrainingJob(
        display_name='mnist-classifier',
        script_path='train_model.py',
        container_uri='gcr.io/cloud-aiplatform/training/tf-cpu.2-8:latest',
        requirements=['tensorflow==2.8.0'],
        args=[
            '--model-dir', 'gs://your-bucket/models/mnist_classifier'
        ]
    )
    
    # æäº¤è®­ç»ƒä½œä¸š
    model = job.run(
        replica_count=1,
        machine_type='n1-standard-4',
        sync=True
    )
    
    return model

if __name__ == '__main__':
    model = submit_custom_training_job()
    print(f"Model deployed at: {model.resource_name}")
```

### 4.3 AutoMLæœåŠ¡

AutoMLå…è®¸éMLä¸“å®¶æ„å»ºé«˜è´¨é‡çš„è‡ªå®šä¹‰æœºå™¨å­¦ä¹ æ¨¡å‹ã€‚

#### ä½¿ç”¨AutoML Visionè¿›è¡Œå›¾åƒåˆ†ç±»

```python
# automl_vision.py
from google.cloud import aiplatform
from google.cloud import vision

def create_dataset():
    """åˆ›å»ºAutoML Visionæ•°æ®é›†"""
    client = vision.AutoMlClient()
    
    # å®šä¹‰æ•°æ®é›†
    dataset = client.create_dataset(
        parent=f"projects/your-project-id/locations/us-central1",
        dataset={
            "display_name": "product_images",
            "image_classification_dataset_metadata": {}
        }
    )
    
    print(f"Dataset created: {dataset.name}")
    return dataset

def import_data(dataset):
    """å¯¼å…¥æ•°æ®åˆ°æ•°æ®é›†"""
    client = vision.AutoMlClient()
    
    # å¯¼å…¥æ•°æ®
    response = client.import_data(
        name=dataset.name,
        input_config={
            "gcs_source": {
                "uris": [
                    "gs://your-bucket/product-images/data.csv"
                ]
            }
        }
    )
    
    print(f"Data import started: {response.operation.name}")

def train_model(dataset):
    """è®­ç»ƒAutoMLæ¨¡å‹"""
    client = vision.AutoMlClient()
    
    # å®šä¹‰æ¨¡å‹
    model = client.create_model(
        parent=f"projects/your-project-id/locations/us-central1",
        model={
            "display_name": "product_classification",
            "dataset_id": dataset.name.split('/')[-1],
            "image_classification_model_metadata": {
                "train_budget": 8,  # è®­ç»ƒæ—¶é—´ï¼ˆèŠ‚ç‚¹å°æ—¶ï¼‰
            }
        }
    )
    
    print(f"Model training started: {model.operation.name}")
    return model

def deploy_model(model):
    """éƒ¨ç½²æ¨¡å‹ç”¨äºé¢„æµ‹"""
    client = vision.AutoMlClient()
    
    # éƒ¨ç½²æ¨¡å‹
    response = client.deploy_model(
        name=model.name
    )
    
    print(f"Model deployment started: {response.operation.name}")

def predict_image(image_path, model_id):
    """ä½¿ç”¨æ¨¡å‹è¿›è¡Œé¢„æµ‹"""
    client = vision.ImageAnnotatorClient()
    
    # è¯»å–å›¾åƒ
    with open(image_path, "rb") as image_file:
        content = image_file.read()
    
    image = vision.Image(content=content)
    
    # è¿›è¡Œé¢„æµ‹
    response = client.image_annotation(
        image=image,
        features=[{"type_": vision.Feature.Type.LABEL_DETECTION}],
        model_id=model_id
    )
    
    # è¾“å‡ºé¢„æµ‹ç»“æœ
    for label in response.label_annotations:
        print(f"Label: {label.description}, Score: {label.score}")

if __name__ == '__main__':
    # åˆ›å»ºæ•°æ®é›†
    dataset = create_dataset()
    
    # å¯¼å…¥æ•°æ®
    import_data(dataset)
    
    # è®­ç»ƒæ¨¡å‹ï¼ˆé€šå¸¸éœ€è¦æ•°å°æ—¶ï¼‰
    model = train_model(dataset)
    
    # éƒ¨ç½²æ¨¡å‹
    deploy_model(model)
    
    # ä½¿ç”¨æ¨¡å‹é¢„æµ‹
    predict_image('test_image.jpg', 'model_id')
```

---

## 5. æ¨¡å‹éƒ¨ç½²ä¸ç®¡ç†

### 5.1 æ¨¡å‹éƒ¨ç½²é€‰é¡¹

GCPæä¾›äº†å¤šç§æ¨¡å‹éƒ¨ç½²é€‰é¡¹ï¼Œé€‚åº”ä¸åŒéœ€æ±‚ï¼š

| éƒ¨ç½²æ–¹å¼ | é€‰é¡¹ | ç‰¹ç‚¹ | é€‚ç”¨åœºæ™¯ |
|----------|------|------|----------|
| **Vertex AI Endpoint** | é¢„æµ‹æœåŠ¡ | æ‰˜ç®¡APIï¼Œè‡ªåŠ¨æ‰©å±• | ç”Ÿäº§ç¯å¢ƒæ¨ç† |
| **AI Platform Prediction** | ä¼ ç»Ÿé¢„æµ‹æœåŠ¡ | æˆç†Ÿç¨³å®šï¼Œç‰ˆæœ¬å…¼å®¹ | ä¼ ç»ŸMLå·¥ä½œæµ |
| **Containerizedæ¨¡å‹** | è‡ªå®šä¹‰å®¹å™¨ | å®Œå…¨æ§åˆ¶ï¼Œè‡ªå®šä¹‰ä¾èµ– | ç‰¹æ®Šè¿è¡Œæ—¶éœ€æ±‚ |
| **Edge deployment** | è¾¹ç¼˜è®¾å¤‡ | æœ¬åœ°æ¨ç†ï¼Œä½å»¶è¿Ÿ | IoTï¼Œå®æ—¶åº”ç”¨ |
| **WebæœåŠ¡** | è‡ªå®šä¹‰WebæœåŠ¡ | çµæ´»è‡ªå®šä¹‰ï¼Œè½»é‡çº§ | ç®€å•æ¨¡å‹ï¼ŒAPIæœåŠ¡ |

### 5.2 Vertex AI Endpointéƒ¨ç½²

#### éƒ¨ç½²æ¨¡å‹åˆ°Vertex AI Endpoint

```python
# deploy_model.py
from google.cloud import aiplatform
from google.protobuf import json_format
from google.protobuf.struct_pb2 import Value

def deploy_model():
    # åˆå§‹åŒ–Vertex AI
    aiplatform.init(
        project='your-project-id',
        location='us-central1',
        staging_bucket='gs://your-bucket/staging'
    )
    
    # ä¸Šä¼ æ¨¡å‹
    model = aiplatform.Model.upload(
        display_name='mnist-classifier',
        artifact_uri='gs://your-bucket/models/mnist_classifier',
        serving_container_image_uri='gcr.io/cloud-aiplatform/prediction/tf2-cpu.2-8:latest'
    )
    
    # åˆ›å»ºç«¯ç‚¹
    endpoint = aiplatform.Endpoint.create(
        display_name='mnist-classifier-endpoint'
    )
    
    # éƒ¨ç½²æ¨¡å‹åˆ°ç«¯ç‚¹
    deployed_model = model.deploy(
        endpoint=endpoint,
        deployed_model_display_name='mnist-classifier-deployed',
        machine_type='n1-standard-2',
        min_replica_count=1,
        max_replica_count=5
    )
    
    return endpoint

def make_prediction(endpoint, instances):
    """ä½¿ç”¨éƒ¨ç½²çš„æ¨¡å‹è¿›è¡Œé¢„æµ‹"""
    response = endpoint.predict(instances=instances)
    return response.predictions

if __name__ == '__main__':
    # éƒ¨ç½²æ¨¡å‹
    endpoint = deploy_model()
    
    # è¿›è¡Œé¢„æµ‹
    instances = [{"image": "base64_encoded_image_data"}]
    predictions = make_prediction(endpoint, instances)
    print(predictions)
```

### 5.3 æ¨¡å‹ç›‘æ§å’Œç®¡ç†

#### è®¾ç½®æ¨¡å‹ç›‘æ§

```python
# model_monitoring.py
from google.cloud import aiplatform
from google.cloud import aiplatform_v1beta1

def configure_model_monitoring(endpoint_id, model_id):
    """é…ç½®æ¨¡å‹ç›‘æ§"""
    aiplatform.init(project='your-project-id', location='us-central1')
    
    # è·å–ç«¯ç‚¹
    endpoint = aiplatform.Endpoint(endpoint_id)
    
    # é…ç½®ç›‘æ§ä½œä¸š
    monitoring_job = aiplatform_v1beta1.ModelDeploymentMonitoringJob(
        display_name='mnist-classifier-monitoring',
        endpoint=endpoint.resource_name,
        monitoring_interval_quantum='3600s',  # æ¯å°æ—¶æ£€æŸ¥ä¸€æ¬¡
        alert_config=aiplatform_v1beta1.ModelDeploymentMonitoringJob.AlertConfig(
            email_alerts=['admin@example.com']
        ),
        objective_configs=[
            aiplatform_v1beta1.ModelDeploymentMonitoringJob.ObjectiveConfig(
                target_sample_rate=0.5,  # é‡‡æ ·50%çš„é¢„æµ‹è¯·æ±‚
                training_dataset='gs://your-bucket/data/training_data.csv'
            )
        ],
        log_sampling_rate=0.1  # é‡‡æ ·10%çš„æ—¥å¿—
    )
    
    # åˆ›å»ºç›‘æ§ä½œä¸š
    monitoring_job = aiplatform_v1beta1.JobServiceClient().create_model_deployment_monitoring_job(
        parent=f'projects/your-project-id/locations/us-central1',
        model_deployment_monitoring_job=monitoring_job
    )
    
    print(f"Model monitoring job created: {monitoring_job.name}")

if __name__ == '__main__':
    configure_model_monitoring(
        endpoint_id='your-endpoint-id',
        model_id='your-model-id'
    )
```

---

## 6. å®éªŒï¼šæ„å»ºæ•°æ®é©±åŠ¨AIåº”ç”¨

è®©æˆ‘ä»¬é€šè¿‡ä¸€ä¸ªç»¼åˆå®éªŒï¼Œå®è·µæœ¬ç« å­¦ä¹ çš„æ•°æ®åˆ†æå’Œæœºå™¨å­¦ä¹ æœåŠ¡ã€‚

### å®éªŒç›®æ ‡

é€šè¿‡æœ¬å®éªŒï¼Œæ‚¨å°†ï¼š

1. æ„å»ºå®æ—¶æ•°æ®é‡‡é›†å’Œå¤„ç†ç®¡é“
2. ä½¿ç”¨BigQueryè¿›è¡Œæ•°æ®åˆ†æå’Œå­˜å‚¨
3. è®­ç»ƒæœºå™¨å­¦ä¹ æ¨¡å‹è¿›è¡Œç”¨æˆ·è¡Œä¸ºé¢„æµ‹
4. éƒ¨ç½²æ¨¡å‹å¹¶æä¾›å®æ—¶é¢„æµ‹æœåŠ¡
5. å®ç°ç«¯åˆ°ç«¯çš„æ•°æ®é©±åŠ¨AIåº”ç”¨

### å®éªŒæ¶æ„

æˆ‘ä»¬å°†æ„å»ºä¸€ä¸ªç”µå•†æ¨èç³»ç»Ÿï¼ŒåŒ…å«å®æ—¶æ•°æ®å¤„ç†å’Œæœºå™¨å­¦ä¹ é¢„æµ‹ï¼š

```mermaid
graph TB
    subgraph "æ•°æ®é‡‡é›†"
        A[ç”µå•†ç½‘ç«™] --> B[æ•°æ®æ”¶é›†]
        C[ç§»åŠ¨åº”ç”¨] --> B
    end
    
    subgraph "å®æ—¶å¤„ç†"
        B --> D[Pub/Sub]
        D --> E[Dataflow]
    end
    
    subgraph "æ•°æ®å­˜å‚¨"
        E --> F[BigQuery]
        G[Cloud Storage] --> F
    end
    
    subgraph "æ¨¡å‹è®­ç»ƒ"
        F --> H[Vertex AI Training]
        I[ç‰¹å¾å­˜å‚¨] --> H
    end
    
    subgraph "é¢„æµ‹æœåŠ¡"
        H --> J[Vertex AI Endpoint]
        J --> K[æ¨èAPI]
    end
    
    subgraph "åº”ç”¨é›†æˆ"
        K --> L[ç”µå•†ç½‘ç«™]
        K --> M[è¥é”€ç³»ç»Ÿ]
    end
```

### å‰ææ¡ä»¶

- å·²åˆ›å»ºGCPé¡¹ç›®å’Œè®¡è´¹è´¦æˆ·
- å·²å¯ç”¨å¿…è¦çš„GCP API
- å·²å®‰è£…Pythonå’ŒGoogle Cloud SDK
- å‡†å¤‡ç¤ºä¾‹ç”µå•†æ•°æ®

### å®éªŒæ­¥éª¤

#### æ­¥éª¤1ï¼šè®¾ç½®æ•°æ®åŸºç¡€è®¾æ–½

```bash
# è®¾ç½®é¡¹ç›®
export PROJECT_ID=$(gcloud config get-value project)
export REGION=us-central1

# åˆ›å»ºBigQueryæ•°æ®é›†
bq mk --dataset $PROJECT_ID:ecommerce_data

# åˆ›å»ºCloud Storageå­˜å‚¨æ¡¶
gsutil mb -l $REGION gs://${PROJECT_ID}-ecommerce-data

# åˆ›å»ºPub/Subä¸»é¢˜å’Œè®¢é˜…
gcloud pubsub topics create user-events
gcloud pubsub subscriptions create user-events-sub --topic=user-events

# ä¸Šä¼ ç¤ºä¾‹æ•°æ®åˆ°Cloud Storage
mkdir -p data
cat > data/products.csv <<EOF
product_id,product_name,category,price,description
1001,Smartphone Pro,Electronics,799.99,Latest smartphone with advanced features
1002,Laptop Ultra,Electronics,1299.99,High-performance laptop for professionals
1003,Wireless Earbuds,Electronics,199.99,Premium wireless earbuds with noise cancellation
1004,Smart Watch,Electronics,349.99,Advanced fitness tracking smartwatch
1005,Tablet Plus,Electronics,599.99,Portable tablet for work and entertainment
EOF

gsutil cp data/products.csv gs://${PROJECT_ID}-ecommerce-data/data/

# åˆ›å»ºBigQueryè¡¨å¹¶å¯¼å…¥æ•°æ®
bq load \
  --source_format=CSV \
  --field_delim="," \
  --skip_leading_rows=1 \
  $PROJECT_ID:ecommerce_data.products \
  gs://${PROJECT_ID}-ecommerce-data/data/products.csv \
  product_id:STRING,product_name:STRING,category:STRING,price:FLOAT,description:STRING
```

#### æ­¥éª¤2ï¼šåˆ›å»ºå®æ—¶æ•°æ®å¤„ç†ç®¡é“

```python
# setup_dataflow_pipeline.py
import os
import argparse
import json
import apache_beam as beam
from apache_beam.options.pipeline_options import PipelineOptions, StandardOptions
from apache_beam.io.gcp.bigquery import WriteToBigQuery
from google.cloud import bigquery
from google.cloud import storage

class ParseUserEvent(beam.DoFn):
    def process(self, element):
        """è§£æç”¨æˆ·äº‹ä»¶æ¶ˆæ¯"""
        try:
            event = json.loads(element.decode('utf-8'))
            return [event]
        except Exception as e:
            print(f"Error parsing event: {e}")
            return []

class EnrichWithProductInfo(beam.DoFn):
    """ä½¿ç”¨äº§å“ä¿¡æ¯ä¸°å¯Œç”¨æˆ·äº‹ä»¶"""
    def process(self, event):
        # è¿™é‡Œç®€åŒ–å¤„ç†ï¼Œå®é™…åº”è¯¥æŸ¥è¯¢äº§å“ä¿¡æ¯
        if 'product_id' in event:
            # æ¨¡æ‹Ÿäº§å“ä¿¡æ¯æŸ¥è¯¢
            product_info = {
                'product_name': f'Product {event["product_id"]}',
                'category': 'Electronics'
            }
            event.update(product_info)
        return [event]

def create_dataflow_pipeline():
    parser = argparse.ArgumentParser()
    parser.add_argument('--project', required=True)
    parser.add_argument('--region', required=True)
    parser.add_argument('--temp_location', required=True)
    parser.add_argument('--runner', default='DataflowRunner')
    
    known_args, pipeline_args = parser.parse_known_args()
    
    pipeline_options = PipelineOptions(pipeline_args)
    pipeline_options.view_as(StandardOptions).streaming = True
    
    with beam.Pipeline(options=pipeline_options) as p:
        events = (
            p
            | 'ReadFromPubSub' >> beam.io.ReadFromPubSub(
                subscription=f'projects/{known_args.project}/subscriptions/user-events-sub'
            )
            | 'ParseEvent' >> beam.ParDo(ParseUserEvent())
            | 'EnrichWithProductInfo' >> beam.ParDo(EnrichWithProductInfo())
            | 'WriteToBigQuery' >> WriteToBigQuery(
                table=f'{known_args.project}:ecommerce_data.user_events',
                schema='event_id:STRING,user_id:STRING,event_type:STRING,timestamp:TIMESTAMP,product_id:STRING,product_name:STRING,category:STRING',
                write_disposition=beam.io.BigQueryDisposition.WRITE_APPEND,
                create_disposition=beam.io.BigQueryDisposition.CREATE_IF_NEEDED
            )
        )

if __name__ == '__main__':
    create_dataflow_pipeline()
```

#### æ­¥éª¤3ï¼šåˆ›å»ºç‰¹å¾å·¥ç¨‹

```sql
-- create_features.sql - BigQueryç‰¹å¾å·¥ç¨‹SQL
-- åˆ›å»ºç”¨æˆ·è¡Œä¸ºç‰¹å¾è¡¨
CREATE OR REPLACE TABLE ecommerce_data.user_features
OPTIONS(
  description="User behavior features for recommendation"
)
AS
SELECT
  user_id,
  COUNT(DISTINCT session_id) as session_count,
  COUNT(DISTINCT product_id) as unique_products_viewed,
  COUNT(*) as total_events,
  SUM(CASE WHEN event_type = 'purchase' THEN 1 ELSE 0 END) as purchase_count,
  SUM(CASE WHEN event_type = 'add_to_cart' THEN 1 ELSE 0 END) as cart_add_count,
  SUM(CASE WHEN event_type = 'view' THEN 1 ELSE 0 END) as view_count,
  APPROX_COUNT_DISTINCT(category) as unique_categories_viewed,
  DATE_DIFF(CURRENT_DATE(), MAX(DATE(timestamp)), DAY) as days_since_last_activity,
  -- ç”¨æˆ·åå¥½ç‰¹å¾
  ARRAY_AGG(DISTINCT category ORDER BY COUNT(*) DESC LIMIT 3) as top_categories,
  -- ä»·æ ¼åå¥½
  AVG(product.price) as avg_price_viewed,
  MAX(product.price) as max_price_viewed,
  MIN(product.price) as min_price_viewed
FROM
  ecommerce_data.user_events AS events
LEFT JOIN
  ecommerce_data.products AS product
ON events.product_id = product.product_id
WHERE
  event_type IN ('view', 'add_to_cart', 'purchase')
GROUP BY
  user_id;

-- åˆ›å»ºäº§å“å…³è”ç‰¹å¾è¡¨
CREATE OR REPLACE TABLE ecommerce_data.product_affinity
OPTIONS(
  description="Product affinity matrix for collaborative filtering"
)
AS
SELECT
  p1.product_id as product_a,
  p2.product_id as product_b,
  COUNT(*) as co_view_count,
  COUNT(DISTINCT e.user_id) as unique_users
FROM
  (SELECT user_id, product_id FROM ecommerce_data.user_events WHERE event_type = 'view') AS p1
JOIN
  (SELECT user_id, product_id FROM ecommerce_data.user_events WHERE event_type = 'view') AS p2
ON
  p1.user_id = p2.user_id AND p1.product_id < p2.product_id
JOIN
  ecommerce_data.user_events AS e
ON
  p1.user_id = e.user_id AND p1.product_id = e.product_id
GROUP BY
  p1.product_id, p2.product_id
HAVING
  unique_users >= 10
ORDER BY
  unique_users DESC;
```

#### æ­¥éª¤4ï¼šè®­ç»ƒæ¨èæ¨¡å‹

```python
# train_recommendation_model.py
import argparse
import os
import json
import pandas as pd
import tensorflow as tf
from google.cloud import bigquery
from google.cloud import storage

def load_data_from_bq(project_id, dataset_id):
    """ä»BigQueryåŠ è½½è®­ç»ƒæ•°æ®"""
    client = bigquery.Client(project=project_id)
    
    # æŸ¥è¯¢è®­ç»ƒæ•°æ®
    query = f"""
    SELECT
      user_id,
      product_id,
      CASE 
        WHEN event_type = 'purchase' THEN 1.0
        WHEN event_type = 'add_to_cart' THEN 0.7
        WHEN event_type = 'view' THEN 0.3
        ELSE 0.0
      END as interaction_score,
      TIMESTAMP_DIFF(CURRENT_TIMESTAMP(), timestamp, HOUR) as recency_hours
    FROM
      `{project_id}.{dataset_id}.user_events`
    WHERE
      event_type IN ('view', 'add_to_cart', 'purchase')
      AND timestamp >= TIMESTAMP_SUB(CURRENT_TIMESTAMP(), INTERVAL 90 DAY)
    ORDER BY
      user_id, product_id
    """
    
    # æ‰§è¡ŒæŸ¥è¯¢å¹¶è·å–ç»“æœ
    df = client.query(query).to_dataframe()
    return df

def preprocess_data(df):
    """æ•°æ®é¢„å¤„ç†"""
    # è·å–ç”¨æˆ·å’Œäº§å“IDæ˜ å°„
    user_ids = df['user_id'].unique()
    product_ids = df['product_id'].unique()
    
    user_id_map = {id: i for i, id in enumerate(user_ids)}
    product_id_map = {id: i for i, id in enumerate(product_ids)}
    
    # æ·»åŠ ç´¢å¼•åˆ—
    df['user_idx'] = df['user_id'].map(user_id_map)
    df['product_idx'] = df['product_id'].map(product_id_map)
    
    # å½’ä¸€åŒ–äº¤äº’åˆ†æ•°
    df['normalized_score'] = df['interaction_score'] / df['interaction_score'].max()
    
    return df, user_id_map, product_id_map

def create_model(num_users, num_products, embedding_dim=50):
    """åˆ›å»ºæ¨èæ¨¡å‹"""
    # ç”¨æˆ·åµŒå…¥
    user_input = tf.keras.layers.Input(shape=(1,), name='user_input')
    user_embedding = tf.keras.layers.Embedding(
        num_users, 
        embedding_dim, 
        name='user_embedding'
    )(user_input)
    user_vec = tf.keras.layers.Flatten(name='user_flatten')(user_embedding)
    
    # äº§å“åµŒå…¥
    product_input = tf.keras.layers.Input(shape=(1,), name='product_input')
    product_embedding = tf.keras.layers.Embedding(
        num_products, 
        embedding_dim, 
        name='product_embedding'
    )(product_input)
    product_vec = tf.keras.layers.Flatten(name='product_flatten')(product_embedding)
    
    # åˆå¹¶å‘é‡
    concat = tf.keras.layers.Concatenate()([user_vec, product_vec])
    
    # å…¨è¿æ¥å±‚
    dense1 = tf.keras.layers.Dense(128, activation='relu')(concat)
    dropout = tf.keras.layers.Dropout(0.2)(dense1)
    dense2 = tf.keras.layers.Dense(64, activation='relu')(dropout)
    
    # è¾“å‡ºå±‚
    output = tf.keras.layers.Dense(1, activation='sigmoid', name='output')(dense2)
    
    # åˆ›å»ºæ¨¡å‹
    model = tf.keras.Model(
        inputs=[user_input, product_input], 
        outputs=output
    )
    
    # ç¼–è¯‘æ¨¡å‹
    model.compile(
        optimizer='adam',
        loss='binary_crossentropy',
        metrics=['accuracy', tf.keras.metrics.AUC()]
    )
    
    return model

def train_model(project_id, dataset_id, model_dir):
    """è®­ç»ƒæ¨èæ¨¡å‹"""
    # åŠ è½½æ•°æ®
    df = load_data_from_bq(project_id, dataset_id)
    print(f"Loaded {len(df)} interactions")
    
    # æ•°æ®é¢„å¤„ç†
    df, user_id_map, product_id_map = preprocess_data(df)
    
    # åˆ›å»ºæ¨¡å‹
    num_users = len(user_id_map)
    num_products = len(product_id_map)
    model = create_model(num_users, num_products)
    model.summary()
    
    # å‡†å¤‡è®­ç»ƒæ•°æ®
    train_inputs = [
        df['user_idx'].values,
        df['product_idx'].values
    ]
    train_targets = df['normalized_score'].values
    
    # è®­ç»ƒæ¨¡å‹
    history = model.fit(
        train_inputs, 
        train_targets,
        epochs=10,
        batch_size=128,
        validation_split=0.2,
        verbose=1
    )
    
    # ä¿å­˜æ¨¡å‹
    os.makedirs(model_dir, exist_ok=True)
    model.save(model_dir)
    
    # ä¿å­˜æ˜ å°„è¡¨
    with open(os.path.join(model_dir, 'user_id_map.json'), 'w') as f:
        json.dump(user_id_map, f)
    
    with open(os.path.join(model_dir, 'product_id_map.json'), 'w') as f:
        json.dump(product_id_map, f)
    
    print(f"Model saved to {model_dir}")
    
    return model, history

def main():
    parser = argparse.ArgumentParser()
    parser.add_argument('--project-id', required=True)
    parser.add_argument('--dataset-id', default='ecommerce_data')
    parser.add_argument('--model-dir', required=True)
    
    args = parser.parse_args()
    
    # è®­ç»ƒæ¨¡å‹
    model, history = train_model(
        args.project_id,
        args.dataset_id,
        args.model_dir
    )
    
    # ä¸Šä¼ æ¨¡å‹åˆ°GCS
    storage_client = storage.Client()
    bucket_name, prefix = args.model_dir.replace('gs://', '').split('/', 1)
    bucket = storage_client.bucket(bucket_name)
    
    for local_file in tf.io.gfile.glob(f'{args.model_dir}/**/*'):
        if tf.io.gfile.isdir(local_file):
            continue
            
        relative_path = local_file.replace(args.model_dir + '/', '')
        blob = bucket.blob(f'{prefix}/{relative_path}')
        blob.upload_from_filename(local_file)

if __name__ == '__main__':
    main()
```

#### æ­¥éª¤5ï¼šéƒ¨ç½²é¢„æµ‹æœåŠ¡

```python
# recommendation_api.py - æ¨èAPIæœåŠ¡
import os
import json
import numpy as np
import tensorflow as tf
from flask import Flask, request, jsonify
from google.cloud import bigquery, storage

app = Flask(__name__)

# å…¨å±€å˜é‡ï¼Œç”¨äºåŠ è½½æ¨¡å‹å’Œæ˜ å°„è¡¨
model = None
user_id_map = None
product_id_map = None
reverse_product_id_map = None
bq_client = None

def load_model(model_dir):
    """åŠ è½½æ¨èæ¨¡å‹"""
    global model, user_id_map, product_id_map, reverse_product_id_map
    
    # åŠ è½½æ¨¡å‹
    model = tf.keras.models.load_model(model_dir)
    
    # åŠ è½½IDæ˜ å°„è¡¨
    with open(os.path.join(model_dir, 'user_id_map.json'), 'r') as f:
        user_id_map = json.load(f)
    
    with open(os.path.join(model_dir, 'product_id_map.json'), 'r') as f:
        product_id_map = json.load(f)
    
    # åˆ›å»ºäº§å“IDåå‘æ˜ å°„
    reverse_product_id_map = {v: k for k, v in product_id_map.items()}
    
    print(f"Model loaded from {model_dir}")

def get_user_features(user_id):
    """è·å–ç”¨æˆ·ç‰¹å¾"""
    query = f"""
    SELECT *
    FROM `{os.environ.get('PROJECT_ID')}.ecommerce_data.user_features`
    WHERE user_id = @user_id
    """
    
    job_config = bigquery.QueryJobConfig(
        query_parameters=[
            bigquery.ScalarQueryParameter("user_id", "STRING", user_id)
        ]
    )
    
    result = bq_client.query(query, job_config=job_config).to_dataframe()
    
    if len(result) == 0:
        return {}
    
    return result.iloc[0].to_dict()

def recommend_products(user_id, num_recommendations=10):
    """ä¸ºç”¨æˆ·æ¨èäº§å“"""
    global model, user_id_map, product_id_map, reverse_product_id_map
    
    # æ£€æŸ¥ç”¨æˆ·æ˜¯å¦å­˜åœ¨
    if user_id not in user_id_map:
        # ä¸ºæ–°ç”¨æˆ·è¿”å›çƒ­é—¨äº§å“
        query = f"""
        SELECT product_id, COUNT(*) as view_count
        FROM `{os.environ.get('PROJECT_ID')}.ecommerce_data.user_events`
        WHERE event_type = 'view'
        GROUP BY product_id
        ORDER BY view_count DESC
        LIMIT {num_recommendations}
        """
        
        result = bq_client.query(query).to_dataframe()
        return result['product_id'].tolist()
    
    # è·å–ç”¨æˆ·ç´¢å¼•
    user_idx = user_id_map[user_id]
    
    # ç”Ÿæˆæ‰€æœ‰äº§å“çš„é¢„æµ‹åˆ†æ•°
    product_indices = np.arange(len(product_id_map))
    user_indices = np.full(len(product_id_map), user_idx)
    
    # æ‰¹é‡é¢„æµ‹
    predictions = model.predict([user_indices, product_indices])
    
    # è·å–æ¨èäº§å“ï¼ˆæ’é™¤ç”¨æˆ·å·²ç»äº¤äº’è¿‡çš„äº§å“ï¼‰
    query = f"""
    SELECT DISTINCT product_id
    FROM `{os.environ.get('PROJECT_ID')}.ecommerce_data.user_events`
    WHERE user_id = @user_id
    """
    
    job_config = bigquery.QueryJobConfig(
        query_parameters=[
            bigquery.ScalarQueryParameter("user_id", "STRING", user_id)
        ]
    )
    
    interacted_products = set(
        bq_client.query(query, job_config=job_config).to_dataframe()['product_id'].tolist()
    )
    
    # æŒ‰é¢„æµ‹åˆ†æ•°æ’åºå¹¶è¿‡æ»¤
    product_scores = list(zip(product_indices, predictions.flatten()))
    product_scores.sort(key=lambda x: x[1], reverse=True)
    
    recommendations = []
    for product_idx, score in product_scores:
        product_id = reverse_product_id_map[product_idx]
        if product_id not in interacted_products:
            recommendations.append(product_id)
            if len(recommendations) >= num_recommendations:
                break
    
    return recommendations

@app.route('/health')
def health_check():
    """å¥åº·æ£€æŸ¥ç«¯ç‚¹"""
    return jsonify({"status": "healthy"})

@app.route('/recommend/<user_id>')
def get_recommendations(user_id):
    """è·å–ç”¨æˆ·æ¨è"""
    try:
        num_recommendations = request.args.get('count', 10, type=int)
        recommendations = recommend_products(user_id, num_recommendations)
        
        # è·å–æ¨èäº§å“çš„è¯¦ç»†ä¿¡æ¯
        if not recommendations:
            return jsonify({"recommendations": []})
        
        product_ids_str = ",".join([f"'{pid}'" for pid in recommendations])
        query = f"""
        SELECT *
        FROM `{os.environ.get('PROJECT_ID')}.ecommerce_data.products`
        WHERE product_id IN ({product_ids_str})
        """
        
        products = bq_client.query(query).to_dataframe().to_dict('records')
        
        return jsonify({
            "user_id": user_id,
            "recommendations": products
        })
    
    except Exception as e:
        return jsonify({"error": str(e)}), 500

@app.route('/event', methods=['POST'])
def log_event():
    """è®°å½•ç”¨æˆ·äº‹ä»¶"""
    try:
        event_data = request.json
        
        # åœ¨ç”Ÿäº§ç¯å¢ƒä¸­ï¼Œè¿™é‡Œåº”è¯¥å°†äº‹ä»¶å‘é€åˆ°Pub/Sub
        # ä½†ä¸ºäº†ç®€åŒ–ï¼Œæˆ‘ä»¬ç›´æ¥æ’å…¥BigQuery
        
        table_id = f"{os.environ.get('PROJECT_ID')}.ecommerce_data.user_events"
        rows_to_insert = [event_data]
        
        errors = bq_client.insert_rows_json(table_id, rows_to_insert)
        
        if not errors:
            return jsonify({"status": "success"})
        else:
            return jsonify({"errors": errors}), 400
    
    except Exception as e:
        return jsonify({"error": str(e)}), 500

if __name__ == '__main__':
    # åŠ è½½æ¨¡å‹
    model_dir = os.environ.get('MODEL_DIR', 'gs://your-project-model-bucket/recommendation_model')
    load_model(model_dir)
    
    # åˆå§‹åŒ–BigQueryå®¢æˆ·ç«¯
    bq_client = bigquery.Client()
    
    # å¯åŠ¨Flaskåº”ç”¨
    app.run(host='0.0.0.0', port=int(os.environ.get('PORT', 8080)))
```

#### æ­¥éª¤6ï¼šéƒ¨ç½²åº”ç”¨åˆ°Cloud Run

```bash
# åˆ›å»ºrequirements.txt
cat > requirements.txt <<EOF
flask==2.2.2
google-cloud-bigquery==3.6.0
google-cloud-storage==2.5.0
tensorflow==2.10.0
pandas==1.5.0
numpy==1.23.3
EOF

# åˆ›å»ºDockerfile
cat > Dockerfile <<EOF
FROM python:3.9-slim

WORKDIR /app

COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

COPY . .

EXPOSE 8080

CMD ["python", "recommendation_api.py"]
EOF

# è®­ç»ƒæ¨¡å‹
python train_recommendation_model.py \
  --project-id=$PROJECT_ID \
  --model-dir=gs://${PROJECT_ID}-ecommerce-data/models/recommendation_model

# æ„å»ºå’Œéƒ¨ç½²Dockeré•œåƒ
export IMAGE_URI="gcr.io/${PROJECT_ID}/ecommerce-recommendation:latest"
gcloud builds submit --tag ${IMAGE_URI}

# éƒ¨ç½²åˆ°Cloud Run
gcloud run deploy ecommerce-recommendation \
  --image ${IMAGE_URI} \
  --platform managed \
  --region $REGION \
  --allow-unauthenticated \
  --memory=2Gi \
  --cpu=1 \
  --set-env-vars "PROJECT_ID=${PROJECT_ID},MODEL_DIR=gs://${PROJECT_ID}-ecommerce-data/models/recommendation_model"

# è·å–æœåŠ¡URL
SERVICE_URL=$(gcloud run services describe ecommerce-recommendation \
  --region $REGION \
  --format='value(status.url)')

echo "Recommendation service deployed at: $SERVICE_URL"
```

#### æ­¥éª¤7ï¼šæµ‹è¯•ç³»ç»Ÿ

```bash
# æµ‹è¯•å¥åº·æ£€æŸ¥
curl $SERVICE_URL/health

# æµ‹è¯•æ¨èAPI
curl "$SERVICE_URL/recommend/user123?count=5"

# æ¨¡æ‹Ÿç”¨æˆ·äº‹ä»¶
curl -X POST "$SERVICE_URL/event" \
  -H "Content-Type: application/json" \
  -d '{
    "user_id": "user123",
    "product_id": "1001",
    "event_type": "view",
    "timestamp": "2023-01-01T12:00:00Z"
  }'

# åˆ›å»ºç®€å•å‰ç«¯åº”ç”¨è¿›è¡Œæµ‹è¯•
mkdir -p frontend
cat > frontend/index.html <<EOF
<!DOCTYPE html>
<html>
<head>
    <title>E-commerce Recommendation Demo</title>
    <style>
        body { font-family: Arial, sans-serif; max-width: 1200px; margin: 0 auto; padding: 20px; }
        .product-grid { display: grid; grid-template-columns: repeat(auto-fill, minmax(300px, 1fr)); gap: 20px; }
        .product { border: 1px solid #ddd; padding: 15px; border-radius: 8px; }
        .product img { width: 100%; height: 200px; object-fit: cover; }
        .product h3 { margin-top: 0; }
        .product p { color: #666; }
        .price { font-weight: bold; color: #e74c3c; }
        button { background-color: #3498db; color: white; border: none; padding: 8px 12px; border-radius: 4px; cursor: pointer; }
    </style>
</head>
<body>
    <h1>E-commerce Recommendation Demo</h1>
    <div id="user-id">User ID: <input type="text" id="userId" value="user123"></div>
    <button id="getRecommendations">Get Recommendations</button>
    <div id="recommendations" class="product-grid"></div>

    <h2>All Products</h2>
    <div id="products" class="product-grid"></div>

    <script>
        const SERVICE_URL = '$SERVICE_URL';
        
        document.getElementById('getRecommendations').addEventListener('click', async () => {
            const userId = document.getElementById('userId').value;
            try {
                const response = await fetch(`${SERVICE_URL}/recommend/${userId}`);
                const data = await response.json();
                
                const recommendationsDiv = document.getElementById('recommendations');
                recommendationsDiv.innerHTML = '';
                
                data.recommendations.forEach(product => {
                    const productDiv = document.createElement('div');
                    productDiv.className = 'product';
                    productDiv.innerHTML = \`
                        <h3>\${product.product_name}</h3>
                        <p>Category: \${product.category}</p>
                        <p class="price">$\${product.price}</p>
                        <p>\${product.description}</p>
                        <button onclick="logEvent('\${userId}', '\${product.product_id}', 'view')">View</button>
                        <button onclick="logEvent('\${userId}', '\${product.product_id}', 'add_to_cart')">Add to Cart</button>
                    \`;
                    recommendationsDiv.appendChild(productDiv);
                });
            } catch (error) {
                console.error('Error fetching recommendations:', error);
                alert('Failed to fetch recommendations');
            }
        });
        
        async function logEvent(userId, productId, eventType) {
            try {
                await fetch(\`\${SERVICE_URL}/event\`, {
                    method: 'POST',
                    headers: {
                        'Content-Type': 'application/json'
                    },
                    body: JSON.stringify({
                        user_id: userId,
                        product_id: productId,
                        event_type: eventType,
                        timestamp: new Date().toISOString()
                    })
                });
                console.log(\`Event logged: \${eventType} for product \${productId}\`);
            } catch (error) {
                console.error('Error logging event:', error);
            }
        }
        
        // åŠ è½½æ‰€æœ‰äº§å“
        (async () => {
            try {
                // è¿™é‡Œåº”è¯¥æœ‰ä¸€ä¸ªäº§å“åˆ—è¡¨APIï¼Œä½†æˆ‘ä»¬ç®€åŒ–å¤„ç†
                const products = [
                    {product_id: "1001", product_name: "Smartphone Pro", category: "Electronics", price: 799.99, description: "Latest smartphone with advanced features"},
                    {product_id: "1002", product_name: "Laptop Ultra", category: "Electronics", price: 1299.99, description: "High-performance laptop for professionals"},
                    {product_id: "1003", product_name: "Wireless Earbuds", category: "Electronics", price: 199.99, description: "Premium wireless earbuds with noise cancellation"},
                    {product_id: "1004", product_name: "Smart Watch", category: "Electronics", price: 349.99, description: "Advanced fitness tracking smartwatch"},
                    {product_id: "1005", product_name: "Tablet Plus", category: "Electronics", price: 599.99, description: "Portable tablet for work and entertainment"}
                ];
                
                const productsDiv = document.getElementById('products');
                
                products.forEach(product => {
                    const productDiv = document.createElement('div');
                    productDiv.className = 'product';
                    productDiv.innerHTML = \`
                        <h3>\${product.product_name}</h3>
                        <p>Category: \${product.category}</p>
                        <p class="price">$\${product.price}</p>
                        <p>\${product.description}</p>
                        <button onclick="logEvent('\${document.getElementById('userId').value}', '\${product.product_id}', 'view')">View</button>
                        <button onclick="logEvent('\${document.getElementById('userId').value}', '\${product.product_id}', 'add_to_cart')">Add to Cart</button>
                    \`;
                    productsDiv.appendChild(productDiv);
                });
            } catch (error) {
                console.error('Error loading products:', error);
            }
        })();
    </script>
</body>
</html>
EOF

# éƒ¨ç½²å‰ç«¯åº”ç”¨
gsutil cp -r frontend/* gs://${PROJECT_ID}-ecommerce-data/frontend/
gsutil web config set -m index.html -e 404.html gs://${PROJECT_ID}-ecommerce-data/frontend/

# è®¾ç½®ç½‘ç«™é…ç½®
gsutil iam ch allUsers:objectViewer gs://${PROJECT_ID}-ecommerce-data/frontend

# è·å–å‰ç«¯URL
echo "Frontend deployed at: https://storage.googleapis.com/${PROJECT_ID}-ecommerce-data/frontend/index.html"
```

#### æ­¥éª¤8ï¼šæ¸…ç†èµ„æº

```bash
# åˆ é™¤Cloud RunæœåŠ¡
gcloud run services delete ecommerce-recommendation --region $REGION

# åˆ é™¤Pub/Subèµ„æº
gcloud pubsub subscriptions delete user-events-sub
gcloud pubsub topics delete user-events

# åˆ é™¤Dataflowä½œä¸š
gcloud dataflow jobs list --region $REGION
gcloud dataflow jobs delete [JOB_ID] --region $REGION

# åˆ é™¤BigQueryæ•°æ®é›†
bq rm -r -f $PROJECT_ID:ecommerce_data

# åˆ é™¤å­˜å‚¨æ¡¶
gsutil -m rm -r gs://${PROJECT_ID}-ecommerce-data
```

### å®éªŒæ€»ç»“

é€šè¿‡è¿™ä¸ªå®éªŒï¼Œæ‚¨å·²ç»ï¼š

1. **æ„å»ºäº†å®æ—¶æ•°æ®é‡‡é›†å’Œå¤„ç†ç®¡é“**ï¼Œä½¿ç”¨Pub/Subå’ŒDataflow
2. **ä½¿ç”¨BigQueryè¿›è¡Œæ•°æ®åˆ†æå’Œå­˜å‚¨**ï¼Œåˆ›å»ºäº†ç‰¹å¾å·¥ç¨‹æµç¨‹
3. **è®­ç»ƒäº†æœºå™¨å­¦ä¹ æ¨¡å‹**ï¼Œå®ç°äº†äº§å“æ¨èç³»ç»Ÿ
4. **éƒ¨ç½²äº†æ¨¡å‹å¹¶æä¾›å®æ—¶é¢„æµ‹æœåŠ¡**ï¼Œä½¿ç”¨Cloud Run
5. **å®ç°äº†ç«¯åˆ°ç«¯çš„æ•°æ®é©±åŠ¨AIåº”ç”¨**ï¼ŒåŒ…æ‹¬æ•°æ®é‡‡é›†ã€å¤„ç†ã€åˆ†æå’Œæ¨è

è¿™ä¸ªå®éªŒå±•ç¤ºäº†å¦‚ä½•ç»“åˆGCPçš„æ•°æ®åˆ†æå’Œæœºå™¨å­¦ä¹ æœåŠ¡ï¼Œæ„å»ºå®Œæ•´çš„æ•°æ®é©±åŠ¨AIåº”ç”¨ã€‚

---

## ğŸ“š æœ¬ç« å°ç»“

æœ¬ç« æˆ‘ä»¬å­¦ä¹ äº†ï¼š

1. **GCPæ•°æ®åˆ†æå¹³å°æ¦‚è§ˆ**ï¼šæ•°æ®ç”Ÿå‘½å‘¨æœŸå’Œæ ¸å¿ƒæœåŠ¡
2. **BigQueryæ•°æ®ä»“åº“**ï¼šæ•°æ®æ¨¡å‹ã€æ“ä½œå’Œé«˜çº§åŠŸèƒ½
3. **å®æ—¶æ•°æ®å¤„ç†**ï¼šPub/Subæ¶ˆæ¯æœåŠ¡å’ŒDataflowæ•°æ®æµå¤„ç†
4. **GCPæœºå™¨å­¦ä¹ å¹³å°**ï¼šVertex AIå¹³å°å’ŒAutoMLæœåŠ¡
5. **æ¨¡å‹éƒ¨ç½²ä¸ç®¡ç†**ï¼šæ¨¡å‹éƒ¨ç½²é€‰é¡¹å’Œç›‘æ§ç®¡ç†
6. **ç»¼åˆå®éªŒ**ï¼šæ„å»ºæ•°æ®é©±åŠ¨AIåº”ç”¨

### ğŸ¯ å…³é”®çŸ¥è¯†ç‚¹å›é¡¾

- **BigQuery** æ˜¯å…¨æ‰˜ç®¡æ•°æ®ä»“åº“ï¼Œé€‚åˆå¤§è§„æ¨¡æ•°æ®åˆ†æå’Œå•†ä¸šæ™ºèƒ½
- **Pub/Sub** æ˜¯æ¶ˆæ¯æœåŠ¡ï¼Œç”¨äºå®æ—¶æ•°æ®é‡‡é›†å’Œäº‹ä»¶é©±åŠ¨æ¶æ„
- **Dataflow** æ˜¯æ•°æ®æµå¤„ç†æœåŠ¡ï¼ŒåŸºäºApache Beamï¼Œæ”¯æŒæ‰¹å¤„ç†å’Œæµå¤„ç†
- **Vertex AI** æ˜¯ç»Ÿä¸€æœºå™¨å­¦ä¹ å¹³å°ï¼Œè¦†ç›–æ¨¡å‹å¼€å‘ã€è®­ç»ƒå’Œéƒ¨ç½²å…¨æµç¨‹
- **AutoML** ä½¿éMLä¸“å®¶ä¹Ÿèƒ½æ„å»ºé«˜è´¨é‡çš„è‡ªå®šä¹‰æœºå™¨å­¦ä¹ æ¨¡å‹
- **æ¨¡å‹ç›‘æ§å’Œç®¡ç†** æ˜¯MLOpsçš„é‡è¦ç»„æˆéƒ¨åˆ†ï¼Œç¡®ä¿æ¨¡å‹åœ¨ç”Ÿäº§ç¯å¢ƒä¸­çš„æ€§èƒ½

### ğŸš€ ä¸‹ä¸€æ­¥

å®Œæˆæœ¬ç« å­¦ä¹ åï¼Œæ‚¨å¯ä»¥ï¼š

1. æ·±å…¥å®è·µå¤§æ•°æ®åˆ†æå’Œæœºå™¨å­¦ä¹ çš„é«˜çº§åŠŸèƒ½
2. æ¢ç´¢MLOpså’Œæ¨¡å‹ç”Ÿå‘½å‘¨æœŸç®¡ç†
3. ç»§ç»­å­¦ä¹ ä¸‹ä¸€ç« "ä¼ä¸šçº§åº”ç”¨ä¸æœ€ä½³å®è·µ"
4. å°è¯•å®Œæˆ[å¤§æ•°æ®åˆ†æç¤ºä¾‹](./code/bigquery-analysis/)å’Œ[æœºå™¨å­¦ä¹ ç®¡é“ç¤ºä¾‹](./code/ml-pipeline/)ä¸­çš„æ›´å¤šå®éªŒ

---

## ğŸ“– å»¶ä¼¸é˜…è¯»

- [BigQuery æ–‡æ¡£](https://cloud.google.com/bigquery/docs)
- [Dataflow æ–‡æ¡£](https://cloud.google.com/dataflow/docs)
- [Pub/Sub æ–‡æ¡£](https://cloud.google.com/pubsub/docs)
- [Vertex AI æ–‡æ¡£](https://cloud.google.com/vertex-ai/docs)
- [AutoML æ–‡æ¡£](https://cloud.google.com/automl/docs)

---

**ğŸ’¡ æç¤ºï¼šæ•°æ®é©±åŠ¨AIåº”ç”¨çš„æˆåŠŸå…³é”®åœ¨äºé«˜è´¨é‡çš„æ•°æ®å’Œç‰¹å¾å·¥ç¨‹ã€‚æŠ•å…¥è¶³å¤Ÿçš„æ—¶é—´åœ¨æ•°æ®å‡†å¤‡å’Œç‰¹å¾å·¥ç¨‹ä¸Šï¼Œå¾€å¾€æ¯”é€‰æ‹©å¤æ‚çš„æ¨¡å‹æ›´æœ‰ä»·å€¼ã€‚**