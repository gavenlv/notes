-- Day 9: ClickHouse ç›‘æŽ§å’Œè¿ç»´ç¤ºä¾‹
-- =============================================

-- 1. ç³»ç»Ÿç›‘æŽ§åŸºç¡€æŸ¥è¯¢
-- =============================================

-- æ£€æŸ¥ClickHouseç‰ˆæœ¬å’ŒåŸºæœ¬ä¿¡æ¯
SELECT 'Basic System Information' as demo_section;
SELECT 
    version() as clickhouse_version,
    hostName() as hostname,
    uptime() as uptime_seconds,
    formatReadableSize(total_memory) as total_memory
FROM system.one
CROSS JOIN (
    SELECT value as total_memory 
    FROM system.asynchronous_metrics 
    WHERE metric = 'MemoryTotal'
);

-- æ£€æŸ¥å½“å‰è¿è¡Œçš„æŸ¥è¯¢
SELECT 'Current Running Queries' as demo_section;
SELECT 
    query_id,
    user,
    elapsed,
    formatReadableSize(memory_usage) as memory_used,
    formatReadableSize(read_bytes) as bytes_read,
    read_rows,
    substring(query, 1, 100) as query_preview
FROM system.processes 
WHERE query != ''
ORDER BY elapsed DESC;

-- 2. æŸ¥è¯¢æ€§èƒ½ç›‘æŽ§
-- =============================================

-- åˆ›å»ºç›‘æŽ§æ•°æ®åº“
CREATE DATABASE IF NOT EXISTS monitoring;
USE monitoring;

-- åˆ›å»ºæŸ¥è¯¢æ€§èƒ½æ±‡æ€»è¡¨
DROP TABLE IF EXISTS query_performance_summary;
CREATE TABLE query_performance_summary (
    date Date,
    hour UInt8,
    query_count UInt32,
    avg_duration_ms Float64,
    p50_duration_ms Float64,
    p95_duration_ms Float64,
    p99_duration_ms Float64,
    total_read_rows UInt64,
    total_read_bytes UInt64,
    error_count UInt32
) ENGINE = MergeTree()
ORDER BY (date, hour)
PARTITION BY toYYYYMM(date);

-- æ’å…¥åŽ†å²æ€§èƒ½æ•°æ®æ±‡æ€»
INSERT INTO query_performance_summary
SELECT 
    toDate(event_time) as date,
    toHour(event_time) as hour,
    count() as query_count,
    avg(query_duration_ms) as avg_duration_ms,
    quantile(0.5)(query_duration_ms) as p50_duration_ms,
    quantile(0.95)(query_duration_ms) as p95_duration_ms,
    quantile(0.99)(query_duration_ms) as p99_duration_ms,
    sum(read_rows) as total_read_rows,
    sum(read_bytes) as total_read_bytes,
    countIf(type = 'ExceptionWhileProcessing') as error_count
FROM system.query_log 
WHERE event_date >= today() - 7
  AND type IN ('QueryFinish', 'ExceptionWhileProcessing')
GROUP BY date, hour
ORDER BY date DESC, hour DESC;

-- æŸ¥çœ‹æ€§èƒ½æ±‡æ€»æ•°æ®
SELECT 'Query Performance Summary (Last 24 Hours)' as demo_section;
SELECT 
    hour,
    query_count,
    round(avg_duration_ms, 2) as avg_duration_ms,
    round(p95_duration_ms, 2) as p95_duration_ms,
    formatReadableSize(total_read_bytes) as total_bytes_read,
    error_count,
    round(error_count * 100.0 / query_count, 2) as error_rate_percent
FROM query_performance_summary 
WHERE date = today()
ORDER BY hour DESC
LIMIT 24;

-- 3. æ…¢æŸ¥è¯¢åˆ†æž
-- =============================================

-- åˆ›å»ºæ…¢æŸ¥è¯¢åˆ†æžè¡¨
DROP TABLE IF EXISTS slow_query_analysis;
CREATE TABLE slow_query_analysis (
    query_date Date,
    normalized_query String,
    execution_count UInt32,
    avg_duration_ms Float64,
    max_duration_ms UInt64,
    avg_memory_usage UInt64,
    avg_read_rows UInt64,
    total_read_bytes UInt64
) ENGINE = MergeTree()
ORDER BY (query_date, avg_duration_ms)
PARTITION BY toYYYYMM(query_date);

-- åˆ†æžæ…¢æŸ¥è¯¢æ¨¡å¼
INSERT INTO slow_query_analysis
SELECT 
    toDate(event_time) as query_date,
    normalizeQuery(query) as normalized_query,
    count() as execution_count,
    avg(query_duration_ms) as avg_duration_ms,
    max(query_duration_ms) as max_duration_ms,
    avg(memory_usage) as avg_memory_usage,
    avg(read_rows) as avg_read_rows,
    sum(read_bytes) as total_read_bytes
FROM system.query_log 
WHERE event_date >= today() - 3
  AND type = 'QueryFinish'
  AND query_duration_ms > 5000  -- æ…¢æŸ¥è¯¢é˜ˆå€¼5ç§’
  AND query NOT LIKE '%system.%'  -- æŽ’é™¤ç³»ç»ŸæŸ¥è¯¢
GROUP BY query_date, normalized_query
HAVING execution_count >= 2  -- è‡³å°‘æ‰§è¡Œ2æ¬¡
ORDER BY avg_duration_ms DESC;

-- æŸ¥çœ‹TOPæ…¢æŸ¥è¯¢
SELECT 'Top Slow Queries Analysis' as demo_section;
SELECT 
    normalized_query,
    execution_count,
    round(avg_duration_ms, 2) as avg_duration_ms,
    max_duration_ms,
    formatReadableSize(avg_memory_usage) as avg_memory_used,
    formatReadableSize(total_read_bytes) as total_bytes_read
FROM slow_query_analysis 
WHERE query_date >= today() - 1
ORDER BY avg_duration_ms DESC
LIMIT 10;

-- 4. ç”¨æˆ·è¡Œä¸ºåˆ†æž
-- =============================================

-- åˆ›å»ºç”¨æˆ·æ´»åŠ¨ç»Ÿè®¡è¡¨
DROP TABLE IF EXISTS user_activity_stats;
CREATE TABLE user_activity_stats (
    date Date,
    user String,
    query_count UInt32,
    avg_duration_ms Float64,
    total_read_rows UInt64,
    total_read_bytes UInt64,
    error_count UInt32,
    unique_queries UInt32
) ENGINE = MergeTree()
ORDER BY (date, user)
PARTITION BY toYYYYMM(date);

-- ç»Ÿè®¡ç”¨æˆ·æ´»åŠ¨æ•°æ®
INSERT INTO user_activity_stats
SELECT 
    toDate(event_time) as date,
    user,
    count() as query_count,
    avg(query_duration_ms) as avg_duration_ms,
    sum(read_rows) as total_read_rows,
    sum(read_bytes) as total_read_bytes,
    countIf(type = 'ExceptionWhileProcessing') as error_count,
    uniq(normalizeQuery(query)) as unique_queries
FROM system.query_log 
WHERE event_date >= today() - 7
  AND user != ''
GROUP BY date, user;

-- ç”¨æˆ·æ´»åŠ¨åˆ†æž
SELECT 'User Activity Analysis (Today)' as demo_section;
SELECT 
    user,
    query_count,
    round(avg_duration_ms, 2) as avg_duration_ms,
    formatReadableSize(total_read_bytes) as data_processed,
    error_count,
    round(error_count * 100.0 / query_count, 2) as error_rate_percent,
    unique_queries
FROM user_activity_stats 
WHERE date = today()
ORDER BY query_count DESC
LIMIT 10;

-- 5. ç³»ç»Ÿèµ„æºç›‘æŽ§
-- =============================================

-- åˆ›å»ºç³»ç»Ÿèµ„æºç›‘æŽ§è¡¨
DROP TABLE IF EXISTS system_resource_monitoring;
CREATE TABLE system_resource_monitoring (
    timestamp DateTime,
    cpu_usage_percent Float64,
    memory_usage_percent Float64,
    disk_usage_percent Float64,
    active_queries UInt32,
    total_connections UInt32
) ENGINE = MergeTree()
ORDER BY timestamp
PARTITION BY toYYYYMM(timestamp);

-- æ¨¡æ‹Ÿç³»ç»Ÿèµ„æºæ•°æ®æ’å…¥ï¼ˆå®žé™…çŽ¯å¢ƒä¸­è¿™äº›æ•°æ®æ¥è‡ªmetric_logï¼‰
INSERT INTO system_resource_monitoring
SELECT 
    toDateTime(toUnixTimestamp(now()) - number * 60) as timestamp,
    rand() % 30 + 20 as cpu_usage_percent,  -- æ¨¡æ‹ŸCPUä½¿ç”¨çŽ‡ 20-50%
    rand() % 40 + 30 as memory_usage_percent,  -- æ¨¡æ‹Ÿå†…å­˜ä½¿ç”¨çŽ‡ 30-70%
    rand() % 20 + 60 as disk_usage_percent,  -- æ¨¡æ‹Ÿç£ç›˜ä½¿ç”¨çŽ‡ 60-80%
    rand() % 10 + 1 as active_queries,  -- æ¨¡æ‹Ÿæ´»è·ƒæŸ¥è¯¢æ•° 1-10
    rand() % 50 + 10 as total_connections  -- æ¨¡æ‹Ÿè¿žæŽ¥æ•° 10-60
FROM numbers(1440)  -- æœ€è¿‘24å°æ—¶ï¼Œæ¯åˆ†é’Ÿä¸€ä¸ªæ•°æ®ç‚¹
WHERE timestamp >= now() - INTERVAL 24 HOUR;

-- èµ„æºä½¿ç”¨è¶‹åŠ¿åˆ†æž
SELECT 'System Resource Trends (Last 6 Hours)' as demo_section;
SELECT 
    toStartOfHour(timestamp) as hour,
    round(avg(cpu_usage_percent), 2) as avg_cpu_percent,
    round(avg(memory_usage_percent), 2) as avg_memory_percent,
    round(avg(disk_usage_percent), 2) as avg_disk_percent,
    round(avg(active_queries), 2) as avg_active_queries,
    round(avg(total_connections), 2) as avg_connections
FROM system_resource_monitoring 
WHERE timestamp >= now() - INTERVAL 6 HOUR
GROUP BY hour
ORDER BY hour DESC;

-- 6. é”™è¯¯ç›‘æŽ§å’Œåˆ†æž
-- =============================================

-- åˆ›å»ºé”™è¯¯ç›‘æŽ§è¡¨
DROP TABLE IF EXISTS error_monitoring;
CREATE TABLE error_monitoring (
    error_date Date,
    error_hour UInt8,
    error_type String,
    error_count UInt32,
    affected_users Array(String),
    sample_queries Array(String)
) ENGINE = MergeTree()
ORDER BY (error_date, error_hour, error_type)
PARTITION BY toYYYYMM(error_date);

-- æ’å…¥é”™è¯¯ç›‘æŽ§æ•°æ®
INSERT INTO error_monitoring
SELECT 
    toDate(event_time) as error_date,
    toHour(event_time) as error_hour,
    if(exception != '', exception, 'Unknown Error') as error_type,
    count() as error_count,
    groupUniqArray(user) as affected_users,
    groupArray(substring(query, 1, 200)) as sample_queries
FROM system.query_log 
WHERE event_date >= today() - 3
  AND type IN ('ExceptionBeforeStart', 'ExceptionWhileProcessing')
GROUP BY error_date, error_hour, error_type
ORDER BY error_date DESC, error_hour DESC;

-- é”™è¯¯ç»Ÿè®¡åˆ†æž
SELECT 'Error Analysis (Last 24 Hours)' as demo_section;
SELECT 
    error_hour,
    error_type,
    error_count,
    length(affected_users) as unique_users_affected,
    if(length(sample_queries) > 0, sample_queries[1], '') as sample_query
FROM error_monitoring 
WHERE error_date = today()
ORDER BY error_count DESC
LIMIT 15;

-- 7. è¡¨å’Œæ•°æ®åº“å­˜å‚¨åˆ†æž
-- =============================================

-- æ•°æ®åº“å­˜å‚¨ç»Ÿè®¡
SELECT 'Database Storage Analysis' as demo_section;
SELECT 
    database,
    count() as table_count,
    sum(rows) as total_rows,
    formatReadableSize(sum(data_compressed_bytes)) as compressed_size,
    formatReadableSize(sum(data_uncompressed_bytes)) as uncompressed_size,
    round(sum(data_compressed_bytes) / sum(data_uncompressed_bytes), 3) as compression_ratio,
    formatReadableSize(sum(primary_key_bytes_in_memory)) as index_size
FROM system.parts 
WHERE active = 1
GROUP BY database
ORDER BY sum(data_compressed_bytes) DESC;

-- åˆ†åŒºå­˜å‚¨åˆ†æž
SELECT 'Partition Storage Analysis' as demo_section;
SELECT 
    database,
    table,
    partition,
    rows,
    formatReadableSize(data_compressed_bytes) as compressed_size,
    round(compression_ratio, 3) as compression_ratio,
    modification_time
FROM system.parts 
WHERE active = 1
  AND database NOT IN ('system', '_temporary_and_external_tables')
ORDER BY data_compressed_bytes DESC
LIMIT 20;

-- 8. æŸ¥è¯¢æ¨¡å¼åˆ†æž
-- =============================================

-- åˆ›å»ºæŸ¥è¯¢æ¨¡å¼åˆ†æžè¡¨
DROP TABLE IF EXISTS query_pattern_analysis;
CREATE TABLE query_pattern_analysis (
    analysis_date Date,
    query_pattern String,
    execution_count UInt32,
    avg_duration_ms Float64,
    total_data_processed UInt64,
    peak_hour UInt8
) ENGINE = MergeTree()
ORDER BY (analysis_date, execution_count)
PARTITION BY toYYYYMM(analysis_date);

-- åˆ†æžæŸ¥è¯¢æ¨¡å¼
INSERT INTO query_pattern_analysis
SELECT 
    toDate(event_time) as analysis_date,
    CASE 
        WHEN query LIKE 'SELECT%FROM%WHERE%' THEN 'Filtered SELECT'
        WHEN query LIKE 'SELECT%FROM%GROUP BY%' THEN 'Aggregation Query'
        WHEN query LIKE 'SELECT%FROM%ORDER BY%' THEN 'Sorted Query'
        WHEN query LIKE 'INSERT%' THEN 'Data Insert'
        WHEN query LIKE 'CREATE%' THEN 'DDL Operation'
        WHEN query LIKE 'ALTER%' THEN 'Table Modification'
        ELSE 'Other Queries'
    END as query_pattern,
    count() as execution_count,
    avg(query_duration_ms) as avg_duration_ms,
    sum(read_bytes) as total_data_processed,
    argMax(toHour(event_time), count()) as peak_hour
FROM system.query_log 
WHERE event_date >= today() - 7
  AND type = 'QueryFinish'
  AND query NOT LIKE '%system.%'
GROUP BY analysis_date, query_pattern
ORDER BY analysis_date DESC, execution_count DESC;

-- æŸ¥è¯¢æ¨¡å¼ç»Ÿè®¡
SELECT 'Query Pattern Analysis (Last 3 Days)' as demo_section;
SELECT 
    query_pattern,
    sum(execution_count) as total_executions,
    round(avg(avg_duration_ms), 2) as avg_duration_ms,
    formatReadableSize(sum(total_data_processed)) as total_data_processed,
    argMax(peak_hour, sum(execution_count)) as most_active_hour
FROM query_pattern_analysis 
WHERE analysis_date >= today() - 3
GROUP BY query_pattern
ORDER BY total_executions DESC;

-- 9. æ€§èƒ½åŸºå‡†æµ‹è¯•
-- =============================================

-- åˆ›å»ºæ€§èƒ½æµ‹è¯•è¡¨
DROP TABLE IF EXISTS performance_benchmark;
CREATE TABLE performance_benchmark (
    test_id UInt32,
    test_name String,
    test_query String,
    execution_time_ms UInt64,
    rows_processed UInt64,
    bytes_processed UInt64,
    memory_used UInt64,
    test_timestamp DateTime
) ENGINE = MergeTree()
ORDER BY test_timestamp;

-- æ’å…¥åŸºå‡†æµ‹è¯•æ•°æ®
INSERT INTO performance_benchmark VALUES
(1, 'Simple Aggregation', 'SELECT count() FROM numbers(1000000)', 0, 0, 0, 0, now()),
(2, 'Complex JOIN', 'SELECT * FROM numbers(100000) n1 JOIN numbers(100000) n2 ON n1.number = n2.number', 0, 0, 0, 0, now()),
(3, 'GROUP BY Query', 'SELECT number % 1000, count() FROM numbers(1000000) GROUP BY number % 1000', 0, 0, 0, 0, now());

-- æ‰§è¡ŒåŸºå‡†æµ‹è¯•æŸ¥è¯¢å¹¶è®°å½•æ€§èƒ½
SELECT 'Performance Benchmark Tests' as demo_section;

-- æµ‹è¯•1: ç®€å•èšåˆ
SET max_execution_time = 300;
SELECT 
    'ç®€å•èšåˆæµ‹è¯•' as test_name,
    count() as result_count,
    'SELECT count() FROM numbers(1000000)' as test_query
FROM numbers(1000000);

-- æµ‹è¯•2: å¤æ‚åˆ†ç»„
SELECT 
    'å¤æ‚åˆ†ç»„æµ‹è¯•' as test_name,
    count() as group_count,
    'SELECT number % 1000, count() FROM numbers(1000000) GROUP BY number % 1000' as test_query
FROM (
    SELECT number % 1000 as group_key, count() as cnt
    FROM numbers(1000000) 
    GROUP BY number % 1000
);

-- æµ‹è¯•3: æ•°æ®å¤„ç†æ€§èƒ½
SELECT 
    'æ•°æ®å¤„ç†æµ‹è¯•' as test_name,
    count() as processed_rows,
    'Complex calculation on large dataset' as test_query
FROM (
    SELECT 
        number,
        sqrt(number) as sqrt_val,
        sin(number / 1000.0) as sin_val,
        toString(number) as str_val
    FROM numbers(500000)
    WHERE number % 7 = 0
);

-- 10. ç›‘æŽ§å‘Šè­¦æ¨¡æ‹Ÿ
-- =============================================

-- åˆ›å»ºå‘Šè­¦è§„åˆ™è¡¨
DROP TABLE IF EXISTS alert_rules;
CREATE TABLE alert_rules (
    rule_id UInt32,
    rule_name String,
    metric_query String,
    threshold_value Float64,
    operator String,
    severity String,
    description String,
    is_active UInt8
) ENGINE = Memory;

-- æ’å…¥å‘Šè­¦è§„åˆ™
INSERT INTO alert_rules VALUES
(1, 'High Query Duration', 'SELECT quantile(0.95)(query_duration_ms) FROM system.query_log WHERE event_time >= now() - 300', 10000, '>', 'warning', '95%æŸ¥è¯¢æ—¶é—´è¶…è¿‡10ç§’', 1),
(2, 'High Error Rate', 'SELECT countIf(type = ''ExceptionWhileProcessing'') * 100.0 / count() FROM system.query_log WHERE event_time >= now() - 300', 5, '>', 'critical', 'é”™è¯¯çŽ‡è¶…è¿‡5%', 1),
(3, 'Low Query Count', 'SELECT count() FROM system.query_log WHERE event_time >= now() - 300', 10, '<', 'warning', 'æŸ¥è¯¢æ•°é‡å¼‚å¸¸åä½Ž', 1),
(4, 'High Memory Usage', 'SELECT avg(memory_usage) FROM system.processes', 1000000000, '>', 'warning', 'å¹³å‡å†…å­˜ä½¿ç”¨è¶…è¿‡1GB', 1);

-- æ‰§è¡Œå‘Šè­¦æ£€æŸ¥
SELECT 'Alert Rules Evaluation' as demo_section;
SELECT 
    rule_name,
    threshold_value,
    operator,
    severity,
    CASE 
        WHEN rule_name = 'High Query Duration' THEN (
            SELECT quantile(0.95)(query_duration_ms) 
            FROM system.query_log 
            WHERE event_time >= now() - 300 
              AND type = 'QueryFinish'
        )
        WHEN rule_name = 'High Error Rate' THEN (
            SELECT if(count() > 0, countIf(type = 'ExceptionWhileProcessing') * 100.0 / count(), 0)
            FROM system.query_log 
            WHERE event_time >= now() - 300
        )
        WHEN rule_name = 'Low Query Count' THEN (
            SELECT count() 
            FROM system.query_log 
            WHERE event_time >= now() - 300
        )
        ELSE 0
    END as current_value,
    CASE 
        WHEN rule_name = 'High Query Duration' AND (
            SELECT quantile(0.95)(query_duration_ms) 
            FROM system.query_log 
            WHERE event_time >= now() - 300 AND type = 'QueryFinish'
        ) > threshold_value THEN 'ALERT'
        WHEN rule_name = 'High Error Rate' AND (
            SELECT if(count() > 0, countIf(type = 'ExceptionWhileProcessing') * 100.0 / count(), 0)
            FROM system.query_log 
            WHERE event_time >= now() - 300
        ) > threshold_value THEN 'ALERT'
        WHEN rule_name = 'Low Query Count' AND (
            SELECT count() 
            FROM system.query_log 
            WHERE event_time >= now() - 300
        ) < threshold_value THEN 'ALERT'
        ELSE 'OK'
    END as alert_status
FROM alert_rules 
WHERE is_active = 1;

-- 11. æ¸…ç†å’Œä¼˜åŒ–
-- =============================================

-- è¡¨ä¼˜åŒ–
OPTIMIZE TABLE query_performance_summary FINAL;
OPTIMIZE TABLE slow_query_analysis FINAL;
OPTIMIZE TABLE user_activity_stats FINAL;
OPTIMIZE TABLE system_resource_monitoring FINAL;

-- ç»Ÿè®¡ä¿¡æ¯æ›´æ–°
SELECT 'Monitoring Tables Summary' as demo_section;
SELECT 
    'monitoring' as database,
    name as table_name,
    total_rows,
    formatReadableSize(total_bytes) as size_on_disk
FROM system.tables 
WHERE database = 'monitoring'
  AND engine LIKE '%MergeTree%'
ORDER BY total_rows DESC;

-- æœ€ç»ˆç›‘æŽ§æŠ¥å‘Š
SELECT '=== Day 9 Monitoring Demo Summary ===' as summary_section;

SELECT 
    'Monitoring Tables Created' as metric,
    toString(count()) as value
FROM system.tables 
WHERE database = 'monitoring'

UNION ALL

SELECT 
    'Performance Records Analyzed' as metric,
    toString(sum(total_rows)) as value
FROM system.tables 
WHERE database = 'monitoring'
  AND name LIKE '%performance%'

UNION ALL

SELECT 
    'Alert Rules Configured' as metric,
    toString(count()) as value
FROM alert_rules

UNION ALL

SELECT 
    'Recent Queries Monitored' as metric,
    toString(count()) as value
FROM system.query_log 
WHERE event_date = today()

UNION ALL

SELECT 
    'System Uptime' as metric,
    toString(uptime()) || ' seconds' as value
FROM system.one;

-- æ¼”ç¤ºå®Œæˆæç¤º
SELECT 
    'ðŸŽ‰ Day 9 ç›‘æŽ§å’Œè¿ç»´æ¼”ç¤ºå®Œæˆï¼' as message,
    'ðŸ“Š å·²åˆ›å»ºå®Œæ•´çš„ç›‘æŽ§ä½“ç³»å’Œåˆ†æžæŠ¥å‘Š' as summary;

-- æ³¨æ„ï¼šæ­¤æ¼”ç¤ºå±•ç¤ºäº†ç›‘æŽ§å’Œè¿ç»´çš„æ ¸å¿ƒæ¦‚å¿µå’Œå®žè·µ
-- ç”Ÿäº§çŽ¯å¢ƒä¸­éœ€è¦ç»“åˆPrometheusã€Grafanaç­‰å¤–éƒ¨ç›‘æŽ§å·¥å…· 