# Day 7: ClickHouse æ•°æ®å¯¼å…¥å¯¼å‡ºå…¨æ”»ç•¥

## å­¦ä¹ ç›®æ ‡ ğŸ¯
- æŒæ¡ClickHouseå¤šç§æ•°æ®æ ¼å¼çš„å¯¼å…¥å¯¼å‡º
- å­¦ä¼šä¸å„ç§å¤–éƒ¨æ•°æ®æºé›†æˆ
- ç†è§£æ‰¹é‡æ•°æ®å¤„ç†å’Œå®æ—¶æ•°æ®æµ
- æŒæ¡æ•°æ®è¿ç§»å’ŒåŒæ­¥ç­–ç•¥
- å­¦ä¼šæ€§èƒ½ä¼˜åŒ–å’Œæœ€ä½³å®è·µ

## ä¸ºä»€ä¹ˆDay 7å­¦æ•°æ®å¯¼å…¥å¯¼å‡ºï¼Ÿ ğŸ¤”

ç»è¿‡å‰6å¤©çš„å­¦ä¹ ï¼š
- âœ… Day 1: ç¯å¢ƒæ­å»º - åŸºç¡€è®¾æ–½å°±ç»ª
- âœ… Day 2: ç†è®ºåŸºç¡€ - æ¶æ„åŸç†æ¸…æ™°
- âœ… Day 3: äº‘ç«¯éƒ¨ç½² - ç”Ÿäº§ç¯å¢ƒå°±ç»ª
- âœ… Day 4: SQLè¯­æ³• - åŸºç¡€æ“ä½œç†Ÿç»ƒ
- âœ… Day 5: è¡¨å¼•æ“ - å­˜å‚¨æœºåˆ¶æŒæ¡
- âœ… Day 6: æŸ¥è¯¢ä¼˜åŒ– - æ€§èƒ½è°ƒä¼˜ç²¾é€š

ç°åœ¨å­¦ä¹ **æ•°æ®å¯¼å…¥å¯¼å‡º**ï¼Œè¿™æ˜¯ClickHouseå®é™…åº”ç”¨çš„æ ¸å¿ƒæŠ€èƒ½ï¼

### å­¦ä¹ è·¯å¾„å›é¡¾
```
Day 1: ç¯å¢ƒæ­å»º âœ… â†’ Day 2: ç†è®ºåŸºç¡€ âœ… â†’ Day 3: äº‘ç«¯éƒ¨ç½² âœ… â†’ Day 4: SQLè¯­æ³• âœ… â†’ Day 5: è¡¨å¼•æ“ âœ… â†’ Day 6: æŸ¥è¯¢ä¼˜åŒ– âœ… â†’ Day 7: æ•°æ®å¯¼å…¥å¯¼å‡º
```

## çŸ¥è¯†è¦ç‚¹ ğŸ“š

### 1. æ•°æ®å¯¼å…¥å¯¼å‡ºæ¦‚è§ˆ

#### 1.1 æ”¯æŒçš„æ•°æ®æ ¼å¼

ClickHouseæ”¯æŒä¸°å¯Œçš„æ•°æ®æ ¼å¼ï¼Œæ»¡è¶³å„ç§åœºæ™¯éœ€æ±‚ã€‚

```mermaid
graph TD
    A[ClickHouseæ•°æ®æ ¼å¼] --> B[æ–‡æœ¬æ ¼å¼]
    A --> C[äºŒè¿›åˆ¶æ ¼å¼]
    A --> D[ç»“æ„åŒ–æ ¼å¼]
    A --> E[æµå¼æ ¼å¼]
    
    B --> B1[CSV - é€—å·åˆ†éš”]
    B --> B2[TSV - åˆ¶è¡¨ç¬¦åˆ†éš”]
    B --> B3[TabSeparated - åˆ¶è¡¨ç¬¦]
    B --> B4[CustomSeparated - è‡ªå®šä¹‰åˆ†éš”ç¬¦]
    
    C --> C1[Native - ClickHouseåŸç”Ÿ]
    C --> C2[RowBinary - è¡ŒäºŒè¿›åˆ¶]
    C --> C3[Parquet - åˆ—å¼å­˜å‚¨]
    C --> C4[ORC - ä¼˜åŒ–è¡Œåˆ—å¼]
    
    D --> D1[JSON - JSONæ ¼å¼]
    D --> D2[JSONEachRow - æ¯è¡ŒJSON]
    D --> D3[XML - XMLæ ¼å¼]
    D --> D4[YAML - YAMLæ ¼å¼]
    
    E --> E1[Kafka - æ¶ˆæ¯é˜Ÿåˆ—]
    E --> E2[Protobuf - åè®®ç¼“å†²]
    E --> E3[Avro - æ•°æ®åºåˆ—åŒ–]
    E --> E4[Arrow - å†…å­˜åˆ—å¼]
```

#### 1.2 æ•°æ®å¯¼å…¥å¯¼å‡ºæ–¹å¼

| æ–¹å¼ | ç”¨é€” | æ€§èƒ½ | é€‚ç”¨åœºæ™¯ |
|------|------|------|----------|
| INSERTè¯­å¥ | å°æ‰¹é‡æ’å…¥ | ä¸­ç­‰ | åº”ç”¨ç¨‹åºé›†æˆ |
| clickhouse-client | å‘½ä»¤è¡Œå¯¼å…¥ | é«˜ | æ‰¹é‡æ•°æ®è¿ç§» |
| HTTPæ¥å£ | RESTful API | é«˜ | Webåº”ç”¨é›†æˆ |
| æ–‡ä»¶å¯¼å…¥ | å¤§æ–‡ä»¶å¤„ç† | æœ€é«˜ | æ•°æ®ä»“åº“ETL |
| æµå¼å¯¼å…¥ | å®æ—¶æ•°æ® | é«˜ | å®æ—¶åˆ†æ |
| è¡¨å¼•æ“é›†æˆ | ç›´æ¥è¿æ¥ | æœ€é«˜ | æ•°æ®æ¹–é›†æˆ |

### 2. æ–‡ä»¶æ ¼å¼å¯¼å…¥å¯¼å‡º

#### 2.1 CSVæ ¼å¼å¤„ç†

```sql
-- åˆ›å»ºæµ‹è¯•è¡¨
CREATE TABLE user_analytics (
    user_id UInt32,
    event_date Date,
    page_views UInt32,
    session_duration UInt32,
    country String,
    device_type String,
    revenue Decimal(10, 2)
) ENGINE = MergeTree()
ORDER BY (user_id, event_date)
PARTITION BY toYYYYMM(event_date);

-- ä»CSVæ–‡ä»¶å¯¼å…¥
-- clickhouse-client --query="INSERT INTO user_analytics FORMAT CSV" < data.csv

-- æˆ–è€…ä½¿ç”¨INFILEè¯­æ³•ï¼ˆéœ€è¦file()å‡½æ•°ï¼‰
INSERT INTO user_analytics 
SELECT * FROM file('data/user_analytics.csv', 'CSV', 
    'user_id UInt32, event_date Date, page_views UInt32, 
     session_duration UInt32, country String, device_type String, revenue Decimal(10, 2)');

-- å¯¼å‡ºåˆ°CSV
SELECT * FROM user_analytics 
WHERE event_date >= '2024-01-01'
FORMAT CSV
INTO OUTFILE 'export_data.csv';

-- å¸¦å¤´éƒ¨çš„CSVå¯¼å‡º
SELECT 'user_id', 'event_date', 'page_views', 'session_duration', 'country', 'device_type', 'revenue'
UNION ALL
SELECT toString(user_id), toString(event_date), toString(page_views), 
       toString(session_duration), country, device_type, toString(revenue)
FROM user_analytics 
FORMAT CSV;
```

#### 2.2 JSONæ ¼å¼å¤„ç†

```sql
-- JSONæ ¼å¼å¯¼å…¥
INSERT INTO user_analytics FORMAT JSONEachRow
{"user_id": 1001, "event_date": "2024-01-01", "page_views": 25, "session_duration": 1800, "country": "China", "device_type": "mobile", "revenue": 99.99}
{"user_id": 1002, "event_date": "2024-01-01", "page_views": 15, "session_duration": 1200, "country": "USA", "device_type": "desktop", "revenue": 149.99}

-- ä»JSONæ–‡ä»¶å¯¼å…¥
INSERT INTO user_analytics 
SELECT * FROM file('data/events.json', 'JSONEachRow', 
    'user_id UInt32, event_date Date, page_views UInt32, 
     session_duration UInt32, country String, device_type String, revenue Decimal(10, 2)');

-- å¯¼å‡ºä¸ºJSON
SELECT * FROM user_analytics 
WHERE country = 'China'
FORMAT JSONEachRow;

-- åµŒå¥—JSONå¤„ç†
SELECT 
    JSONExtractUInt(raw_data, 'user_id') as user_id,
    JSONExtractString(raw_data, 'country') as country,
    JSONExtractFloat(raw_data, 'revenue') as revenue
FROM (
    SELECT '{"user_id": 1001, "country": "China", "revenue": 99.99}' as raw_data
);
```

#### 2.3 Parquetæ ¼å¼å¤„ç†

```sql
-- Parquetæ˜¯é«˜æ•ˆçš„åˆ—å¼å­˜å‚¨æ ¼å¼
-- å¯¼å…¥Parquetæ–‡ä»¶
INSERT INTO user_analytics 
SELECT * FROM file('data/analytics.parquet', 'Parquet');

-- å¯¼å‡ºä¸ºParquetæ ¼å¼
SELECT * FROM user_analytics 
FORMAT Parquet 
INTO OUTFILE 'analytics_export.parquet';

-- Parquetæ ¼å¼ç‰¹åˆ«é€‚åˆå¤§æ•°æ®åœºæ™¯
-- æŸ¥çœ‹Parquetæ–‡ä»¶ç»“æ„
SELECT * FROM file('data/analytics.parquet', 'Parquet') LIMIT 5;
```

### 3. æ•°æ®åº“é›†æˆ

#### 3.1 MySQLé›†æˆ

```sql
-- åˆ›å»ºMySQLè¡¨å¼•æ“è¿æ¥
CREATE TABLE mysql_users (
    id UInt32,
    name String,
    email String,
    created_at DateTime,
    status String
) ENGINE = MySQL('mysql_host:3306', 'database_name', 'users', 'username', 'password');

-- ä»MySQLå¯¼å…¥æ•°æ®åˆ°ClickHouse
INSERT INTO user_analytics 
SELECT 
    id as user_id,
    toDate(created_at) as event_date,
    0 as page_views,
    0 as session_duration,
    'Unknown' as country,
    'Unknown' as device_type,
    0.00 as revenue
FROM mysql_users 
WHERE status = 'active';

-- ä½¿ç”¨MySQLå‡½æ•°è¿›è¡Œå¤æ‚è½¬æ¢
CREATE TABLE mysql_orders (
    order_id UInt32,
    user_id UInt32,
    amount Decimal(10, 2),
    order_date DateTime,
    status String
) ENGINE = MySQL('mysql_host:3306', 'ecommerce', 'orders', 'user', 'pass');

-- èšåˆMySQLæ•°æ®å¯¼å…¥
INSERT INTO user_analytics 
SELECT 
    user_id,
    toDate(order_date) as event_date,
    count() as page_views,
    0 as session_duration,
    'Unknown' as country,
    'Unknown' as device_type,
    sum(amount) as revenue
FROM mysql_orders 
WHERE order_date >= '2024-01-01'
GROUP BY user_id, toDate(order_date);
```

#### 3.2 PostgreSQLé›†æˆ

```sql
-- PostgreSQLè¡¨å¼•æ“
CREATE TABLE postgres_events (
    event_id UInt64,
    user_id UInt32,
    event_type String,
    event_time DateTime,
    properties String
) ENGINE = PostgreSQL('postgres_host:5432', 'analytics', 'events', 'username', 'password');

-- ä»PostgreSQLå¯¼å…¥JSONæ•°æ®
INSERT INTO user_analytics 
SELECT 
    user_id,
    toDate(event_time) as event_date,
    JSONExtractUInt(properties, 'page_views') as page_views,
    JSONExtractUInt(properties, 'session_duration') as session_duration,
    JSONExtractString(properties, 'country') as country,
    JSONExtractString(properties, 'device_type') as device_type,
    JSONExtractFloat(properties, 'revenue') as revenue
FROM postgres_events 
WHERE event_type = 'user_summary';
```

#### 3.3 MongoDBé›†æˆï¼ˆé€šè¿‡å¤–éƒ¨å·¥å…·ï¼‰

```bash
# ä½¿ç”¨mongoexportå¯¼å‡ºæ•°æ®
mongoexport --host mongodb_host:27017 \
    --db analytics \
    --collection user_events \
    --type json \
    --out events.json

# è½¬æ¢ä¸ºClickHouseå¯è¯»æ ¼å¼
cat events.json | jq -c '{
    user_id: .user_id,
    event_date: .event_date,
    page_views: .page_views,
    session_duration: .session_duration,
    country: .country,
    device_type: .device_type,
    revenue: .revenue
}' > events_formatted.json

# å¯¼å…¥åˆ°ClickHouse
clickhouse-client --query="INSERT INTO user_analytics FORMAT JSONEachRow" < events_formatted.json
```

### 4. æµå¼æ•°æ®å¤„ç†

#### 4.1 Kafkaé›†æˆ

```sql
-- åˆ›å»ºKafkaè¡¨å¼•æ“
CREATE TABLE kafka_events (
    user_id UInt32,
    event_time DateTime,
    event_type String,
    page_url String,
    session_id String,
    properties String
) ENGINE = Kafka()
SETTINGS 
    kafka_broker_list = 'kafka_host:9092',
    kafka_topic_list = 'user_events',
    kafka_group_name = 'clickhouse_consumer',
    kafka_format = 'JSONEachRow',
    kafka_num_consumers = 3,
    kafka_max_block_size = 1048576;

-- åˆ›å»ºç‰©åŒ–è§†å›¾å®ç°å®æ—¶ETL
CREATE MATERIALIZED VIEW kafka_to_analytics AS
SELECT 
    user_id,
    toDate(event_time) as event_date,
    countIf(event_type = 'page_view') as page_views,
    maxIf(toUInt32(JSONExtractInt(properties, 'session_duration')), 
          event_type = 'session_end') as session_duration,
    any(JSONExtractString(properties, 'country')) as country,
    any(JSONExtractString(properties, 'device_type')) as device_type,
    sumIf(JSONExtractFloat(properties, 'amount'), 
          event_type = 'purchase') as revenue
FROM kafka_events 
GROUP BY user_id, toDate(event_time);

-- ç›®æ ‡è¡¨
CREATE TABLE real_time_analytics (
    user_id UInt32,
    event_date Date,
    page_views UInt32,
    session_duration UInt32,
    country String,
    device_type String,
    revenue Decimal(10, 2)
) ENGINE = SummingMergeTree((page_views, session_duration, revenue))
ORDER BY (user_id, event_date)
PARTITION BY toYYYYMM(event_date);

-- åˆ›å»ºç‰©åŒ–è§†å›¾å†™å…¥ç›®æ ‡è¡¨
CREATE MATERIALIZED VIEW kafka_consumer_mv TO real_time_analytics AS
SELECT 
    user_id,
    event_date,
    page_views,
    session_duration,
    country,
    device_type,
    revenue
FROM kafka_to_analytics;
```

#### 4.2 å®æ—¶æ•°æ®æµå¤„ç†

```sql
-- åˆ›å»ºBufferè¡¨ä¼˜åŒ–å†™å…¥æ€§èƒ½
CREATE TABLE analytics_buffer AS real_time_analytics
ENGINE = Buffer(currentDatabase(), real_time_analytics, 16, 10, 100, 10000, 1000000, 10000000, 100000000);

-- åº”ç”¨ç¨‹åºå†™å…¥Bufferè¡¨
INSERT INTO analytics_buffer VALUES
(1001, '2024-01-01', 10, 300, 'China', 'mobile', 50.00),
(1002, '2024-01-01', 15, 450, 'USA', 'desktop', 75.00);

-- å®šæœŸåˆ·æ–°Buffer
OPTIMIZE TABLE analytics_buffer;

-- å®æ—¶ç›‘æ§æµå¼æ•°æ®
SELECT 
    toStartOfMinute(now()) as minute,
    count() as events_per_minute,
    uniq(user_id) as unique_users,
    sum(revenue) as total_revenue
FROM analytics_buffer 
WHERE event_date = today()
GROUP BY minute
ORDER BY minute DESC
LIMIT 10;
```

### 5. å¤§æ•°æ®é›†æˆ

#### 5.1 HDFSé›†æˆ

```sql
-- ä»HDFSè¯»å–æ•°æ®
INSERT INTO user_analytics 
SELECT * FROM hdfs('hdfs://namenode:9000/data/analytics/*.parquet', 'Parquet',
    'user_id UInt32, event_date Date, page_views UInt32, 
     session_duration UInt32, country String, device_type String, revenue Decimal(10, 2)');

-- å¯¼å‡ºæ•°æ®åˆ°HDFS
INSERT INTO FUNCTION hdfs('hdfs://namenode:9000/export/analytics_export.parquet', 'Parquet',
    'user_id UInt32, event_date Date, page_views UInt32, 
     session_duration UInt32, country String, device_type String, revenue Decimal(10, 2)')
SELECT * FROM user_analytics 
WHERE event_date >= '2024-01-01';
```

#### 5.2 S3é›†æˆ

```sql
-- ä»S3è¯»å–æ•°æ®
INSERT INTO user_analytics 
SELECT * FROM s3('https://s3.amazonaws.com/my-bucket/data/*.csv', 'CSV',
    'user_id UInt32, event_date Date, page_views UInt32, 
     session_duration UInt32, country String, device_type String, revenue Decimal(10, 2)');

-- å¯¼å‡ºåˆ°S3
INSERT INTO FUNCTION s3('https://s3.amazonaws.com/my-bucket/export/analytics.parquet', 
    'access_key', 'secret_key', 'Parquet',
    'user_id UInt32, event_date Date, page_views UInt32, 
     session_duration UInt32, country String, device_type String, revenue Decimal(10, 2)')
SELECT * FROM user_analytics;

-- åˆ†åŒºå¯¼å‡ºåˆ°S3
INSERT INTO FUNCTION s3('https://s3.amazonaws.com/my-bucket/export/year={year}/month={month}/data.parquet', 
    'access_key', 'secret_key', 'Parquet',
    'user_id UInt32, event_date Date, page_views UInt32, 
     session_duration UInt32, country String, device_type String, revenue Decimal(10, 2)')
SELECT 
    user_id, event_date, page_views, session_duration, country, device_type, revenue,
    toYear(event_date) as year,
    toMonth(event_date) as month
FROM user_analytics;
```

### 6. æ€§èƒ½ä¼˜åŒ–æŠ€å·§

#### 6.1 æ‰¹é‡å¯¼å…¥ä¼˜åŒ–

```sql
-- ä¼˜åŒ–å¯¼å…¥æ€§èƒ½çš„è®¾ç½®
SET max_insert_block_size = 1048576;          -- å¢åŠ æ’å…¥å—å¤§å°
SET min_insert_block_size_rows = 1048576;     -- æœ€å°æ’å…¥è¡Œæ•°
SET min_insert_block_size_bytes = 268435456;  -- æœ€å°æ’å…¥å­—èŠ‚æ•°
SET max_threads = 16;                         -- å¢åŠ å¹¶è¡Œçº¿ç¨‹æ•°

-- ç¦ç”¨åŒæ­¥å†™å…¥åŠ é€Ÿå¯¼å…¥
SET insert_quorum = 0;
SET insert_quorum_timeout = 0;

-- æ‰¹é‡å¯¼å…¥å¤§æ–‡ä»¶
INSERT INTO user_analytics 
SELECT * FROM file('/path/to/large_file.csv', 'CSV',
    'user_id UInt32, event_date Date, page_views UInt32, 
     session_duration UInt32, country String, device_type String, revenue Decimal(10, 2)')
SETTINGS 
    max_threads = 16,
    max_insert_block_size = 1048576;
```

#### 6.2 å¹¶è¡Œå¯¼å…¥ç­–ç•¥

```bash
#!/bin/bash
# å¹¶è¡Œå¯¼å…¥å¤šä¸ªæ–‡ä»¶

# åˆ†å‰²å¤§æ–‡ä»¶
split -l 1000000 large_data.csv data_chunk_

# å¹¶è¡Œå¯¼å…¥
for file in data_chunk_*; do
    clickhouse-client --query="INSERT INTO user_analytics FORMAT CSV" < "$file" &
done

# ç­‰å¾…æ‰€æœ‰ä»»åŠ¡å®Œæˆ
wait

echo "æ‰€æœ‰æ–‡ä»¶å¯¼å…¥å®Œæˆ"
```

#### 6.3 å†…å­˜ä¼˜åŒ–

```sql
-- å†…å­˜ä¼˜åŒ–è®¾ç½®
SET max_memory_usage = 20000000000;           -- 20GBå†…å­˜é™åˆ¶
SET max_bytes_before_external_group_by = 10000000000;  -- å¤–éƒ¨GROUP BYé˜ˆå€¼
SET max_bytes_before_external_sort = 10000000000;      -- å¤–éƒ¨æ’åºé˜ˆå€¼

-- ä½¿ç”¨LIMITä¼˜åŒ–å¤§æ•°æ®å¯¼å‡º
SELECT * FROM huge_table 
WHERE event_date >= '2024-01-01'
ORDER BY user_id, event_date
LIMIT 10000000, 1000000  -- åˆ†é¡µå¯¼å‡º
FORMAT Parquet;
```

### 7. æ•°æ®éªŒè¯å’Œç›‘æ§

#### 7.1 æ•°æ®è´¨é‡æ£€æŸ¥

```sql
-- å¯¼å…¥åæ•°æ®éªŒè¯
SELECT 
    'Data Quality Report' as report_type,
    count() as total_rows,
    count(DISTINCT user_id) as unique_users,
    min(event_date) as min_date,
    max(event_date) as max_date,
    countIf(user_id = 0) as invalid_user_ids,
    countIf(event_date < '2020-01-01') as invalid_dates,
    countIf(revenue < 0) as negative_revenue,
    round(avg(revenue), 2) as avg_revenue
FROM user_analytics;

-- æ•°æ®å®Œæ•´æ€§æ£€æŸ¥
WITH 
    expected_count AS (
        SELECT count() as cnt FROM source_table
    ),
    actual_count AS (
        SELECT count() as cnt FROM user_analytics
    )
SELECT 
    expected_count.cnt as expected_rows,
    actual_count.cnt as actual_rows,
    actual_count.cnt / expected_count.cnt as completion_rate
FROM expected_count, actual_count;
```

#### 7.2 å¯¼å…¥ç›‘æ§

```sql
-- ç›‘æ§å¯¼å…¥è¿›åº¦
SELECT 
    table,
    partition,
    rows,
    bytes_on_disk,
    modification_time
FROM system.parts 
WHERE table = 'user_analytics' 
  AND modification_time >= now() - INTERVAL 1 HOUR
ORDER BY modification_time DESC;

-- æŸ¥çœ‹å½“å‰å¯¼å…¥ä»»åŠ¡
SELECT 
    query_id,
    user,
    query,
    elapsed,
    read_rows,
    read_bytes,
    written_rows,
    written_bytes
FROM system.processes 
WHERE query LIKE '%INSERT%'
  AND query LIKE '%user_analytics%';
```

### 8. é”™è¯¯å¤„ç†å’Œæ¢å¤

#### 8.1 é”™è¯¯å¤„ç†ç­–ç•¥

```sql
-- è®¾ç½®é”™è¯¯å®¹å¿åº¦
SET input_format_allow_errors_num = 1000;     -- å…è®¸1000ä¸ªé”™è¯¯
SET input_format_allow_errors_ratio = 0.01;   -- å…è®¸1%çš„é”™è¯¯ç‡

-- è·³è¿‡é”™è¯¯è¡Œå¯¼å…¥
INSERT INTO user_analytics 
SELECT * FROM file('data_with_errors.csv', 'CSV',
    'user_id UInt32, event_date Date, page_views UInt32, 
     session_duration UInt32, country String, device_type String, revenue Decimal(10, 2)')
SETTINGS 
    input_format_allow_errors_num = 100,
    input_format_allow_errors_ratio = 0.05;

-- è®°å½•é”™è¯¯ä¿¡æ¯
CREATE TABLE import_errors (
    timestamp DateTime DEFAULT now(),
    table_name String,
    error_message String,
    row_content String
) ENGINE = Log;
```

#### 8.2 æ•°æ®å¤‡ä»½å’Œæ¢å¤

```sql
-- åˆ›å»ºå¤‡ä»½è¡¨
CREATE TABLE user_analytics_backup AS user_analytics;

-- å¢é‡å¤‡ä»½
INSERT INTO user_analytics_backup 
SELECT * FROM user_analytics 
WHERE event_date >= today() - INTERVAL 1 DAY;

-- å¯¼å‡ºå¤‡ä»½
SELECT * FROM user_analytics_backup 
FORMAT Native 
INTO OUTFILE 'backup_20240101.native';

-- ä»å¤‡ä»½æ¢å¤
INSERT INTO user_analytics 
SELECT * FROM file('backup_20240101.native', 'Native',
    'user_id UInt32, event_date Date, page_views UInt32, 
     session_duration UInt32, country String, device_type String, revenue Decimal(10, 2)');
```

## å®è·µç»ƒä¹  ğŸ› ï¸

### ç»ƒä¹ 1ï¼šå¤šæ ¼å¼æ•°æ®å¯¼å…¥

```sql
-- åˆ›å»ºç»¼åˆæµ‹è¯•è¡¨
CREATE TABLE multi_format_test (
    id UInt32,
    name String,
    timestamp DateTime,
    value Float64,
    tags Array(String)
) ENGINE = MergeTree()
ORDER BY id;

-- æµ‹è¯•ä¸åŒæ ¼å¼å¯¼å…¥
-- CSVæ ¼å¼
INSERT INTO multi_format_test FORMAT CSV
1,"Alice","2024-01-01 10:00:00",99.5,"['tag1','tag2']"
2,"Bob","2024-01-01 11:00:00",87.3,"['tag3']"

-- JSONæ ¼å¼  
INSERT INTO multi_format_test FORMAT JSONEachRow
{"id": 3, "name": "Charlie", "timestamp": "2024-01-01 12:00:00", "value": 95.7, "tags": ["tag4", "tag5"]}

-- TSVæ ¼å¼
INSERT INTO multi_format_test FORMAT TSV
4	David	2024-01-01 13:00:00	92.1	['tag6']
```

### ç»ƒä¹ 2ï¼šå®æ—¶æ•°æ®æµæ¨¡æ‹Ÿ

è¿è¡Œday7çš„ç¤ºä¾‹æ–‡ä»¶ï¼š
```bash
clickhouse-client < day7/examples/import-export-demo.sql
```

### ç»ƒä¹ 3ï¼šæ€§èƒ½æµ‹è¯•

```sql
-- åˆ›å»ºå¤§æ•°æ®é‡æµ‹è¯•
INSERT INTO user_analytics 
SELECT 
    number % 100000 + 1 as user_id,
    toDate('2024-01-01') + toIntervalDay(number % 365) as event_date,
    (number % 50) + 1 as page_views,
    (number % 3600) + 300 as session_duration,
    ['China', 'USA', 'Japan', 'Germany', 'UK'][number % 5 + 1] as country,
    ['mobile', 'desktop', 'tablet'][number % 3 + 1] as device_type,
    round((number % 1000) * 0.99, 2) as revenue
FROM numbers(10000000);

-- æµ‹è¯•å¯¼å‡ºæ€§èƒ½
SELECT count() FROM user_analytics;
SELECT * FROM user_analytics FORMAT Parquet INTO OUTFILE 'performance_test.parquet';
```

## æœ€ä½³å®è·µæ€»ç»“ ğŸ’¡

### 1. æ ¼å¼é€‰æ‹©
- **CSV**: ç®€å•æ–‡æœ¬æ•°æ®ï¼Œæ˜“äºå¤„ç†
- **JSON**: åŠç»“æ„åŒ–æ•°æ®ï¼Œçµæ´»æ€§å¥½
- **Parquet**: å¤§æ•°æ®é‡ï¼Œé«˜å‹ç¼©æ¯”
- **Native**: ClickHouseé—´è¿ç§»ï¼Œæ€§èƒ½æœ€ä½³

### 2. æ€§èƒ½ä¼˜åŒ–
- **æ‰¹é‡å¯¼å…¥**: ä½¿ç”¨å¤§çš„block_size
- **å¹¶è¡Œå¤„ç†**: å¤šçº¿ç¨‹/å¤šè¿›ç¨‹å¯¼å…¥
- **å†…å­˜ç®¡ç†**: åˆç†è®¾ç½®å†…å­˜é™åˆ¶
- **ç½‘ç»œä¼˜åŒ–**: å‹ç¼©ä¼ è¾“ï¼Œæœ¬åœ°å¤„ç†

### 3. æ•°æ®è´¨é‡
- **é¢„éªŒè¯**: å¯¼å…¥å‰æ£€æŸ¥æ•°æ®æ ¼å¼
- **å®¹é”™æœºåˆ¶**: è®¾ç½®åˆç†çš„é”™è¯¯å®¹å¿åº¦
- **ç›‘æ§å‘Šè­¦**: å®æ—¶ç›‘æ§å¯¼å…¥çŠ¶æ€
- **å¤‡ä»½ç­–ç•¥**: é‡è¦æ•°æ®åŠæ—¶å¤‡ä»½

### 4. è¿ç»´ç®¡ç†
- **åˆ†åŒºç­–ç•¥**: æŒ‰æ—¶é—´åˆ†åŒºä¾¿äºç®¡ç†
- **æ¸…ç†æœºåˆ¶**: å®šæœŸæ¸…ç†è¿‡æœŸæ•°æ®
- **æƒé™æ§åˆ¶**: ä¸¥æ ¼çš„æ•°æ®è®¿é—®æƒé™
- **æ–‡æ¡£è®°å½•**: å®Œæ•´çš„æ•°æ®è¡€ç¼˜å…³ç³»

## å¸¸è§é—®é¢˜ â“

### Q1: å¯¼å…¥å¤§æ–‡ä»¶æ—¶å†…å­˜ä¸è¶³æ€ä¹ˆåŠï¼Ÿ
**A**: è§£å†³æ–¹æ¡ˆï¼š
- åˆ†å‰²å¤§æ–‡ä»¶ä¸ºå°æ–‡ä»¶å¹¶è¡Œå¯¼å…¥
- å¢åŠ max_memory_usageè®¾ç½®
- ä½¿ç”¨æµå¼å¯¼å…¥æ–¹å¼
- å¯ç”¨å¤–éƒ¨æ’åºå’Œåˆ†ç»„

### Q2: å¦‚ä½•å¤„ç†è„æ•°æ®ï¼Ÿ
**A**: å¤„ç†ç­–ç•¥ï¼š
- è®¾ç½®input_format_allow_errors_*å‚æ•°
- é¢„å¤„ç†æ¸…æ´—æ•°æ®
- ä½¿ç”¨Nullableç±»å‹å¤„ç†ç¼ºå¤±å€¼
- å»ºç«‹æ•°æ®è´¨é‡ç›‘æ§

### Q3: å®æ—¶å¯¼å…¥å»¶è¿Ÿé«˜æ€ä¹ˆä¼˜åŒ–ï¼Ÿ
**A**: ä¼˜åŒ–æ–¹æ³•ï¼š
- ä½¿ç”¨Bufferè¡¨ç¼“å†²å°æ‰¹é‡å†™å…¥
- è°ƒæ•´Kafkaæ¶ˆè´¹è€…æ•°é‡
- ä¼˜åŒ–ç½‘ç»œå’Œç£ç›˜I/O
- åˆç†è®¾è®¡åˆ†åŒºç­–ç•¥

### Q4: å¦‚ä½•é€‰æ‹©æœ€é€‚åˆçš„æ•°æ®æ ¼å¼ï¼Ÿ
**A**: é€‰æ‹©æŒ‡å—ï¼š
- **æ•°æ®é‡å°**: CSV/JSON
- **æ•°æ®é‡å¤§**: Parquet/ORC
- **å®æ—¶æµ**: JSONEachRow
- **ClickHouseé—´**: Native

## ä»Šæ—¥æ€»ç»“ ğŸ“‹

ä»Šå¤©æˆ‘ä»¬å…¨é¢å­¦ä¹ äº†ï¼š
- âœ… å¤šç§æ•°æ®æ ¼å¼çš„å¯¼å…¥å¯¼å‡º
- âœ… æ•°æ®åº“å’Œå¤–éƒ¨ç³»ç»Ÿé›†æˆ
- âœ… æµå¼æ•°æ®å’Œå®æ—¶å¤„ç†
- âœ… æ€§èƒ½ä¼˜åŒ–å’Œæœ€ä½³å®è·µ
- âœ… é”™è¯¯å¤„ç†å’Œç›‘æ§æ–¹æ¡ˆ

**ä¸‹ä¸€æ­¥**: Day 8 - é›†ç¾¤ç®¡ç†ä¸é«˜å¯ç”¨ï¼Œå­¦ä¹ ClickHouseçš„åˆ†å¸ƒå¼éƒ¨ç½²

---
*å­¦ä¹ è¿›åº¦: Day 7/14 å®Œæˆ* ğŸ‰ 