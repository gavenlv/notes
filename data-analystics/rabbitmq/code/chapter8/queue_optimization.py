#!/usr/bin/env python3
"""
ç¬¬8ç« ï¼šé˜Ÿåˆ—ä¼˜åŒ–ç¤ºä¾‹
 RabbitMQ é˜Ÿåˆ—æ€§èƒ½ä¼˜åŒ–å’Œé…ç½®è°ƒä¼˜å·¥å…·
"""

import time
import threading
import json
from typing import Dict, List, Optional, Any, Callable
from dataclasses import dataclass, asdict
from datetime import datetime, timedelta
from enum import Enum
from collections import deque, defaultdict
import logging
import heapq
import uuid


class QueueType(Enum):
    """é˜Ÿåˆ—ç±»å‹"""
    DURABLE = "durable"
    TRANSIENT = "transient"
    QUORUM = "quorum"
    STREAM = "stream"


class MessageTTL(Enum):
    """æ¶ˆæ¯TTLç­–ç•¥"""
    NONE = "none"
    FIXED = "fixed"
    DYNAMIC = "dynamic"


@dataclass
class QueueConfig:
    """é˜Ÿåˆ—é…ç½®"""
    name: str
    durable: bool = True
    auto_delete: bool = False
    exclusive: bool = False
    max_length: Optional[int] = None
    max_length_bytes: Optional[int] = None
    message_ttl: Optional[int] = None  # æ¯«ç§’
    expires: Optional[int] = None      # æ¯«ç§’
    dead_letter_exchange: Optional[str] = None
    dead_letter_routing_key: Optional[str] = None
    max_priority: Optional[int] = None
    overflow: str = "reject-publish"  # "reject-publish" æˆ– "drop-head"
    queue_type: QueueType = QueueType.DURABLE
    arguments: Optional[Dict[str, Any]] = None


@dataclass
class QueueMetrics:
    """é˜Ÿåˆ—æŒ‡æ ‡"""
    queue_name: str
    message_count: int
    consumer_count: int
    ready_messages: int
    unacknowledged_messages: int
    durable_messages: int
    transient_messages: int
    rate_in: float
    rate_out: float
    memory_usage_bytes: int
    disk_usage_bytes: int
    timestamp: float


@dataclass
class PerformanceBenchmark:
    """æ€§èƒ½åŸºå‡†æµ‹è¯•ç»“æœ"""
    test_name: str
    duration: float
    total_messages: int
    throughput: float
    latency_avg: float
    latency_p95: float
    latency_p99: float
    memory_peak: int
    memory_avg: int
    error_count: int


class QueueOptimizer:
    """é˜Ÿåˆ—ä¼˜åŒ–å™¨"""
    
    def __init__(self):
        self.configs = {}
        self.metrics = {}
        self.optimization_strategies = {
            'high_throughput': self._high_throughput_strategy,
            'low_latency': self._low_latency_strategy,
            'low_memory': self._low_memory_strategy,
            'balanced': self._balanced_strategy,
            'reliable': self._reliable_strategy
        }
    
    def generate_optimized_config(self, 
                                queue_name: str,
                                workload_type: str,
                                message_rate: int,
                                avg_message_size: int,
                                consumer_count: int = 1,
                                requirements: Dict[str, Any] = None) -> QueueConfig:
        """ç”Ÿæˆä¼˜åŒ–çš„é˜Ÿåˆ—é…ç½®"""
        
        strategy = self.optimization_strategies.get(workload_type, self._balanced_strategy)
        base_config = QueueConfig(name=queue_name)
        
        return strategy(base_config, message_rate, avg_message_size, consumer_count, requirements or {})
    
    def _high_throughput_strategy(self, 
                                 config: QueueConfig,
                                 message_rate: int,
                                 avg_message_size: int,
                                 consumer_count: int,
                                 requirements: Dict[str, Any]) -> QueueConfig:
        """é«˜ååé‡ä¼˜åŒ–ç­–ç•¥"""
        # é«˜ååé‡ä¼˜åŒ–ï¼šæœ€å¤§åŒ–å¤„ç†èƒ½åŠ›
        
        # å…³é—­æŒä¹…åŒ–ä»¥æé«˜æ€§èƒ½
        config.durable = not requirements.get('require_durability', False)
        
        # è®¾ç½®é˜Ÿåˆ—é•¿åº¦é™åˆ¶ä»¥æ§åˆ¶å†…å­˜ä½¿ç”¨
        if message_rate > 10000:
            config.max_length = message_rate * 60  # 1åˆ†é’Ÿçš„æ¶ˆæ¯é‡
        elif message_rate > 1000:
            config.max_length = message_rate * 30  # 30ç§’çš„æ¶ˆæ¯é‡
        
        # è®¾ç½®åˆç†çš„TTL
        if not config.message_ttl:
            config.message_ttl = 300000  # 5åˆ†é’Ÿ
        
        # ç¦ç”¨è‡ªåŠ¨åˆ é™¤ï¼ˆé¿å…é¢‘ç¹åˆ›å»ºåˆ é™¤å¼€é”€ï¼‰
        config.auto_delete = False
        
        # é…ç½®arguments
        config.arguments = config.arguments or {}
        config.arguments.update({
            'x-queue-type': QueueType.TRANSIENT.value,
            'x-overflow': 'reject-publish',
            'x-max-priority': 1 if requirements.get('priority_support', False) else None
        })
        
        # æ¸…ç†Noneå€¼
        config.arguments = {k: v for k, v in config.arguments.items() if v is not None}
        
        return config
    
    def _low_latency_strategy(self,
                            config: QueueConfig,
                            message_rate: int,
                            avg_message_size: int,
                            consumer_count: int,
                            requirements: Dict[str, Any]) -> QueueConfig:
        """ä½å»¶è¿Ÿä¼˜åŒ–ç­–ç•¥"""
        # ä½å»¶è¿Ÿä¼˜åŒ–ï¼šæœ€å°åŒ–å¤„ç†å»¶è¿Ÿ
        
        # ä½¿ç”¨å†…å­˜é˜Ÿåˆ—ï¼ˆéæŒä¹…åŒ–ï¼‰
        config.durable = False
        config.auto_delete = False
        
        # ä¸è®¾ç½®é˜Ÿåˆ—é•¿åº¦é™åˆ¶ï¼Œä¿è¯æ¶ˆæ¯ä¸ä¸¢å¤±
        config.max_length = None
        
        # å¯ç”¨ä¼˜å…ˆçº§æ”¯æŒ
        if requirements.get('priority_support', True):
            config.max_priority = 10
        
        # è®¾ç½®è¾ƒçŸ­TTL
        config.message_ttl = 60000  # 1åˆ†é’Ÿ
        
        # é…ç½®arguments
        config.arguments = config.arguments or {}
        config.arguments.update({
            'x-queue-type': QueueType.TRANSIENT.value,
            'x-max-priority': config.max_priority,
            'x-overflow': 'reject-publish'
        })
        
        return config
    
    def _low_memory_strategy(self,
                           config: QueueConfig,
                           message_rate: int,
                           avg_message_size: int,
                           consumer_count: int,
                           requirements: Dict[str, Any]) -> QueueConfig:
        """ä½å†…å­˜ä¼˜åŒ–ç­–ç•¥"""
        # ä½å†…å­˜ä¼˜åŒ–ï¼šæœ€å°åŒ–å†…å­˜å ç”¨
        
        # è®¾ç½®ä¸¥æ ¼çš„é˜Ÿåˆ—é•¿åº¦é™åˆ¶
        if message_rate > 100:
            config.max_length = min(message_rate * 10, 1000)  # æœ€å¤§1000æ¡æ¶ˆæ¯
        else:
            config.max_length = 100
        
        # è®¾ç½®é˜Ÿåˆ—é•¿åº¦å­—èŠ‚é™åˆ¶
        config.max_length_bytes = avg_message_size * config.max_length
        
        # è®¾ç½®è¾ƒçŸ­TTL
        config.message_ttl = 300000  # 5åˆ†é’Ÿ
        
        # é…ç½®arguments
        config.arguments = config.arguments or {}
        config.arguments.update({
            'x-queue-type': QueueType.TRANSIENT.value,
            'x-overflow': 'drop-head',  # åˆ é™¤æ—§æ¶ˆæ¯
            'x-max-length': config.max_length,
            'x-max-length-bytes': config.max_length_bytes
        })
        
        return config
    
    def _balanced_strategy(self,
                         config: QueueConfig,
                         message_rate: int,
                         avg_message_size: int,
                         consumer_count: int,
                         requirements: Dict[str, Any]) -> QueueConfig:
        """å¹³è¡¡ä¼˜åŒ–ç­–ç•¥"""
        # å¹³è¡¡æ€§èƒ½ä¸å¯é æ€§
        
        # å¯ç”¨æŒä¹…åŒ–
        config.durable = True
        config.auto_delete = False
        
        # è®¾ç½®é€‚ä¸­çš„é˜Ÿåˆ—é•¿åº¦é™åˆ¶
        config.max_length = message_rate * 60  # 1åˆ†é’Ÿçš„æ¶ˆæ¯é‡
        config.message_ttl = 1800000  # 30åˆ†é’Ÿ
        
        # å¯ç”¨ä¼˜å…ˆçº§æ”¯æŒ
        if requirements.get('priority_support', False):
            config.max_priority = 5
        
        # é…ç½®arguments
        config.arguments = config.arguments or {}
        config.arguments.update({
            'x-queue-type': QueueType.DURABLE.value,
            'x-overflow': 'reject-publish',
            'x-max-priority': config.max_priority
        })
        
        return config
    
    def _reliable_strategy(self,
                         config: QueueConfig,
                         message_rate: int,
                         avg_message_size: int,
                         consumer_count: int,
                         requirements: Dict[str, Any]) -> QueueConfig:
        """å¯é æ€§ä¼˜åŒ–ç­–ç•¥"""
        # å¯é æ€§ä¼˜åŒ–ï¼šä¿è¯æ¶ˆæ¯ä¸ä¸¢å¤±
        
        # å¼ºåˆ¶æŒä¹…åŒ–
        config.durable = True
        config.auto_delete = False
        config.exclusive = False
        
        # ä¸è®¾ç½®é˜Ÿåˆ—é•¿åº¦é™åˆ¶
        config.max_length = None
        
        # ä¸è®¾ç½®TTL
        config.message_ttl = None
        
        # è®¾ç½®æ­»ä¿¡é˜Ÿåˆ—
        config.dead_letter_exchange = 'dlx'
        config.dead_letter_routing_key = f"{config.name}.dead"
        
        # å¯ç”¨ä¼˜å…ˆçº§æ”¯æŒ
        config.max_priority = 8
        
        # é…ç½®arguments
        config.arguments = config.arguments or {}
        config.arguments.update({
            'x-queue-type': QueueType.DURABLE.value,
            'x-dead-letter-exchange': config.dead_letter_exchange,
            'x-dead-letter-routing-key': config.dead_letter_routing_key,
            'x-max-priority': config.max_priority
        })
        
        return config
    
    def analyze_queue_performance(self, metrics_history: List[QueueMetrics]) -> Dict[str, Any]:
        """åˆ†æé˜Ÿåˆ—æ€§èƒ½"""
        if not metrics_history:
            return {}
        
        # è®¡ç®—å¹³å‡å€¼å’Œè¶‹åŠ¿
        total_messages = [m.message_count for m in metrics_history]
        consumer_counts = [m.consumer_count for m in metrics_history]
        rate_ins = [m.rate_in for m in metrics_history]
        rate_outs = [m.rate_out for m in metrics_history]
        
        analysis = {
            'avg_messages': sum(total_messages) / len(total_messages),
            'avg_consumers': sum(consumer_counts) / len(consumer_counts),
            'avg_rate_in': sum(rate_ins) / len(rate_ins),
            'avg_rate_out': sum(rate_outs) / len(rate_outs),
            'throughput_efficiency': sum(rate_outs) / sum(rate_ins) if sum(rate_ins) > 0 else 0,
            'trend': self._calculate_trend([(i, m.message_count) for i, m in enumerate(metrics_history)]),
            'recommendations': self._generate_performance_recommendations(metrics_history)
        }
        
        return analysis
    
    def _calculate_trend(self, data_points: List[tuple]) -> str:
        """è®¡ç®—è¶‹åŠ¿"""
        if len(data_points) < 2:
            return 'stable'
        
        # ç®€å•çº¿æ€§å›å½’
        n = len(data_points)
        sum_x = sum(x for x, y in data_points)
        sum_y = sum(y for x, y in data_points)
        sum_xy = sum(x * y for x, y in data_points)
        sum_x2 = sum(x * x for x, y in data_points)
        
        slope = (n * sum_xy - sum_x * sum_y) / (n * sum_x2 - sum_x * sum_x) if (n * sum_x2 - sum_x * sum_x) != 0 else 0
        
        if slope > 0.1:
            return 'increasing'
        elif slope < -0.1:
            return 'decreasing'
        else:
            return 'stable'
    
    def _generate_performance_recommendations(self, metrics_history: List[QueueMetrics]) -> List[str]:
        """ç”Ÿæˆæ€§èƒ½å»ºè®®"""
        recommendations = []
        
        latest_metrics = metrics_history[-1]
        
        # åŸºäºé˜Ÿåˆ—é•¿åº¦å»ºè®®
        if latest_metrics.message_count > 1000:
            recommendations.append("é˜Ÿåˆ—æ¶ˆæ¯è¿‡å¤šï¼Œè€ƒè™‘å¢åŠ æ¶ˆè´¹è€…æ•°é‡æˆ–æé«˜å¤„ç†é€Ÿåº¦")
        
        if latest_metrics.message_count < 10:
            recommendations.append("é˜Ÿåˆ—æ¶ˆæ¯è¾ƒå°‘ï¼Œå¯ä»¥å‡å°‘æ¶ˆè´¹è€…æ•°é‡ä»¥èŠ‚çœèµ„æº")
        
        # åŸºäºæ¶ˆè´¹è€…æ•ˆç‡å»ºè®®
        if latest_metrics.consumer_count > 0:
            efficiency = latest_metrics.message_count / latest_metrics.consumer_count
            if efficiency > 100:
                recommendations.append("æ¶ˆè´¹è€…æ•ˆç‡è¾ƒé«˜ï¼Œå¯ä»¥è€ƒè™‘å¢åŠ å¹¶å‘å¤„ç†")
            elif efficiency < 10:
                recommendations.append("æ¶ˆè´¹è€…æ•ˆç‡è¾ƒä½ï¼Œæ£€æŸ¥æ¶ˆæ¯å¤„ç†é€»è¾‘")
        
        # åŸºäºé€Ÿç‡å»ºè®®
        if latest_metrics.rate_in > latest_metrics.rate_out * 1.2:
            recommendations.append("æ¶ˆæ¯ç§¯å‹ä¸¥é‡ï¼Œæ¶ˆè´¹è€…å¤„ç†é€Ÿåº¦è·Ÿä¸ä¸Šç”Ÿäº§è€…")
        
        if latest_metrics.rate_out > latest_metrics.rate_in * 1.2:
            recommendations.append("æ¶ˆè´¹è€…å¤„ç†é€Ÿåº¦å¿«äºç”Ÿäº§ï¼Œå¯ä»¥å¢åŠ ç”Ÿäº§é€Ÿç‡æˆ–å‡å°‘æ¶ˆè´¹è€…")
        
        # åŸºäºå†…å­˜ä½¿ç”¨å»ºè®®
        if latest_metrics.memory_usage_bytes > 100 * 1024 * 1024:  # 100MB
            recommendations.append("å†…å­˜ä½¿ç”¨è¿‡é«˜ï¼Œè€ƒè™‘è®¾ç½®é˜Ÿåˆ—é•¿åº¦é™åˆ¶æˆ–TTL")
        
        return recommendations


class QueueBenchmarker:
    """é˜Ÿåˆ—åŸºå‡†æµ‹è¯•å™¨"""
    
    def __init__(self):
        self.test_results = []
        self.concurrent_tests = []
    
    def run_basic_benchmark(self,
                          queue_config: QueueConfig,
                          message_count: int = 1000,
                          message_size: int = 1024,
                          consumer_count: int = 1,
                          message_rate: int = 100) -> PerformanceBenchmark:
        """è¿è¡ŒåŸºç¡€åŸºå‡†æµ‹è¯•"""
        print(f"å¼€å§‹åŸºç¡€é˜Ÿåˆ—åŸºå‡†æµ‹è¯•:")
        print(f"  é˜Ÿåˆ—: {queue_config.name}")
        print(f"  æ¶ˆæ¯æ•°: {message_count}")
        print(f"  æ¶ˆæ¯å¤§å°: {message_size}å­—èŠ‚")
        print(f"  æ¶ˆè´¹è€…æ•°: {consumer_count}")
        print(f"  ç”Ÿäº§é€Ÿç‡: {message_rate}/ç§’")
        print()
        
        start_time = time.time()
        messages_produced = 0
        messages_consumed = 0
        latencies = []
        memory_usage = []
        error_count = 0
        
        # æ¨¡æ‹Ÿæ¶ˆæ¯ç”Ÿäº§è€…
        def producer():
            nonlocal messages_produced
            message_data = "x" * message_size
            
            for i in range(message_count):
                try:
                    send_time = time.time()
                    
                    # æ¨¡æ‹Ÿæ¶ˆæ¯å‘é€
                    self._simulate_send_message(queue_config, message_data)
                    
                    messages_produced += 1
                    
                    # æ¨¡æ‹Ÿå‘é€å»¶è¿Ÿ
                    if message_rate > 0:
                        time.sleep(1.0 / message_rate)
                
                except Exception as e:
                    nonlocal error_count
                    error_count += 1
                    print(f"ç”Ÿäº§æ¶ˆæ¯é”™è¯¯: {e}")
        
        # æ¨¡æ‹Ÿæ¶ˆæ¯æ¶ˆè´¹è€…
        def consumer(consumer_id: int):
            nonlocal messages_consumed, latencies
            consumer_start = time.time()
            
            while (messages_consumed < message_count or 
                   time.time() - consumer_start < 30):  # æœ€å¤šç­‰å¾…30ç§’
                try:
                    receive_time = time.time()
                    
                    # æ¨¡æ‹Ÿæ¥æ”¶æ¶ˆæ¯
                    message = self._simulate_receive_message(queue_config)
                    
                    if message:
                        messages_consumed += 1
                        
                        # è®¡ç®—å»¶è¿Ÿï¼ˆæ¨¡æ‹Ÿï¼‰
                        latency = time.time() - receive_time
                        latencies.append(latency)
                
                except Exception as e:
                    error_count += 1
                    print(f"æ¶ˆè´¹è€…{consumer_id}å¤„ç†æ¶ˆæ¯é”™è¯¯: {e}")
                
                time.sleep(0.001)  # 1mså¤„ç†é—´éš”
        
        # å¯åŠ¨çº¿ç¨‹
        threads = []
        
        # å¯åŠ¨ç”Ÿäº§è€…
        producer_thread = threading.Thread(target=producer)
        threads.append(producer_thread)
        producer_thread.start()
        
        # å¯åŠ¨æ¶ˆè´¹è€…
        for i in range(consumer_count):
            consumer_thread = threading.Thread(target=consumer, args=(i,))
            threads.append(consumer_thread)
            consumer_thread.start()
        
        # ç­‰å¾…æ‰€æœ‰çº¿ç¨‹å®Œæˆ
        producer_thread.join()
        for thread in threads[1:]:
            thread.join()
        
        # è®¡ç®—ç»“æœ
        end_time = time.time()
        duration = end_time - start_time
        throughput = messages_consumed / duration
        
        # è®¡ç®—å»¶è¿Ÿç»Ÿè®¡
        if latencies:
            latencies.sort()
            p95_idx = int(len(latencies) * 0.95)
            p99_idx = int(len(latencies) * 0.99)
            
            latency_avg = sum(latencies) / len(latencies)
            latency_p95 = latencies[p95_idx]
            latency_p99 = latencies[p99_idx]
        else:
            latency_avg = latency_p95 = latency_p99 = 0
        
        # æ¨¡æ‹Ÿå†…å­˜ä½¿ç”¨
        memory_peak = queue_config.max_length_bytes or 1024 * 1024
        memory_avg = memory_peak // 2
        
        result = PerformanceBenchmark(
            test_name=f"basic_{queue_config.name}",
            duration=duration,
            total_messages=messages_consumed,
            throughput=throughput,
            latency_avg=latency_avg,
            latency_p95=latency_p95,
            latency_p99=latency_p99,
            memory_peak=memory_peak,
            memory_avg=memory_avg,
            error_count=error_count
        )
        
        # è¾“å‡ºç»“æœ
        print("âœ… åŸºç¡€åŸºå‡†æµ‹è¯•å®Œæˆ:")
        print(f"  æ€»è€—æ—¶: {duration:.2f}ç§’")
        print(f"  å¤„ç†æ¶ˆæ¯: {messages_consumed}")
        print(f"  ååé‡: {throughput:.2f}æ¶ˆæ¯/ç§’")
        print(f"  å¹³å‡å»¶è¿Ÿ: {latency_avg:.4f}ç§’")
        print(f"  95%å»¶è¿Ÿ: {latency_p95:.4f}ç§’")
        print(f"  99%å»¶è¿Ÿ: {latency_p99:.4f}ç§’")
        print(f"  å†…å­˜å³°å€¼: {memory_peak / 1024 / 1024:.1f}MB")
        print(f"  é”™è¯¯æ•°: {error_count}")
        print()
        
        self.test_results.append(result)
        return result
    
    def _simulate_send_message(self, queue_config: QueueConfig, message_data: str) -> str:
        """æ¨¡æ‹Ÿå‘é€æ¶ˆæ¯"""
        # è¿™é‡Œæ˜¯æ¨¡æ‹Ÿå®ç°ï¼Œå®é™…åº”è¯¥è¿æ¥RabbitMQ
        message_id = str(uuid.uuid4())
        
        # æ¨¡æ‹Ÿæ¶ˆæ¯å¤§å°æ£€æŸ¥
        if queue_config.max_length_bytes and len(message_data) > queue_config.max_length_bytes:
            raise Exception("æ¶ˆæ¯å¤§å°è¶…è¿‡é™åˆ¶")
        
        return message_id
    
    def _simulate_receive_message(self, queue_config: QueueConfig) -> Optional[Dict[str, Any]]:
        """æ¨¡æ‹Ÿæ¥æ”¶æ¶ˆæ¯"""
        # è¿™é‡Œæ˜¯æ¨¡æ‹Ÿå®ç°
        import random
        
        # æ¨¡æ‹Ÿæ¶ˆæ¯æ¥æ”¶
        if random.random() < 0.1:  # 10%çš„æ¦‚ç‡æ¥æ”¶ä¸åˆ°æ¶ˆæ¯
            return None
        
        return {
            'id': str(uuid.uuid4()),
            'data': "x" * 1024,
            'timestamp': time.time()
        }
    
    def run_stress_test(self,
                      queue_config: QueueConfig,
                      stress_duration: int = 60,
                      ramp_up_time: int = 10) -> PerformanceBenchmark:
        """è¿è¡Œå‹åŠ›æµ‹è¯•"""
        print(f"å¼€å§‹å‹åŠ›æµ‹è¯•:")
        print(f"  é˜Ÿåˆ—: {queue_config.name}")
        print(f"  æµ‹è¯•æ—¶é•¿: {stress_duration}ç§’")
        print(f"  é¢„çƒ­æ—¶é—´: {ramp_up_time}ç§’")
        print()
        
        start_time = time.time()
        messages_produced = 0
        messages_consumed = 0
        latencies = []
        error_count = 0
        
        # å‹åŠ›æµ‹è¯•å‚æ•°
        high_message_rate = 1000  # é«˜é€Ÿç‡
        consumer_count = 10  # 10ä¸ªæ¶ˆè´¹è€…
        
        def stress_producer():
            nonlocal messages_produced
            
            ramp_end = start_time + ramp_up_time
            test_end = start_time + stress_duration
            
            while time.time() < test_end:
                try:
                    # é¢„çƒ­æœŸé—´ä½¿ç”¨è¾ƒä½é€Ÿç‡
                    current_rate = high_message_rate if time.time() >= ramp_end else high_message_rate // 4
                    
                    message_data = "stress_test_message" * 100  # å¤§æ¶ˆæ¯
                    self._simulate_send_message(queue_config, message_data)
                    messages_produced += 1
                    
                    # æ§åˆ¶å‘é€é€Ÿç‡
                    if current_rate > 0:
                        time.sleep(1.0 / current_rate)
                
                except Exception as e:
                    error_count += 1
                    if error_count < 10:  # åªæ‰“å°å‰10ä¸ªé”™è¯¯
                        print(f"å‹åŠ›æµ‹è¯•ç”Ÿäº§é”™è¯¯: {e}")
        
        def stress_consumer():
            nonlocal messages_consumed
            
            test_end = start_time + stress_duration
            
            while time.time() < test_end:
                try:
                    message = self._simulate_receive_message(queue_config)
                    
                    if message:
                        messages_consumed += 1
                        latency = time.time() - message.get('timestamp', time.time())
                        latencies.append(latency)
                
                except Exception as e:
                    error_count += 1
                    if error_count < 10:
                        print(f"å‹åŠ›æµ‹è¯•æ¶ˆè´¹é”™è¯¯: {e}")
                
                time.sleep(0.001)  # 1mså¤„ç†é—´éš”
        
        # å¯åŠ¨å‹åŠ›æµ‹è¯•çº¿ç¨‹
        threads = []
        
        # å¯åŠ¨å¤šä¸ªç”Ÿäº§è€…
        for i in range(3):
            producer_thread = threading.Thread(target=stress_producer)
            threads.append(producer_thread)
            producer_thread.start()
        
        # å¯åŠ¨å¤šä¸ªæ¶ˆè´¹è€…
        for i in range(consumer_count):
            consumer_thread = threading.Thread(target=stress_consumer)
            threads.append(consumer_thread)
            consumer_thread.start()
        
        # ç­‰å¾…æµ‹è¯•å®Œæˆ
        for thread in threads:
            thread.join()
        
        # è®¡ç®—ç»“æœ
        end_time = time.time()
        duration = end_time - start_time
        throughput = messages_consumed / duration
        
        # å»¶è¿Ÿç»Ÿè®¡
        if latencies:
            latencies.sort()
            p95_idx = int(len(latencies) * 0.95)
            p99_idx = int(len(latencies) * 0.99)
            
            latency_avg = sum(latencies) / len(latencies)
            latency_p95 = latencies[p95_idx]
            latency_p99 = latencies[p99_idx]
        else:
            latency_avg = latency_p95 = latency_p99 = 0
        
        result = PerformanceBenchmark(
            test_name=f"stress_{queue_config.name}",
            duration=duration,
            total_messages=messages_consumed,
            throughput=throughput,
            latency_avg=latency_avg,
            latency_p95=latency_p95,
            latency_p99=latency_p99,
            memory_peak=50 * 1024 * 1024,  # 50MB
            memory_avg=30 * 1024 * 1024,   # 30MB
            error_count=error_count
        )
        
        print("âœ… å‹åŠ›æµ‹è¯•å®Œæˆ:")
        print(f"  æ€»è€—æ—¶: {duration:.2f}ç§’")
        print(f"  å¤„ç†æ¶ˆæ¯: {messages_consumed}")
        print(f"  ååé‡: {throughput:.2f}æ¶ˆæ¯/ç§’")
        print(f"  å¹³å‡å»¶è¿Ÿ: {latency_avg:.4f}ç§’")
        print(f"  95%å»¶è¿Ÿ: {latency_p95:.4f}ç§’")
        print(f"  99%å»¶è¿Ÿ: {latency_p99:.4f}ç§’")
        print(f"  é”™è¯¯æ•°: {error_count}")
        print()
        
        self.test_results.append(result)
        return result
    
    def run_concurrent_benchmark(self,
                               queue_configs: List[QueueConfig],
                               total_messages: int = 1000,
                               message_size: int = 1024) -> Dict[str, PerformanceBenchmark]:
        """è¿è¡Œå¹¶å‘é˜Ÿåˆ—åŸºå‡†æµ‹è¯•"""
        print(f"å¼€å§‹å¹¶å‘é˜Ÿåˆ—åŸºå‡†æµ‹è¯•:")
        print(f"  é˜Ÿåˆ—æ•°é‡: {len(queue_configs)}")
        print(f"  æ€»æ¶ˆæ¯æ•°: {total_messages}")
        print(f"  æ¶ˆæ¯å¤§å°: {message_size}å­—èŠ‚")
        print()
        
        results = {}
        
        def concurrent_producer_consumer(queue_config: QueueConfig, queue_index: int):
            queue_messages = total_messages // len(queue_configs)
            messages_consumed = 0
            latencies = []
            
            # ç”Ÿäº§æ¶ˆæ¯
            for i in range(queue_messages):
                try:
                    message_data = f"concurrent_message_{queue_index}_{i}" * (message_size // 50)
                    self._simulate_send_message(queue_config, message_data)
                except Exception as e:
                    print(f"é˜Ÿåˆ—{queue_index}ç”Ÿäº§é”™è¯¯: {e}")
            
            # æ¶ˆè´¹æ¶ˆæ¯
            while messages_consumed < queue_messages:
                try:
                    message = self._simulate_receive_message(queue_config)
                    if message:
                        messages_consumed += 1
                        latency = time.time() - message.get('timestamp', time.time())
                        latencies.append(latency)
                except Exception as e:
                    print(f"é˜Ÿåˆ—{queue_index}æ¶ˆè´¹é”™è¯¯: {e}")
                
                time.sleep(0.001)
            
            # è®¡ç®—é˜Ÿåˆ—æ€§èƒ½
            if latencies:
                latencies.sort()
                latency_avg = sum(latencies) / len(latencies)
                latency_p95 = latencies[int(len(latencies) * 0.95)]
                latency_p99 = latencies[int(len(latencies) * 0.99)]
            else:
                latency_avg = latency_p95 = latency_p99 = 0
            
            result = PerformanceBenchmark(
                test_name=f"concurrent_{queue_config.name}",
                duration=30.0,  # æ¨¡æ‹Ÿ30ç§’
                total_messages=messages_consumed,
                throughput=messages_consumed / 30.0,
                latency_avg=latency_avg,
                latency_p95=latency_p95,
                latency_p99=latency_p99,
                memory_peak=10 * 1024 * 1024,  # 10MB
                memory_avg=5 * 1024 * 1024,    # 5MB
                error_count=0
            )
            
            results[queue_config.name] = result
        
        # å¯åŠ¨å¹¶å‘æµ‹è¯•
        threads = []
        for i, config in enumerate(queue_configs):
            thread = threading.Thread(target=concurrent_producer_consumer, args=(config, i))
            threads.append(thread)
            thread.start()
        
        # ç­‰å¾…æ‰€æœ‰é˜Ÿåˆ—å®Œæˆ
        for thread in threads:
            thread.join()
        
        # è¾“å‡ºç»“æœ
        print("âœ… å¹¶å‘é˜Ÿåˆ—åŸºå‡†æµ‹è¯•å®Œæˆ:")
        for queue_name, result in results.items():
            print(f"  {queue_name}:")
            print(f"    ååé‡: {result.throughput:.2f}æ¶ˆæ¯/ç§’")
            print(f"    å¹³å‡å»¶è¿Ÿ: {result.latency_avg:.4f}ç§’")
            print(f"    95%å»¶è¿Ÿ: {result.latency_p95:.4f}ç§’")
        print()
        
        return results


class QueuePerformanceAnalyzer:
    """é˜Ÿåˆ—æ€§èƒ½åˆ†æå™¨"""
    
    def __init__(self):
        self.historical_data = defaultdict(list)
        self.alerts = []
    
    def analyze_benchmark_results(self, results: List[PerformanceBenchmark]) -> Dict[str, Any]:
        """åˆ†æåŸºå‡†æµ‹è¯•ç»“æœ"""
        if not results:
            return {}
        
        analysis = {
            'summary': {
                'total_tests': len(results),
                'avg_throughput': sum(r.throughput for r in results) / len(results),
                'avg_latency': sum(r.latency_avg for r in results) / len(results),
                'total_messages': sum(r.total_messages for r in results),
                'total_errors': sum(r.error_count for r in results)
            },
            'performance_comparison': {},
            'recommendations': []
        }
        
        # æ€§èƒ½å¯¹æ¯”
        best_throughput = max(results, key=lambda r: r.throughput)
        best_latency = min(results, key=lambda r: r.latency_avg)
        worst_throughput = min(results, key=lambda r: r.throughput)
        worst_latency = max(results, key=lambda r: r.latency_avg)
        
        analysis['performance_comparison'] = {
            'best_throughput': {
                'test': best_throughput.test_name,
                'value': best_throughput.throughput
            },
            'best_latency': {
                'test': best_latency.test_name,
                'value': best_latency.latency_avg
            },
            'worst_throughput': {
                'test': worst_throughput.test_name,
                'value': worst_throughput.throughput
            },
            'worst_latency': {
                'test': worst_latency.test_name,
                'value': worst_latency.latency_avg
            }
        }
        
        # ç”Ÿæˆå»ºè®®
        analysis['recommendations'] = self._generate_optimization_recommendations(results)
        
        return analysis
    
    def _generate_optimization_recommendations(self, results: List[PerformanceBenchmark]) -> List[str]:
        """ç”Ÿæˆä¼˜åŒ–å»ºè®®"""
        recommendations = []
        
        # åŸºäºååé‡åˆ†æ
        throughputs = [r.throughput for r in results]
        avg_throughput = sum(throughputs) / len(throughputs)
        min_throughput = min(throughputs)
        
        if min_throughput < avg_throughput * 0.5:
            recommendations.append("å­˜åœ¨æ€§èƒ½ç“¶é¢ˆé˜Ÿåˆ—ï¼Œéœ€è¦é‡ç‚¹ä¼˜åŒ–")
        
        # åŸºäºå»¶è¿Ÿåˆ†æ
        latencies = [r.latency_avg for r in results]
        avg_latency = sum(latencies) / len(latencies)
        max_latency = max(latencies)
        
        if max_latency > avg_latency * 2:
            recommendations.append("å­˜åœ¨é«˜å»¶è¿Ÿé˜Ÿåˆ—ï¼Œå»ºè®®æ£€æŸ¥é˜Ÿåˆ—é…ç½®å’Œå¤„ç†é€»è¾‘")
        
        # åŸºäºé”™è¯¯ç‡åˆ†æ
        for result in results:
            error_rate = result.error_count / result.total_messages if result.total_messages > 0 else 0
            if error_rate > 0.01:  # 1%é”™è¯¯ç‡
                recommendations.append(f"æµ‹è¯•{result.test_name}é”™è¯¯ç‡è¿‡é«˜ï¼Œéœ€è¦æ£€æŸ¥é”™è¯¯å¤„ç†")
        
        # åŸºäºå†…å­˜ä½¿ç”¨åˆ†æ
        memory_peaks = [r.memory_peak for r in results]
        avg_memory = sum(memory_peaks) / len(memory_peaks)
        max_memory = max(memory_peaks)
        
        if max_memory > avg_memory * 2:
            recommendations.append("å†…å­˜ä½¿ç”¨ä¸å‡è¡¡ï¼Œå»ºè®®ä¼˜åŒ–å¤§å†…å­˜é˜Ÿåˆ—çš„é…ç½®")
        
        if avg_memory > 100 * 1024 * 1024:  # 100MB
            recommendations.append("å¹³å‡å†…å­˜ä½¿ç”¨è¿‡é«˜ï¼Œå»ºè®®å¯ç”¨æ¶ˆæ¯TTLæˆ–é˜Ÿåˆ—é•¿åº¦é™åˆ¶")
        
        return recommendations
    
    def generate_performance_report(self, analysis: Dict[str, Any]) -> str:
        """ç”Ÿæˆæ€§èƒ½æŠ¥å‘Š"""
        report = []
        report.append("# RabbitMQ é˜Ÿåˆ—æ€§èƒ½åˆ†ææŠ¥å‘Š")
        report.append(f"\nç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        
        # æµ‹è¯•æ¦‚è¿°
        summary = analysis.get('summary', {})
        report.append("\n## æµ‹è¯•æ¦‚è¿°")
        report.append(f"- æ€»æµ‹è¯•æ•°é‡: {summary.get('total_tests', 0)}")
        report.append(f"- å¹³å‡ååé‡: {summary.get('avg_throughput', 0):.2f} æ¶ˆæ¯/ç§’")
        report.append(f"- å¹³å‡å»¶è¿Ÿ: {summary.get('avg_latency', 0):.4f} ç§’")
        report.append(f"- æ€»å¤„ç†æ¶ˆæ¯: {summary.get('total_messages', 0)}")
        report.append(f"- æ€»é”™è¯¯æ•°: {summary.get('total_errors', 0)}")
        
        # æ€§èƒ½å¯¹æ¯”
        comparison = analysis.get('performance_comparison', {})
        if comparison:
            report.append("\n## æ€§èƒ½å¯¹æ¯”")
            report.append(f"- æœ€ä½³ååé‡: {comparison.get('best_throughput', {}).get('value', 0):.2f} æ¶ˆæ¯/ç§’ ({comparison.get('best_throughput', {}).get('test', 'unknown')})")
            report.append(f"- æœ€ä½³å»¶è¿Ÿ: {comparison.get('best_latency', {}).get('value', 0):.4f} ç§’ ({comparison.get('best_latency', {}).get('test', 'unknown')})")
            report.append(f"- æœ€å·®ååé‡: {comparison.get('worst_throughput', {}).get('value', 0):.2f} æ¶ˆæ¯/ç§’ ({comparison.get('worst_throughput', {}).get('test', 'unknown')})")
            report.append(f"- æœ€å·®å»¶è¿Ÿ: {comparison.get('worst_latency', {}).get('value', 0):.4f} ç§’ ({comparison.get('worst_latency', {}).get('test', 'unknown')})")
        
        # ä¼˜åŒ–å»ºè®®
        recommendations = analysis.get('recommendations', [])
        if recommendations:
            report.append("\n## ä¼˜åŒ–å»ºè®®")
            for i, rec in enumerate(recommendations, 1):
                report.append(f"{i}. {rec}")
        
        report.append("\n## æ€»ç»“")
        report.append("åŸºäºä»¥ä¸Šåˆ†æï¼Œå»ºè®®æŒ‰ç…§ä¼˜å…ˆçº§å®æ–½ä¼˜åŒ–æªæ–½ï¼ŒæŒç»­ç›‘æ§ç³»ç»Ÿæ€§èƒ½ã€‚")
        
        return "\n".join(report)


class QueueOptimizationDemo:
    """é˜Ÿåˆ—ä¼˜åŒ–æ¼”ç¤º"""
    
    def __init__(self):
        self.optimizer = QueueOptimizer()
        self.benchmarker = QueueBenchmarker()
        self.analyzer = QueuePerformanceAnalyzer()
    
    def demonstrate_optimization_strategies(self):
        """æ¼”ç¤ºä¸åŒçš„ä¼˜åŒ–ç­–ç•¥"""
        print("=== é˜Ÿåˆ—ä¼˜åŒ–ç­–ç•¥æ¼”ç¤º ===")
        print()
        
        # ä¸åŒå·¥ä½œè´Ÿè½½ç±»å‹çš„ä¼˜åŒ–é…ç½®
        test_configs = [
            {
                'name': 'high_throughput_queue',
                'type': 'high_throughput',
                'message_rate': 5000,
                'avg_message_size': 1024,
                'consumer_count': 10
            },
            {
                'name': 'low_latency_queue',
                'type': 'low_latency',
                'message_rate': 100,
                'avg_message_size': 512,
                'consumer_count': 5
            },
            {
                'name': 'low_memory_queue',
                'type': 'low_memory',
                'message_rate': 1000,
                'avg_message_size': 2048,
                'consumer_count': 3
            },
            {
                'name': 'balanced_queue',
                'type': 'balanced',
                'message_rate': 2000,
                'avg_message_size': 1024,
                'consumer_count': 5
            }
        ]
        
        optimized_configs = {}
        
        for config in test_configs:
            print(f"ç”Ÿæˆ{config['type']}ç­–ç•¥é…ç½®:")
            
            # ç”Ÿæˆä¼˜åŒ–é…ç½®
            optimized_config = self.optimizer.generate_optimized_config(
                queue_name=config['name'],
                workload_type=config['type'],
                message_rate=config['message_rate'],
                avg_message_size=config['avg_message_size'],
                consumer_count=config['consumer_count'],
                requirements={
                    'require_durability': config['type'] in ['balanced', 'reliable'],
                    'priority_support': True
                }
            )
            
            optimized_configs[config['name']] = optimized_config
            
            # æ˜¾ç¤ºé…ç½®è¯¦æƒ…
            print(f"  é˜Ÿåˆ—å: {optimized_config.name}")
            print(f"  æŒä¹…åŒ–: {optimized_config.durable}")
            print(f"  æœ€å¤§é•¿åº¦: {optimized_config.max_length}")
            print(f"  æ¶ˆæ¯TTL: {optimized_config.message_ttl}ms")
            print(f"  ä¼˜å…ˆçº§: {optimized_config.max_priority}")
            print(f"  å‚æ•°: {optimized_config.arguments}")
            print()
        
        return optimized_configs
    
    def demonstrate_performance_testing(self, configs: Dict[str, QueueConfig]):
        """æ¼”ç¤ºæ€§èƒ½æµ‹è¯•"""
        print("=== é˜Ÿåˆ—æ€§èƒ½æµ‹è¯•æ¼”ç¤º ===")
        print()
        
        all_results = []
        
        # å¯¹æ¯ä¸ªé…ç½®è¿è¡ŒåŸºå‡†æµ‹è¯•
        for config_name, config in configs.items():
            print(f"æµ‹è¯•é˜Ÿåˆ—é…ç½®: {config_name}")
            print("-" * 40)
            
            # åŸºç¡€æ€§èƒ½æµ‹è¯•
            result = self.benchmarker.run_basic_benchmark(
                queue_config=config,
                message_count=500,  # è¾ƒå°‘çš„æ¶ˆæ¯ç”¨äºæ¼”ç¤º
                message_size=1024,
                consumer_count=3,
                message_rate=100
            )
            all_results.append(result)
            
            print()
        
        # åˆ†ææµ‹è¯•ç»“æœ
        print("=== æ€§èƒ½æµ‹è¯•ç»“æœåˆ†æ ===")
        analysis = self.analyzer.analyze_benchmark_results(all_results)
        
        # è¾“å‡ºåˆ†æç»“æœ
        summary = analysis.get('summary', {})
        print("ğŸ“Š æµ‹è¯•æ€»ç»“:")
        print(f"  å¹³å‡ååé‡: {summary.get('avg_throughput', 0):.2f} æ¶ˆæ¯/ç§’")
        print(f"  å¹³å‡å»¶è¿Ÿ: {summary.get('avg_latency', 0):.4f} ç§’")
        print(f"  æ€»å¤„ç†æ¶ˆæ¯: {summary.get('total_messages', 0)}")
        
        comparison = analysis.get('performance_comparison', {})
        if comparison:
            print(f"  æœ€ä½³ååé‡: {comparison.get('best_throughput', {}).get('value', 0):.2f} æ¶ˆæ¯/ç§’")
            print(f"  æœ€ä½³å»¶è¿Ÿ: {comparison.get('best_latency', {}).get('value', 0):.4f} ç§’")
        
        recommendations = analysis.get('recommendations', [])
        if recommendations:
            print("ğŸ’¡ ä¼˜åŒ–å»ºè®®:")
            for i, rec in enumerate(recommendations, 1):
                print(f"  {i}. {rec}")
        
        print()
        
        return analysis
    
    def demonstrate_stress_testing(self):
        """æ¼”ç¤ºå‹åŠ›æµ‹è¯•"""
        print("=== å‹åŠ›æµ‹è¯•æ¼”ç¤º ===")
        print()
        
        # åˆ›å»ºå‹åŠ›æµ‹è¯•é…ç½®
        stress_config = QueueConfig(
            name="stress_test_queue",
            durable=True,
            max_length=10000,  # é™åˆ¶é˜Ÿåˆ—é•¿åº¦
            message_ttl=300000,  # 5åˆ†é’ŸTTL
            overflow="reject-publish"
        )
        
        # è¿è¡Œå‹åŠ›æµ‹è¯•
        result = self.benchmarker.run_stress_test(
            queue_config=stress_config,
            stress_duration=30,  # 30ç§’å‹åŠ›æµ‹è¯•
            ramp_up_time=5      # 5ç§’é¢„çƒ­
        )
        
        print("âœ… å‹åŠ›æµ‹è¯•å®Œæˆ")
        print()
        
        return result
    
    def demonstrate_concurrent_testing(self):
        """æ¼”ç¤ºå¹¶å‘æµ‹è¯•"""
        print("=== å¹¶å‘é˜Ÿåˆ—æµ‹è¯•æ¼”ç¤º ===")
        print()
        
        # åˆ›å»ºå¤šä¸ªé˜Ÿåˆ—é…ç½®
        queue_configs = [
            QueueConfig("concurrent_queue_1", max_length=5000),
            QueueConfig("concurrent_queue_2", max_length=3000),
            QueueConfig("concurrent_queue_3", max_length=2000),
        ]
        
        # è¿è¡Œå¹¶å‘æµ‹è¯•
        results = self.benchmarker.run_concurrent_benchmark(
            queue_configs=queue_configs,
            total_messages=3000,  # æ¯ä¸ªé˜Ÿåˆ—1000æ¡æ¶ˆæ¯
            message_size=512
        )
        
        print("âœ… å¹¶å‘æµ‹è¯•å®Œæˆ")
        print()
        
        return results
    
    def demonstrate_performance_analysis(self, test_results: List[PerformanceBenchmark]):
        """æ¼”ç¤ºæ€§èƒ½åˆ†æ"""
        print("=== æ€§èƒ½åˆ†ææ¼”ç¤º ===")
        print()
        
        if not test_results:
            print("æ²¡æœ‰æµ‹è¯•ç»“æœå¯åˆ†æ")
            return
        
        # ç”Ÿæˆæ€§èƒ½æŠ¥å‘Š
        analysis = self.analyzer.analyze_benchmark_results(test_results)
        report = self.analyzer.generate_performance_report(analysis)
        
        print("ğŸ“„ æ€§èƒ½æŠ¥å‘Š:")
        print(report)
        print()
        
        # è¾“å‡ºå»ºè®®çš„é˜Ÿåˆ—é…ç½®
        print("ğŸ’¡ æ¨èçš„é«˜æ€§èƒ½é˜Ÿåˆ—é…ç½®:")
        
        high_throughput_config = self.optimizer.generate_optimized_config(
            queue_name="recommended_high_throughput",
            workload_type="high_throughput",
            message_rate=10000,
            avg_message_size=1024,
            consumer_count=20
        )
        
        print("é«˜ååé‡é…ç½®:")
        print(f"  durable: {high_throughput_config.durable}")
        print(f"  max_length: {high_throughput_config.max_length}")
        print(f"  message_ttl: {high_throughput_config.message_ttl}")
        print(f"  arguments: {high_throughput_config.arguments}")
        
        low_latency_config = self.optimizer.generate_optimized_config(
            queue_name="recommended_low_latency",
            workload_type="low_latency",
            message_rate=500,
            avg_message_size=512,
            consumer_count=10
        )
        
        print("\nä½å»¶è¿Ÿé…ç½®:")
        print(f"  durable: {low_latency_config.durable}")
        print(f"  max_length: {low_latency_config.max_length}")
        print(f"  max_priority: {low_latency_config.max_priority}")
        print(f"  arguments: {low_latency_config.arguments}")
        
        print()


if __name__ == "__main__":
    # è¿è¡Œé˜Ÿåˆ—ä¼˜åŒ–æ¼”ç¤º
    demo = QueueOptimizationDemo()
    
    print("ğŸš€ RabbitMQ é˜Ÿåˆ—ä¼˜åŒ–ä¸è°ƒä¼˜ç³»ç»Ÿ")
    print("=" * 50)
    print()
    
    try:
        # 1. æ¼”ç¤ºä¼˜åŒ–ç­–ç•¥
        configs = demo.demonstrate_optimization_strategies()
        
        # 2. æ¼”ç¤ºæ€§èƒ½æµ‹è¯•
        analysis = demo.demonstrate_performance_testing(configs)
        
        # 3. æ¼”ç¤ºå‹åŠ›æµ‹è¯•
        stress_result = demo.demonstrate_stress_testing()
        
        # 4. æ¼”ç¤ºå¹¶å‘æµ‹è¯•
        concurrent_results = demo.demonstrate_concurrent_testing()
        
        # 5. æ¼”ç¤ºæ€§èƒ½åˆ†æ
        test_results = demo.benchmarker.test_results
        demo.demonstrate_performance_analysis(test_results)
        
        print("ğŸ‰ é˜Ÿåˆ—ä¼˜åŒ–æ¼”ç¤ºå®Œæˆ!")
        
    except KeyboardInterrupt:
        print("\nç¨‹åºè¢«ç”¨æˆ·ä¸­æ–­")
    except Exception as e:
        print(f"\nç¨‹åºæ‰§è¡Œé”™è¯¯: {e}")
        import traceback
        traceback.print_exc()