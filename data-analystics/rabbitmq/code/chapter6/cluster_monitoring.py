#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
RabbitMQé›†ç¾¤ç›‘æ§ä¸å‘Šè­¦ç³»ç»Ÿ

è¿™ä¸ªæ¨¡å—æä¾›äº†å®Œæ•´çš„é›†ç¾¤ç›‘æ§åŠŸèƒ½ï¼š
- å®æ—¶èŠ‚ç‚¹çŠ¶æ€ç›‘æ§
- é˜Ÿåˆ—æ€§èƒ½æŒ‡æ ‡æ”¶é›†
- å‘Šè­¦è§„åˆ™é…ç½®
- æ€§èƒ½æŠ¥å‘Šç”Ÿæˆ
- å†å²æ•°æ®åˆ†æ
"""

import pika
import json
import time
import threading
import requests
import psutil
from datetime import datetime, timedelta
from typing import Dict, List, Optional, Callable
from dataclasses import dataclass, asdict
from collections import defaultdict, deque
import logging

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

@dataclass
class ClusterMetrics:
    """é›†ç¾¤æŒ‡æ ‡æ•°æ®ç±»"""
    timestamp: str
    node_name: str
    queue_name: str
    message_count: int
    consumer_count: int
    memory_usage: int
    cpu_usage: float
    connection_count: int
    channel_count: int

@dataclass
class AlertRule:
    """å‘Šè­¦è§„åˆ™æ•°æ®ç±»"""
    name: str
    metric: str
    condition: str  # '>', '<', '>=', '<='
    threshold: float
    duration: int  # æŒç»­æ—¶é—´ï¼ˆç§’ï¼‰
    severity: str  # 'info', 'warning', 'critical'
    enabled: bool = True

class ClusterNodeMonitor:
    """å•ä¸ªèŠ‚ç‚¹ç›‘æ§å™¨"""
    
    def __init__(self, node_name: str, host: str, port: int = 5672, 
                 username: str = 'admin', password: str = 'admin123'):
        self.node_name = node_name
        self.host = host
        self.port = port
        self.username = username
        self.password = password
        self.connection = None
        self.channel = None
        self.is_connected = False
        
        # ç›‘æ§å†å²æ•°æ®
        self.metrics_history = deque(maxlen=1000)  # ä¿ç•™1000æ¡è®°å½•
        self.alerts = []
        
    def connect(self) -> bool:
        """è¿æ¥åˆ°èŠ‚ç‚¹"""
        try:
            credentials = pika.PlainCredentials(self.username, self.password)
            connection_params = pika.ConnectionParameters(
                host=self.host,
                port=self.port,
                credentials=credentials,
                heartbeat=30,
                connection_attempts=3,
                retry_delay=5
            )
            
            self.connection = pika.BlockingConnection(connection_params)
            self.channel = self.connection.channel()
            self.is_connected = True
            
            logger.info(f"âœ… è¿æ¥åˆ°èŠ‚ç‚¹: {self.node_name}")
            return True
            
        except Exception as e:
            logger.error(f"âŒ è¿æ¥èŠ‚ç‚¹å¤±è´¥ {self.node_name}: {e}")
            self.is_connected = False
            return False
    
    def collect_system_metrics(self) -> Dict:
        """æ”¶é›†ç³»ç»ŸæŒ‡æ ‡"""
        try:
            cpu_percent = psutil.cpu_percent(interval=1)
            memory = psutil.virtual_memory()
            disk = psutil.disk_usage('/')
            
            return {
                'cpu_usage': cpu_percent,
                'memory_usage': memory.percent,
                'memory_total': memory.total,
                'memory_available': memory.available,
                'disk_usage': disk.percent,
                'disk_free': disk.free,
                'timestamp': datetime.now().isoformat()
            }
            
        except Exception as e:
            logger.error(f"âŒ æ”¶é›†ç³»ç»ŸæŒ‡æ ‡å¤±è´¥ {self.node_name}: {e}")
            return {}
    
    def collect_queue_metrics(self) -> List[Dict]:
        """æ”¶é›†é˜Ÿåˆ—æŒ‡æ ‡"""
        if not self.is_connected:
            return []
        
        queue_metrics = []
        
        try:
            # å£°æ˜ä¸´æ—¶é˜Ÿåˆ—è·å–é˜Ÿåˆ—åˆ—è¡¨
            result = self.channel.queue_declare('', exclusive=True, auto_delete=True)
            
            # è·å–æ‰€æœ‰é˜Ÿåˆ—ä¿¡æ¯ï¼ˆä½¿ç”¨HTTP APIï¼‰
            api_url = f"http://{self.host}:15672/api/queues"
            auth = (self.username, self.password)
            
            response = requests.get(api_url, auth=auth, timeout=10)
            if response.status_code == 200:
                queues_data = response.json()
                
                for queue_data in queues_data:
                    metrics = ClusterMetrics(
                        timestamp=datetime.now().isoformat(),
                        node_name=self.node_name,
                        queue_name=queue_data.get('name', ''),
                        message_count=queue_data.get('messages', 0),
                        consumer_count=queue_data.get('consumers', 0),
                        memory_usage=queue_data.get('memory', 0),
                        cpu_usage=0.0,  # é€šè¿‡ç³»ç»Ÿæ”¶é›†
                        connection_count=len(queue_data.get('node_details', {}).get('channels', [])),
                        channel_count=queue_data.get('channels', 0)
                    )
                    
                    queue_metrics.append(asdict(metrics))
                    
        except Exception as e:
            logger.error(f"âŒ æ”¶é›†é˜Ÿåˆ—æŒ‡æ ‡å¤±è´¥ {self.node_name}: {e}")
        
        return queue_metrics
    
    def collect_cluster_metrics(self) -> Dict:
        """æ”¶é›†é›†ç¾¤æ•´ä½“æŒ‡æ ‡"""
        if not self.is_connected:
            return {}
        
        try:
            api_url = f"http://{self.host}:15672/api/overview"
            auth = (self.username, self.password)
            
            response = requests.get(api_url, auth=auth, timeout=10)
            if response.status_code == 200:
                data = response.json()
                
                return {
                    'total_connections': data.get('connection_totals', {}).get('current', 0),
                    'total_channels': data.get('channel_totals', {}).get('current', 0),
                    'total_queues': data.get('queue_totals', {}).get('messages', 0),
                    'object_totals': data.get('object_totals', {}),
                    'timestamp': datetime.now().isoformat()
                }
                
        except Exception as e:
            logger.error(f"âŒ æ”¶é›†é›†ç¾¤æŒ‡æ ‡å¤±è´¥ {self.node_name}: {e}")
        
        return {}
    
    def collect_all_metrics(self) -> Dict:
        """æ”¶é›†æ‰€æœ‰æŒ‡æ ‡"""
        timestamp = datetime.now().isoformat()
        
        # æ”¶é›†ç³»ç»ŸæŒ‡æ ‡
        system_metrics = self.collect_system_metrics()
        
        # æ”¶é›†é˜Ÿåˆ—æŒ‡æ ‡
        queue_metrics = self.collect_queue_metrics()
        
        # æ”¶é›†é›†ç¾¤æŒ‡æ ‡
        cluster_metrics = self.collect_cluster_metrics()
        
        metrics_data = {
            'node_name': self.node_name,
            'timestamp': timestamp,
            'system': system_metrics,
            'queues': queue_metrics,
            'cluster': cluster_metrics,
            'status': 'connected' if self.is_connected else 'disconnected'
        }
        
        # ä¿å­˜åˆ°å†å²è®°å½•
        self.metrics_history.append(metrics_data)
        
        return metrics_data
    
    def disconnect(self):
        """æ–­å¼€è¿æ¥"""
        if self.connection and not self.connection.is_closed:
            self.connection.close()
            self.is_connected = False
            logger.info(f"ğŸ”Œ æ–­å¼€èŠ‚ç‚¹è¿æ¥: {self.node_name}")

class ClusterAlertManager:
    """é›†ç¾¤å‘Šè­¦ç®¡ç†å™¨"""
    
    def __init__(self, notification_callbacks: Optional[List[Callable]] = None):
        self.rules = []
        self.active_alerts = {}
        self.alert_history = deque(maxlen=1000)
        self.notification_callbacks = notification_callbacks or []
        
        # é»˜è®¤å‘Šè­¦è§„åˆ™
        self._setup_default_rules()
    
    def _setup_default_rules(self):
        """è®¾ç½®é»˜è®¤å‘Šè­¦è§„åˆ™"""
        default_rules = [
            AlertRule("é«˜å†…å­˜ä½¿ç”¨", "memory_usage", ">", 80, 60, "warning"),
            AlertRule("æé«˜å†…å­˜ä½¿ç”¨", "memory_usage", ">", 95, 30, "critical"),
            AlertRule("é«˜CPUä½¿ç”¨", "cpu_usage", ">", 80, 60, "warning"),
            AlertRule("æé«˜CPUä½¿ç”¨", "cpu_usage", ">", 95, 30, "critical"),
            AlertRule("é˜Ÿåˆ—æ¶ˆæ¯ç§¯å‹", "queue_messages", ">", 1000, 120, "warning"),
            AlertRule("é˜Ÿåˆ—æ¶ˆæ¯ä¸¥é‡ç§¯å‹", "queue_messages", ">", 10000, 60, "critical"),
            AlertRule("èŠ‚ç‚¹ç¦»çº¿", "status", "==", 0, 10, "critical"),
            AlertRule("æ— æ¶ˆè´¹è€…", "consumers", "<", 1, 180, "warning")
        ]
        
        self.rules = default_rules
        logger.info(f"ğŸ“‹ è®¾ç½®äº† {len(default_rules)} ä¸ªé»˜è®¤å‘Šè­¦è§„åˆ™")
    
    def add_rule(self, rule: AlertRule):
        """æ·»åŠ å‘Šè­¦è§„åˆ™"""
        self.rules.append(rule)
        logger.info(f"â• æ·»åŠ å‘Šè­¦è§„åˆ™: {rule.name}")
    
    def remove_rule(self, rule_name: str):
        """åˆ é™¤å‘Šè­¦è§„åˆ™"""
        self.rules = [rule for rule in self.rules if rule.name != rule_name]
        logger.info(f"â– åˆ é™¤å‘Šè­¦è§„åˆ™: {rule_name}")
    
    def evaluate_metrics(self, metrics_data: Dict) -> List[Dict]:
        """è¯„ä¼°æŒ‡æ ‡å¹¶ç”Ÿæˆå‘Šè­¦"""
        triggered_alerts = []
        
        for rule in self.rules:
            if not rule.enabled:
                continue
            
            alert = self._check_rule(rule, metrics_data)
            if alert:
                triggered_alerts.append(alert)
        
        return triggered_alerts
    
    def _check_rule(self, rule: AlertRule, metrics_data: Dict) -> Optional[Dict]:
        """æ£€æŸ¥å•ä¸ªå‘Šè­¦è§„åˆ™"""
        try:
            metric_value = self._get_metric_value(metrics_data, rule.metric)
            if metric_value is None:
                return None
            
            # æ£€æŸ¥æ¡ä»¶
            condition_met = self._evaluate_condition(metric_value, rule.condition, rule.threshold)
            
            if condition_met:
                # åˆ›å»ºå‘Šè­¦è®°å½•
                alert = {
                    'id': f"{rule.name}_{metrics_data['node_name']}_{int(time.time())}",
                    'name': rule.name,
                    'rule': rule,
                    'metric': rule.metric,
                    'value': metric_value,
                    'threshold': rule.threshold,
                    'condition': rule.condition,
                    'node': metrics_data['node_name'],
                    'timestamp': metrics_data['timestamp'],
                    'severity': rule.severity,
                    'status': 'triggered'
                }
                
                # æ£€æŸ¥æ˜¯å¦ä¸ºæŒç»­å‘Šè­¦
                if rule.duration > 0:
                    alert_key = f"{rule.name}_{metrics_data['node_name']}"
                    if alert_key not in self.active_alerts:
                        self.active_alerts[alert_key] = {
                            'first_triggered': time.time(),
                            'alert': alert
                        }
                    else:
                        duration_passed = time.time() - self.active_alerts[alert_key]['first_triggered']
                        if duration_passed < rule.duration:
                            return None  # æŒç»­æ—¶é—´æœªåˆ°ï¼Œä¸è§¦å‘å‘Šè­¦
                        else:
                            alert['duration'] = duration_passed
                else:
                    # éæŒç»­å‘Šè­¦ï¼Œç«‹å³è§¦å‘
                    alert_key = f"{rule.name}_{metrics_data['node_name']}"
                    self.active_alerts[alert_key] = {
                        'first_triggered': time.time(),
                        'alert': alert
                    }
                
                logger.warning(f"ğŸš¨ è§¦å‘å‘Šè­¦: {rule.name} - {metrics_data['node_name']}: {metric_value}")
                
                # å‘é€é€šçŸ¥
                self._send_notifications(alert)
                
                return alert
        
        except Exception as e:
            logger.error(f"âŒ æ£€æŸ¥å‘Šè­¦è§„åˆ™å¤±è´¥ {rule.name}: {e}")
        
        return None
    
    def _get_metric_value(self, metrics_data: Dict, metric: str) -> Optional[float]:
        """è·å–æŒ‡æ ‡å€¼"""
        try:
            if metric == "memory_usage":
                return metrics_data.get('system', {}).get('memory_usage', 0)
            elif metric == "cpu_usage":
                return metrics_data.get('system', {}).get('cpu_usage', 0)
            elif metric == "status":
                return 1.0 if metrics_data.get('status') == 'connected' else 0.0
            elif metric.startswith("queue_"):
                queue_name = metric.split("_")[1]  # ç®€åŒ–å¤„ç†
                for queue_metrics in metrics_data.get('queues', []):
                    return float(queue_metrics.get('message_count', 0))
            elif metric == "queue_messages":
                # è¿”å›æœ€å¤§é˜Ÿåˆ—æ¶ˆæ¯æ•°
                max_messages = 0
                for queue_metrics in metrics_data.get('queues', []):
                    max_messages = max(max_messages, queue_metrics.get('message_count', 0))
                return float(max_messages)
            elif metric == "consumers":
                # è¿”å›æœ€å°æ¶ˆè´¹è€…æ•°
                min_consumers = float('inf')
                for queue_metrics in metrics_data.get('queues', []):
                    min_consumers = min(min_consumers, queue_metrics.get('consumer_count', 0))
                return min_consumers if min_consumers != float('inf') else 0.0
            
        except Exception as e:
            logger.error(f"âŒ è·å–æŒ‡æ ‡å€¼å¤±è´¥ {metric}: {e}")
        
        return None
    
    def _evaluate_condition(self, value: float, condition: str, threshold: float) -> bool:
        """è¯„ä¼°æ¡ä»¶"""
        if condition == ">":
            return value > threshold
        elif condition == ">=":
            return value >= threshold
        elif condition == "<":
            return value < threshold
        elif condition == "<=":
            return value <= threshold
        elif condition == "==":
            return value == threshold
        elif condition == "!=":
            return value != threshold
        
        return False
    
    def _send_notifications(self, alert: Dict):
        """å‘é€å‘Šè­¦é€šçŸ¥"""
        for callback in self.notification_callbacks:
            try:
                callback(alert)
            except Exception as e:
                logger.error(f"âŒ å‘Šè­¦é€šçŸ¥å‘é€å¤±è´¥: {e}")
    
    def resolve_alert(self, alert_id: str):
        """è§£å†³å‘Šè­¦"""
        for alert_key, alert_info in list(self.active_alerts.items()):
            if alert_info['alert']['id'] == alert_id:
                alert_info['alert']['status'] = 'resolved'
                alert_info['alert']['resolved_at'] = datetime.now().isoformat()
                
                # æ·»åŠ åˆ°å†å²è®°å½•
                self.alert_history.append(alert_info['alert'])
                
                # ä»æ´»åŠ¨å‘Šè­¦ä¸­ç§»é™¤
                del self.active_alerts[alert_key]
                
                logger.info(f"âœ… å‘Šè­¦å·²è§£å†³: {alert_id}")
                break

class ClusterMonitor:
    """é›†ç¾¤ç›‘æ§ä¸»ç±»"""
    
    def __init__(self, cluster_config: List[Dict], alert_config: Optional[Dict] = None):
        self.cluster_config = cluster_config
        self.nodes = []
        self.alert_manager = ClusterAlertManager()
        
        # ç›‘æ§é…ç½®
        self.monitoring_interval = 30  # 30ç§’
        self.is_monitoring = False
        
        # æ•°æ®å­˜å‚¨
        self.global_metrics_history = deque(maxlen=10000)
        
        # è®¾ç½®å‘Šè­¦å›è°ƒ
        if alert_config:
            self._setup_alert_callbacks(alert_config)
        
        # åˆå§‹åŒ–èŠ‚ç‚¹
        self._initialize_nodes()
    
    def _initialize_nodes(self):
        """åˆå§‹åŒ–èŠ‚ç‚¹ç›‘æ§å™¨"""
        for config in self.cluster_config:
            node_monitor = ClusterNodeMonitor(
                node_name=config['name'],
                host=config['host'],
                port=config.get('port', 5672),
                username=config.get('username', 'admin'),
                password=config.get('password', 'admin123')
            )
            self.nodes.append(node_monitor)
        
        logger.info(f"ğŸ”§ åˆå§‹åŒ–äº† {len(self.nodes)} ä¸ªèŠ‚ç‚¹ç›‘æ§å™¨")
    
    def _setup_alert_callbacks(self, alert_config: Dict):
        """è®¾ç½®å‘Šè­¦å›è°ƒå‡½æ•°"""
        def log_alert(alert):
            logger.warning(f"ğŸš¨ å‘Šè­¦: {alert['name']} - {alert['node']} - {alert['value']}")
        
        def email_alert(alert):
            if 'email' in alert_config:
                # è¿™é‡Œå¯ä»¥é›†æˆé‚®ä»¶å‘é€åŠŸèƒ½
                logger.info(f"ğŸ“§ é‚®ä»¶å‘Šè­¦: {alert['name']} - {alert['node']}")
        
        def webhook_alert(alert):
            if 'webhook_url' in alert_config:
                try:
                    payload = {
                        'alert_name': alert['name'],
                        'node': alert['node'],
                        'severity': alert['severity'],
                        'value': alert['value'],
                        'timestamp': alert['timestamp']
                    }
                    requests.post(alert_config['webhook_url'], json=payload, timeout=5)
                except Exception as e:
                    logger.error(f"âŒ Webhookå‘Šè­¦å‘é€å¤±è´¥: {e}")
        
        self.alert_manager.notification_callbacks = [log_alert]
        if 'email' in alert_config:
            self.alert_manager.notification_callbacks.append(email_alert)
        if 'webhook_url' in alert_config:
            self.alert_manager.notification_callbacks.append(webhook_alert)
    
    def connect_all_nodes(self) -> Dict[str, bool]:
        """è¿æ¥æ‰€æœ‰èŠ‚ç‚¹"""
        results = {}
        threads = []
        
        def connect_node(node):
            results[node.node_name] = node.connect()
        
        for node in self.nodes:
            thread = threading.Thread(target=connect_node, args=(node,))
            thread.start()
            threads.append(thread)
        
        for thread in threads:
            thread.join()
        
        connected_count = sum(results.values())
        logger.info(f"ğŸ“Š èŠ‚ç‚¹è¿æ¥ç»“æœ: {connected_count}/{len(self.nodes)} è¿æ¥æˆåŠŸ")
        
        return results
    
    def start_monitoring(self):
        """å¼€å§‹ç›‘æ§"""
        logger.info("ğŸ” å¼€å§‹é›†ç¾¤ç›‘æ§...")
        self.is_monitoring = True
        
        while self.is_monitoring:
            try:
                # æ”¶é›†æ‰€æœ‰èŠ‚ç‚¹çš„æŒ‡æ ‡
                all_metrics = []
                for node in self.nodes:
                    if node.is_connected:
                        metrics = node.collect_all_metrics()
                        if metrics:
                            all_metrics.append(metrics)
                
                # å…¨å±€å‘Šè­¦æ£€æŸ¥
                for metrics in all_metrics:
                    alerts = self.alert_manager.evaluate_metrics(metrics)
                    # å‘Šè­¦å·²é€šè¿‡å›è°ƒå¤„ç†
                
                # ä¿å­˜å…¨å±€å†å²æ•°æ®
                self.global_metrics_history.append({
                    'timestamp': datetime.now().isoformat(),
                    'nodes_metrics': all_metrics,
                    'cluster_summary': self._generate_cluster_summary(all_metrics)
                })
                
                # è¾“å‡ºç›‘æ§æ‘˜è¦
                if all_metrics:
                    summary = self._generate_cluster_summary(all_metrics)
                    logger.info(f"ğŸ“Š é›†ç¾¤ç›‘æ§æ‘˜è¦: {summary}")
                
                time.sleep(self.monitoring_interval)
                
            except Exception as e:
                logger.error(f"âŒ ç›‘æ§å¼‚å¸¸: {e}")
                time.sleep(5)
        
        logger.info("ğŸ é›†ç¾¤ç›‘æ§åœæ­¢")
    
    def stop_monitoring(self):
        """åœæ­¢ç›‘æ§"""
        self.is_monitoring = False
        logger.info("â¹ï¸  æ­£åœ¨åœæ­¢ç›‘æ§...")
    
    def _generate_cluster_summary(self, all_metrics: List[Dict]) -> Dict:
        """ç”Ÿæˆé›†ç¾¤æ‘˜è¦"""
        if not all_metrics:
            return {}
        
        summary = {
            'total_nodes': len(self.nodes),
            'connected_nodes': len(all_metrics),
            'total_messages': 0,
            'total_consumers': 0,
            'average_memory': 0,
            'average_cpu': 0,
            'timestamp': datetime.now().isoformat()
        }
        
        memory_usage = []
        cpu_usage = []
        
        for metrics in all_metrics:
            # ç»Ÿè®¡é˜Ÿåˆ—æŒ‡æ ‡
            for queue_metrics in metrics.get('queues', []):
                summary['total_messages'] += queue_metrics.get('message_count', 0)
                summary['total_consumers'] += queue_metrics.get('consumer_count', 0)
            
            # ç³»ç»ŸæŒ‡æ ‡
            system_metrics = metrics.get('system', {})
            memory_usage.append(system_metrics.get('memory_usage', 0))
            cpu_usage.append(system_metrics.get('cpu_usage', 0))
        
        # è®¡ç®—å¹³å‡å€¼
        if memory_usage:
            summary['average_memory'] = sum(memory_usage) / len(memory_usage)
        if cpu_usage:
            summary['average_cpu'] = sum(cpu_usage) / len(cpu_usage)
        
        return summary
    
    def generate_performance_report(self, duration_hours: int = 24) -> Dict:
        """ç”Ÿæˆæ€§èƒ½æŠ¥å‘Š"""
        end_time = datetime.now()
        start_time = end_time - timedelta(hours=duration_hours)
        
        # ç­›é€‰æ—¶é—´èŒƒå›´å†…çš„æ•°æ®
        recent_data = [
            data for data in self.global_metrics_history
            if start_time <= datetime.fromisoformat(data['timestamp']) <= end_time
        ]
        
        if not recent_data:
            return {'error': 'æ²¡æœ‰è¶³å¤Ÿçš„å†å²æ•°æ®ç”ŸæˆæŠ¥å‘Š'}
        
        # åˆ†ææ•°æ®
        messages_over_time = []
        cpu_over_time = []
        memory_over_time = []
        alert_counts = defaultdict(int)
        
        for data in recent_data:
            cluster_summary = data.get('cluster_summary', {})
            messages_over_time.append(cluster_summary.get('total_messages', 0))
            cpu_over_time.append(cluster_summary.get('average_cpu', 0))
            memory_over_time.append(cluster_summary.get('average_memory', 0))
        
        # å‘Šè­¦ç»Ÿè®¡
        for alert in self.alert_manager.alert_history:
            alert_counts[alert['severity']] += 1
        
        report = {
            'report_period': {
                'start_time': start_time.isoformat(),
                'end_time': end_time.isoformat(),
                'duration_hours': duration_hours
            },
            'statistics': {
                'max_messages': max(messages_over_time) if messages_over_time else 0,
                'min_messages': min(messages_over_time) if messages_over_time else 0,
                'avg_messages': sum(messages_over_time) / len(messages_over_time) if messages_over_time else 0,
                'max_cpu': max(cpu_over_time) if cpu_over_time else 0,
                'min_cpu': min(cpu_over_time) if cpu_over_time else 0,
                'avg_cpu': sum(cpu_over_time) / len(cpu_over_time) if cpu_over_time else 0,
                'max_memory': max(memory_over_time) if memory_over_time else 0,
                'min_memory': min(memory_over_time) if memory_over_time else 0,
                'avg_memory': sum(memory_over_time) / len(memory_over_time) if memory_over_time else 0
            },
            'alerts_summary': dict(alert_counts),
            'recommendations': self._generate_recommendations(recent_data)
        }
        
        return report
    
    def _generate_recommendations(self, data: List[Dict]) -> List[str]:
        """ç”Ÿæˆä¼˜åŒ–å»ºè®®"""
        recommendations = []
        
        # åˆ†æå†å²æ•°æ®ç”Ÿæˆå»ºè®®
        if len(data) >= 10:
            # æ£€æŸ¥è¶‹åŠ¿
            last_10_summaries = [d.get('cluster_summary', {}) for d in data[-10:]]
            
            avg_messages = [s.get('total_messages', 0) for s in last_10_summaries]
            avg_cpu = [s.get('average_cpu', 0) for s in last_10_summaries]
            avg_memory = [s.get('average_memory', 0) for s in last_10_summaries]
            
            if sum(avg_messages) / len(avg_messages) > 1000:
                recommendations.append("é˜Ÿåˆ—æ¶ˆæ¯ç§¯å‹è¾ƒå¤šï¼Œå»ºè®®å¢åŠ æ¶ˆè´¹è€…æ•°é‡æˆ–ä¼˜åŒ–æ¶ˆæ¯å¤„ç†é€Ÿåº¦")
            
            if sum(avg_cpu) / len(avg_cpu) > 80:
                recommendations.append("CPUä½¿ç”¨ç‡è¾ƒé«˜ï¼Œå»ºè®®å‡çº§ç¡¬ä»¶æˆ–ä¼˜åŒ–åº”ç”¨ç¨‹åºæ€§èƒ½")
            
            if sum(avg_memory) / len(avg_memory) > 80:
                recommendations.append("å†…å­˜ä½¿ç”¨ç‡è¾ƒé«˜ï¼Œå»ºè®®å¢åŠ å†…å­˜å®¹é‡æˆ–ä¼˜åŒ–å†…å­˜ä½¿ç”¨")
        
        return recommendations
    
    def cleanup(self):
        """æ¸…ç†èµ„æº"""
        logger.info("ğŸ§¹ æ¸…ç†ç›‘æ§èµ„æº...")
        
        self.stop_monitoring()
        
        for node in self.nodes:
            node.disconnect()
        
        logger.info("âœ… ç›‘æ§èµ„æºæ¸…ç†å®Œæˆ")

def main():
    """ä¸»å‡½æ•° - æ¼”ç¤ºç›‘æ§åŠŸèƒ½"""
    
    # é›†ç¾¤é…ç½®
    cluster_config = [
        {'name': 'node1', 'host': 'rabbitmq-node1', 'port': 5672},
        {'name': 'node2', 'host': 'rabbitmq-node2', 'port': 5672},
        {'name': 'node3', 'host': 'rabbitmq-node3', 'port': 5672}
    ]
    
    # å‘Šè­¦é…ç½®
    alert_config = {
        'email': 'admin@company.com',  # é‚®ä»¶æ¥æ”¶è€…
        'webhook_url': 'https://hooks.slack.com/services/YOUR/SLACK/WEBHOOK'  # Slack webhook
    }
    
    # åˆ›å»ºé›†ç¾¤ç›‘æ§å™¨
    monitor = ClusterMonitor(cluster_config, alert_config)
    
    try:
        # è¿æ¥æ‰€æœ‰èŠ‚ç‚¹
        logger.info("ğŸ”— è¿æ¥é›†ç¾¤èŠ‚ç‚¹...")
        monitor.connect_all_nodes()
        
        # å¼€å§‹ç›‘æ§ï¼ˆåå°è¿è¡Œï¼‰
        logger.info("ğŸ” å¯åŠ¨ç›‘æ§...")
        monitoring_thread = threading.Thread(target=monitor.start_monitoring)
        monitoring_thread.start()
        
        # è¿è¡Œ5åˆ†é’Ÿç›‘æ§æ¼”ç¤º
        print("ğŸ“Š å¼€å§‹5åˆ†é’Ÿç›‘æ§æ¼”ç¤º...")
        time.sleep(300)  # 5åˆ†é’Ÿ
        
        # ç”Ÿæˆæ€§èƒ½æŠ¥å‘Š
        logger.info("ğŸ“ˆ ç”Ÿæˆæ€§èƒ½æŠ¥å‘Š...")
        report = monitor.generate_performance_report(duration_hours=1)
        logger.info(f"ğŸ“‹ æ€§èƒ½æŠ¥å‘Š: {json.dumps(report, indent=2, ensure_ascii=False)}")
        
    except KeyboardInterrupt:
        logger.info("\nâ¹ï¸  ç”¨æˆ·ä¸­æ–­")
    finally:
        monitor.cleanup()

if __name__ == '__main__':
    main()