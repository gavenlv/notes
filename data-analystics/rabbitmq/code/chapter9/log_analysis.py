"""
RabbitMQæ—¥å¿—åˆ†ææ¨¡å—
æä¾›æ—¥å¿—è§£æã€åˆ†æã€ç›‘æ§å’Œå‘Šè­¦åŠŸèƒ½
"""

import re
import json
import time
import threading
from datetime import datetime, timedelta
from typing import Dict, List, Optional, Any, Pattern, Callable
from dataclasses import dataclass, asdict
from enum import Enum
from collections import defaultdict, deque
import logging
import os
from pathlib import Path


class LogLevel(Enum):
    """æ—¥å¿—çº§åˆ«"""
    DEBUG = "debug"
    INFO = "info"
    WARNING = "warning"
    ERROR = "error"
    CRITICAL = "critical"


class LogEventType(Enum):
    """æ—¥å¿—äº‹ä»¶ç±»å‹"""
    CONNECTION = "connection"
    CHANNEL = "channel"
    QUEUE = "queue"
    MESSAGE = "message"
    AUTHENTICATION = "authentication"
    CLUSTER = "cluster"
    GARBAGE_COLLECTION = "gc"
    DISK = "disk"
    MEMORY = "memory"
    UNKNOWN = "unknown"


@dataclass
class LogEntry:
    """æ—¥å¿—æ¡ç›®"""
    timestamp: datetime
    level: LogLevel
    source: str
    message: str
    event_type: LogEventType
    details: Dict[str, Any]
    raw_line: str


@dataclass
class LogStats:
    """æ—¥å¿—ç»Ÿè®¡"""
    total_entries: int
    entries_by_level: Dict[str, int]
    entries_by_type: Dict[str, int]
    entries_by_source: Dict[str, int]
    error_rate: float
    time_range: tuple


@dataclass
class LogPattern:
    """æ—¥å¿—æ¨¡å¼"""
    name: str
    pattern: str
    event_type: LogEventType
    extract_fields: List[str]
    description: str


class LogParser:
    """æ—¥å¿—è§£æå™¨"""
    
    def __init__(self):
        self.patterns = []
        self._setup_default_patterns()
    
    def _setup_default_patterns(self):
        """è®¾ç½®é»˜è®¤æ¨¡å¼"""
        # è¿æ¥æ—¥å¿—æ¨¡å¼
        connection_pattern = LogPattern(
            name="connection_pattern",
            pattern=r'(\d{4}-\d{2}-\d{2} \d{2}:\d{2}:\d{2}\.\d{3}) \[(info|warning|error)\] <\d+> (rabbit_\d+) started\. TCP connection from (\d+\.\d+\.\d+\.\d+:\d+)',
            event_type=LogEventType.CONNECTION,
            extract_fields=['timestamp', 'level', 'pid', 'source'],
            description="è¿æ¥å¯åŠ¨æ—¥å¿—"
        )
        
        # é”™è¯¯æ—¥å¿—æ¨¡å¼
        error_pattern = LogPattern(
            name="error_pattern",
            pattern=r'(\d{4}-\d{2}-\d{2} \d{2}:\d{2}:\d{2}\.\d{3}) \[(error|critical)\] <\d+> (rabbit_\d+) (.*?)(?: \{(\{.*?\})\})?',
            event_type=LogEventType.UNKNOWN,
            extract_fields=['timestamp', 'level', 'pid', 'message', 'context'],
            description="é”™è¯¯æ—¥å¿—æ¨¡å¼"
        )
        
        # é˜Ÿåˆ—æ—¥å¿—æ¨¡å¼
        queue_pattern = LogPattern(
            name="queue_pattern",
            pattern=r'(\d{4}-\d{2}-\d{2} \d{2}:\d{2}:\d{2}\.\d{3}) \[(info|warning)\] <\d+> rabbit_queue (created|deleted|declaring|deleting)\s+(.*?)(?: \(.*? (\d+)\))?',
            event_type=LogEventType.QUEUE,
            extract_fields=['timestamp', 'level', 'action', 'queue_name'],
            description="é˜Ÿåˆ—æ“ä½œæ—¥å¿—"
        )
        
        # è®¤è¯æ—¥å¿—æ¨¡å¼
        auth_pattern = LogPattern(
            name="auth_pattern",
            pattern=r'(\d{4}-\d{2}-\d{2} \d{2}:\d{2}:\d{2}\.\d{3}) \[(info|warning)\] <\d+> rabbit_access_control (.*?)(?: denied for user (\'.*?\'))?(?: in vhost (\'.*?\'))?',
            event_type=LogEventType.AUTHENTICATION,
            extract_fields=['timestamp', 'level', 'action', 'user', 'vhost'],
            description="è®¤è¯æ—¥å¿—æ¨¡å¼"
        )
        
        # GCæ—¥å¿—æ¨¡å¼
        gc_pattern = LogPattern(
            name="gc_pattern",
            pattern=r'(\d{4}-\d{2}-\d{2} \d{2}:\d{2}:\d{2}\.\d{3}) \[info\] <\d+> (\w+): GC started',
            event_type=LogEventType.GARBAGE_COLLECTION,
            extract_fields=['timestamp', 'type'],
            description="åƒåœ¾å›æ”¶æ—¥å¿—"
        )
        
        # ç£ç›˜æ—¥å¿—æ¨¡å¼
        disk_pattern = LogPattern(
            name="disk_pattern",
            pattern=r'(\d{4}-\d{2}-\d{2} \d{2}:\d{2}:\d{2}\.\d{3}) \[(info|warning)\] <\d+> Disk free: (\d+)\. Space available: (\d+)',
            event_type=LogEventType.DISK,
            extract_fields=['timestamp', 'level', 'free_bytes', 'available_bytes'],
            description="ç£ç›˜ç©ºé—´æ—¥å¿—"
        )
        
        self.patterns = [
            connection_pattern,
            error_pattern,
            queue_pattern,
            auth_pattern,
            gc_pattern,
            disk_pattern
        ]
    
    def add_pattern(self, pattern: LogPattern):
        """æ·»åŠ è‡ªå®šä¹‰æ¨¡å¼"""
        self.patterns.append(pattern)
    
    def parse_log_line(self, line: str) -> Optional[LogEntry]:
        """è§£æå•è¡Œæ—¥å¿—"""
        line = line.strip()
        if not line:
            return None
        
        for pattern in self.patterns:
            match = re.match(pattern.pattern, line)
            if match:
                try:
                    groups = match.groups()
                    details = {}
                    
                    # æå–è¯¦ç»†å­—æ®µ
                    for i, field in enumerate(pattern.extract_fields):
                        if i < len(groups):
                            details[field] = groups[i]
                    
                    # è§£ææ—¶é—´æˆ³
                    timestamp_str = details.get('timestamp', '')
                    if timestamp_str:
                        timestamp = datetime.strptime(timestamp_str, '%Y-%m-%d %H:%M:%S.%f')
                    else:
                        timestamp = datetime.now()
                    
                    # è§£ææ—¥å¿—çº§åˆ«
                    level_str = details.get('level', 'info')
                    level = self._parse_log_level(level_str)
                    
                    # ç¡®å®šäº‹ä»¶ç±»å‹
                    event_type = self._determine_event_type(line, pattern.event_type)
                    
                    return LogEntry(
                        timestamp=timestamp,
                        level=level,
                        source=details.get('pid', 'unknown'),
                        message=details.get('message', line),
                        event_type=event_type,
                        details=details,
                        raw_line=line
                    )
                except Exception as e:
                    logging.warning(f"è§£ææ—¥å¿—è¡Œå¤±è´¥: {e}, è¡Œ: {line}")
                    continue
        
        # å¦‚æœæ²¡æœ‰åŒ¹é…çš„æ¨¡å¼ï¼Œåˆ›å»ºé€šç”¨æ¡ç›®
        return self._create_generic_entry(line)
    
    def _parse_log_level(self, level_str: str) -> LogLevel:
        """è§£ææ—¥å¿—çº§åˆ«"""
        level_str = level_str.lower()
        level_mapping = {
            'debug': LogLevel.DEBUG,
            'info': LogLevel.INFO,
            'warning': LogLevel.WARNING,
            'error': LogLevel.ERROR,
            'critical': LogLevel.CRITICAL
        }
        return level_mapping.get(level_str, LogLevel.INFO)
    
    def _determine_event_type(self, line: str, pattern_type: LogEventType) -> LogEventType:
        """ç¡®å®šäº‹ä»¶ç±»å‹"""
        line_lower = line.lower()
        
        # åŸºäºå…³é”®è¯è¿›ä¸€æ­¥åˆ†ç±»
        if 'connection' in line_lower:
            return LogEventType.CONNECTION
        elif 'channel' in line_lower:
            return LogEventType.CHANNEL
        elif 'queue' in line_lower:
            return LogEventType.QUEUE
        elif 'message' in line_lower:
            return LogEventType.MESSAGE
        elif 'auth' in line_lower or 'login' in line_lower:
            return LogEventType.AUTHENTICATION
        elif 'cluster' in line_lower:
            return LogEventType.CLUSTER
        elif 'gc' in line_lower or 'garbage' in line_lower:
            return LogEventType.GARBAGE_COLLECTION
        elif 'disk' in line_lower:
            return LogEventType.DISK
        elif 'memory' in line_lower:
            return LogEventType.MEMORY
        else:
            return pattern_type
    
    def _create_generic_entry(self, line: str) -> LogEntry:
        """åˆ›å»ºé€šç”¨æ—¥å¿—æ¡ç›®"""
        return LogEntry(
            timestamp=datetime.now(),
            level=LogLevel.INFO,
            source='unknown',
            message=line,
            event_type=LogEventType.UNKNOWN,
            details={},
            raw_line=line
        )


class LogFileReader:
    """æ—¥å¿—æ–‡ä»¶é˜…è¯»å™¨"""
    
    def __init__(self, log_file: str):
        self.log_file = Path(log_file)
        self.parser = LogParser()
        self.following = False
        self.reader_thread = None
        self.callbacks = []
        self.logger = logging.getLogger(__name__)
        
        # è·Ÿè¸ªæ–‡ä»¶ä½ç½®
        self.last_position = 0
    
    def add_log_callback(self, callback: Callable[[LogEntry], None]):
        """æ·»åŠ æ—¥å¿—å›è°ƒ"""
        self.callbacks.append(callback)
    
    def start_following(self, seek_to_end: bool = True):
        """å¼€å§‹è·Ÿè¸ªæ—¥å¿—æ–‡ä»¶"""
        if self.following:
            return
        
        self.following = True
        
        # å¦‚æœä»æ–‡ä»¶æœ«å°¾å¼€å§‹
        if seek_to_end and self.log_file.exists():
            self.last_position = self.log_file.stat().st_size
        
        self.reader_thread = threading.Thread(target=self._follow_loop, daemon=True)
        self.reader_thread.start()
        self.logger.info(f"å¼€å§‹è·Ÿè¸ªæ—¥å¿—æ–‡ä»¶: {self.log_file}")
    
    def stop_following(self):
        """åœæ­¢è·Ÿè¸ªæ—¥å¿—æ–‡ä»¶"""
        self.following = False
        if self.reader_thread:
            self.reader_thread.join(timeout=5)
        self.logger.info("åœæ­¢è·Ÿè¸ªæ—¥å¿—æ–‡ä»¶")
    
    def _follow_loop(self):
        """è·Ÿè¸ªå¾ªç¯"""
        while self.following:
            try:
                if not self.log_file.exists():
                    time.sleep(1)
                    continue
                
                # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦è¢«è½®æ¢
                current_size = self.log_file.stat().st_size
                if current_size < self.last_position:
                    # æ–‡ä»¶è¢«è½®æ¢ï¼Œé‡ç½®ä½ç½®
                    self.last_position = 0
                
                with open(self.log_file, 'r', encoding='utf-8', errors='ignore') as f:
                    f.seek(self.last_position)
                    
                    for line in f:
                        entry = self.parser.parse_log_line(line)
                        if entry:
                            self._notify_callbacks(entry)
                    
                    self.last_position = f.tell()
                
                time.sleep(0.1)  # 100msé—´éš”
            except Exception as e:
                self.logger.error(f"è·Ÿè¸ªæ—¥å¿—æ–‡ä»¶æ—¶å‘ç”Ÿé”™è¯¯: {e}")
                time.sleep(1)
    
    def _notify_callbacks(self, entry: LogEntry):
        """é€šçŸ¥å›è°ƒå‡½æ•°"""
        for callback in self.callbacks:
            try:
                callback(entry)
            except Exception as e:
                self.logger.error(f"å›è°ƒå‡½æ•°æ‰§è¡Œå¤±è´¥: {e}")
    
    def read_historical_logs(self, start_time: Optional[datetime] = None, 
                           end_time: Optional[datetime] = None,
                           max_entries: int = 10000) -> List[LogEntry]:
        """è¯»å–å†å²æ—¥å¿—"""
        entries = []
        
        if not self.log_file.exists():
            return entries
        
        try:
            with open(self.log_file, 'r', encoding='utf-8', errors='ignore') as f:
                for line in f:
                    entry = self.parser.parse_log_line(line)
                    if entry:
                        # æ—¶é—´èŒƒå›´è¿‡æ»¤
                        if start_time and entry.timestamp < start_time:
                            continue
                        if end_time and entry.timestamp > end_time:
                            break
                        
                        entries.append(entry)
                        
                        if len(entries) >= max_entries:
                            break
            
            self.logger.info(f"è¯»å–å†å²æ—¥å¿—: {len(entries)} æ¡è®°å½•")
            return entries
        
        except Exception as e:
            self.logger.error(f"è¯»å–å†å²æ—¥å¿—å¤±è´¥: {e}")
            return entries


class LogAnalyzer:
    """æ—¥å¿—åˆ†æå™¨"""
    
    def __init__(self, max_entries: int = 100000):
        self.max_entries = max_entries
        self.entries = deque(maxlen=max_entries)
        self.analysis_callbacks = []
        self.logger = logging.getLogger(__name__)
    
    def add_analysis_callback(self, callback: Callable[[LogEntry], None]):
        """æ·»åŠ åˆ†æå›è°ƒ"""
        self.analysis_callbacks.append(callback)
    
    def add_entry(self, entry: LogEntry):
        """æ·»åŠ æ—¥å¿—æ¡ç›®"""
        self.entries.append(entry)
        
        # é€šçŸ¥åˆ†æå›è°ƒ
        for callback in self.analysis_callbacks:
            try:
                callback(entry)
            except Exception as e:
                self.logger.error(f"åˆ†æå›è°ƒæ‰§è¡Œå¤±è´¥: {e}")
    
    def analyze_errors(self, time_window_minutes: int = 60) -> Dict[str, Any]:
        """åˆ†æé”™è¯¯"""
        end_time = datetime.now()
        start_time = end_time - timedelta(minutes=time_window_minutes)
        
        # è¿‡æ»¤æ—¶é—´èŒƒå›´å†…çš„é”™è¯¯
        recent_errors = [
            entry for entry in self.entries
            if entry.level in [LogLevel.ERROR, LogLevel.CRITICAL] and
            start_time <= entry.timestamp <= end_time
        ]
        
        if not recent_errors:
            return {
                'error_count': 0,
                'error_rate': 0.0,
                'error_types': {},
                'error_sources': {},
                'time_range_minutes': time_window_minutes
            }
        
        # é”™è¯¯åˆ†ç±»
        error_types = defaultdict(int)
        error_sources = defaultdict(int)
        error_messages = defaultdict(int)
        
        for error in recent_errors:
            error_types[error.event_type.value] += 1
            error_sources[error.source] += 1
            error_messages[error.message] += 1
        
        # è®¡ç®—é”™è¯¯ç‡
        total_entries = len([
            entry for entry in self.entries
            if start_time <= entry.timestamp <= end_time
        ])
        
        error_rate = len(recent_errors) / max(total_entries, 1) * 100
        
        return {
            'error_count': len(recent_errors),
            'error_rate': round(error_rate, 2),
            'error_types': dict(error_types),
            'error_sources': dict(error_sources),
            'common_messages': dict(sorted(error_messages.items(), key=lambda x: x[1], reverse=True)[:10]),
            'time_range_minutes': time_window_minutes
        }
    
    def analyze_performance(self, time_window_minutes: int = 60) -> Dict[str, Any]:
        """åˆ†ææ€§èƒ½"""
        end_time = datetime.now()
        start_time = end_time - timedelta(minutes=time_window_minutes)
        
        # è¿‡æ»¤æ—¶é—´èŒƒå›´å†…çš„æ—¥å¿—
        recent_entries = [
            entry for entry in self.entries
            if start_time <= entry.timestamp <= end_time
        ]
        
        if not recent_entries:
            return {
                'total_entries': 0,
                'entries_per_minute': 0.0,
                'level_distribution': {},
                'event_type_distribution': {},
                'time_range_minutes': time_window_minutes
            }
        
        # æŒ‰çº§åˆ«ç»Ÿè®¡
        level_dist = defaultdict(int)
        type_dist = defaultdict(int)
        
        for entry in recent_entries:
            level_dist[entry.level.value] += 1
            type_dist[entry.event_type.value] += 1
        
        return {
            'total_entries': len(recent_entries),
            'entries_per_minute': len(recent_entries) / time_window_minutes,
            'level_distribution': dict(level_dist),
            'event_type_distribution': dict(type_dist),
            'time_range_minutes': time_window_minutes
        }
    
    def detect_anomalies(self, time_window_minutes: int = 60) -> List[Dict[str, Any]]:
        """æ£€æµ‹å¼‚å¸¸"""
        anomalies = []
        end_time = datetime.now()
        start_time = end_time - timedelta(minutes=time_window_minutes)
        
        recent_entries = [
            entry for entry in self.entries
            if start_time <= entry.timestamp <= end_time
        ]
        
        if len(recent_entries) < 10:
            return anomalies
        
        # æ£€æµ‹é”™è¯¯ç‡å¼‚å¸¸
        error_count = len([e for e in recent_entries if e.level in [LogLevel.ERROR, LogLevel.CRITICAL]])
        error_rate = error_count / len(recent_entries)
        
        if error_rate > 0.1:  # è¶…è¿‡10%é”™è¯¯ç‡
            anomalies.append({
                'type': 'high_error_rate',
                'severity': 'high' if error_rate > 0.2 else 'medium',
                'description': f'é”™è¯¯ç‡å¼‚å¸¸: {error_rate:.2%}',
                'error_rate': error_rate,
                'timestamp': end_time
            })
        
        # æ£€æµ‹æ¶ˆæ¯é‡å¼‚å¸¸
        entries_per_minute = len(recent_entries) / time_window_minutes
        
        # ä¸å†å²å¹³å‡æ¯”è¾ƒ
        historical_minute_counts = []
        for i in range(6):  # è¿‡å»6ä¸ªæ—¶é—´æ®µ
            hist_start = start_time - timedelta(minutes=(i+1) * time_window_minutes)
            hist_end = hist_start + timedelta(minutes=time_window_minutes)
            
            hist_count = len([
                e for e in self.entries
                if hist_start <= e.timestamp <= hist_end
            ])
            historical_minute_counts.append(hist_count)
        
        if historical_minute_counts:
            avg_historical = sum(historical_minute_counts) / len(historical_minute_counts)
            
            if entries_per_minute > avg_historical * 3:  # è¶…è¿‡å¹³å‡å€¼3å€
                anomalies.append({
                    'type': 'high_log_volume',
                    'severity': 'high',
                    'description': f'æ—¥å¿—é‡å¼‚å¸¸é«˜: {entries_per_minute:.1f}/åˆ†é’Ÿ (å¹³å‡: {avg_historical:.1f}/åˆ†é’Ÿ)',
                    'current_rate': entries_per_minute,
                    'historical_average': avg_historical,
                    'timestamp': end_time
                })
        
        return anomalies
    
    def get_statistics(self, time_window_minutes: int = 60) -> LogStats:
        """è·å–ç»Ÿè®¡ä¿¡æ¯"""
        end_time = datetime.now()
        start_time = end_time - timedelta(minutes=time_window_minutes)
        
        recent_entries = [
            entry for entry in self.entries
            if start_time <= entry.timestamp <= end_time
        ]
        
        # ç»Ÿè®¡
        level_dist = defaultdict(int)
        type_dist = defaultdict(int)
        source_dist = defaultdict(int)
        
        for entry in recent_entries:
            level_dist[entry.level.value] += 1
            type_dist[entry.event_type.value] += 1
            source_dist[entry.source] += 1
        
        # è®¡ç®—é”™è¯¯ç‡
        error_count = level_dist.get('error', 0) + level_dist.get('critical', 0)
        error_rate = error_count / max(len(recent_entries), 1) * 100
        
        return LogStats(
            total_entries=len(recent_entries),
            entries_by_level=dict(level_dist),
            entries_by_type=dict(type_dist),
            entries_by_source=dict(source_dist),
            error_rate=round(error_rate, 2),
            time_range=(start_time, end_time)
        )


class LogAlertManager:
    """æ—¥å¿—å‘Šè­¦ç®¡ç†å™¨"""
    
    def __init__(self):
        self.alert_rules = []
        self.active_alerts = {}
        self.alert_callbacks = []
        self.logger = logging.getLogger(__name__)
    
    def add_alert_rule(self, name: str, condition: str, threshold: float, 
                      time_window_minutes: int, level: LogLevel = LogLevel.ERROR,
                      description: str = ""):
        """æ·»åŠ å‘Šè­¦è§„åˆ™"""
        self.alert_rules.append({
            'name': name,
            'condition': condition,
            'threshold': threshold,
            'time_window_minutes': time_window_minutes,
            'level': level,
            'description': description
        })
    
    def add_alert_callback(self, callback: Callable[[Dict], None]):
        """æ·»åŠ å‘Šè­¦å›è°ƒ"""
        self.alert_callbacks.append(callback)
    
    def check_alerts(self, analyzer: LogAnalyzer) -> List[Dict]:
        """æ£€æŸ¥å‘Šè­¦"""
        alerts = []
        
        # è·å–é”™è¯¯åˆ†æ
        error_analysis = analyzer.analyze_errors()
        
        for rule in self.alert_rules:
            alert_triggered = False
            alert_message = ""
            
            if rule['condition'] == 'error_rate_high':
                if error_analysis['error_rate'] > rule['threshold']:
                    alert_triggered = True
                    alert_message = f"é”™è¯¯ç‡è¿‡é«˜: {error_analysis['error_rate']:.2f}% (é˜ˆå€¼: {rule['threshold']}%)"
            
            elif rule['condition'] == 'error_count_high':
                if error_analysis['error_count'] > rule['threshold']:
                    alert_triggered = True
                    alert_message = f"é”™è¯¯æ•°é‡è¿‡å¤š: {error_analysis['error_count']} (é˜ˆå€¼: {rule['threshold']})"
            
            elif rule['condition'] == 'critical_error':
                # æ£€æŸ¥æ˜¯å¦æœ‰Criticalçº§åˆ«çš„é”™è¯¯
                critical_entries = [
                    entry for entry in analyzer.entries
                    if entry.level == LogLevel.CRITICAL and
                    entry.timestamp >= datetime.now() - timedelta(minutes=rule['time_window_minutes'])
                ]
                if critical_entries:
                    alert_triggered = True
                    alert_message = f"æ£€æµ‹åˆ° {len(critical_entries)} ä¸ªä¸¥é‡é”™è¯¯"
            
            alert_key = rule['name']
            
            if alert_triggered:
                if alert_key not in self.active_alerts:
                    alert = {
                        'id': alert_key,
                        'name': rule['name'],
                        'level': rule['level'],
                        'condition': rule['condition'],
                        'message': alert_message,
                        'triggered_at': datetime.now(),
                        'rule': rule
                    }
                    
                    self.active_alerts[alert_key] = alert
                    alerts.append(alert)
                    
                    # é€šçŸ¥å›è°ƒ
                    for callback in self.alert_callbacks:
                        try:
                            callback(alert)
                        except Exception as e:
                            self.logger.error(f"å‘Šè­¦å›è°ƒå¤±è´¥: {e}")
            
            else:
                # å‘Šè­¦æ¢å¤
                if alert_key in self.active_alerts:
                    del self.active_alerts[alert_key]
                    self.logger.info(f"å‘Šè­¦æ¢å¤: {rule['name']}")
        
        return alerts
    
    def get_active_alerts(self) -> List[Dict]:
        """è·å–æ´»è·ƒå‘Šè­¦"""
        return list(self.active_alerts.values())


class LogAnalysisDemo:
    """æ—¥å¿—åˆ†ææ¼”ç¤º"""
    
    def __init__(self, log_file: str = None):
        self.log_file = log_file
        self.reader = None
        self.analyzer = LogAnalyzer()
        self.alert_manager = LogAlertManager()
        
        # è®¾ç½®é»˜è®¤å‘Šè­¦è§„åˆ™
        self._setup_default_alerts()
        
        # æ·»åŠ åˆ†æå›è°ƒ
        self.analyzer.add_analysis_callback(self._analyze_entry)
        
        # æ·»åŠ å‘Šè­¦å›è°ƒ
        self.alert_manager.add_alert_callback(self._alert_callback)
        
        self.logger = logging.getLogger(__name__)
    
    def _setup_default_alerts(self):
        """è®¾ç½®é»˜è®¤å‘Šè­¦è§„åˆ™"""
        self.alert_manager.add_alert_rule(
            name='high_error_rate',
            condition='error_rate_high',
            threshold=5.0,
            time_window_minutes=10,
            level=LogLevel.WARNING,
            description='é”™è¯¯ç‡è¶…è¿‡5%æ—¶è§¦å‘å‘Šè­¦'
        )
        
        self.alert_manager.add_alert_rule(
            name='critical_errors',
            condition='critical_error',
            threshold=1.0,
            time_window_minutes=5,
            level=LogLevel.CRITICAL,
            description='æ£€æµ‹åˆ°Criticalçº§åˆ«é”™è¯¯æ—¶ç«‹å³å‘Šè­¦'
        )
    
    def _analyze_entry(self, entry: LogEntry):
        """åˆ†ææ—¥å¿—æ¡ç›®"""
        # è¿™é‡Œå¯ä»¥å®ç°å®æ—¶åˆ†æé€»è¾‘
        pass
    
    def _alert_callback(self, alert: Dict):
        """å‘Šè­¦å›è°ƒ"""
        level_icons = {
            LogLevel.DEBUG: 'ğŸ›',
            LogLevel.INFO: 'â„¹ï¸',
            LogLevel.WARNING: 'âš ï¸',
            LogLevel.ERROR: 'âŒ',
            LogLevel.CRITICAL: 'ğŸš¨'
        }
        
        icon = level_icons.get(alert['level'], 'â“')
        print(f"{icon} æ—¥å¿—å‘Šè­¦: {alert['name']} - {alert['message']}")
    
    def demo_log_file_parsing(self):
        """æ¼”ç¤ºæ—¥å¿—æ–‡ä»¶è§£æ"""
        print("=== æ—¥å¿—æ–‡ä»¶è§£ææ¼”ç¤º ===")
        
        # åˆ›å»ºæµ‹è¯•æ—¥å¿—æ–‡ä»¶
        test_log_file = "test_rabbitmq.log"
        self._create_test_log_file(test_log_file)
        
        try:
            reader = LogFileReader(test_log_file)
            
            # è¯»å–å†å²æ—¥å¿—
            print(f"è§£ææ—¥å¿—æ–‡ä»¶: {test_log_file}")
            entries = reader.read_historical_logs(max_entries=50)
            
            print(f"è§£æåˆ° {len(entries)} æ¡æ—¥å¿—è®°å½•")
            
            # æ˜¾ç¤ºå‰å‡ æ¡è®°å½•
            for i, entry in enumerate(entries[:5]):
                print(f"\næ—¥å¿—æ¡ç›® {i+1}:")
                print(f"  æ—¶é—´: {entry.timestamp}")
                print(f"  çº§åˆ«: {entry.level.value}")
                print(f"  æº: {entry.source}")
                print(f"  ç±»å‹: {entry.event_type.value}")
                print(f"  æ¶ˆæ¯: {entry.message}")
            
            # åˆ†æç»Ÿè®¡
            for entry in entries:
                self.analyzer.add_entry(entry)
            
            stats = self.analyzer.get_statistics()
            print(f"\næ—¥å¿—ç»Ÿè®¡:")
            print(f"  æ€»è®°å½•æ•°: {stats.total_entries}")
            print(f"  é”™è¯¯ç‡: {stats.error_rate}%")
            print(f"  çº§åˆ«åˆ†å¸ƒ: {stats.entries_by_level}")
            print(f"  ç±»å‹åˆ†å¸ƒ: {stats.entries_by_type}")
            
        finally:
            # æ¸…ç†æµ‹è¯•æ–‡ä»¶
            if os.path.exists(test_log_file):
                os.remove(test_log_file)
    
    def _create_test_log_file(self, filename: str):
        """åˆ›å»ºæµ‹è¯•æ—¥å¿—æ–‡ä»¶"""
        test_logs = [
            "2024-01-15 10:00:00.123 [info] <1234> rabbit_123 started. TCP connection from 192.168.1.100:12345",
            "2024-01-15 10:01:30.456 [warning] <1234> rabbit_queue declaring queue: test_queue",
            "2024-01-15 10:02:15.789 [error] <1234> Connection closed by client",
            "2024-01-15 10:03:00.012 [info] <1234> Disk free: 1000000000. Space available: 800000000",
            "2024-01-15 10:04:45.345 [warning] <1235> rabbit_access_control denied for user 'test_user' in vhost '/test'",
            "2024-01-15 10:05:30.678 [critical] <1234> Memory allocation failed",
            "2024-01-15 10:06:00.901 [info] <1234> Garbage collection started",
            "2024-01-15 10:07:15.234 [error] <1234> Channel closed: channel error",
            "2024-01-15 10:08:00.567 [info] <1234> Queue created: test_queue",
            "2024-01-15 10:09:30.890 [error] <1236> Authentication failed for user 'admin'"
        ]
        
        with open(filename, 'w') as f:
            for log in test_logs:
                f.write(log + '\n')
    
    def demo_real_time_monitoring(self):
        """æ¼”ç¤ºå®æ—¶ç›‘æ§"""
        print("\n=== å®æ—¶æ—¥å¿—ç›‘æ§æ¼”ç¤º ===")
        
        # åˆ›å»ºå®æ—¶æ—¥å¿—æ–‡ä»¶
        real_time_log = "realtime_rabbitmq.log"
        self._create_real_time_log_stream(real_time_log)
        
        try:
            reader = LogFileReader(real_time_log)
            self.reader = reader
            
            # æ·»åŠ æ—¥å¿—å›è°ƒ
            reader.add_log_callback(self._log_entry_callback)
            
            # å¼€å§‹è·Ÿè¸ª
            print("å¼€å§‹å®æ—¶è·Ÿè¸ªæ—¥å¿—...")
            reader.start_following()
            
            # è¿è¡Œ30ç§’
            time.sleep(30)
            
            # åœæ­¢è·Ÿè¸ª
            reader.stop_following()
            
        except KeyboardInterrupt:
            print("\nç›‘æ§è¢«ç”¨æˆ·ä¸­æ–­")
            if self.reader:
                self.reader.stop_following()
        finally:
            # æ¸…ç†æµ‹è¯•æ–‡ä»¶
            if os.path.exists(real_time_log):
                os.remove(real_time_log)
    
    def _log_entry_callback(self, entry: LogEntry):
        """æ—¥å¿—æ¡ç›®å›è°ƒ"""
        self.analyzer.add_entry(entry)
        
        # æ˜¾ç¤ºå…³é”®æ—¥å¿—
        if entry.level in [LogLevel.ERROR, LogLevel.CRITICAL]:
            icon = "âŒ" if entry.level == LogLevel.ERROR else "ğŸš¨"
            print(f"{icon} {entry.timestamp.strftime('%H:%M:%S')} - {entry.event_type.value}: {entry.message}")
        
        # æ¯10æ¡è®°å½•æ˜¾ç¤ºä¸€æ¬¡ç»Ÿè®¡
        if len(self.analyzer.entries) % 10 == 0:
            stats = self.analyzer.get_statistics(time_window_minutes=60)
            print(f"\nğŸ“Š ç»Ÿè®¡æ›´æ–° - æ€»è®°å½•: {stats.total_entries}, é”™è¯¯ç‡: {stats.error_rate}%")
    
    def _create_real_time_log_stream(self, filename: str):
        """åˆ›å»ºå®æ—¶æ—¥å¿—æµ"""
        import threading
        
        # æ¸…ç©ºæ–‡ä»¶
        with open(filename, 'w') as f:
            f.write("")
        
        def log_writer():
            log_messages = [
                "2024-01-15 15:00:00.000 [info] <1001> rabbit_1001 started. TCP connection from 10.0.0.1:54321",
                "2024-01-15 15:00:05.000 [info] <1001> Queue operation: test_queue",
                "2024-01-15 15:00:10.000 [warning] <1002> Disk space running low",
                "2024-01-15 15:00:15.000 [error] <1001> Message processing failed",
                "2024-01-15 15:00:20.000 [info] <1003> Connection established",
                "2024-01-15 15:00:25.000 [critical] <1004> System overload detected",
                "2024-01-15 15:00:30.000 [info] <1002> GC started",
                "2024-01-15 15:00:35.000 [warning] <1003> High memory usage",
                "2024-01-15 15:00:40.000 [error] <1001> Connection lost",
                "2024-01-15 15:00:45.000 [info] <1002> System recovered"
            ]
            
            for i, message in enumerate(log_messages * 10):  # é‡å¤10æ¬¡
                with open(filename, 'a') as f:
                    f.write(message + '\n')
                time.sleep(3)
        
        # åœ¨åå°çº¿ç¨‹ä¸­å†™å…¥æ—¥å¿—
        writer_thread = threading.Thread(target=log_writer, daemon=True)
        writer_thread.start()
    
    def demo_error_analysis(self):
        """æ¼”ç¤ºé”™è¯¯åˆ†æ"""
        print("\n=== é”™è¯¯åˆ†ææ¼”ç¤º ===")
        
        # æ·»åŠ ä¸€äº›æµ‹è¯•æ—¥å¿—æ¡ç›®
        test_entries = self._generate_test_entries()
        
        for entry in test_entries:
            self.analyzer.add_entry(entry)
        
        # åˆ†æé”™è¯¯
        error_analysis = self.analyzer.analyze_errors(time_window_minutes=60)
        
        print(f"é”™è¯¯åˆ†æç»“æœ:")
        print(f"  é”™è¯¯æ€»æ•°: {error_analysis['error_count']}")
        print(f"  é”™è¯¯ç‡: {error_analysis['error_rate']}%")
        print(f"  é”™è¯¯ç±»å‹åˆ†å¸ƒ: {error_analysis['error_types']}")
        print(f"  é”™è¯¯æºåˆ†å¸ƒ: {error_analysis['error_sources']}")
        
        if error_analysis['common_messages']:
            print(f"  å¸¸è§é”™è¯¯ä¿¡æ¯:")
            for message, count in list(error_analysis['common_messages'].items())[:5]:
                print(f"    - {message} ({count}æ¬¡)")
        
        # æ£€æµ‹å¼‚å¸¸
        anomalies = self.analyzer.detect_anomalies(time_window_minutes=60)
        if anomalies:
            print(f"\næ£€æµ‹åˆ° {len(anomalies)} ä¸ªå¼‚å¸¸:")
            for anomaly in anomalies:
                print(f"  - {anomaly['type']}: {anomaly['description']} (ä¸¥é‡ç¨‹åº¦: {anomaly['severity']})")
        else:
            print("\næœªæ£€æµ‹åˆ°å¼‚å¸¸")
    
    def _generate_test_entries(self) -> List[LogEntry]:
        """ç”Ÿæˆæµ‹è¯•æ—¥å¿—æ¡ç›®"""
        entries = []
        base_time = datetime.now() - timedelta(hours=1)
        
        # æ­£å¸¸æ—¥å¿—
        for i in range(100):
            entries.append(LogEntry(
                timestamp=base_time + timedelta(minutes=i),
                level=LogLevel.INFO,
                source=f"rabbit_{1000 + i % 10}",
                message="Normal operation",
                event_type=LogEventType.CONNECTION,
                details={},
                raw_line=""
            ))
        
        # é”™è¯¯æ—¥å¿—
        error_sources = ["rabbit_1001", "rabbit_1002", "rabbit_1003"]
        error_messages = ["Connection timeout", "Memory error", "Disk full", "Network error"]
        
        for i in range(15):  # 15%é”™è¯¯ç‡
            entries.append(LogEntry(
                timestamp=base_time + timedelta(minutes=i * 4),
                level=LogLevel.ERROR,
                source=error_sources[i % len(error_sources)],
                message=error_messages[i % len(error_messages)],
                event_type=LogEventType.CONNECTION if i % 3 == 0 else LogEventType.QUEUE,
                details={},
                raw_line=""
            ))
        
        # ä¸¥é‡é”™è¯¯
        for i in range(3):
            entries.append(LogEntry(
                timestamp=base_time + timedelta(minutes=i * 20),
                level=LogLevel.CRITICAL,
                source="rabbit_system",
                message="System failure",
                event_type=LogEventType.CLUSTER,
                details={},
                raw_line=""
            ))
        
        return entries
    
    def demo_alert_system(self):
        """æ¼”ç¤ºå‘Šè­¦ç³»ç»Ÿ"""
        print("\n=== å‘Šè­¦ç³»ç»Ÿæ¼”ç¤º ===")
        
        # æ·»åŠ æµ‹è¯•æ—¥å¿—
        test_entries = self._generate_test_entries()
        
        for entry in test_entries:
            self.analyzer.add_entry(entry)
        
        # æ£€æŸ¥å‘Šè­¦
        print("æ£€æŸ¥å‘Šè­¦...")
        alerts = self.alert_manager.check_alerts(self.analyzer)
        
        if alerts:
            print(f"è§¦å‘ {len(alerts)} ä¸ªå‘Šè­¦:")
            for alert in alerts:
                level_icon = {
                    LogLevel.DEBUG: 'ğŸ›',
                    LogLevel.INFO: 'â„¹ï¸',
                    LogLevel.WARNING: 'âš ï¸',
                    LogLevel.ERROR: 'âŒ',
                    LogLevel.CRITICAL: 'ğŸš¨'
                }
                icon = level_icon.get(alert['level'], 'â“')
                print(f"  {icon} {alert['name']}: {alert['message']}")
        else:
            print("æœªè§¦å‘å‘Šè­¦")
        
        # æ˜¾ç¤ºæ´»è·ƒå‘Šè­¦
        active_alerts = self.alert_manager.get_active_alerts()
        print(f"\nå½“å‰æ´»è·ƒå‘Šè­¦: {len(active_alerts)} ä¸ª")
        
        for alert in active_alerts:
            print(f"  - {alert['name']}: {alert['message']}")
    
    def run_complete_demo(self):
        """è¿è¡Œå®Œæ•´æ¼”ç¤º"""
        print("RabbitMQæ—¥å¿—åˆ†ææ¼”ç¤ºå¼€å§‹")
        print("=" * 50)
        
        try:
            # 1. æ—¥å¿—æ–‡ä»¶è§£æ
            self.demo_log_file_parsing()
            
            # 2. é”™è¯¯åˆ†æ
            self.demo_error_analysis()
            
            # 3. å‘Šè­¦ç³»ç»Ÿ
            self.demo_alert_system()
            
            # 4. å®æ—¶ç›‘æ§
            self.demo_real_time_monitoring()
            
            print("\næ¼”ç¤ºå®Œæˆ!")
            
        except KeyboardInterrupt:
            print("\næ¼”ç¤ºè¢«ç”¨æˆ·ä¸­æ–­")
            if self.reader:
                self.reader.stop_following()
        except Exception as e:
            print(f"\næ¼”ç¤ºè¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")


if __name__ == "__main__":
    # é…ç½®æ—¥å¿—
    logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
    
    demo = LogAnalysisDemo()
    demo.run_complete_demo()