#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ç¬¬2ç« ï¼šAMQPåè®®æ·±å…¥ç†è§£
AMQPæ¶ˆæ¯æµåˆ†æå’Œç›‘æ§å·¥å…·

åŠŸèƒ½ï¼š
- ç›‘æ§AMQPè¿æ¥çŠ¶æ€
- è·Ÿè¸ªæ¶ˆæ¯è·¯ç”±è·¯å¾„
- åˆ†ææ¶ˆæ¯æ€§èƒ½æŒ‡æ ‡
- è°ƒè¯•æ¶ˆæ¯æµé—®é¢˜
- ç”Ÿæˆæ¶ˆæ¯æµæŠ¥å‘Š

ä½œè€…ï¼šRabbitMQå­¦ä¹ æ•™ç¨‹
åˆ›å»ºæ—¶é—´ï¼š2025å¹´11æœˆ
"""

import pika
import time
import json
import threading
import logging
from datetime import datetime, timedelta
from typing import Dict, List, Any, Optional, Tuple
from dataclasses import dataclass, asdict
from collections import defaultdict, deque
import queue


@dataclass
class MessageFlow:
    """æ¶ˆæ¯æµè®°å½•"""
    message_id: str
    timestamp: float
    event_type: str  # publish, route, consume, ack, reject, expire
    source: str  # publish, queue, exchange
    destination: str  # exchange, queue, consumer
    routing_key: str
    properties: Dict[str, Any]
    payload_size: int
    processing_time: Optional[float] = None


@dataclass
class ConnectionStats:
    """è¿æ¥ç»Ÿè®¡ä¿¡æ¯"""
    connection_id: str
    created_time: float
    channel_count: int
    message_count: int
    error_count: int
    last_activity: float
    status: str  # connected, disconnected, error


class AMQPFlowAnalyzer:
    """AMQPæ¶ˆæ¯æµåˆ†æå™¨"""
    
    def __init__(self, host='localhost', port=5672, max_history=10000):
        self.host = host
        self.port = port
        self.max_history = max_history
        self.connection = None
        self.channel = None
        
        # æ¶ˆæ¯æµå†å²
        self.message_flows: deque = deque(maxlen=max_history)
        self.connection_stats: Dict[str, ConnectionStats] = {}
        
        # æ€§èƒ½ç»Ÿè®¡
        self.performance_metrics = {
            'total_messages': 0,
            'total_throughput': 0,
            'average_latency': 0,
            'error_rate': 0,
            'connections_active': 0
        }
        
        # ç›‘æ§çº¿ç¨‹
        self.monitoring_active = False
        self.monitor_thread = None
        
        # é…ç½®æ—¥å¿—
        logging.basicConfig(level=logging.INFO)
        self.logger = logging.getLogger(__name__)
        
    def connect(self):
        """å»ºç«‹è¿æ¥"""
        try:
            self.connection = pika.BlockingConnection(
                pika.ConnectionParameters(
                    host=self.host,
                    port=self.port,
                    heartbeat=30,
                    blocked_connection_timeout=10,
                    connection_attempts=3,
                    retry_delay=5
                )
            )
            self.channel = self.connection.channel()
            
            # è®¾ç½®è¿æ¥çŠ¶æ€å›è°ƒ
            self.connection.add_on_connection_blocked_callback(self._on_connection_blocked)
            self.connection.add_on_connection_unblocked_callback(self._on_connection_unblocked)
            self.connection.add_on_connection_closed_callback(self._on_connection_closed)
            
            self.logger.info(f"âœ… è¿æ¥åˆ° RabbitMQ: {self.host}:{self.port}")
            return True
            
        except Exception as e:
            self.logger.error(f"âŒ è¿æ¥å¤±è´¥: {e}")
            return False
    
    def _on_connection_blocked(self, connection):
        """è¿æ¥è¢«é˜»å¡å›è°ƒ"""
        self.logger.warning("âš ï¸ è¿æ¥è¢«é˜»å¡")
        self._update_connection_stats('blocked')
    
    def _on_connection_unblocked(self, connection):
        """è¿æ¥è§£é™¤é˜»å¡å›è°ƒ"""
        self.logger.info("âœ… è¿æ¥å·²è§£é™¤é˜»å¡")
        self._update_connection_stats('connected')
    
    def _on_connection_closed(self, connection, reply_code, reply_text):
        """è¿æ¥å…³é—­å›è°ƒ"""
        self.logger.warning(f"ğŸ”Œ è¿æ¥å…³é—­: {reply_code} - {reply_text}")
        self._update_connection_stats('disconnected', error=True)
    
    def _update_connection_stats(self, status: str, error: bool = False):
        """æ›´æ–°è¿æ¥ç»Ÿè®¡"""
        if self.connection:
            conn_id = id(self.connection)
            
            if conn_id in self.connection_stats:
                stats = self.connection_stats[conn_id]
                stats.status = status
                stats.last_activity = time.time()
                if error:
                    stats.error_count += 1
            else:
                stats = ConnectionStats(
                    connection_id=str(conn_id),
                    created_time=time.time(),
                    channel_count=1,
                    message_count=0,
                    error_count=1 if error else 0,
                    last_activity=time.time(),
                    status=status
                )
                self.connection_stats[conn_id] = stats
    
    def setup_monitoring_queues(self):
        """è®¾ç½®ç›‘æ§é˜Ÿåˆ—"""
        # åˆ›å»ºç›‘æ§äº¤æ¢æœº
        self.channel.exchange_declare(
            exchange='monitoring_exchange',
            exchange_type='topic',
            durable=True,
            auto_delete=False
        )
        
        # åˆ›å»ºç›‘æ§é˜Ÿåˆ—
        monitoring_queues = {
            'flow_tracking_queue': {'durable': True, 'auto_delete': False},
            'performance_metrics_queue': {'durable': True, 'auto_delete': False},
            'connection_alerts_queue': {'durable': True, 'auto_delete': False}
        }
        
        for queue_name, args in monitoring_queues.items():
            self.channel.queue_declare(queue=queue_name, **args)
            self.channel.queue_bind(
                exchange='monitoring_exchange',
                queue=queue_name,
                routing_key=f'monitor.{queue_name}'
            )
        
        self.logger.info("âœ… ç›‘æ§é˜Ÿåˆ—åˆ›å»ºå®Œæˆ")
    
    def instrument_producer(self, exchange_name: str = '', queue_name: str = None):
        """ä¸ºç”Ÿäº§è€…æ·»åŠ ç›‘æ§"""
        
        def instrumented_publish(method, properties, body):
            """è¢«ç›‘æ§çš„å‘å¸ƒæ–¹æ³•"""
            try:
                # è®°å½•å‘å¸ƒäº‹ä»¶
                flow = MessageFlow(
                    message_id=properties.message_id or f"msg-{int(time.time() * 1000)}",
                    timestamp=time.time(),
                    event_type='publish',
                    source='producer',
                    destination=exchange_name or queue_name,
                    routing_key=properties.correlation_id or '',
                    properties=asdict(properties),
                    payload_size=len(body)
                )
                
                self.message_flows.append(flow)
                self.performance_metrics['total_messages'] += 1
                
                self.logger.info(f"ğŸ“¤ ç›‘æ§å‘å¸ƒ: {flow.message_id}")
                
                # è°ƒç”¨åŸå§‹å‘å¸ƒæ–¹æ³•
                return method()
                
            except Exception as e:
                self.logger.error(f"âŒ å‘å¸ƒç›‘æ§å¤±è´¥: {e}")
                return method()
        
        return instrumented_publish
    
    def track_message_routing(self, exchange_name: str, routing_key: str, properties: pika.BasicProperties):
        """è·Ÿè¸ªæ¶ˆæ¯è·¯ç”±"""
        flow = MessageFlow(
            message_id=properties.message_id or f"route-{int(time.time() * 1000)}",
            timestamp=time.time(),
            event_type='route',
            source=exchange_name,
            destination=routing_key,
            routing_key=routing_key,
            properties=asdict(properties),
            payload_size=0
        )
        
        self.message_flows.append(flow)
        self.logger.info(f"ğŸ”„ è·Ÿè¸ªè·¯ç”±: {exchange_name} -> {routing_key}")
    
    def track_message_consumption(self, channel, method, properties, body):
        """è·Ÿè¸ªæ¶ˆæ¯æ¶ˆè´¹"""
        flow = MessageFlow(
            message_id=properties.message_id or f"cons-{int(time.time() * 1000)}",
            timestamp=time.time(),
            event_type='consume',
            source=method.routing_key,
            destination='consumer',
            routing_key=method.routing_key,
            properties=asdict(properties),
            payload_size=len(body)
        )
        
        self.message_flows.append(flow)
        self.performance_metrics['total_messages'] += 1
        
        self.logger.info(f"ğŸ“¥ è·Ÿè¸ªæ¶ˆè´¹: {flow.message_id}")
        
        return channel.basic_consume(
            queue=method.routing_key,
            on_message_callback=self._consume_with_tracking,
            auto_ack=False
        )
    
    def _consume_with_tracking(self, channel, method, properties, body):
        """å¸¦è·Ÿè¸ªçš„æ¶ˆè´¹å›è°ƒ"""
        start_time = time.time()
        
        try:
            # è®°å½•æ¶ˆè´¹äº‹ä»¶
            flow = MessageFlow(
                message_id=properties.message_id or f"consumed-{int(time.time() * 1000)}",
                timestamp=time.time(),
                event_type='consume',
                source=method.routing_key,
                destination='consumer',
                routing_key=method.routing_key,
                properties=asdict(properties),
                payload_size=len(body)
            )
            
            self.message_flows.append(flow)
            
            # æ¨¡æ‹Ÿæ¶ˆæ¯å¤„ç†
            time.sleep(0.1)  # æ¨¡æ‹Ÿå¤„ç†æ—¶é—´
            
            processing_time = time.time() - start_time
            flow.processing_time = processing_time
            
            # ç¡®è®¤æ¶ˆæ¯
            channel.basic_ack(delivery_tag=method.delivery_tag)
            
            # è®°å½•ç¡®è®¤äº‹ä»¶
            ack_flow = MessageFlow(
                message_id=f"{flow.message_id}_ack",
                timestamp=time.time(),
                event_type='ack',
                source='consumer',
                destination=method.routing_key,
                routing_key=method.routing_key,
                properties=asdict(properties),
                payload_size=len(body),
                processing_time=processing_time
            )
            
            self.message_flows.append(ack_flow)
            
            self.logger.info(f"âœ… æ¶ˆè´¹ç¡®è®¤: {flow.message_id} (å¤„ç†æ—¶é—´: {processing_time:.3f}s)")
            
        except Exception as e:
            self.logger.error(f"âŒ æ¶ˆè´¹å¤±è´¥: {e}")
            # æ‹’ç»æ¶ˆæ¯
            channel.basic_nack(delivery_tag=method.delivery_tag, requeue=False)
            
            # è®°å½•æ‹’ç»äº‹ä»¶
            nack_flow = MessageFlow(
                message_id=f"{flow.message_id}_nack",
                timestamp=time.time(),
                event_type='reject',
                source='consumer',
                destination=method.routing_key,
                routing_key=method.routing_key,
                properties=asdict(properties),
                payload_size=len(body)
            )
            
            self.message_flows.append(nack_flow)
    
    def analyze_message_flow(self, message_id: str = None, time_window: int = 60):
        """åˆ†ææ¶ˆæ¯æµ"""
        current_time = time.time()
        start_time = current_time - time_window
        
        # è¿‡æ»¤æ¶ˆæ¯æµ
        flows = [
            flow for flow in self.message_flows
            if flow.timestamp >= start_time and
               (message_id is None or message_id in flow.message_id)
        ]
        
        if not flows:
            self.logger.warning("âš ï¸ æœªæ‰¾åˆ°ç¬¦åˆæ¡ä»¶çš„æ¶ˆæ¯æµ")
            return None
        
        # åˆ†æç»Ÿè®¡
        analysis = {
            'total_events': len(flows),
            'time_window': time_window,
            'event_types': defaultdict(int),
            'source_distribution': defaultdict(int),
            'destination_distribution': defaultdict(int),
            'routing_keys': defaultdict(int),
            'processing_times': [],
            'payload_sizes': [],
            'latency_analysis': {}
        }
        
        for flow in flows:
            analysis['event_types'][flow.event_type] += 1
            analysis['source_distribution'][flow.source] += 1
            analysis['destination_distribution'][flow.destination] += 1
            analysis['routing_keys'][flow.routing_key] += 1
            
            if flow.processing_time:
                analysis['processing_times'].append(flow.processing_time)
            
            if flow.payload_size > 0:
                analysis['payload_sizes'].append(flow.payload_size)
        
        # è®¡ç®—æ€§èƒ½æŒ‡æ ‡
        if analysis['processing_times']:
            analysis['avg_processing_time'] = sum(analysis['processing_times']) / len(analysis['processing_times'])
            analysis['max_processing_time'] = max(analysis['processing_times'])
            analysis['min_processing_time'] = min(analysis['processing_times'])
        
        if analysis['payload_sizes']:
            analysis['avg_payload_size'] = sum(analysis['payload_sizes']) / len(analysis['payload_sizes'])
            analysis['total_payload_size'] = sum(analysis['payload_sizes'])
        
        analysis['throughput'] = len(flows) / time_window
        
        return analysis
    
    def generate_flow_report(self, analysis: Dict[str, Any]):
        """ç”Ÿæˆæ¶ˆæ¯æµæŠ¥å‘Š"""
        print("\nğŸ“Š AMQPæ¶ˆæ¯æµåˆ†ææŠ¥å‘Š")
        print("=" * 80)
        
        if not analysis:
            print("âŒ æ— åˆ†ææ•°æ®")
            return
        
        print(f"ğŸ“… åˆ†ææ—¶é—´çª—å£: {analysis['time_window']}ç§’")
        print(f"ğŸ“ˆ æ€»äº‹ä»¶æ•°: {analysis['total_events']}")
        print(f"âš¡ ååé‡: {analysis['throughput']:.2f} äº‹ä»¶/ç§’")
        
        # äº‹ä»¶ç±»å‹åˆ†å¸ƒ
        print(f"\nğŸ“‹ äº‹ä»¶ç±»å‹åˆ†å¸ƒ:")
        for event_type, count in analysis['event_types'].items():
            percentage = (count / analysis['total_events']) * 100
            print(f"   {event_type}: {count} ({percentage:.1f}%)")
        
        # æºåˆ†å¸ƒ
        print(f"\nğŸ“¤ æ¶ˆæ¯æºåˆ†å¸ƒ:")
        for source, count in analysis['source_distribution'].items():
            percentage = (count / analysis['total_events']) * 100
            print(f"   {source}: {count} ({percentage:.1f}%)")
        
        # ç›®æ ‡åˆ†å¸ƒ
        print(f"\nğŸ“¥ æ¶ˆæ¯ç›®æ ‡åˆ†å¸ƒ:")
        for dest, count in analysis['destination_distribution'].items():
            percentage = (count / analysis['total_events']) * 100
            print(f"   {dest}: {count} ({percentage:.1f}%)")
        
        # æ€§èƒ½åˆ†æ
        if 'avg_processing_time' in analysis:
            print(f"\nâ±ï¸ æ€§èƒ½åˆ†æ:")
            print(f"   å¹³å‡å¤„ç†æ—¶é—´: {analysis['avg_processing_time']:.3f}ç§’")
            print(f"   æœ€å¤§å¤„ç†æ—¶é—´: {analysis['max_processing_time']:.3f}ç§’")
            print(f"   æœ€å°å¤„ç†æ—¶é—´: {analysis['min_processing_time']:.3f}ç§’")
        
        if 'avg_payload_size' in analysis:
            print(f"\nğŸ“¦ è´Ÿè½½åˆ†æ:")
            print(f"   å¹³å‡è´Ÿè½½å¤§å°: {analysis['avg_payload_size']:.1f}å­—èŠ‚")
            print(f"   æ€»è´Ÿè½½å¤§å°: {analysis['total_payload_size']:.1f}å­—èŠ‚")
        
        # è·¯ç”±é”®ç»Ÿè®¡
        print(f"\nğŸ”‘ è·¯ç”±é”®ä½¿ç”¨ç»Ÿè®¡:")
        for routing_key, count in sorted(analysis['routing_keys'].items(), 
                                       key=lambda x: x[1], reverse=True):
            percentage = (count / analysis['total_events']) * 100
            print(f"   {routing_key}: {count} ({percentage:.1f}%)")
    
    def start_monitoring(self):
        """å¯åŠ¨ç›‘æ§"""
        self.monitoring_active = True
        self.monitor_thread = threading.Thread(target=self._monitor_loop)
        self.monitor_thread.daemon = True
        self.monitor_thread.start()
        self.logger.info("ğŸš€ æ¶ˆæ¯æµç›‘æ§å·²å¯åŠ¨")
    
    def stop_monitoring(self):
        """åœæ­¢ç›‘æ§"""
        self.monitoring_active = False
        if self.monitor_thread:
            self.monitor_thread.join()
        self.logger.info("â¹ï¸ æ¶ˆæ¯æµç›‘æ§å·²åœæ­¢")
    
    def _monitor_loop(self):
        """ç›‘æ§å¾ªç¯"""
        while self.monitoring_active:
            try:
                # æ£€æŸ¥è¿æ¥çŠ¶æ€
                if not self.connection or not self.connection.is_open:
                    self.logger.warning("âš ï¸ è¿æ¥å·²æ–­å¼€ï¼Œå°è¯•é‡è¿...")
                    time.sleep(5)
                    continue
                
                # æ›´æ–°æ€§èƒ½æŒ‡æ ‡
                self._update_performance_metrics()
                
                # æ£€æŸ¥å¼‚å¸¸æƒ…å†µ
                self._check_anomalies()
                
                time.sleep(1)
                
            except Exception as e:
                self.logger.error(f"âŒ ç›‘æ§å¾ªç¯é”™è¯¯: {e}")
                time.sleep(5)
    
    def _update_performance_metrics(self):
        """æ›´æ–°æ€§èƒ½æŒ‡æ ‡"""
        current_time = time.time()
        
        # æ´»è·ƒè¿æ¥æ•°
        active_connections = sum(
            1 for stats in self.connection_stats.values()
            if stats.status == 'connected' and 
               current_time - stats.last_activity < 30
        )
        
        self.performance_metrics['connections_active'] = active_connections
        
        # é”™è¯¯ç‡
        total_errors = sum(stats.error_count for stats in self.connection_stats.values())
        total_operations = self.performance_metrics['total_messages'] + total_errors
        
        if total_operations > 0:
            self.performance_metrics['error_rate'] = total_errors / total_operations
    
    def _check_anomalies(self):
        """æ£€æŸ¥å¼‚å¸¸æƒ…å†µ"""
        current_time = time.time()
        
        # æ£€æŸ¥é•¿æ—¶é—´æ— æ´»åŠ¨çš„è¿æ¥
        for stats in self.connection_stats.values():
            if (stats.status == 'connected' and 
                current_time - stats.last_activity > 60):
                self.logger.warning(f"âš ï¸ è¿æ¥ {stats.connection_id} é•¿æ—¶é—´æ— æ´»åŠ¨")
    
    def get_connection_info(self):
        """è·å–è¿æ¥ä¿¡æ¯"""
        if not self.connection:
            return None
        
        info = {
            'is_open': self.connection.is_open,
            'is_closed': self.connection.is_closed,
            'has_open_channels': len(self.connection._channels) > 0,
            'socket_timeout': self.connection.socket_timeout,
            'heartbeat': self.connection.heartbeat,
            'connected_time': getattr(self.connection, '_tune_connection', {}).get('start_time')
        }
        
        return info
    
    def run_flow_demonstration(self):
        """è¿è¡Œæ¶ˆæ¯æµæ¼”ç¤º"""
        print("\nğŸ¬ AMQPæ¶ˆæ¯æµæ¼”ç¤º")
        print("=" * 60)
        
        if not self.connect():
            return False
        
        try:
            # è®¾ç½®ç›‘æ§
            self.setup_monitoring_queues()
            self.start_monitoring()
            
            # åˆ›å»ºæµ‹è¯•äº¤æ¢æœºå’Œé˜Ÿåˆ—
            self.channel.exchange_declare(
                exchange='demo_exchange',
                exchange_type='topic',
                durable=True
            )
            
            self.channel.queue_declare(queue='demo_queue', durable=True)
            self.channel.queue_bind(
                exchange='demo_exchange',
                queue='demo_queue',
                routing_key='demo.*'
            )
            
            print("âœ… æµ‹è¯•ç¯å¢ƒè®¾ç½®å®Œæˆ")
            
            # å‘é€æµ‹è¯•æ¶ˆæ¯
            print("\nğŸ“¤ å‘é€æµ‹è¯•æ¶ˆæ¯...")
            for i in range(10):
                properties = pika.BasicProperties(
                    message_id=f'demo-{i}',
                    timestamp=time.time(),
                    priority=i % 3,
                    correlation_id=f'corr-{i}',
                    content_type='application/json'
                )
                
                self.channel.basic_publish(
                    exchange='demo_exchange',
                    routing_key=f'demo.{i % 2}',
                    body=json.dumps({
                        'id': i,
                        'data': f'test_data_{i}',
                        'timestamp': time.time()
                    }),
                    properties=properties
                )
                
                # è·Ÿè¸ªè·¯ç”±
                self.track_message_routing('demo_exchange', f'demo.{i % 2}', properties)
                print(f"   å‘é€æ¶ˆæ¯ {i}: demo.{i % 2}")
                
                time.sleep(0.1)
            
            # å¯åŠ¨æ¶ˆè´¹è€…
            print("\nğŸ“¥ å¯åŠ¨æ¶ˆæ¯æ¶ˆè´¹...")
            self.channel.basic_consume(
                queue='demo_queue',
                on_message_callback=self._consume_with_tracking,
                auto_ack=False
            )
            
            # å¼€å§‹æ¶ˆè´¹
            try:
                self.channel.start_consuming()
            except KeyboardInterrupt:
                self.channel.stop_consuming()
            
            # ç­‰å¾…ä¸€äº›æ¶ˆæ¯å¤„ç†å®Œæˆ
            time.sleep(2)
            
            # åœæ­¢ç›‘æ§
            self.stop_monitoring()
            
            # åˆ†ææ¶ˆæ¯æµ
            analysis = self.analyze_message_flow(time_window=30)
            if analysis:
                self.generate_flow_report(analysis)
            
            return True
            
        except Exception as e:
            self.logger.error(f"âŒ æ¼”ç¤ºå¤±è´¥: {e}")
            return False
        
        finally:
            self.close()
    
    def close(self):
        """å…³é—­è¿æ¥"""
        if self.monitoring_active:
            self.stop_monitoring()
            
        if self.connection and self.connection.is_open:
            self.connection.close()
            self.logger.info("ğŸ”’ è¿æ¥å·²å…³é—­")


def interactive_flow_analysis():
    """äº¤äº’å¼æ¶ˆæ¯æµåˆ†æ"""
    print("\nğŸ¯ äº¤äº’å¼AMQPæ¶ˆæ¯æµåˆ†æ")
    print("=" * 60)
    
    analyzer = AMQPFlowAnalyzer()
    
    while True:
        print("\nè¯·é€‰æ‹©æ“ä½œ:")
        print("1. å¯åŠ¨æ¶ˆæ¯æµç›‘æ§")
        print("2. è¿è¡Œæ¼”ç¤º")
        print("3. åˆ†ææ¶ˆæ¯æµ")
        print("4. æŸ¥çœ‹è¿æ¥ä¿¡æ¯")
        print("5. æŸ¥çœ‹æ€§èƒ½æŒ‡æ ‡")
        print("6. å¯¼å‡ºæŠ¥å‘Š")
        print("7. é€€å‡º")
        
        choice = input("\nè¯·è¾“å…¥é€‰æ‹© (1-7): ").strip()
        
        if choice == '1':
            if analyzer.connect():
                analyzer.start_monitoring()
                print("âœ… ç›‘æ§å·²å¯åŠ¨ï¼ŒæŒ‰ Ctrl+C åœæ­¢")
                try:
                    while True:
                        time.sleep(1)
                except KeyboardInterrupt:
                    analyzer.stop_monitoring()
                    analyzer.close()
                    print("âœ… ç›‘æ§å·²åœæ­¢")
        
        elif choice == '2':
            analyzer.run_flow_demonstration()
        
        elif choice == '3':
            time_window = int(input("è¯·è¾“å…¥åˆ†ææ—¶é—´çª—å£(ç§’ï¼Œé»˜è®¤60): ") or "60")
            message_id = input("è¯·è¾“å…¥è¦åˆ†æçš„æ¶ˆæ¯ID(ç•™ç©ºåˆ†ææ‰€æœ‰): ").strip() or None
            
            analysis = analyzer.analyze_message_flow(
                message_id=message_id,
                time_window=time_window
            )
            if analysis:
                analyzer.generate_flow_report(analysis)
        
        elif choice == '4':
            info = analyzer.get_connection_info()
            if info:
                print("\nğŸ”Œ è¿æ¥ä¿¡æ¯:")
                for key, value in info.items():
                    print(f"   {key}: {value}")
            else:
                print("âŒ æ— è¿æ¥ä¿¡æ¯")
        
        elif choice == '5':
            print("\nğŸ“Š æ€§èƒ½æŒ‡æ ‡:")
            for key, value in analyzer.performance_metrics.items():
                print(f"   {key}: {value}")
        
        elif choice == '6':
            filename = input("è¯·è¾“å…¥æ–‡ä»¶å: ").strip() or f"flow_report_{int(time.time())}.json"
            analysis = analyzer.analyze_message_flow()
            
            with open(filename, 'w', encoding='utf-8') as f:
                report = {
                    'timestamp': time.time(),
                    'performance_metrics': analyzer.performance_metrics,
                    'analysis': analysis
                }
                json.dump(report, f, indent=2, ensure_ascii=False)
            
            print(f"âœ… æŠ¥å‘Šå·²å¯¼å‡ºåˆ°: {filename}")
        
        elif choice == '7':
            print("ğŸ‘‹ é€€å‡ºåˆ†æ")
            break
        
        else:
            print("âŒ æ— æ•ˆé€‰æ‹©")
        
        input("\næŒ‰å›è½¦é”®ç»§ç»­...")


if __name__ == "__main__":
    import argparse
    
    parser = argparse.ArgumentParser(description="AMQPæ¶ˆæ¯æµåˆ†æå™¨")
    parser.add_argument('--host', default='localhost', help='RabbitMQä¸»æœºåœ°å€')
    parser.add_argument('--port', type=int, default=5672, help='RabbitMQç«¯å£')
    parser.add_argument('--interactive', action='store_true', help='äº¤äº’æ¨¡å¼')
    parser.add_argument('--demo', action='store_true', help='è¿è¡Œæ¼”ç¤º')
    
    args = parser.parse_args()
    
    # åˆ›å»ºåˆ†æå™¨
    analyzer = AMQPFlowAnalyzer(host=args.host, port=args.port)
    
    if args.interactive:
        interactive_flow_analysis()
    elif args.demo:
        analyzer.run_flow_demonstration()
    else:
        # è¿è¡Œå®Œæ•´åˆ†æ
        analyzer.run_flow_demonstration()