# ç¬¬5ç« ï¼šé˜Ÿåˆ—ç®¡ç†ä¸è´Ÿè½½å‡è¡¡

## 5.1 é˜Ÿåˆ—ç®¡ç†åŸºç¡€

### 5.1.1 é˜Ÿåˆ—çš„ç”Ÿå‘½å‘¨æœŸ

RabbitMQä¸­çš„é˜Ÿåˆ—å…·æœ‰å®Œæ•´çš„ç”Ÿå‘½å‘¨æœŸç®¡ç†ï¼š

**1. é˜Ÿåˆ—åˆ›å»º**
```python
# åŸºç¡€é˜Ÿåˆ—åˆ›å»º
channel.queue_declare(queue='my_queue')

# é«˜çº§é˜Ÿåˆ—é…ç½®
channel.queue_declare(
    queue='my_queue',
    durable=True,           # é˜Ÿåˆ—æŒä¹…åŒ–
    exclusive=False,        # éç‹¬å 
    auto_delete=False,      # ä¸è‡ªåŠ¨åˆ é™¤
    arguments={
        'x-message-ttl': 300000,        # æ¶ˆæ¯TTL (5åˆ†é’Ÿ)
        'x-max-length': 1000,           # æœ€å¤§é˜Ÿåˆ—é•¿åº¦
        'x-max-length-bytes': 10485760, # æœ€å¤§é˜Ÿåˆ—å¤§å°(10MB)
        'x-dead-letter-exchange': 'dlx', # æ­»ä¿¡äº¤æ¢æœº
        'x-dead-letter-routing-key': 'dead_letter'
    }
)
```

**2. é˜Ÿåˆ—å±æ€§è¯¦è§£**

| å±æ€§ | è¯´æ˜ | ç¤ºä¾‹å€¼ |
|------|------|-------|
| `durable` | é˜Ÿåˆ—æ˜¯å¦æŒä¹…åŒ– | `True/False` |
| `exclusive` | é˜Ÿåˆ—æ˜¯å¦ç‹¬å  | `True/False` |
| `auto_delete` | æ˜¯å¦è‡ªåŠ¨åˆ é™¤ | `True/False` |
| `x-message-ttl` | æ¶ˆæ¯ç”Ÿå­˜æ—¶é—´ | `300000ms` |
| `x-max-length` | æœ€å¤§é˜Ÿåˆ—é•¿åº¦ | `1000` |
| `x-max-priority` | æœ€å¤§ä¼˜å…ˆçº§ | `10` |

**3. é˜Ÿåˆ—ç›‘æ§ä¸ç»´æŠ¤**
```python
import pika
import time

class QueueMonitor:
    def __init__(self, host='localhost'):
        self.connection = pika.BlockingConnection(
            pika.ConnectionParameters(host)
        )
        self.channel = self.connection.channel()
    
    def get_queue_info(self, queue_name):
        """è·å–é˜Ÿåˆ—ä¿¡æ¯"""
        result = self.channel.queue_declare(
            queue=queue_name, passive=True
        )
        return {
            'message_count': result.method.message_count,
            'consumer_count': result.method.consumer_count,
            'queue_name': queue_name
        }
    
    def get_all_queues(self):
        """è·å–æ‰€æœ‰é˜Ÿåˆ—ä¿¡æ¯"""
        queues = []
        result = self.channel.queue_declare(
            queue='', passive=True
        )
        # è¿™é‡Œéœ€è¦ä½¿ç”¨list_queueså‘½ä»¤
        return queues
    
    def cleanup_queues(self, pattern='test_'):
        """æ¸…ç†æµ‹è¯•é˜Ÿåˆ—"""
        # å®ç°é˜Ÿåˆ—æ¸…ç†é€»è¾‘
        pass
```

### 5.1.2 é˜Ÿåˆ—æœ€ä½³å®è·µ

**1. é˜Ÿåˆ—å‘½åè§„èŒƒ**
```python
# âœ… å¥½çš„å‘½å
production_orders_queue
user_notifications_queue
financial_transactions_queue

# âŒ é¿å…çš„å‘½å
q1                    # æ— æ„ä¹‰
test                  # å¤ªé€šç”¨
very_long_queue_name_that_exceeds_limits  # è¿‡é•¿
```

**2. é˜Ÿåˆ—é…ç½®å»ºè®®**
```python
# å»ºè®®çš„é˜Ÿåˆ—é…ç½®
def create_production_queue(channel, queue_name):
    channel.queue_declare(
        queue=queue_name,
        durable=True,                    # ç”Ÿäº§ç¯å¢ƒå¿…é¡»æŒä¹…åŒ–
        arguments={
            'x-message-ttl': 3600000,    # 1å°æ—¶TTL
            'x-max-length': 10000,       # é˜²æ­¢å†…å­˜æº¢å‡º
            'x-overflow': 'reject-publish', # æ‹’ç»æ–°æ¶ˆæ¯
            'x-dead-letter-exchange': 'dlx',
            'x-dead-letter-routing-key': f'dead_letter_{queue_name}'
        }
    )
```

## 5.2 è´Ÿè½½å‡è¡¡æœºåˆ¶

### 5.2.1 æ¶ˆè´¹è€…è´Ÿè½½å‡è¡¡

**1. è½®è¯¢åˆ†å‘ï¼ˆé»˜è®¤ï¼‰**
```python
# è½®è¯¢åˆ†å‘ç¤ºä¾‹
def round_robin_example():
    # å¯åŠ¨å¤šä¸ªæ¶ˆè´¹è€…
    consumers = []
    
    for i in range(3):
        def consumer_callback(ch, method, properties, body):
            print(f"æ¶ˆè´¹è€… {i} å¤„ç†: {body}")
            # æ¨¡æ‹Ÿå¤„ç†æ—¶é—´
            time.sleep(1)
            ch.basic_ack(delivery_tag=method.delivery_tag)
        
        # ç»‘å®šåˆ°é˜Ÿåˆ—
        channel.basic_consume(
            queue='round_robin_queue',
            on_message_callback=consumer_callback
        )
    
    # RabbitMQæŒ‰é¡ºåºè½®è¯¢åˆ†å‘ç»™æ¶ˆè´¹è€…
```

**2. å…¬å¹³åˆ†å‘**
```python
# è®¾ç½®prefetch_countå®ç°å…¬å¹³åˆ†å‘
channel.basic_qos(prefetch_count=1)

def fair_distribution_example():
    def callback(ch, method, properties, body):
        print(f"å¤„ç†æ¶ˆæ¯: {body}")
        time.sleep(1)  # æ¨¡æ‹Ÿå¤„ç†
        ch.basic_ack(delivery_tag=method.delivery_tag)
    
    channel.basic_consume(
        queue='fair_queue',
        on_message_callback=callback
    )
```

### 5.2.2 é«˜çº§è´Ÿè½½å‡è¡¡ç­–ç•¥

**1. åŸºäºä¼˜å…ˆçº§çš„è´Ÿè½½å‡è¡¡**
```python
class PriorityLoadBalancer:
    def __init__(self, channel):
        self.channel = channel
        self.high_priority_queue = 'high_priority_queue'
        self.normal_priority_queue = 'normal_priority_queue'
    
    def send_with_priority(self, message, priority=0):
        properties = pika.BasicProperties(
            priority=priority  # 0-255, æ•°å­—è¶Šå¤§ä¼˜å…ˆçº§è¶Šé«˜
        )
        
        queue = self.high_priority_queue if priority > 5 else self.normal_priority_queue
        
        self.channel.basic_publish(
            exchange='',
            routing_key=queue,
            body=message,
            properties=properties
        )
    
    def priority_consumer(self):
        def callback(ch, method, properties, body):
            priority = properties.priority if properties.priority else 0
            print(f"å¤„ç†ä¼˜å…ˆçº§ {priority} çš„æ¶ˆæ¯: {body}")
            ch.basic_ack(delivery_tag=method.delivery_tag)
        
        # ä¼˜å…ˆæ¶ˆè´¹é«˜ä¼˜å…ˆçº§é˜Ÿåˆ—
        self.channel.basic_consume(
            queue=self.high_priority_queue,
            on_message_callback=callback
        )
```

**2. åŸºäºæƒé‡çš„è´Ÿè½½å‡è¡¡**
```python
class WeightedLoadBalancer:
    def __init__(self, channel):
        self.channel = channel
        self.weights = {
            'slow_consumer': 1,
            'fast_consumer': 3,
            'premium_consumer': 5
        }
    
    def setup_weighted_queues(self):
        # åˆ›å»ºä¸åŒæƒé‡çš„é˜Ÿåˆ—
        for consumer, weight in self.weights.items():
            queue_name = f'weighted_queue_{consumer}'
            
            # ä¸ºæƒé‡é«˜çš„æ¶ˆè´¹è€…è®¾ç½®æ›´å¤§çš„prefetch
            self.channel.queue_declare(queue=queue_name)
            self.channel.queue_bind(
                exchange='weighted_exchange',
                queue=queue_name,
                routing_key=consumer
            )
    
    def send_weighted_message(self, message, consumer_type='normal'):
        routing_key = consumer_type
        if consumer_type not in self.weights:
            routing_key = 'slow_consumer'  # é»˜è®¤åˆ†é…ç»™æ…¢æ¶ˆè´¹è€…
        
        self.channel.basic_publish(
            exchange='weighted_exchange',
            routing_key=routing_key,
            body=message
        )
```

## 5.3 é˜Ÿåˆ—ä¼¸ç¼©ä¸æ‰©å®¹

### 5.3.1 åŠ¨æ€æ‰©ç¼©å®¹

**1. æ¶ˆè´¹è€…åŠ¨æ€ç®¡ç†**
```python
import threading
import time
from concurrent.futures import ThreadPoolExecutor

class DynamicScaler:
    def __init__(self, queue_name, min_consumers=1, max_consumers=10):
        self.queue_name = queue_name
        self.min_consumers = min_consumers
        self.max_consumers = max_consumers
        self.consumers = []
        self.scaling_interval = 30  # 30ç§’æ£€æŸ¥ä¸€æ¬¡
        self.running = False
    
    def start(self):
        """å¯åŠ¨åŠ¨æ€ä¼¸ç¼©"""
        self.running = True
        
        # å¯åŠ¨åˆå§‹æ¶ˆè´¹è€…
        for i in range(self.min_consumers):
            self.add_consumer()
        
        # å¯åŠ¨ä¼¸ç¼©çº¿ç¨‹
        scaling_thread = threading.Thread(target=self._scaling_loop)
        scaling_thread.start()
    
    def add_consumer(self):
        """æ·»åŠ æ¶ˆè´¹è€…"""
        if len(self.consumers) >= self.max_consumers:
            return False
        
        consumer = DynamicConsumer(self.queue_name, len(self.consumers))
        consumer.start()
        self.consumers.append(consumer)
        
        print(f"æ·»åŠ æ¶ˆè´¹è€… {consumer.consumer_id}")
        return True
    
    def remove_consumer(self):
        """ç§»é™¤æ¶ˆè´¹è€…"""
        if len(self.consumers) <= self.min_consumers:
            return False
        
        consumer = self.consumers.pop()
        consumer.stop()
        
        print(f"ç§»é™¤æ¶ˆè´¹è€… {consumer.consumer_id}")
        return True
    
    def _scaling_loop(self):
        """ä¼¸ç¼©å¾ªç¯"""
        while self.running:
            try:
                metrics = self._get_queue_metrics()
                
                if metrics['queue_length'] > 1000 and len(self.consumers) < self.max_consumers:
                    self.add_consumer()
                elif metrics['queue_length'] < 10 and len(self.consumers) > self.min_consumers:
                    self.remove_consumer()
                
                time.sleep(self.scaling_interval)
                
            except Exception as e:
                print(f"ä¼¸ç¼©æ£€æŸ¥é”™è¯¯: {e}")
    
    def _get_queue_metrics(self):
        """è·å–é˜Ÿåˆ—æŒ‡æ ‡"""
        # å®ç°é˜Ÿåˆ—æŒ‡æ ‡è·å–é€»è¾‘
        return {'queue_length': 0, 'consumer_count': len(self.consumers)}
```

**2. åŸºäºè´Ÿè½½çš„è‡ªåŠ¨æ‰©å®¹**
```python
import psutil
import threading
from queue import Queue

class LoadBasedScaler:
    def __init__(self, channel, queue_name):
        self.channel = channel
        self.queue_name = queue_name
        self.message_queue = Queue(maxsize=1000)
        self.consumers = []
        self.monitoring = False
    
    def start_monitoring(self):
        """å¼€å§‹ç›‘æ§"""
        self.monitoring = True
        
        monitor_thread = threading.Thread(target=self._monitor_loop)
        monitor_thread.start()
    
    def _monitor_loop(self):
        """ç›‘æ§å¾ªç¯"""
        while self.monitoring:
            try:
                # è·å–ç³»ç»Ÿè´Ÿè½½
                cpu_percent = psutil.cpu_percent(interval=1)
                memory_percent = psutil.virtual_memory().percent
                
                # è·å–é˜Ÿåˆ—è´Ÿè½½
                queue_length = self._get_queue_length()
                
                # å†³å®šæ‰©å®¹æˆ–ç¼©å®¹
                if (cpu_percent < 70 and memory_percent < 70 and 
                    queue_length > 100 and len(self.consumers) < 8):
                    self.scale_up()
                elif (cpu_percent > 90 or memory_percent > 90 or 
                      queue_length < 5) and len(self.consumers) > 1:
                    self.scale_down()
                
                time.sleep(10)  # æ¯10ç§’æ£€æŸ¥ä¸€æ¬¡
                
            except Exception as e:
                print(f"ç›‘æ§é”™è¯¯: {e}")
    
    def _get_queue_length(self):
        """è·å–é˜Ÿåˆ—é•¿åº¦"""
        try:
            result = self.channel.queue_declare(
                queue=self.queue_name, passive=True
            )
            return result.method.message_count
        except:
            return 0
    
    def scale_up(self):
        """æ‰©å®¹"""
        consumer = LoadAwareConsumer(self.queue_name, len(self.consumers))
        consumer.start()
        self.consumers.append(consumer)
        print(f"æ‰©å®¹è‡³ {len(self.consumers)} ä¸ªæ¶ˆè´¹è€…")
    
    def scale_down(self):
        """ç¼©å®¹"""
        if self.consumers:
            consumer = self.consumers.pop()
            consumer.stop()
            print(f"ç¼©å®¹è‡³ {len(self.consumers)} ä¸ªæ¶ˆè´¹è€…")
```

### 5.3.2 åˆ†ç‰‡é˜Ÿåˆ—

**1. åˆ†ç‰‡ç­–ç•¥**
```python
class ShardedQueue:
    def __init__(self, base_queue_name, shard_count=4):
        self.base_queue_name = base_queue_name
        self.shard_count = shard_count
        self.shards = [f"{base_queue_name}_shard_{i}" for i in range(shard_count)]
    
    def get_shard(self, message_key):
        """æ ¹æ®æ¶ˆæ¯é”®ç¡®å®šåˆ†ç‰‡"""
        hash_value = hash(str(message_key)) % self.shard_count
        return self.shards[hash_value]
    
    def send_to_shard(self, message, message_key):
        """å‘é€æ¶ˆæ¯åˆ°æŒ‡å®šåˆ†ç‰‡"""
        shard = self.get_shard(message_key)
        
        # è¿™é‡Œéœ€è¦å®é™…çš„é€šé“å¯¹è±¡
        return shard
    
    def create_sharded_queues(self, channel):
        """åˆ›å»ºåˆ†ç‰‡é˜Ÿåˆ—"""
        for shard in self.shards:
            channel.queue_declare(
                queue=shard,
                durable=True,
                arguments={
                    'x-message-ttl': 300000,
                    'x-max-length': 1000
                }
            )
```

**2. åˆ†ç‰‡æ¶ˆè´¹è€…ç®¡ç†**
```python
class ShardedConsumer:
    def __init__(self, channel, shard_queues):
        self.channel = channel
        self.shard_queues = shard_queues
        self.consumers = {}
    
    def start_shard_consumers(self):
        """å¯åŠ¨åˆ†ç‰‡æ¶ˆè´¹è€…"""
        for shard_queue in self.shard_queues:
            consumer = ShardConsumerWorker(
                self.channel, shard_queue, shard_queue.split('_')[-1]
            )
            consumer.start()
            self.consumers[shard_queue] = consumer
    
    def stop_all_consumers(self):
        """åœæ­¢æ‰€æœ‰æ¶ˆè´¹è€…"""
        for consumer in self.consumers.values():
            consumer.stop()

class ShardConsumerWorker:
    def __init__(self, channel, queue_name, shard_id):
        self.channel = channel
        self.queue_name = queue_name
        self.shard_id = shard_id
        self.running = False
    
    def start(self):
        """å¯åŠ¨æ¶ˆè´¹è€…"""
        self.running = True
        
        def callback(ch, method, properties, body):
            print(f"åˆ†ç‰‡ {self.shard_id} å¤„ç†æ¶ˆæ¯: {body}")
            # åˆ†ç‰‡ç‰¹å®šçš„å¤„ç†é€»è¾‘
            time.sleep(0.5)  # æ¨¡æ‹Ÿå¤„ç†æ—¶é—´
            ch.basic_ack(delivery_tag=method.delivery_tag)
        
        self.channel.basic_consume(
            queue=self.queue_name,
            on_message_callback=callback
        )
        
        # å¯åŠ¨æ¶ˆè´¹çº¿ç¨‹
        consumer_thread = threading.Thread(target=self._consume_loop)
        consumer_thread.start()
    
    def _consume_loop(self):
        """æ¶ˆè´¹å¾ªç¯"""
        try:
            while self.running:
                self.channel.connection.process_data_events(time_limit=1.0)
        except Exception as e:
            print(f"åˆ†ç‰‡ {self.shard_id} æ¶ˆè´¹é”™è¯¯: {e}")
    
    def stop(self):
        """åœæ­¢æ¶ˆè´¹è€…"""
        self.running = False
```

## 5.4 é˜Ÿåˆ—ç›‘æ§ä¸å¥åº·æ£€æŸ¥

### 5.4.1 é˜Ÿåˆ—å¥åº·ç›‘æ§

**1. åŸºç¡€å¥åº·æ£€æŸ¥**
```python
class QueueHealthChecker:
    def __init__(self, connection_params):
        self.connection_params = connection_params
    
    def check_queue_health(self, queue_name):
        """æ£€æŸ¥é˜Ÿåˆ—å¥åº·çŠ¶æ€"""
        try:
            connection = pika.BlockingConnection(self.connection_params)
            channel = connection.channel()
            
            # è·å–é˜Ÿåˆ—ä¿¡æ¯
            result = channel.queue_declare(queue=queue_name, passive=True)
            message_count = result.method.message_count
            consumer_count = result.method.consumer_count
            
            # è®¡ç®—å¥åº·æŒ‡æ ‡
            health_status = {
                'status': 'healthy',
                'message_count': message_count,
                'consumer_count': consumer_count,
                'queue_name': queue_name,
                'timestamp': time.time()
            }
            
            # å¼‚å¸¸æ£€æµ‹
            if message_count > 1000:
                health_status['status'] = 'warning'
                health_status['reason'] = 'æ¶ˆæ¯ç§¯å‹è¿‡å¤š'
            elif consumer_count == 0:
                health_status['status'] = 'critical'
                health_status['reason'] = 'æ— æ¶ˆè´¹è€…'
            
            connection.close()
            return health_status
            
        except Exception as e:
            return {
                'status': 'error',
                'error': str(e),
                'timestamp': time.time()
            }
    
    def check_multiple_queues(self, queue_names):
        """æ£€æŸ¥å¤šä¸ªé˜Ÿåˆ—"""
        results = {}
        for queue_name in queue_names:
            results[queue_name] = self.check_queue_health(queue_name)
        return results
```

**2. å®æ—¶ç›‘æ§ç³»ç»Ÿ**
```python
import json
from datetime import datetime
from dataclasses import dataclass
from typing import List, Dict

@dataclass
class QueueMetrics:
    queue_name: str
    message_count: int
    consumer_count: int
    timestamp: float
    cpu_usage: float = 0.0
    memory_usage: float = 0.0

class QueueMonitoringSystem:
    def __init__(self, connection_params, check_interval=10):
        self.connection_params = connection_params
        self.check_interval = check_interval
        self.metrics_history = {}
        self.alert_thresholds = {
            'max_queue_length': 1000,
            'min_consumer_count': 1,
            'max_processing_time': 5.0
        }
    
    def start_monitoring(self, queue_names):
        """å¼€å§‹ç›‘æ§"""
        import threading
        self.monitoring = True
        self.queue_names = queue_names
        
        monitor_thread = threading.Thread(
            target=self._monitoring_loop,
            args=(queue_names,)
        )
        monitor_thread.start()
    
    def _monitoring_loop(self, queue_names):
        """ç›‘æ§å¾ªç¯"""
        while self.monitoring:
            try:
                for queue_name in queue_names:
                    metrics = self._collect_metrics(queue_name)
                    self._store_metrics(metrics)
                    self._check_alerts(metrics)
                
                time.sleep(self.check_interval)
                
            except Exception as e:
                print(f"ç›‘æ§é”™è¯¯: {e}")
                time.sleep(self.check_interval)
    
    def _collect_metrics(self, queue_name) -> QueueMetrics:
        """æ”¶é›†é˜Ÿåˆ—æŒ‡æ ‡"""
        try:
            connection = pika.BlockingConnection(self.connection_params)
            channel = connection.channel()
            
            result = channel.queue_declare(queue=queue_name, passive=True)
            
            # ç³»ç»Ÿèµ„æºæŒ‡æ ‡
            cpu_usage = psutil.cpu_percent(interval=0.1)
            memory_usage = psutil.virtual_memory().percent
            
            metrics = QueueMetrics(
                queue_name=queue_name,
                message_count=result.method.message_count,
                consumer_count=result.method.consumer_count,
                timestamp=time.time(),
                cpu_usage=cpu_usage,
                memory_usage=memory_usage
            )
            
            connection.close()
            return metrics
            
        except Exception as e:
            print(f"æ”¶é›†æŒ‡æ ‡é”™è¯¯ {queue_name}: {e}")
            return QueueMetrics(queue_name, 0, 0, time.time())
    
    def _store_metrics(self, metrics: QueueMetrics):
        """å­˜å‚¨æŒ‡æ ‡"""
        if metrics.queue_name not in self.metrics_history:
            self.metrics_history[metrics.queue_name] = []
        
        self.metrics_history[metrics.queue_name].append(metrics)
        
        # ä¿æŒæœ€è¿‘1å°æ—¶çš„æ•°æ®
        cutoff_time = time.time() - 3600
        self.metrics_history[metrics.queue_name] = [
            m for m in self.metrics_history[metrics.queue_name] 
            if m.timestamp > cutoff_time
        ]
    
    def _check_alerts(self, metrics: QueueMetrics):
        """æ£€æŸ¥å‘Šè­¦"""
        alerts = []
        
        if metrics.message_count > self.alert_thresholds['max_queue_length']:
            alerts.append({
                'type': 'queue_full',
                'queue': metrics.queue_name,
                'message_count': metrics.message_count,
                'threshold': self.alert_thresholds['max_queue_length']
            })
        
        if metrics.consumer_count < self.alert_thresholds['min_consumer_count']:
            alerts.append({
                'type': 'no_consumers',
                'queue': metrics.queue_name,
                'consumer_count': metrics.consumer_count
            })
        
        if metrics.cpu_usage > 90:
            alerts.append({
                'type': 'high_cpu',
                'queue': metrics.queue_name,
                'cpu_usage': metrics.cpu_usage
            })
        
        # å¤„ç†å‘Šè­¦
        for alert in alerts:
            self._handle_alert(alert)
    
    def _handle_alert(self, alert):
        """å¤„ç†å‘Šè­¦"""
        print(f"ğŸš¨ å‘Šè­¦: {alert}")
        
        # è¿™é‡Œå¯ä»¥å®ç°å‘Šè­¦é€šçŸ¥é€»è¾‘
        # - å‘é€é‚®ä»¶
        # - è°ƒç”¨Webhook
        # - è®°å½•åˆ°ç›‘æ§ç³»ç»Ÿ
    
    def get_queue_status_report(self) -> Dict:
        """ç”Ÿæˆé˜Ÿåˆ—çŠ¶æ€æŠ¥å‘Š"""
        report = {
            'timestamp': time.time(),
            'queues': {}
        }
        
        for queue_name, metrics_list in self.metrics_history.items():
            if not metrics_list:
                continue
            
            latest = metrics_list[-1]
            recent_metrics = [m for m in metrics_list if time.time() - m.timestamp < 300]
            
            if recent_metrics:
                avg_queue_length = sum(m.message_count for m in recent_metrics) / len(recent_metrics)
                avg_consumer_count = sum(m.consumer_count for m in recent_metrics) / len(recent_metrics)
            else:
                avg_queue_length = latest.message_count
                avg_consumer_count = latest.consumer_count
            
            report['queues'][queue_name] = {
                'current_messages': latest.message_count,
                'current_consumers': latest.consumer_count,
                'avg_messages_5min': avg_queue_length,
                'avg_consumers_5min': avg_consumer_count,
                'status': self._determine_status(latest)
            }
        
        return report
    
    def _determine_status(self, metrics: QueueMetrics) -> str:
        """ç¡®å®šé˜Ÿåˆ—çŠ¶æ€"""
        if metrics.message_count > self.alert_thresholds['max_queue_length']:
            return 'critical'
        elif metrics.consumer_count == 0:
            return 'warning'
        else:
            return 'healthy'
```

### 5.4.2 æ€§èƒ½åˆ†æ

**1. ååé‡åˆ†æ**
```python
class ThroughputAnalyzer:
    def __init__(self, connection_params):
        self.connection_params = connection_params
        self.throughput_data = {}
    
    def measure_throughput(self, queue_name, duration_seconds=60):
        """æµ‹é‡é˜Ÿåˆ—ååé‡"""
        start_time = time.time()
        initial_count = self._get_queue_length(queue_name)
        
        # ç­‰å¾…æŒ‡å®šæ—¶é—´
        time.sleep(duration_seconds)
        
        end_time = time.time()
        final_count = self._get_queue_length(queue_name)
        
        # è®¡ç®—ååé‡
        elapsed_time = end_time - start_time
        processed_messages = final_count - initial_count
        throughput = processed_messages / elapsed_time
        
        return {
            'queue_name': queue_name,
            'duration': elapsed_time,
            'messages_processed': processed_messages,
            'throughput_per_second': throughput,
            'start_time': start_time,
            'end_time': end_time
        }
    
    def _get_queue_length(self, queue_name):
        """è·å–é˜Ÿåˆ—é•¿åº¦"""
        try:
            connection = pika.BlockingConnection(self.connection_params)
            channel = connection.channel()
            
            result = channel.queue_declare(queue=queue_name, passive=True)
            message_count = result.method.message_count
            
            connection.close()
            return message_count
            
        except Exception as e:
            print(f"è·å–é˜Ÿåˆ—é•¿åº¦é”™è¯¯: {e}")
            return 0
    
    def benchmark_queues(self, queue_names, duration=30):
        """æ‰¹é‡æµ‹è¯•é˜Ÿåˆ—ååé‡"""
        benchmarks = {}
        
        for queue_name in queue_names:
            print(f"æµ‹è¯•é˜Ÿåˆ—: {queue_name}")
            benchmarks[queue_name] = self.measure_throughput(queue_name, duration)
        
        return benchmarks
    
    def generate_performance_report(self, benchmarks):
        """ç”Ÿæˆæ€§èƒ½æŠ¥å‘Š"""
        report = {
            'timestamp': time.time(),
            'summary': {
                'total_queues': len(benchmarks),
                'best_throughput': 0,
                'worst_throughput': float('inf'),
                'average_throughput': 0
            },
            'queues': {}
        }
        
        total_throughput = 0
        throughputs = []
        
        for queue_name, benchmark in benchmarks.items():
            throughput = benchmark['throughput_per_second']
            throughputs.append(throughput)
            total_throughput += throughput
            
            report['queues'][queue_name] = {
                'throughput': throughput,
                'messages_processed': benchmark['messages_processed'],
                'duration': benchmark['duration']
            }
        
        if throughputs:
            report['summary']['best_throughput'] = max(throughputs)
            report['summary']['worst_throughput'] = min(throughputs)
            report['summary']['average_throughput'] = total_throughput / len(throughputs)
        
        return report
```

## 5.5 æ•…éšœå¤„ç†ä¸æ¢å¤

### 5.5.1 é˜Ÿåˆ—æ•…éšœæ£€æµ‹

**1. æ•…éšœç±»å‹æ£€æµ‹**
```python
class QueueFaultDetector:
    def __init__(self, connection_params):
        self.connection_params = connection_params
    
    def detect_faults(self, queue_name):
        """æ£€æµ‹é˜Ÿåˆ—æ•…éšœ"""
        faults = []
        
        try:
            # æ£€æŸ¥è¿æ¥çŠ¶æ€
            connection_healthy = self._test_connection()
            if not connection_healthy:
                faults.append({
                    'type': 'connection_failure',
                    'severity': 'critical',
                    'description': 'æ— æ³•è¿æ¥åˆ°RabbitMQæœåŠ¡å™¨'
                })
            
            # æ£€æŸ¥é˜Ÿåˆ—æ˜¯å¦å­˜åœ¨
            queue_exists = self._check_queue_exists(queue_name)
            if not queue_exists:
                faults.append({
                    'type': 'queue_missing',
                    'severity': 'critical',
                    'description': f'é˜Ÿåˆ— {queue_name} ä¸å­˜åœ¨'
                })
            
            # æ£€æŸ¥é˜Ÿåˆ—é…ç½®
            queue_config = self._check_queue_configuration(queue_name)
            if not queue_config['valid']:
                faults.append({
                    'type': 'configuration_error',
                    'severity': 'warning',
                    'description': f'é˜Ÿåˆ—é…ç½®é”™è¯¯: {queue_config["errors"]}'
                })
            
            # æ£€æŸ¥æ¶ˆæ¯ç§¯å‹
            message_backlog = self._check_message_backlog(queue_name)
            if message_backlog['critical']:
                faults.append({
                    'type': 'message_backlog_critical',
                    'severity': 'critical',
                    'description': f'æ¶ˆæ¯ä¸¥é‡ç§¯å‹: {message_backlog["count"]} æ¡'
                })
            elif message_backlog['warning']:
                faults.append({
                    'type': 'message_backlog_warning',
                    'severity': 'warning',
                    'description': f'æ¶ˆæ¯ç§¯å‹: {message_backlog["count"]} æ¡'
                })
            
            # æ£€æŸ¥æ¶ˆè´¹è€…çŠ¶æ€
            consumer_status = self._check_consumer_status(queue_name)
            if consumer_status['no_consumers']:
                faults.append({
                    'type': 'no_consumers',
                    'severity': 'critical',
                    'description': 'é˜Ÿåˆ—æ— æ¶ˆè´¹è€…'
                })
            elif consumer_status['low_consumers']:
                faults.append({
                    'type': 'low_consumer_count',
                    'severity': 'warning',
                    'description': f'æ¶ˆè´¹è€…æ•°é‡ä¸è¶³: {consumer_status["count"]}'
                })
        
        except Exception as e:
            faults.append({
                'type': 'detection_error',
                'severity': 'critical',
                'description': f'æ•…éšœæ£€æµ‹é”™è¯¯: {str(e)}'
            })
        
        return faults
    
    def _test_connection(self):
        """æµ‹è¯•è¿æ¥"""
        try:
            connection = pika.BlockingConnection(self.connection_params)
            connection.close()
            return True
        except:
            return False
    
    def _check_queue_exists(self, queue_name):
        """æ£€æŸ¥é˜Ÿåˆ—æ˜¯å¦å­˜åœ¨"""
        try:
            connection = pika.BlockingConnection(self.connection_params)
            channel = connection.channel()
            
            # å°è¯•å£°æ˜é˜Ÿåˆ—ï¼ˆè¢«åŠ¨æ¨¡å¼ï¼‰
            channel.queue_declare(queue=queue_name, passive=True)
            connection.close()
            return True
            
        except pika.exceptions.QueueNotFound:
            return False
        except:
            return False
    
    def _check_queue_configuration(self, queue_name):
        """æ£€æŸ¥é˜Ÿåˆ—é…ç½®"""
        # è¿™é‡Œå¯ä»¥å®ç°æ›´è¯¦ç»†çš„é…ç½®æ£€æŸ¥
        return {
            'valid': True,
            'errors': []
        }
    
    def _check_message_backlog(self, queue_name):
        """æ£€æŸ¥æ¶ˆæ¯ç§¯å‹"""
        try:
            connection = pika.BlockingConnection(self.connection_params)
            channel = connection.channel()
            
            result = channel.queue_declare(queue=queue_name, passive=True)
            message_count = result.method.message_count
            
            connection.close()
            
            # å®šä¹‰ç§¯å‹é˜ˆå€¼
            critical_threshold = 1000
            warning_threshold = 500
            
            return {
                'count': message_count,
                'critical': message_count > critical_threshold,
                'warning': message_count > warning_threshold
            }
            
        except:
            return {'count': 0, 'critical': False, 'warning': False}
    
    def _check_consumer_status(self, queue_name):
        """æ£€æŸ¥æ¶ˆè´¹è€…çŠ¶æ€"""
        try:
            connection = pika.BlockingConnection(self.connection_params)
            channel = connection.channel()
            
            result = channel.queue_declare(queue=queue_name, passive=True)
            consumer_count = result.method.consumer_count
            
            connection.close()
            
            return {
                'count': consumer_count,
                'no_consumers': consumer_count == 0,
                'low_consumers': 0 < consumer_count < 2  # å°‘äº2ä¸ªæ¶ˆè´¹è€…è®¤ä¸ºä¸è¶³
            }
            
        except:
            return {'count': 0, 'no_consumers': True, 'low_consumers': True}
```

### 5.5.2 è‡ªåŠ¨æ¢å¤æœºåˆ¶

**1. æ•…éšœæ¢å¤ç®¡ç†å™¨**
```python
import threading
import logging
from typing import Callable

class QueueRecoveryManager:
    def __init__(self, connection_params):
        self.connection_params = connection_params
        self.recovery_strategies = {}
        self.monitoring_active = False
        self.recovery_thread = None
        self.logger = logging.getLogger(__name__)
        
    def register_recovery_strategy(self, fault_type: str, strategy: Callable):
        """æ³¨å†Œæ¢å¤ç­–ç•¥"""
        self.recovery_strategies[fault_type] = strategy
    
    def start_monitoring(self, queue_names, check_interval=30):
        """å¼€å§‹ç›‘æ§å’Œæ¢å¤"""
        self.monitoring_active = True
        self.queue_names = queue_names
        self.check_interval = check_interval
        
        self.recovery_thread = threading.Thread(
            target=self._monitoring_and_recovery_loop
        )
        self.recovery_thread.start()
        
        # æ³¨å†Œé»˜è®¤æ¢å¤ç­–ç•¥
        self._register_default_strategies()
    
    def _register_default_strategies(self):
        """æ³¨å†Œé»˜è®¤æ¢å¤ç­–ç•¥"""
        
        # è¿æ¥æ•…éšœæ¢å¤
        self.register_recovery_strategy('connection_failure', self._recover_connection)
        
        # é˜Ÿåˆ—ç¼ºå¤±æ¢å¤
        self.register_recovery_strategy('queue_missing', self._recreate_queue)
        
        # æ— æ¶ˆè´¹è€…æ¢å¤
        self.register_recovery_strategy('no_consumers', self._restart_consumers)
        
        # æ¶ˆæ¯ç§¯å‹æ¢å¤
        self.register_recovery_strategy('message_backlog_critical', self._handle_backlog)
        
        # é˜Ÿåˆ—é…ç½®é”™è¯¯æ¢å¤
        self.register_recovery_strategy('configuration_error', self._fix_configuration)
    
    def _monitoring_and_recovery_loop(self):
        """ç›‘æ§å’Œæ¢å¤å¾ªç¯"""
        detector = QueueFaultDetector(self.connection_params)
        
        while self.monitoring_active:
            try:
                for queue_name in self.queue_names:
                    faults = detector.detect_faults(queue_name)
                    
                    for fault in faults:
                        self.logger.warning(f"æ£€æµ‹åˆ°æ•…éšœ: {fault['type']} in {queue_name}")
                        self._handle_fault(queue_name, fault)
                
                time.sleep(self.check_interval)
                
            except Exception as e:
                self.logger.error(f"ç›‘æ§å¾ªç¯é”™è¯¯: {e}")
                time.sleep(self.check_interval)
    
    def _handle_fault(self, queue_name, fault):
        """å¤„ç†æ•…éšœ"""
        fault_type = fault['type']
        
        if fault_type in self.recovery_strategies:
            try:
                strategy = self.recovery_strategies[fault_type]
                success = strategy(queue_name, fault)
                
                if success:
                    self.logger.info(f"æ•…éšœæ¢å¤æˆåŠŸ: {fault_type} in {queue_name}")
                else:
                    self.logger.error(f"æ•…éšœæ¢å¤å¤±è´¥: {fault_type} in {queue_name}")
                    
            except Exception as e:
                self.logger.error(f"æ¢å¤ç­–ç•¥æ‰§è¡Œé”™è¯¯: {e}")
        else:
            self.logger.warning(f"æœªæ‰¾åˆ°æ•…éšœç±»å‹ {fault_type} çš„æ¢å¤ç­–ç•¥")
    
    def _recover_connection(self, queue_name, fault):
        """æ¢å¤è¿æ¥"""
        try:
            # ç­‰å¾…ä¸€æ®µæ—¶é—´åé‡è¯•è¿æ¥
            time.sleep(5)
            test_connection = pika.BlockingConnection(self.connection_params)
            test_connection.close()
            return True
        except:
            return False
    
    def _recreate_queue(self, queue_name, fault):
        """é‡æ–°åˆ›å»ºé˜Ÿåˆ—"""
        try:
            connection = pika.BlockingConnection(self.connection_params)
            channel = connection.channel()
            
            # é‡æ–°åˆ›å»ºé˜Ÿåˆ—ï¼ˆé»˜è®¤é…ç½®ï¼‰
            channel.queue_declare(queue=queue_name, durable=True)
            
            connection.close()
            return True
            
        except Exception as e:
            self.logger.error(f"é‡æ–°åˆ›å»ºé˜Ÿåˆ—å¤±è´¥: {e}")
            return False
    
    def _restart_consumers(self, queue_name, fault):
        """é‡å¯æ¶ˆè´¹è€…"""
        try:
            # è¿™é‡Œå¯ä»¥å®ç°é‡å¯æ¶ˆè´¹è€…çš„é€»è¾‘
            # ä¾‹å¦‚å‘é€é‡å¯ä¿¡å·ç»™æ¶ˆè´¹è€…ç®¡ç†å™¨
            self.logger.info(f"é‡å¯é˜Ÿåˆ— {queue_name} çš„æ¶ˆè´¹è€…")
            return True
        except Exception as e:
            self.logger.error(f"é‡å¯æ¶ˆè´¹è€…å¤±è´¥: {e}")
            return False
    
    def _handle_backlog(self, queue_name, fault):
        """å¤„ç†æ¶ˆæ¯ç§¯å‹"""
        try:
            message_count = fault.get('count', 0)
            
            if message_count > 10000:
                # ä¸¥é‡ç§¯å‹ï¼Œå°è¯•æ‰©å®¹æ¶ˆè´¹è€…
                self._scale_up_consumers(queue_name)
                return True
            elif message_count > 5000:
                # ä¸­åº¦ç§¯å‹ï¼Œæ¸…ç†æ—§æ¶ˆæ¯
                self._clear_old_messages(queue_name)
                return True
            
            return False
            
        except Exception as e:
            self.logger.error(f"å¤„ç†æ¶ˆæ¯ç§¯å‹å¤±è´¥: {e}")
            return False
    
    def _scale_up_consumers(self, queue_name):
        """æ‰©å®¹æ¶ˆè´¹è€…"""
        # å®ç°æ‰©å®¹é€»è¾‘
        self.logger.info(f"æ‰©å®¹é˜Ÿåˆ— {queue_name} çš„æ¶ˆè´¹è€…")
    
    def _clear_old_messages(self, queue_name):
        """æ¸…ç†æ—§æ¶ˆæ¯"""
        try:
            connection = pika.BlockingConnection(self.connection_params)
            channel = connection.channel()
            
            # æ¸…ç†é˜Ÿåˆ—
            channel.queue_purge(queue=queue_name)
            
            connection.close()
            self.logger.info(f"æ¸…ç†é˜Ÿåˆ— {queue_name} çš„æ¶ˆæ¯")
            return True
            
        except Exception as e:
            self.logger.error(f"æ¸…ç†æ¶ˆæ¯å¤±è´¥: {e}")
            return False
    
    def _fix_configuration(self, queue_name, fault):
        """ä¿®å¤é…ç½®"""
        # å®ç°é…ç½®ä¿®å¤é€»è¾‘
        self.logger.info(f"ä¿®å¤é˜Ÿåˆ— {queue_name} çš„é…ç½®")
        return True
    
    def stop_monitoring(self):
        """åœæ­¢ç›‘æ§"""
        self.monitoring_active = False
        if self.recovery_thread and self.recovery_thread.is_alive():
            self.recovery_thread.join()
```

## 5.6 å®é™…åº”ç”¨æ¡ˆä¾‹

### 5.6.1 ç”µå•†è®¢å•å¤„ç†ç³»ç»Ÿ

**1. è®¢å•é˜Ÿåˆ—æ¶æ„**
```python
class OrderProcessingSystem:
    def __init__(self, connection_params):
        self.connection_params = connection_params
        self.queues = {
            'order_created': 'order_created_queue',
            'payment_processing': 'payment_processing_queue',
            'inventory_check': 'inventory_check_queue',
            'order_fulfillment': 'order_fulfillment_queue',
            'notification': 'order_notification_queue'
        }
        
        # è®¢å•ä¼˜å…ˆçº§é…ç½®
        self.priority_levels = {
            'vip': 10,
            'express': 8,
            'normal': 5,
            'bulk': 2
        }
    
    def setup_order_queues(self):
        """è®¾ç½®è®¢å•å¤„ç†é˜Ÿåˆ—"""
        connection = pika.BlockingConnection(self.connection_params)
        channel = connection.channel()
        
        # åˆ›å»ºä¸»è¦é˜Ÿåˆ—
        for queue_name in self.queues.values():
            channel.queue_declare(
                queue=queue_name,
                durable=True,
                arguments={
                    'x-message-ttl': 1800000,        # 30åˆ†é’ŸTTL
                    'x-max-length': 10000,           # æœ€å¤§é˜Ÿåˆ—é•¿åº¦
                    'x-dead-letter-exchange': 'dlx_order',
                    'x-dead-letter-routing-key': 'dead_letter'
                }
            )
        
        # åˆ›å»ºæ­»ä¿¡é˜Ÿåˆ—
        channel.queue_declare(
            queue='dead_letter_order',
            durable=True
        )
        
        # åˆ›å»ºäº¤æ¢æœº
        channel.exchange_declare(
            exchange='order_exchange',
            exchange_type='topic',
            durable=True
        )
        
        # ç»‘å®šé˜Ÿåˆ—
        for routing_key, queue_name in self.queues.items():
            channel.queue_bind(
                exchange='order_exchange',
                queue=queue_name,
                routing_key=routing_key
            )
        
        connection.close()
        print("âœ… è®¢å•å¤„ç†é˜Ÿåˆ—è®¾ç½®å®Œæˆ")
    
    def process_order(self, order_data):
        """å¤„ç†è®¢å•"""
        try:
            priority = self.priority_levels.get(
                order_data.get('priority', 'normal'), 5
            )
            
            connection = pika.BlockingConnection(self.connection_params)
            channel = connection.channel()
            
            properties = pika.BasicProperties(
                priority=priority,
                delivery_mode=2,  # æŒä¹…åŒ–
                message_id=order_data['order_id'],
                timestamp=int(time.time())
            )
            
            # å‘é€è®¢å•åˆ›å»ºäº‹ä»¶
            channel.basic_publish(
                exchange='order_exchange',
                routing_key='order_created',
                body=json.dumps(order_data),
                properties=properties
            )
            
            connection.close()
            return True
            
        except Exception as e:
            print(f"å¤„ç†è®¢å•å¤±è´¥: {e}")
            return False

class OrderConsumer:
    def __init__(self, queue_name, consumer_id):
        self.queue_name = queue_name
        self.consumer_id = consumer_id
        self.connection_params = pika.ConnectionParameters(host='localhost')
    
    def start_consuming(self):
        """å¼€å§‹æ¶ˆè´¹è®¢å•"""
        connection = pika.BlockingConnection(self.connection_params)
        channel = connection.channel()
        
        # è®¾ç½®å…¬å¹³åˆ†å‘
        channel.basic_qos(prefetch_count=10)
        
        def process_order_callback(ch, method, properties, body):
            try:
                order_data = json.loads(body.decode())
                print(f"æ¶ˆè´¹è€… {self.consumer_id} å¤„ç†è®¢å•: {order_data['order_id']}")
                
                # æ ¹æ®é˜Ÿåˆ—åç§°æ‰§è¡Œä¸åŒçš„å¤„ç†é€»è¾‘
                if 'payment' in self.queue_name:
                    self._process_payment(order_data)
                elif 'inventory' in self.queue_name:
                    self._check_inventory(order_data)
                elif 'fulfillment' in self.queue_name:
                    self._fulfill_order(order_data)
                elif 'notification' in self.queue_name:
                    self._send_notification(order_data)
                
                ch.basic_ack(delivery_tag=method.delivery_tag)
                
            except Exception as e:
                print(f"è®¢å•å¤„ç†é”™è¯¯: {e}")
                ch.basic_nack(delivery_tag=method.delivery_tag, requeue=True)
        
        channel.basic_consume(
            queue=self.queue_name,
            on_message_callback=process_order_callback
        )
        
        print(f"ğŸš€ æ¶ˆè´¹è€… {self.consumer_id} å¼€å§‹å¤„ç†é˜Ÿåˆ— {self.queue_name}")
        
        try:
            channel.start_consuming()
        except KeyboardInterrupt:
            channel.stop_consuming()
        finally:
            connection.close()
    
    def _process_payment(self, order_data):
        """å¤„ç†æ”¯ä»˜"""
        print(f"å¤„ç†æ”¯ä»˜: {order_data['order_id']}")
        time.sleep(0.5)  # æ¨¡æ‹Ÿæ”¯ä»˜å¤„ç†
    
    def _check_inventory(self, order_data):
        """æ£€æŸ¥åº“å­˜"""
        print(f"æ£€æŸ¥åº“å­˜: {order_data['order_id']}")
        time.sleep(0.3)  # æ¨¡æ‹Ÿåº“å­˜æ£€æŸ¥
    
    def _fulfill_order(self, order_data):
        """å±¥çº¦è®¢å•"""
        print(f"å±¥çº¦è®¢å•: {order_data['order_id']}")
        time.sleep(1.0)  # æ¨¡æ‹Ÿå±¥çº¦è¿‡ç¨‹
    
    def _send_notification(self, order_data):
        """å‘é€é€šçŸ¥"""
        print(f"å‘é€é€šçŸ¥: {order_data['order_id']}")
        time.sleep(0.2)  # æ¨¡æ‹Ÿé€šçŸ¥å‘é€
```

### 5.6.2 æ—¥å¿—å¤„ç†ç³»ç»Ÿ

**1. æ—¥å¿—æ”¶é›†å’Œå¤„ç†æ¶æ„**
```python
import gzip
from datetime import datetime
from typing import List, Dict

class LogProcessingSystem:
    def __init__(self, connection_params):
        self.connection_params = connection_params
        self.log_levels = {
            'ERROR': 4,
            'WARN': 3,
            'INFO': 2,
            'DEBUG': 1
        }
    
    def setup_log_queues(self):
        """è®¾ç½®æ—¥å¿—å¤„ç†é˜Ÿåˆ—"""
        connection = pika.BlockingConnection(self.connection_params)
        channel = connection.channel()
        
        # æŒ‰æ—¥å¿—çº§åˆ«åˆ›å»ºé˜Ÿåˆ—
        for level in self.log_levels.keys():
            queue_name = f'log_{level.lower()}_queue'
            
            channel.queue_declare(
                queue=queue_name,
                durable=True,
                arguments={
                    'x-message-ttl': 3600000,        # 1å°æ—¶TTL
                    'x-max-length': 50000,           # å¤§é‡æ—¥å¿—ç¼“å†²
                    'x-overflow': 'drop-head'        # ä¸¢å¼ƒæ—§æ¶ˆæ¯
                }
            )
        
        # åˆ›å»ºæ­»ä¿¡é˜Ÿåˆ—
        channel.queue_declare(
            queue='log_dead_letter',
            durable=True
        )
        
        # åˆ›å»ºæ—¥å¿—äº¤æ¢æœº
        channel.exchange_declare(
            exchange='log_exchange',
            exchange_type='topic',
            durable=True
        )
        
        # ç»‘å®šé˜Ÿåˆ—
        for level in self.log_levels.keys():
            queue_name = f'log_{level.lower()}_queue'
            channel.queue_bind(
                exchange='log_exchange',
                queue=queue_name,
                routing_key=f'logs.{level.lower()}'
            )
        
        connection.close()
        print("âœ… æ—¥å¿—å¤„ç†é˜Ÿåˆ—è®¾ç½®å®Œæˆ")
    
    def collect_log(self, log_data):
        """æ”¶é›†æ—¥å¿—"""
        try:
            log_level = log_data.get('level', 'INFO').upper()
            if log_level not in self.log_levels:
                log_level = 'INFO'
            
            properties = pika.BasicProperties(
                priority=self.log_levels[log_level],
                delivery_mode=2,  # æŒä¹…åŒ–æ—¥å¿—
                timestamp=int(time.time())
            )
            
            connection = pika.BlockingConnection(self.connection_params)
            channel = connection.channel()
            
            # å‹ç¼©æ—¥å¿—æ•°æ®
            compressed_data = gzip.compress(json.dumps(log_data).encode())
            
            channel.basic_publish(
                exchange='log_exchange',
                routing_key=f'logs.{log_level.lower()}',
                body=compressed_data,
                properties=properties
            )
            
            connection.close()
            return True
            
        except Exception as e:
            print(f"æ”¶é›†æ—¥å¿—å¤±è´¥: {e}")
            return False

class LogProcessor:
    def __init__(self, log_level, processor_id):
        self.log_level = log_level
        self.processor_id = processor_id
        self.queue_name = f'log_{log_level.lower()}_queue'
        self.connection_params = pika.ConnectionParameters(host='localhost')
        self.stats = {
            'processed': 0,
            'errors': 0,
            'start_time': time.time()
        }
    
    def start_processing(self):
        """å¼€å§‹å¤„ç†æ—¥å¿—"""
        connection = pika.BlockingConnection(self.connection_params)
        channel = connection.channel()
        
        # é«˜å¹¶å‘å¤„ç†
        channel.basic_qos(prefetch_count=100)
        
        def log_callback(ch, method, properties, body):
            try:
                # è§£å‹æ—¥å¿—æ•°æ®
                decompressed_data = gzip.decompress(body)
                log_entry = json.loads(decompressed_data.decode())
                
                # å¤„ç†æ—¥å¿—
                self._process_log_entry(log_entry)
                
                self.stats['processed'] += 1
                ch.basic_ack(delivery_tag=method.delivery_tag)
                
            except Exception as e:
                self.stats['errors'] += 1
                print(f"æ—¥å¿—å¤„ç†é”™è¯¯: {e}")
                ch.basic_nack(delivery_tag=method.delivery_tag, requeue=True)
        
        channel.basic_consume(
            queue=self.queue_name,
            on_message_callback=log_callback
        )
        
        print(f"ğŸš€ æ—¥å¿—å¤„ç†å™¨ {self.processor_id} å¼€å§‹å¤„ç† {self.log_level} æ—¥å¿—")
        
        try:
            channel.start_consuming()
        except KeyboardInterrupt:
            channel.stop_consuming()
        finally:
            connection.close()
            self._print_statistics()
    
    def _process_log_entry(self, log_entry):
        """å¤„ç†å•ä¸ªæ—¥å¿—æ¡ç›®"""
        timestamp = datetime.fromtimestamp(log_entry.get('timestamp', time.time()))
        
        if self.log_level == 'ERROR':
            self._handle_error_log(log_entry)
        elif self.log_level == 'WARN':
            self._handle_warning_log(log_entry)
        elif self.log_level == 'INFO':
            self._handle_info_log(log_entry)
        else:  # DEBUG
            self._handle_debug_log(log_entry)
    
    def _handle_error_log(self, log_entry):
        """å¤„ç†é”™è¯¯æ—¥å¿—"""
        print(f"[ERROR] {log_entry['timestamp']}: {log_entry['message']}")
        # å¯ä»¥å‘é€åˆ°å‘Šè­¦ç³»ç»Ÿ
        self._send_alert(log_entry)
    
    def _handle_warning_log(self, log_entry):
        """å¤„ç†è­¦å‘Šæ—¥å¿—"""
        print(f"[WARN] {log_entry['timestamp']}: {log_entry['message']}")
    
    def _handle_info_log(self, log_entry):
        """å¤„ç†ä¿¡æ¯æ—¥å¿—"""
        print(f"[INFO] {log_entry['timestamp']}: {log_entry['message']}")
    
    def _handle_debug_log(self, log_entry):
        """å¤„ç†è°ƒè¯•æ—¥å¿—"""
        if self.processor_id % 4 == 0:  # åªæ‰“å°éƒ¨åˆ†è°ƒè¯•æ—¥å¿—
            print(f"[DEBUG] {log_entry['timestamp']}: {log_entry['message']}")
    
    def _send_alert(self, log_entry):
        """å‘é€å‘Šè­¦"""
        # å®ç°å‘Šè­¦é€»è¾‘
        pass
    
    def _print_statistics(self):
        """æ‰“å°ç»Ÿè®¡ä¿¡æ¯"""
        duration = time.time() - self.stats['start_time']
        throughput = self.stats['processed'] / duration if duration > 0 else 0
        
        print(f"\nğŸ“Š å¤„ç†å™¨ {self.processor_id} ç»Ÿè®¡:")
        print(f"  å¤„ç†æ—¥å¿—æ•°: {self.stats['processed']}")
        print(f"  é”™è¯¯æ•°: {self.stats['errors']}")
        print(f"  ååé‡: {throughput:.2f} æ—¥å¿—/ç§’")
        print(f"  è¿è¡Œæ—¶é—´: {duration:.2f} ç§’")
```

## 5.7 æ€§èƒ½ä¼˜åŒ–ä¸è°ƒä¼˜

### 5.7.1 é˜Ÿåˆ—æ€§èƒ½è°ƒä¼˜

**1. é¢„å–æ•°é‡ä¼˜åŒ–**
```python
class PrefetchOptimizer:
    def __init__(self, queue_name):
        self.queue_name = queue_name
        self.performance_history = []
        self.optimal_prefetch = None
    
    def find_optimal_prefetch(self, test_range=(1, 50)):
        """æ‰¾åˆ°æœ€ä¼˜é¢„å–æ•°é‡"""
        results = []
        
        for prefetch_count in range(test_range[0], test_range[1] + 1):
            print(f"æµ‹è¯•é¢„å–æ•°é‡: {prefetch_count}")
            
            # è¿è¡Œæµ‹è¯•
            result = self._test_prefetch_performance(prefetch_count)
            results.append({
                'prefetch_count': prefetch_count,
                'throughput': result['throughput'],
                'latency': result['avg_latency'],
                'cpu_usage': result['cpu_usage']
            })
        
        # åˆ†æç»“æœ
        self.optimal_prefetch = self._analyze_performance(results)
        return self.optimal_prefetch
    
    def _test_prefetch_performance(self, prefetch_count):
        """æµ‹è¯•ç‰¹å®šé¢„å–æ•°é‡çš„æ€§èƒ½"""
        # è¿™é‡Œéœ€è¦å®ç°æ€§èƒ½æµ‹è¯•é€»è¾‘
        # å¯ä»¥æ¨¡æ‹Ÿä¸åŒçš„é¢„å–è®¾ç½®å¹¶æµ‹é‡ååé‡ã€å»¶è¿Ÿç­‰
        return {
            'throughput': 100,  # æ¨¡æ‹Ÿæ•°æ®
            'avg_latency': 50,  # æ¨¡æ‹Ÿæ•°æ®
            'cpu_usage': 30     # æ¨¡æ‹Ÿæ•°æ®
        }
    
    def _analyze_performance(self, results):
        """åˆ†ææ€§èƒ½ç»“æœ"""
        # æ‰¾åˆ°æœ€ä½³å¹³è¡¡ç‚¹ï¼ˆé«˜ååé‡ã€ä½å»¶è¿Ÿã€ä½CPUä½¿ç”¨ï¼‰
        best_score = 0
        best_prefetch = 1
        
        for result in results:
            # ç»¼åˆè¯„åˆ†ï¼šååé‡æƒé‡40%ï¼Œå»¶è¿Ÿæƒé‡40%ï¼ŒCPUæƒé‡20%
            score = (result['throughput'] * 0.4 + 
                    (100 - result['latency']) * 0.4 + 
                    (100 - result['cpu_usage']) * 0.2)
            
            if score > best_score:
                best_score = score
                best_prefetch = result['prefetch_count']
        
        return best_prefetch
```

**2. æ‰¹å¤„ç†ä¼˜åŒ–**
```python
class BatchProcessor:
    def __init__(self, queue_name, batch_size=100, flush_interval=5):
        self.queue_name = queue_name
        self.batch_size = batch_size
        self.flush_interval = flush_interval
        self.current_batch = []
        self.last_flush = time.time()
        self.connection_params = pika.ConnectionParameters(host='localhost')
    
    def add_to_batch(self, message):
        """æ·»åŠ æ¶ˆæ¯åˆ°æ‰¹æ¬¡"""
        self.current_batch.append(message)
        
        # æ£€æŸ¥æ˜¯å¦éœ€è¦flushæ‰¹æ¬¡
        if (len(self.current_batch) >= self.batch_size or 
            time.time() - self.last_flush > self.flush_interval):
            self.flush_batch()
    
    def flush_batch(self):
        """flushå½“å‰æ‰¹æ¬¡"""
        if not self.current_batch:
            return
        
        try:
            connection = pika.BlockingConnection(self.connection_params)
            channel = connection.channel()
            
            # å‘é€æ‰¹æ¬¡æ¶ˆæ¯
            for message in self.current_batch:
                channel.basic_publish(
                    exchange='',
                    routing_key=self.queue_name,
                    body=json.dumps(message),
                    properties=pika.BasicProperties(delivery_mode=2)
                )
            
            connection.close()
            
            print(f"å‘é€æ‰¹æ¬¡: {len(self.current_batch)} æ¡æ¶ˆæ¯")
            self.current_batch.clear()
            self.last_flush = time.time()
            
        except Exception as e:
            print(f"å‘é€æ‰¹æ¬¡å¤±è´¥: {e}")
            # é‡è¯•é€»è¾‘
            self._retry_batch()
    
    def _retry_batch(self):
        """é‡è¯•å¤±è´¥æ‰¹æ¬¡"""
        if self.current_batch:
            # å°è¯•é‡æ–°å‘é€ï¼ˆå¯èƒ½éœ€è¦æŒ‡æ•°é€€é¿ï¼‰
            time.sleep(1)
            self.flush_batch()
```

### 5.7.2 å†…å­˜å’Œç£ç›˜ä¼˜åŒ–

**1. å†…å­˜ä½¿ç”¨ä¼˜åŒ–**
```python
class MemoryOptimizedConsumer:
    def __init__(self, queue_name):
        self.queue_name = queue_name
        self.memory_limit_mb = 512  # 512MBå†…å­˜é™åˆ¶
        self.message_buffer = []
        self.buffer_size_limit = 1000
    
    def consume_with_memory_limit(self):
        """å†…å­˜é™åˆ¶æ¶ˆè´¹"""
        connection = pika.BlockingConnection(self.connection_params)
        channel = connection.channel()
        
        # è®¾ç½®è¾ƒä½é¢„å–
        channel.basic_qos(prefetch_count=10)
        
        def callback(ch, method, properties, body):
            try:
                # æ£€æŸ¥å†…å­˜ä½¿ç”¨
                if self._check_memory_usage():
                    # å†…å­˜è¿‡é«˜ï¼Œæš‚åœæ¶ˆè´¹
                    print("å†…å­˜ä½¿ç”¨è¿‡é«˜ï¼Œæš‚åœæ¶ˆè´¹")
                    time.sleep(1)
                
                # æ¸…ç†ç¼“å†²åŒº
                if len(self.message_buffer) > self.buffer_size_limit:
                    self.message_buffer = self.message_buffer[-self.buffer_size_limit//2:]
                
                # å¤„ç†æ¶ˆæ¯
                message = json.loads(body.decode())
                self._process_message(message)
                
                ch.basic_ack(delivery_tag=method.delivery_tag)
                
            except Exception as e:
                print(f"å¤„ç†æ¶ˆæ¯é”™è¯¯: {e}")
                ch.basic_nack(delivery_tag=method.delivery_tag, requeue=True)
        
        channel.basic_consume(
            queue=self.queue_name,
            on_message_callback=callback
        )
        
        channel.start_consuming()
    
    def _check_memory_usage(self):
        """æ£€æŸ¥å†…å­˜ä½¿ç”¨"""
        import psutil
        memory_percent = psutil.virtual_memory().percent
        return memory_percent > 80  # å†…å­˜ä½¿ç”¨è¶…è¿‡80%æ—¶è¿”å›True
    
    def _process_message(self, message):
        """å¤„ç†æ¶ˆæ¯"""
        # å¤„ç†æ¶ˆæ¯ï¼Œä½†é¿å…å¤§é‡å†…å­˜ç§¯ç´¯
        self.message_buffer.append(message)
        # ç«‹å³å¤„ç†ï¼Œä¸åœ¨ç¼“å†²åŒºä¸­ä¿å­˜è¿‡ä¹…
```

**2. ç£ç›˜I/Oä¼˜åŒ–**
```python
import os
import tempfile

class DiskOptimizedProcessor:
    def __init__(self, queue_name, temp_dir=None):
        self.queue_name = queue_name
        self.temp_dir = temp_dir or tempfile.gettempdir()
        self.pending_files = []
        self.processing_files = []
    
    def process_with_disk_spillover(self, messages):
        """ç£ç›˜æº¢å‡ºå¤„ç†"""
        memory_messages = []
        
        # å†…å­˜ä¸­ä¿å­˜çš„æ¶ˆæ¯æ•°é‡é™åˆ¶
        memory_limit = 100
        
        for message in messages:
            if len(memory_messages) < memory_limit:
                memory_messages.append(message)
            else:
                # å†…å­˜æ»¡ï¼Œå†™å…¥ç£ç›˜
                self._write_to_disk(message)
    
    def _write_to_disk(self, message):
        """å°†æ¶ˆæ¯å†™å…¥ç£ç›˜"""
        try:
            file_name = f"{self.queue_name}_{int(time.time())}_{len(self.pending_files)}.tmp"
            file_path = os.path.join(self.temp_dir, file_name)
            
            with open(file_path, 'w') as f:
                json.dump(message, f)
            
            self.pending_files.append(file_path)
            
            # å®šæœŸæ¸…ç†ä¸´æ—¶æ–‡ä»¶
            if len(self.pending_files) > 1000:
                self._cleanup_temp_files()
                
        except Exception as e:
            print(f"å†™å…¥ç£ç›˜å¤±è´¥: {e}")
    
    def _cleanup_temp_files(self):
        """æ¸…ç†ä¸´æ—¶æ–‡ä»¶"""
        # ç§»åŠ¨ä¸€äº›æ–‡ä»¶åˆ°å¤„ç†åˆ—è¡¨
        while self.pending_files and len(self.processing_files) < 10:
            file_path = self.pending_files.pop(0)
            self.processing_files.append(file_path)
    
    def _process_disk_files(self):
        """å¤„ç†ç£ç›˜æ–‡ä»¶"""
        while self.processing_files:
            file_path = self.processing_files.pop(0)
            
            try:
                with open(file_path, 'r') as f:
                    message = json.load(f)
                
                # å¤„ç†æ¶ˆæ¯
                self._process_message(message)
                
                # åˆ é™¤ä¸´æ—¶æ–‡ä»¶
                os.remove(file_path)
                
            except Exception as e:
                print(f"å¤„ç†ç£ç›˜æ–‡ä»¶é”™è¯¯: {e}")
                # æ–‡ä»¶å¯èƒ½åœ¨ä¸‹æ¬¡æ¸…ç†æ—¶è¢«åˆ é™¤
```

## 5.8 æ€»ç»“

é˜Ÿåˆ—ç®¡ç†ä¸è´Ÿè½½å‡è¡¡æ˜¯RabbitMQåº”ç”¨ä¸­çš„æ ¸å¿ƒæŠ€èƒ½ã€‚æœ¬ç« è¦†ç›–äº†ï¼š

### 5.8.1 å…³é”®æ¦‚å¿µ
- **é˜Ÿåˆ—ç”Ÿå‘½å‘¨æœŸç®¡ç†**ï¼šåˆ›å»ºã€é…ç½®ã€ç›‘æ§ã€æ¸…ç†
- **è´Ÿè½½å‡è¡¡ç­–ç•¥**ï¼šè½®è¯¢ã€å…¬å¹³åˆ†å‘ã€ä¼˜å…ˆçº§ã€æƒé‡
- **åŠ¨æ€æ‰©ç¼©å®¹**ï¼šåŸºäºè´Ÿè½½çš„è‡ªåŠ¨æ‰©å®¹å’Œç¼©å®¹
- **åˆ†ç‰‡é˜Ÿåˆ—**ï¼šæé«˜å¹¶è¡Œå¤„ç†èƒ½åŠ›

### 5.8.2 å®é™…åº”ç”¨
- **ç”µå•†è®¢å•å¤„ç†**ï¼šå¤šé˜Ÿåˆ—ã€å¤šæ¶ˆè´¹è€…ã€ä¼˜å…ˆçº§å¤„ç†
- **æ—¥å¿—å¤„ç†ç³»ç»Ÿ**ï¼šåˆ†çº§å¤„ç†ã€æ‰¹é‡æ“ä½œã€å‹ç¼©å­˜å‚¨

### 5.8.3 æ€§èƒ½ä¼˜åŒ–
- **é¢„å–æ•°é‡è°ƒä¼˜**ï¼šæ‰¾åˆ°æœ€ä½³prefetch_count
- **æ‰¹å¤„ç†ä¼˜åŒ–**ï¼šå‡å°‘ç½‘ç»œå¼€é”€
- **å†…å­˜å’Œç£ç›˜ä¼˜åŒ–**ï¼šé˜²æ­¢å†…å­˜æº¢å‡ºå’ŒI/Oç“¶é¢ˆ

### 5.8.4 ç›‘æ§ä¸æ•…éšœå¤„ç†
- **å¥åº·æ£€æŸ¥**ï¼šå®æ—¶ç›‘æ§é˜Ÿåˆ—çŠ¶æ€
- **æ•…éšœæ£€æµ‹**ï¼šè‡ªåŠ¨è¯†åˆ«å„ç±»æ•…éšœ
- **è‡ªåŠ¨æ¢å¤**ï¼šæ•…éšœè‡ªåŠ¨ä¿®å¤æœºåˆ¶

### 5.8.5 æœ€ä½³å®è·µ
1. **åˆç†çš„é˜Ÿåˆ—é…ç½®**ï¼šæ ¹æ®ä¸šåŠ¡éœ€æ±‚è®¾ç½®TTLã€æœ€å¤§é•¿åº¦ç­‰
2. **é€‚å½“çš„é¢„å–è®¾ç½®**ï¼šå¹³è¡¡ååé‡å’ŒæœåŠ¡è´¨é‡
3. **å¤šæ¶ˆè´¹è€…æ¨¡å¼**ï¼šæé«˜å¤„ç†èƒ½åŠ›
4. **ç›‘æ§å’Œå‘Šè­¦**ï¼šåŠæ—¶å‘ç°é—®é¢˜
5. **è‡ªåŠ¨æ‰©ç¼©å®¹**ï¼šåº”å¯¹è´Ÿè½½å˜åŒ–

æŒæ¡è¿™äº›æŠ€èƒ½ï¼Œèƒ½å¤Ÿæ„å»ºé«˜æ•ˆã€å¯é ã€å¯æ‰©å±•çš„æ¶ˆæ¯é˜Ÿåˆ—ç³»ç»Ÿã€‚