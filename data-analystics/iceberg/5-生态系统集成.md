# 第五章：Iceberg 生态系统集成

Apache Iceberg 作为一个开放的表格式，设计初衷就是与各种大数据生态系统组件无缝集成。本章将详细介绍 Iceberg 如何与不同的计算引擎、存储系统和数据治理工具集成。

## 1. 计算引擎集成

### 1.1 Spark 集成

Apache Spark 是使用 Iceberg 最广泛的计算引擎之一。Iceberg 提供了原生的 Spark 支持，可以通过多种方式配置。

#### 1.1.1 添加依赖

```xml
<dependency>
    <groupId>org.apache.iceberg</groupId>
    <artifactId>iceberg-spark-runtime-3.3_2.12</artifactId>
    <version>1.1.0</version>
</dependency>
```

#### 1.1.2 配置 Spark Session

```scala
import org.apache.spark.sql.SparkSession

val spark = SparkSession.builder()
  .appName("Iceberg Integration")
  .config("spark.sql.extensions", "org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions")
  .config("spark.sql.catalog.spark_catalog", "org.apache.iceberg.spark.SparkSessionCatalog")
  .config("spark.sql.catalog.spark_catalog.type", "hadoop")
  .config("spark.sql.catalog.spark_catalog.warehouse", "hdfs://namenode:8020/iceberg/warehouse")
  .getOrCreate()
```

#### 1.1.3 创建和查询 Iceberg 表

```scala
// 创建 Iceberg 表
spark.sql("""
  CREATE TABLE spark_catalog.default.users (
    id BIGINT,
    name STRING,
    email STRING,
    created_at TIMESTAMP
  )
  USING iceberg
  PARTITIONED BY (days(created_at))
""")

// 插入数据
spark.sql("""
  INSERT INTO spark_catalog.default.users VALUES
  (1, 'Alice', 'alice@example.com', now()),
  (2, 'Bob', 'bob@example.com', now())
""")

// 查询数据
spark.sql("SELECT * FROM spark_catalog.default.users").show()
```

### 1.2 Flink 集成

Apache Flink 也提供了对 Iceberg 的原生支持，特别适合流处理场景。

#### 1.2.1 添加依赖

```xml
<dependency>
    <groupId>org.apache.iceberg</groupId>
    <artifactId>iceberg-flink-runtime-1.16</artifactId>
    <version>1.1.0</version>
</dependency>
```

#### 1.2.2 配置 Flink 环境

```java
import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;
import org.apache.flink.table.api.bridge.java.StreamTableEnvironment;

StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();
StreamTableEnvironment tableEnv = StreamTableEnvironment.create(env);

// 配置 Iceberg Catalog
tableEnv.executeSql("CREATE CATALOG iceberg WITH (" +
    "'type'='iceberg'," +
    "'catalog-type'='hadoop'," +
    "'warehouse'='hdfs://namenode:8020/iceberg/warehouse'" +
    ")");

tableEnv.executeSql("USE CATALOG iceberg");
```

#### 1.2.3 流式写入 Iceberg 表

```java
// 创建表
tableEnv.executeSql("CREATE TABLE default.events (" +
    "event_time TIMESTAMP(3)," +
    "user_id BIGINT," +
    "event_type STRING," +
    "data STRING" +
    ") WITH (" +
    "'connector'='kafka'," +
    "'topic'='events'," +
    "'properties.bootstrap.servers'='localhost:9092'," +
    "'format'='json'" +
    ")");

// 将 Kafka 数据流式写入 Iceberg
tableEnv.executeSql("INSERT INTO iceberg.default.events SELECT * FROM default.events");
```

### 1.3 Trino/Presto 集成

Trino 和 Presto 提供了对 Iceberg 的优秀支持，适合交互式查询。

#### 1.3.1 Trino 配置

在 Trino 的 `etc/catalog/iceberg.properties` 文件中：

```properties
connector.name=iceberg
iceberg.catalog.type=hive_metastore
hive.metastore.uri=thrift://localhost:9083
```

#### 1.3.2 查询 Iceberg 表

```sql
-- 查询 Iceberg 表
SELECT * FROM iceberg.default.users WHERE created_at > DATE '2023-01-01';

-- 聚合查询
SELECT 
    DATE_TRUNC('month', created_at) as month,
    COUNT(*) as user_count
FROM iceberg.default.users
GROUP BY DATE_TRUNC('month', created_at)
ORDER BY month;
```

## 2. 存储系统集成

### 2.1 HDFS 存储配置

Iceberg 原生支持 HDFS，这是最常见的部署方式。

#### 2.1.1 基本配置

```xml
<!-- core-site.xml -->
<configuration>
    <property>
        <name>fs.defaultFS</name>
        <value>hdfs://namenode:8020</value>
    </property>
</configuration>

<!-- hdfs-site.xml -->
<configuration>
    <property>
        <name>dfs.replication</name>
        <value>3</value>
    </property>
</configuration>
```

#### 2.1.2 Iceberg 表创建

```sql
CREATE TABLE hdfs_table (
    id BIGINT,
    data STRING,
    ts TIMESTAMP
)
USING iceberg
LOCATION 'hdfs://namenode:8020/warehouse/hdfs_table'
```

### 2.2 S3 存储集成

Iceberg 可以很好地与 Amazon S3 集成。

#### 2.2.1 Spark 配置

```scala
val spark = SparkSession.builder()
  .appName("Iceberg S3 Integration")
  .config("spark.sql.extensions", "org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions")
  .config("spark.sql.catalog.s3_catalog", "org.apache.iceberg.spark.SparkCatalog")
  .config("spark.sql.catalog.s3_catalog.type", "hadoop")
  .config("spark.sql.catalog.s3_catalog.warehouse", "s3a://my-bucket/iceberg/warehouse")
  .config("spark.hadoop.fs.s3a.access.key", "YOUR_ACCESS_KEY")
  .config("spark.hadoop.fs.s3a.secret.key", "YOUR_SECRET_KEY")
  .getOrCreate()
```

#### 2.2.2 表创建和使用

```scala
// 创建 S3 上的 Iceberg 表
spark.sql("""
  CREATE TABLE s3_catalog.default.s3_users (
    id BIGINT,
    name STRING,
    email STRING
  )
  USING iceberg
  LOCATION 's3a://my-bucket/iceberg/tables/s3_users'
""")

// 插入和查询数据
spark.sql("INSERT INTO s3_catalog.default.s3_users VALUES (1, 'Alice', 'alice@example.com')")
spark.sql("SELECT * FROM s3_catalog.default.s3_users").show()
```

### 2.3 本地文件系统支持

Iceberg 也可以在本地文件系统上使用，适合开发和测试环境。

#### 2.3.1 配置示例

```scala
val spark = SparkSession.builder()
  .appName("Iceberg Local FS")
  .config("spark.sql.extensions", "org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions")
  .config("spark.sql.catalog.local", "org.apache.iceberg.spark.SparkCatalog")
  .config("spark.sql.catalog.local.type", "hadoop")
  .config("spark.sql.catalog.local.warehouse", "file:///tmp/iceberg/warehouse")
  .getOrCreate()
```

## 3. 数据治理工具集成

### 3.1 Apache Atlas 集成

Apache Atlas 是一个元数据管理和数据治理平台，可以与 Iceberg 集成。

#### 3.1.1 配置 Atlas Hook

```xml
<!-- atlas-application.properties -->
atlas.kafka.bootstrap.servers=localhost:9092
atlas.rest.address=http://localhost:21000
```

#### 3.1.2 在 Spark 中启用 Atlas

```bash
spark-submit \
  --conf "spark.extraListeners=org.apache.atlas.spark.SparkAtlasEventTracker" \
  --conf "spark.sql.queryExecutionListeners=org.apache.atlas.spark.SparkAtlasEventTracker" \
  --jars "/path/to/atlas-spark-hook.jar" \
  your-application.jar
```

### 3.2 数据血缘追踪

Iceberg 提供了丰富的元数据，可用于构建数据血缘关系。

#### 3.2.1 查看表历史

```sql
-- 查看表的历史快照
SELECT * FROM default.users.history;

-- 查看表的元数据日志
SELECT * FROM default.users.metadata_log_entries;
```

#### 3.2.2 使用 Spark API 查看血缘

```scala
import org.apache.iceberg.spark.Spark3Util

// 获取表的血缘信息
val catalog = Spark3Util.loadIcebergCatalog(spark, "spark_catalog")
val table = catalog.loadTable(org.apache.iceberg.catalog.TableIdentifier.of("default", "users"))

// 打印表的快照信息
table.currentSnapshot().summary().forEach { (key, value) =>
  println(s"$key: $value")
}
```

## 4. 可视化工具集成

### 4.1 Superset 集成

Apache Superset 可以通过 Trino 或其他查询引擎连接到 Iceberg 表。

#### 4.1.1 配置数据库连接

在 Superset 中添加数据库连接：
- SQLAlchemy URI: `trino://user@localhost:8080/iceberg`
- 其他配置保持默认

#### 4.1.2 创建数据集

1. 在 Superset 中选择刚刚配置的数据库连接
2. 选择 Iceberg 表：`iceberg.default.users`
3. 配置指标和维度字段

### 4.2 Grafana 集成

Grafana 也可以通过相应的数据源插件连接到 Iceberg。

#### 4.2.1 配置数据源

在 Grafana 中添加 PostgreSQL 或 MySQL 数据源（通过 Trino 连接 Iceberg）：
- Host: localhost
- Port: 8080
- Database: iceberg
- User: trino

#### 4.2.2 创建仪表板

使用以下查询创建面板：

```sql
SELECT 
    DATE_TRUNC('day', created_at) AS day,
    COUNT(*) AS user_count
FROM default.users
WHERE created_at >= $__timeFrom() AND created_at < $__timeTo()
GROUP BY DATE_TRUNC('day', created_at)
ORDER BY day
```

## 5. 最佳实践

### 5.1 性能优化建议

1. **合理设置文件大小**：
   ```sql
   ALTER TABLE default.users SET TBLPROPERTIES (
     'write.target-file-size-bytes'='536870912' -- 512MB
   )
   ```

2. **启用元数据缓存**：
   ```sql
   ALTER TABLE default.users SET TBLPROPERTIES (
     'metadata.previous-versions-max'='20',
     'metadata.delete-after-commit.enabled'='true'
   )
   ```

3. **使用合适的分区策略**：
   ```sql
   CREATE TABLE events (
     event_time TIMESTAMP,
     user_id BIGINT,
     event_data STRING
   )
   USING iceberg
   PARTITIONED BY (days(event_time), user_id)
   ```

### 5.2 安全配置

1. **启用 Kerberos 认证**：
   ```properties
   # core-site.xml
   <property>
     <name>hadoop.security.authentication</name>
     <value>kerberos</value>
   </property>
   ```

2. **配置 Ranger 权限控制**：
   ```sql
   -- 在 Ranger 中为 Iceberg 表配置访问策略
   -- 表: iceberg.default.users
   -- 权限: SELECT, INSERT, UPDATE, DELETE
   ```

## 总结

本章详细介绍了 Apache Iceberg 与各种大数据生态系统的集成方式。通过这些集成，Iceberg 可以成为统一的数据湖平台的核心组件，为企业提供高性能、可扩展且易于管理的数据存储和分析能力。

关键要点：
1. Iceberg 与主流计算引擎（Spark、Flink、Trino）都有良好的集成支持
2. 支持多种存储系统，包括 HDFS、S3 和本地文件系统
3. 可以与数据治理工具集成，实现元数据管理和血缘追踪
4. 能够与可视化工具结合，构建完整的数据分析平台