# 第二章：Iceberg 环境搭建与配置

## 2.1 环境准备

在开始使用 Apache Iceberg 之前，我们需要准备好相应的环境。Iceberg 作为一个表格式，需要与计算引擎配合使用才能发挥其威力。本章将介绍如何搭建 Iceberg 环境，并与主流计算引擎集成。

### 2.1.1 系统要求

- Java 8 或更高版本
- Scala 2.11 或 2.12（取决于使用的计算引擎版本）
- 足够的磁盘空间用于存储数据
- 网络连接（用于下载依赖包）

### 2.1.2 支持的计算引擎

Iceberg 支持多种计算引擎，包括：
- Apache Spark（推荐）
- Apache Flink
- Presto/Trino
- Apache Hive

## 2.2 Iceberg 与 Spark 集成

Apache Spark 是使用 Iceberg 最广泛的计算引擎之一。下面我们将详细介绍如何配置 Spark 与 Iceberg 的集成。

### 2.2.1 Spark 环境准备

首先确保已安装 Spark，可以通过以下命令验证：

```bash
spark-submit --version
```

如果未安装，请参考官方文档进行安装。

### 2.2.2 添加 Iceberg 依赖

Iceberg 提供了针对不同 Spark 版本的运行时包。根据你的 Spark 版本选择合适的 Iceberg 包：

```bash
# 对于 Spark 3.5
spark-shell --packages org.apache.iceberg:iceberg-spark-runtime-3.5_2.12:1.6.1

# 对于 Spark 3.4
spark-shell --packages org.apache.iceberg:iceberg-spark-runtime-3.4_2.12:1.6.1

# 对于 Spark 3.3
spark-shell --packages org.apache.iceberg:iceberg-spark-runtime-3.3_2.12:1.6.1
```

### 2.2.3 Spark Session 配置

要在 Spark Session 中启用 Iceberg 支持，需要进行如下配置：

```scala
import org.apache.spark.sql.SparkSession

val spark = SparkSession.builder()
  .appName("Iceberg Integration")
  .config("spark.sql.extensions", "org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions")
  .config("spark.sql.catalog.spark_catalog", "org.apache.iceberg.spark.SparkSessionCatalog")
  .config("spark.sql.catalog.spark_catalog.type", "hive")
  .config("spark.sql.catalog.local", "org.apache.iceberg.spark.SparkCatalog")
  .config("spark.sql.catalog.local.type", "hadoop")
  .config("spark.sql.catalog.local.warehouse", "file:///tmp/iceberg/warehouse")
  .getOrCreate()
```

### 2.2.4 创建 Iceberg 表

配置完成后，就可以创建 Iceberg 表了：

```sql
-- 创建数据库
CREATE DATABASE IF NOT EXISTS local.iceberg_db;

-- 创建 Iceberg 表
CREATE TABLE local.iceberg_db.users (
  id BIGINT COMMENT '用户ID',
  name STRING COMMENT '用户名',
  age INT COMMENT '年龄',
  email STRING COMMENT '邮箱'
) USING iceberg
PARTITIONED BY (age)
TBLPROPERTIES ('format-version'='2');
```

## 2.3 Iceberg 与 Flink 集成

Apache Flink 是一个流处理框架，与 Iceberg 结合可以实现流批一体化的数据处理。

### 2.3.1 Flink 环境准备

确保已安装 Flink，并配置好环境变量。

### 2.3.2 添加 Iceberg 依赖

对于 Flink，需要添加相应的 Iceberg 运行时包：

```xml
<!-- Maven 依赖 -->
<dependency>
  <groupId>org.apache.iceberg</groupId>
  <artifactId>iceberg-flink-runtime-1.18</artifactId>
  <version>1.6.1</version>
</dependency>
```

或者在启动 Flink 作业时通过 `--jar` 参数指定：

```bash
./bin/flink run \
  --jarfile /path/to/iceberg-flink-runtime-1.18-1.6.1.jar \
  your-application.jar
```

### 2.3.3 Flink SQL 配置

在 Flink SQL 中使用 Iceberg，需要配置 Catalog：

```sql
-- 创建 Iceberg Catalog
CREATE CATALOG iceberg_catalog WITH (
  'type'='iceberg',
  'catalog-type'='hadoop',
  'warehouse'='file:///tmp/iceberg/warehouse'
);

-- 使用 Catalog
USE CATALOG iceberg_catalog;

-- 创建表
CREATE TABLE iceberg_catalog.`default`.user_events (
  user_id BIGINT,
  event_type STRING,
  ts TIMESTAMP(3),
  WATERMARK FOR ts AS ts - INTERVAL '5' SECOND
) WITH (
  'connector'='iceberg'
);
```

## 2.4 Iceberg 与 Presto/Trino 集成

Presto/Trino 是高性能的分布式 SQL 查询引擎，可以用来查询 Iceberg 表。

### 2.4.1 配置 Catalog

在 Presto/Trino 的配置目录下创建 Iceberg Catalog 配置文件：

```properties
# etc/catalog/iceberg.properties
connector.name=iceberg
iceberg.catalog.type=hive_metastore
hive.metastore.uri=thrift://localhost:9083
```

### 2.4.2 查询 Iceberg 表

配置完成后，就可以在 Presto/Trino 中查询 Iceberg 表：

```sql
SELECT * FROM iceberg.iceberg_db.users WHERE age > 18;
```

## 2.5 最佳实践建议

### 2.5.1 版本兼容性

确保 Iceberg 版本与计算引擎版本兼容：
- Iceberg 1.6.x 支持 Spark 3.3+
- Iceberg 1.6.x 支持 Flink 1.17+

### 2.5.2 性能优化配置

```properties
# Spark 性能优化配置
spark.sql.iceberg.metadata.cache-enabled=true
spark.sql.iceberg.metadata.expire-after-commit.ms=600000
spark.sql.iceberg.plan.parallelism=8
```

### 2.5.3 存储配置

建议使用分布式文件系统（如 HDFS、S3）作为底层存储：

```properties
# HDFS 存储配置
spark.sql.catalog.local.type=hadoop
spark.sql.catalog.local.warehouse=hdfs://namenode:port/path/to/warehouse

# S3 存储配置
spark.sql.catalog.local.type=hadoop
spark.sql.catalog.local.warehouse=s3a://bucket/path/to/warehouse
spark.sql.catalog.local.fs.s3a.access.key=YOUR_ACCESS_KEY
spark.sql.catalog.local.fs.s3a.secret.key=YOUR_SECRET_KEY
```

## 2.6 故障排除

### 2.6.1 常见问题

1. **ClassNotFoundException**: 确保正确添加了 Iceberg 运行时依赖
2. **Catalog 无法连接**: 检查 Metastore 服务是否正常运行
3. **权限问题**: 确保有足够的权限访问存储系统

### 2.6.2 日志调试

启用详细日志可以帮助诊断问题：

```properties
log4j.logger.org.apache.iceberg=DEBUG
log4j.logger.org.apache.spark.sql.execution.datasources.v2=DEBUG
```

## 2.7 本章小结

本章详细介绍了如何搭建 Iceberg 环境并与主流计算引擎集成：
1. 介绍了 Iceberg 与 Spark、Flink、Presto/Trino 的集成方法
2. 提供了详细的配置示例和代码片段
3. 给出了最佳实践建议和故障排除方法

掌握了这些内容后，你就能够在本地环境中成功运行 Iceberg，并开始进行数据操作了。下一章我们将深入探讨 Iceberg 的表格式和核心概念。