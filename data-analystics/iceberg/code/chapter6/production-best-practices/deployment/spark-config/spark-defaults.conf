# Spark Configuration for Iceberg Production Environment
# ================================================

# Spark Core Configuration
spark.app.name                           IcebergProductionApp
spark.master                             yarn
spark.submit.deployMode                  cluster

# Resource Allocation
spark.executor.instances                 20
spark.executor.cores                     4
spark.executor.memory                    8g
spark.executor.memoryOverhead            2g
spark.driver.memory                      4g
spark.driver.memoryOverhead              1g
spark.driver.maxResultSize               2g

# Dynamic Resource Allocation
spark.dynamicAllocation.enabled          true
spark.dynamicAllocation.minExecutors     5
spark.dynamicAllocation.maxExecutors     50
spark.dynamicAllocation.initialExecutors 10

# Serialization
spark.serializer                         org.apache.spark.serializer.KryoSerializer
spark.kryo.registrationRequired          false

# Memory Management
spark.sql.execution.arrow.pyspark.enabled true
spark.sql.adaptive.enabled               true
spark.sql.adaptive.coalescePartitions.enabled true
spark.sql.adaptive.skewJoin.enabled      true

# Shuffle Configuration
spark.sql.shuffle.partitions             400
spark.default.parallelism                400
spark.sql.adaptive.advisoryPartitionSizeInBytes 128MB

# Iceberg Specific Configuration
spark.sql.catalog.spark_catalog          org.apache.iceberg.spark.SparkCatalog
spark.sql.catalog.spark_catalog.type     hive
spark.sql.catalog.spark_catalog.uri      thrift://hive-metastore:9083
spark.sql.catalog.spark_catalog.warehouse hdfs://namenode:9000/warehouse

# Iceberg Runtime Configuration
spark.sql.extensions                      org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions

# S3 Configuration (if using S3)
spark.hadoop.fs.s3a.impl                 org.apache.hadoop.fs.s3a.S3AFileSystem
spark.hadoop.fs.s3a.access.key           YOUR_ACCESS_KEY
spark.hadoop.fs.s3a.secret.key           YOUR_SECRET_KEY
spark.hadoop.fs.s3a.endpoint             s3.amazonaws.com
spark.hadoop.fs.s3a.path.style.access    true

# Hive Metastore Configuration
spark.hadoop.hive.metastore.uris         thrift://hive-metastore:9083
spark.hadoop.hive.metastore.schema.verification false
spark.hadoop.javax.jdo.option.ConnectionURL jdbc:mysql://metastore-db:3306/metastore
spark.hadoop.javax.jdo.option.ConnectionDriverName com.mysql.cj.jdbc.Driver
spark.hadoop.javax.jdo.option.ConnectionUserName metastore_user
spark.hadoop.javax.jdo.option.ConnectionPassword metastore_password

# Security Configuration
spark.yarn.principal                     spark-user/_HOST@YOUR-REALM.COM
spark.yarn.keytab                        /etc/security/keytabs/spark.keytab

# Logging Configuration
spark.eventLog.enabled                   true
spark.eventLog.dir                       hdfs://namenode:9000/spark-logs
spark.sql.ui.retainedExecutions          100
spark.ui.retainedJobs                    100
spark.ui.retainedStages                  100

# Network Configuration
spark.network.timeout                    800s
spark.sql.broadcastTimeout               1200s
spark.rpc.askTimeout                     800s

# Compression
spark.sql.parquet.compression.codec      snappy
spark.io.compression.codec               snappy
spark.sql.orc.compression.codec          snappy