# 第7章：高可用与负载均衡

## 1. 高可用性概述

### 1.1 高可用性概念

高可用性(High Availability, HA)是指系统在面对各种故障时，能够持续提供服务的能力。对于数据库系统而言，高可用性意味着即使硬件、软件或网络发生故障，数据库服务仍然可用，或者能够在极短时间内恢复服务。

#### 高可用性的核心指标

- **可用性百分比(Availability Percentage)**：系统可提供服务的时间占总时间的百分比
  - 99.9%可用性：每年停机时间约8.76小时
  - 99.99%可用性：每年停机时间约52.6分钟
  - 99.999%可用性：每年停机时间约5.26分钟

- **平均无故障时间(MTBF, Mean Time Between Failures)**：系统平均能够正常运行的时间
- **平均修复时间(MTTR, Mean Time To Repair)**：系统故障后平均恢复时间
- **恢复时间目标(RTO, Recovery Time Objective)**：系统可接受的最大恢复时间
- **恢复点目标(RPO, Recovery Point Objective)**：可接受的最大数据丢失量

#### 高可用性的层次

1. **数据层面高可用**：通过数据冗余保证数据不丢失
2. **服务层面高可用**：通过服务冗余保证服务不中断
3. **应用层面高可用**：通过应用设计减少单点故障
4. **网络层面高可用**：通过网络冗余保证连接可靠

### 1.2 高可用架构模式

#### 主从模式(Master-Slave)

```
[应用服务器] --> [主服务器] --> [从服务器]
                      ↑           ↑
                   [写入操作]    [读取操作]
```

- **优点**：实现简单，成本低
- **缺点**：主节点是单点故障，切换需要手动干预

#### 主从切换模式(Master-Slave with Failover)

```
[应用服务器] --> [主服务器] --> [从服务器]
      ↑            ↑           ↑
      |            |           |
   [监控组件]   [自动切换]   [备用主]
```

- **优点**：实现自动故障转移，恢复时间短
- **缺点**：切换可能丢失部分数据，配置复杂

#### 多主模式(Multi-Master)

```
[应用服务器] --> [主服务器1] <--> [主服务器2]
      ↑               ↑             ↑
   [读写操作]      [读写操作]     [读写操作]
```

- **优点**：写入负载分散，无单点故障
- **缺点**：数据一致性处理复杂

#### 集群模式(Cluster)

```
[应用服务器] --> [负载均衡器] --> [SQL节点] <--> [数据节点]
                                        ↑
                                   [管理节点]
```

- **优点**：高可用、高扩展性、自动分区
- **缺点**：实现复杂，成本高

### 1.3 故障类型与应对策略

#### 硬件故障

- **服务器故障**：通过冗余服务器和自动切换应对
- **存储故障**：通过RAID、存储复制和分布式存储应对
- **网络故障**：通过多路径网络和链路聚合应对

#### 软件故障

- **数据库软件故障**：通过进程监控和自动重启应对
- **操作系统故障**：通过虚拟化和容器化应对
- **应用程序故障**：通过应用冗余和健康检查应对

#### 数据故障

- **数据损坏**：通过备份和二进制日志恢复
- **数据不一致**：通过数据校验和修复工具处理
- **数据丢失**：通过冗余存储和定期备份应对

## 2. MySQL高可用解决方案

### 2.1 基于复制的高可用

#### 传统主从复制

```sql
-- 主服务器配置
[mysqld]
server-id=1
log-bin=mysql-bin
binlog-format=ROW
sync-binlog=1
innodb-flush-log-at-trx-commit=1

-- 从服务器配置
[mysqld]
server-id=2
relay-log=relay-bin
read-only=1
skip-slave-start=1

-- 设置主从关系
CHANGE MASTER TO
    MASTER_HOST='master_host',
    MASTER_USER='repl_user',
    MASTER_PASSWORD='repl_password',
    MASTER_AUTO_POSITION=1;
START SLAVE;
```

#### GTID复制

```sql
-- 主服务器配置
[mysqld]
server-id=1
log-bin=mysql-bin
binlog-format=ROW
gtid-mode=ON
enforce-gtid-consistency=1
log-slave-updates=1

-- 从服务器配置
[mysqld]
server-id=2
gtid-mode=ON
enforce-gtid-consistency=1
log-slave-updates=1
master-info-repository=TABLE
relay-log-info-repository=TABLE

-- 使用GTID设置主从
CHANGE MASTER TO
    MASTER_HOST='master_host',
    MASTER_USER='repl_user',
    MASTER_PASSWORD='repl_password',
    MASTER_AUTO_POSITION=1;
START SLAVE;
```

#### 半同步复制

```sql
-- 主服务器安装插件
INSTALL PLUGIN rpl_semi_sync_master SONAME 'semisync_master.so';
SET GLOBAL rpl_semi_sync_master_enabled = 1;
SET GLOBAL rpl_semi_sync_master_timeout = 1000;  -- 1秒超时

-- 从服务器安装插件
INSTALL PLUGIN rpl_semi_sync_slave SONAME 'semisync_slave.so';
SET GLOBAL rpl_semi_sync_slave_enabled = 1;

-- 监控半同步复制
SHOW STATUS LIKE 'Rpl_semi_sync%';
```

### 2.2 基于集群的高可用

#### MySQL Group Replication

```sql
-- 所有节点配置
[mysqld]
server-id=1  # 每个节点不同
gtid-mode=ON
enforce-gtid-consistency=1
log-slave-updates=1
binlog-format=ROW
binlog-checksum=NONE
master-info-repository=TABLE
relay-log-info-repository=TABLE
relay-log-recovery=ON
transaction-write-set-extraction=XXHASH64
loose-group-replication-start-on-boot=off
loose-group-replication-local-address=192.168.1.101:33061  # 每个节点不同
loose-group-replication-group-seeds=192.168.1.101:33061,192.168.1.102:33061,192.168.1.103:33061
loose-group-replication-single-primary-mode=ON
loose-group-replication-ip-whitelist=192.168.1.0/24

-- 在第一个节点启动组复制
SET GLOBAL group_replication_bootstrap_group=ON;
START GROUP_REPLICATION;
SET GLOBAL group_replication_bootstrap_group=OFF;

-- 在其他节点加入组
START GROUP_REPLICATION;

-- 监控组复制状态
SELECT * FROM performance_schema.replication_group_members;
SELECT * FROM performance_schema.replication_group_member_stats;
```

#### MySQL NDB Cluster

```
# 管理节点配置
[ndbd default]
NoOfReplicas=2

[ndb_mgmd]
NodeId=1
HostName=192.168.1.101
DataDir=/var/lib/mysql-cluster

[ndbd]
NodeId=2
HostName=192.168.1.102
DataDir=/var/lib/mysql-cluster

[ndbd]
NodeId=3
HostName=192.168.1.103
DataDir=/var/lib/mysql-cluster

[mysqld]
NodeId=4
HostName=192.168.1.104
```

### 2.3 基于第三方的高可用

#### MHA (Master High Availability)

```bash
# 安装MHA Manager
yum install -y mha4mysql-manager

# 安装MHA Node
yum install -y mha4mysql-node

# MHA配置文件
[server default]
user=mha_manager
password=mha_password
manager_workdir=/var/log/mha/app1
manager_log=/var/log/mha/app1/app1.log
remote_workdir=/var/log/mha/app1
ssh_user=root
repl_user=repl_user
repl_password=repl_password
ping_interval=1

[server1]
hostname=192.168.1.101
candidate_master=1

[server2]
hostname=192.168.1.102
candidate_master=1

[server3]
hostname=192.168.1.103
no_master=1

# 启动MHA Manager
nohup masterha_manager --conf=/etc/masterha/app1.cnf > /var/log/mha/app1/manager.log 2>&1 &
```

#### Orchestrator

```json
// Orchestrator配置
{
    "ListenAddress": ":3000",
    "MySQLTopologyUser": "orchestrator",
    "MySQLTopologyPassword": "orchestrator_password",
    "MySQLTopologyCredentialsConfigFile": "/etc/orchestrator/.my.topology.cnf",
    
    "Raft": {
        "Enabled": true,
        "DataDir": "/var/lib/orchestrator",
        "BindAddress": "192.168.1.101:10008"
    },
    
    "Discovery": {
        "MaxQueryLength": 4000,
        "IterationFrequency": "5s"
    }
}
```

#### ProxySQL

```sql
-- 配置后端MySQL服务器
INSERT INTO mysql_servers (
    hostgroup_id, 
    hostname, 
    port, 
    weight, 
    max_connections, 
    max_replication_lag
) VALUES (
    10, 
    '192.168.1.101', 
    3306, 
    1, 
    1000, 
    10
);

INSERT INTO mysql_servers (
    hostgroup_id, 
    hostname, 
    port, 
    weight, 
    max_connections, 
    max_replication_lag
) VALUES (
    20, 
    '192.168.1.102', 
    3306, 
    1, 
    1000, 
    10
);

-- 配置查询规则
INSERT INTO mysql_query_rules (
    rule_id, 
    active, 
    match_digest, 
    destination_hostgroup, 
    apply
) VALUES (
    1, 
    1, 
    '^SELECT.*FOR UPDATE', 
    10, 
    1
);

INSERT INTO mysql_query_rules (
    rule_id, 
    active, 
    match_digest, 
    destination_hostgroup, 
    apply
) VALUES (
    2, 
    1, 
    '^SELECT', 
    20, 
    1
);

-- 加载配置到运行时
LOAD MYSQL SERVERS TO RUNTIME;
SAVE MYSQL SERVERS TO DISK;

LOAD MYSQL QUERY RULES TO RUNTIME;
SAVE MYSQL QUERY RULES TO DISK;
```

## 3. 负载均衡技术

### 3.1 读写分离

#### 基于代理的读写分离

```python
# Python读写分离示例
import mysql.connector
from mysql.connector import pooling

class ReadWriteRouter:
    def __init__(self, master_config, slave_configs):
        self.master_config = master_config
        self.slave_configs = slave_configs
        self.master_pool = pooling.MySQLConnectionPool(
            pool_name="master_pool",
            pool_size=5,
            **master_config
        )
        self.slave_pools = []
        for i, config in enumerate(slave_configs):
            pool = pooling.MySQLConnectionPool(
                pool_name=f"slave_pool_{i}",
                pool_size=5,
                **config
            )
            self.slave_pools.append(pool)
    
    def get_master_connection(self):
        """获取主库连接"""
        return self.master_pool.get_connection()
    
    def get_slave_connection(self):
        """获取从库连接（轮询）"""
        import random
        pool = random.choice(self.slave_pools)
        return pool.get_connection()
    
    def execute_read(self, query, params=None):
        """执行读操作"""
        conn = None
        cursor = None
        try:
            conn = self.get_slave_connection()
            cursor = conn.cursor(dictionary=True)
            cursor.execute(query, params)
            result = cursor.fetchall()
            return result
        finally:
            if cursor:
                cursor.close()
            if conn:
                conn.close()
    
    def execute_write(self, query, params=None):
        """执行写操作"""
        conn = None
        cursor = None
        try:
            conn = self.get_master_connection()
            cursor = conn.cursor(dictionary=True)
            cursor.execute(query, params)
            conn.commit()
            return cursor.lastrowid if cursor.lastrowid else cursor.rowcount
        finally:
            if cursor:
                cursor.close()
            if conn:
                conn.close()
```

#### 基于框架的读写分离

```java
// Java Spring Boot读写分离配置
@Configuration
public class DataSourceConfig {
    
    @Bean
    @Primary
    public DataSource masterDataSource() {
        return DataSourceBuilder.create()
                .url("jdbc:mysql://master-host:3306/db")
                .username("user")
                .password("password")
                .build();
    }
    
    @Bean
    public DataSource slaveDataSource() {
        return DataSourceBuilder.create()
                .url("jdbc:mysql://slave-host:3306/db")
                .username("user")
                .password("password")
                .build();
    }
    
    @Bean
    public DataSource routingDataSource() {
        RoutingDataSource routingDataSource = new RoutingDataSource();
        Map<Object, Object> dataSourceMap = new HashMap<>();
        dataSourceMap.put("master", masterDataSource());
        dataSourceMap.put("slave", slaveDataSource());
        routingDataSource.setTargetDataSources(dataSourceMap);
        routingDataSource.setDefaultTargetDataSource(masterDataSource());
        return routingDataSource;
    }
}

// 读写分离注解
@Target({ElementType.METHOD, ElementType.TYPE})
@Retention(RetentionPolicy.RUNTIME)
public @interface ReadOnly {
}

// 动态数据源路由
public class DynamicDataSource extends AbstractRoutingDataSource {
    @Override
    protected Object determineCurrentLookupKey() {
        return DataSourceContextHolder.getDataSourceType();
    }
}

// AOP切面
@Aspect
@Component
public class DataSourceAspect {
    
    @Before("@annotation(readOnly)")
    public void setReadDataSource(ReadOnly readOnly) {
        DataSourceContextHolder.setDataSourceType("slave");
    }
    
    @After("@annotation(readOnly)")
    public void clearDataSource(ReadOnly readOnly) {
        DataSourceContextHolder.clearDataSourceType();
    }
}
```

### 3.2 分片负载均衡

#### 基于哈希的分片

```python
class HashShardingRouter:
    def __init__(self, shard_configs):
        self.shard_configs = shard_configs
        self.shard_count = len(shard_configs)
        self.connections = []
        for config in shard_configs:
            conn = mysql.connector.connect(**config)
            self.connections.append(conn)
    
    def get_shard(self, key):
        """根据键值计算分片"""
        hash_value = int(hashlib.md5(key.encode()).hexdigest(), 16)
        shard_index = hash_value % self.shard_count
        return shard_index
    
    def execute_query(self, query, params=None, sharding_key=None):
        """执行查询（需要提供分片键）"""
        if sharding_key is None:
            raise ValueError("必须提供分片键")
        
        shard_index = self.get_shard(sharding_key)
        conn = self.connections[shard_index]
        cursor = conn.cursor(dictionary=True)
        cursor.execute(query, params)
        result = cursor.fetchall()
        cursor.close()
        return result
    
    def execute_cross_shard_query(self, query, params=None):
        """执行跨分片查询"""
        results = []
        for conn in self.connections:
            cursor = conn.cursor(dictionary=True)
            cursor.execute(query, params)
            results.extend(cursor.fetchall())
            cursor.close()
        return results
```

#### 基于范围的分片

```python
class RangeShardingRouter:
    def __init__(self, shard_configs, ranges):
        self.shard_configs = shard_configs
        self.ranges = ranges  # [(min_val, max_val), ...]
        self.connections = []
        for config in shard_configs:
            conn = mysql.connector.connect(**config)
            self.connections.append(conn)
    
    def get_shard(self, value):
        """根据值范围计算分片"""
        for i, (min_val, max_val) in enumerate(self.ranges):
            if min_val <= value <= max_val:
                return i
        return len(self.ranges) - 1  # 默认最后一个分片
    
    def execute_query(self, query, params=None, range_key=None):
        """执行查询（需要提供范围键）"""
        if range_key is None:
            raise ValueError("必须提供范围键")
        
        shard_index = self.get_shard(range_key)
        conn = self.connections[shard_index]
        cursor = conn.cursor(dictionary=True)
        cursor.execute(query, params)
        result = cursor.fetchall()
        cursor.close()
        return result
```

### 3.3 连接池管理

#### HikariCP配置

```properties
# HikariCP连接池配置
spring.datasource.hikari.maximum-pool-size=20
spring.datasource.hikari.minimum-idle=10
spring.datasource.hikari.idle-timeout=30000
spring.datasource.hikari.max-lifetime=1800000
spring.datasource.hikari.connection-timeout=30000
spring.datasource.hikari.connection-test-query=SELECT 1

# 多数据源配置
spring.datasource.master.hikari.maximum-pool-size=20
spring.datasource.slave.hikari.maximum-pool-size=40
```

#### Python连接池配置

```python
from mysql.connector import pooling

# 创建连接池
dbconfig = {
    "host": "localhost",
    "user": "user",
    "password": "password",
    "database": "database"
}

# 主库连接池
master_pool = pooling.MySQLConnectionPool(
    pool_name="master_pool",
    pool_size=10,
    **dbconfig
)

# 从库连接池
slave_pool = pooling.MySQLConnectionPool(
    pool_name="slave_pool",
    pool_size=20,
    **dbconfig
)

# 使用连接池
def execute_query(query, is_write=False):
    pool = master_pool if is_write else slave_pool
    conn = pool.get_connection()
    try:
        cursor = conn.cursor(dictionary=True)
        cursor.execute(query)
        result = cursor.fetchall()
        return result
    finally:
        cursor.close()
        conn.close()
```

## 4. 健康检查与监控

### 4.1 数据库健康检查

#### 基础健康检查

```python
import mysql.connector
import time
import logging

class MySQLHealthChecker:
    def __init__(self, config):
        self.config = config
        self.logger = logging.getLogger(__name__)
    
    def check_connection(self):
        """检查数据库连接"""
        try:
            conn = mysql.connector.connect(**self.config)
            conn.close()
            return True, "连接正常"
        except mysql.connector.Error as e:
            return False, f"连接失败: {e}"
    
    def check_query_performance(self):
        """检查查询性能"""
        try:
            conn = mysql.connector.connect(**self.config)
            cursor = conn.cursor()
            
            # 执行简单查询测试性能
            start_time = time.time()
            cursor.execute("SELECT 1")
            cursor.fetchone()
            elapsed_time = time.time() - start_time
            
            cursor.close()
            conn.close()
            
            if elapsed_time < 0.1:  # 100ms以内认为正常
                return True, f"查询性能正常: {elapsed_time:.4f}秒"
            else:
                return False, f"查询性能异常: {elapsed_time:.4f}秒"
        except Exception as e:
            return False, f"性能检查失败: {e}"
    
    def check_replication_lag(self):
        """检查复制延迟"""
        try:
            conn = mysql.connector.connect(**self.config)
            cursor = conn.cursor(dictionary=True)
            
            cursor.execute("SHOW SLAVE STATUS")
            slave_status = cursor.fetchone()
            
            cursor.close()
            conn.close()
            
            if not slave_status:
                return True, "未配置复制"
            
            lag = slave_status.get('Seconds_Behind_Master')
            if lag is None:
                return False, "复制未运行"
            elif lag <= 5:  # 5秒以内认为正常
                return True, f"复制延迟正常: {lag}秒"
            else:
                return False, f"复制延迟过大: {lag}秒"
        except Exception as e:
            return False, f"复制检查失败: {e}"
    
    def comprehensive_check(self):
        """综合健康检查"""
        results = {
            'connection': self.check_connection(),
            'performance': self.check_query_performance(),
            'replication': self.check_replication_lag()
        }
        
        all_healthy = all(result[0] for result in results.values())
        
        return all_healthy, results
```

#### 服务发现集成

```python
import consul

class MySQLServiceDiscovery:
    def __init__(self, consul_host='localhost', consul_port=8500):
        self.consul = consul.Consul(host=consul_host, port=consul_port)
    
    def register_service(self, service_id, service_name, address, port, tags=None):
        """注册MySQL服务"""
        self.consul.agent.service.register(
            name=service_name,
            service_id=service_id,
            address=address,
            port=port,
            tags=tags or [],
            check=consul.Check.mysql(
                host=address,
                port=port,
                interval="10s",
                timeout="5s"
            )
        )
    
    def discover_services(self, service_name):
        """发现MySQL服务"""
        services = self.consul.health.service(service_name, passing=True)[1]
        return [{"id": s['Service']['ID'], 
                "address": s['Service']['Address'], 
                "port": s['Service']['Port']} 
                for s in services]
    
    def deregister_service(self, service_id):
        """注销服务"""
        self.consul.agent.service.deregister(service_id)
```

### 4.2 监控指标收集

#### Prometheus监控配置

```yaml
# Prometheus配置
global:
  scrape_interval: 15s

scrape_configs:
  - job_name: 'mysql'
    static_configs:
      - targets: ['localhost:9104']
    relabel_configs:
      - source_labels: [__address__]
        target_label: __param_target
      - source_labels: [__param_target]
        target_label: instance
      - target_label: __address__
        replacement: 'localhost:9104'
```

#### MySQL监控指标

```sql
-- 创建监控用户
CREATE USER 'exporter'@'localhost' IDENTIFIED BY 'exporter_password';
GRANT PROCESS, REPLICATION CLIENT, SELECT ON *.* TO 'exporter'@'localhost';

-- 查询数据库状态
SELECT 
    VARIABLE_NAME, 
    VARIABLE_VALUE 
FROM performance_schema.global_status 
WHERE VARIABLE_NAME IN (
    'Connections',
    'Threads_connected',
    'Threads_running',
    'QPS',
    'TPS',
    'Innodb_buffer_pool_read_requests',
    'Innodb_buffer_pool_reads',
    'Innodb_row_lock_waits',
    'Innodb_row_lock_time'
);

-- 查询表大小
SELECT 
    table_schema,
    table_name,
    ROUND(((data_length + index_length) / 1024 / 1024), 2) AS size_mb
FROM information_schema.tables 
WHERE table_schema = 'your_database'
ORDER BY size_mb DESC;

-- 查询慢查询
SELECT 
    start_time,
    query_time,
    lock_time,
    rows_sent,
    rows_examined,
    sql_text
FROM mysql.slow_log
ORDER BY start_time DESC
LIMIT 10;
```

### 4.3 告警规则

#### Prometheus告警规则

```yaml
groups:
  - name: mysql
    rules:
      - alert: MySQLDown
        expr: mysql_up == 0
        for: 1m
        labels:
          severity: critical
        annotations:
          summary: "MySQL实例宕机"
          description: "MySQL实例 {{ $labels.instance }} 已宕机超过1分钟"
      
      - alert: MySQLHighConnections
        expr: mysql_global_status_threads_connected / mysql_global_variables_max_connections * 100 > 80
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "MySQL连接数过高"
          description: "MySQL实例 {{ $labels.instance }} 连接数使用率超过80%"
      
      - alert: MySQLReplicationLag
        expr: mysql_slave_lag_seconds > 30
        for: 2m
        labels:
          severity: warning
        annotations:
          summary: "MySQL复制延迟"
          description: "MySQL实例 {{ $labels.instance }} 复制延迟为 {{ $value }} 秒"
      
      - alert: MySQLSlowQueries
        expr: rate(mysql_global_status_slow_queries[5m]) > 0.1
        for: 2m
        labels:
          severity: warning
        annotations:
          summary: "MySQL慢查询增加"
          description: "MySQL实例 {{ $labels.instance }} 慢查询频率为 {{ $value }} queries/second"
```

## 5. 故障转移

### 5.1 自动故障转移

#### MHA故障转移

```bash
# MHA监控脚本
#!/bin/bash

# 检查主服务器状态
check_master() {
    local master_host=$1
    local master_port=$2
    
    mysql -h $master_host -P $master_port -e "SELECT 1" > /dev/null 2>&1
    return $?
}

# 执行故障转移
do_failover() {
    local new_master=$1
    
    echo "开始故障转移到 $new_master"
    
    # 停止其他从服务器的复制
    for slave in ${SLAVE_LIST[@]}; do
        if [ "$slave" != "$new_master" ]; then
            mysql -h $slave -e "STOP SLAVE"
        fi
    done
    
    # 提升新主服务器
    mysql -h $new_master -e "STOP SLAVE"
    mysql -h $new_master -e "RESET MASTER"
    mysql -h $new_master -e "SET GLOBAL read_only=0"
    
    # 重新配置其他从服务器指向新主服务器
    for slave in ${SLAVE_LIST[@]}; do
        if [ "$slave" != "$new_master" ]; then
            mysql -h $slave -e "
                CHANGE MASTER TO 
                    MASTER_HOST='$new_master',
                    MASTER_PORT=3306,
                    MASTER_AUTO_POSITION=1;
                START SLAVE;
            "
        fi
    done
    
    echo "故障转移完成"
}

# 主循环
while true; do
    if ! check_master $MASTER_HOST $MASTER_PORT; then
        # 主服务器故障，选择新的主服务器
        NEW_MASTER=${SLAVE_LIST[0]}  # 简化选择第一个从服务器
        do_failover $NEW_MASTER
        break
    fi
    
    sleep 10
done
```

#### Orchestrator自动故障转移

```go
// Orchestrator故障转移示例代码
package main

import (
    "fmt"
    "log"
    "time"
)

type OrchestratorClient struct {
    BaseURL string
    APIKey  string
}

func (c *OrchestratorClient) DetectFailover(clusterName string) (bool, error) {
    // 获取集群拓扑
    topology, err := c.GetClusterTopology(clusterName)
    if err != nil {
        return false, err
    }
    
    // 检查主服务器状态
    master := topology.GetMaster()
    if !master.IsUp {
        return true, nil
    }
    
    return false, nil
}

func (c *OrchestratorClient) PerformFailover(clusterName string) error {
    // 获取主服务器候选
    candidates, err := c.GetMasterCandidates(clusterName)
    if err != nil {
        return err
    }
    
    // 选择最佳候选
    bestCandidate := c.SelectBestCandidate(candidates)
    
    // 执行故障转移
    return c.GracefulMasterTakeover(clusterName, bestCandidate.Key)
}

func main() {
    client := OrchestratorClient{
        BaseURL: "http://orchestrator:3000",
        APIKey:  "your_api_key",
    }
    
    // 监控循环
    for {
        shouldFailover, err := client.DetectFailover("production-cluster")
        if err != nil {
            log.Printf("检测故障转移失败: %v", err)
            time.Sleep(10 * time.Second)
            continue
        }
        
        if shouldFailover {
            log.Println("检测到故障，开始故障转移")
            err = client.PerformFailover("production-cluster")
            if err != nil {
                log.Printf("故障转移失败: %v", err)
            } else {
                log.Println("故障转移成功")
                break
            }
        }
        
        time.Sleep(10 * time.Second)
    }
}
```

### 5.2 手动故障转移

#### 基于GTID的手动切换

```bash
#!/bin/bash
# 手动故障转移脚本

MASTER_HOST="master.example.com"
SLAVE_HOSTS=("slave1.example.com" "slave2.example.com")
NEW_MASTER="slave1.example.com"

# 1. 检查从服务器状态
check_slave_status() {
    local slave=$1
    echo "检查从服务器 $slave 的状态:"
    mysql -h $slave -e "SHOW SLAVE STATUS\G" | grep -E "(Slave_IO_Running|Slave_SQL_Running|Seconds_Behind_Master)"
}

# 2. 选择新的主服务器
select_new_master() {
    local best_slave=""
    local min_lag=999999
    
    for slave in "${SLAVE_HOSTS[@]}"; do
        lag=$(mysql -h $slave -e "SHOW SLAVE STATUS" | awk '/Seconds_Behind_Master/ {print $2}')
        if [[ $lag != "NULL" && $lag -lt $min_lag ]]; then
            min_lag=$lag
            best_slave=$slave
        fi
    done
    
    echo "最佳新主服务器: $best_slave (延迟: $min_lag 秒)"
    echo $best_slave
}

# 3. 停止从服务器复制
stop_slave_replication() {
    local slave=$1
    echo "停止从服务器 $slave 的复制:"
    mysql -h $slave -e "STOP SLAVE"
}

# 4. 提升从服务器为主服务器
promote_to_master() {
    local new_master=$1
    echo "提升 $new_master 为主服务器:"
    mysql -h $new_master -e "STOP SLAVE"
    mysql -h $new_master -e "RESET MASTER"
    mysql -h $new_master -e "SET GLOBAL read_only=0"
}

# 5. 重新配置从服务器指向新主服务器
reconfigure_slaves() {
    local new_master=$1
    local new_master_ip=$(dig +short $new_master)
    
    for slave in "${SLAVE_LIST[@]}"; do
        if [ "$slave" != "$new_master" ]; then
            echo "重新配置从服务器 $slave 指向新主服务器 $new_master:"
            mysql -h $slave -e "
                CHANGE MASTER TO 
                    MASTER_HOST='$new_master_ip',
                    MASTER_PORT=3306,
                    MASTER_AUTO_POSITION=1;
                START SLAVE;
            "
        fi
    done
}

# 执行故障转移流程
echo "开始手动故障转移..."

# 选择新主服务器
NEW_MASTER=$(select_new_master)
if [ -z "$NEW_MASTER" ]; then
    echo "无法选择新主服务器"
    exit 1
fi

# 停止所有从服务器的复制
for slave in "${SLAVE_HOSTS[@]}"; do
    check_slave_status $slave
    stop_slave_replication $slave
done

# 提升新主服务器
promote_to_master $NEW_MASTER

# 重新配置其他从服务器
reconfigure_slaves $NEW_MASTER

echo "故障转移完成"
```

### 5.3 切换后的验证

#### 数据一致性检查

```python
import mysql.connector
import hashlib

class ConsistencyChecker:
    def __init__(self, master_config, slave_config):
        self.master_config = master_config
        self.slave_config = slave_config
    
    def table_checksum(self, table_name, primary_key='id'):
        """计算表的校验和"""
        def calculate_checksum(config):
            conn = mysql.connector.connect(**config)
            cursor = conn.cursor()
            
            cursor.execute(f"""
                SELECT MD5(GROUP_CONCAT(
                    CONCAT_WS('|', {primary_key}, MD5(CONCAT_WS('|', *)))
                    ORDER BY {primary_key}
                )) as checksum
                FROM {table_name}
            """)
            
            result = cursor.fetchone()
            cursor.close()
            conn.close()
            return result[0]
        
        master_checksum = calculate_checksum(self.master_config)
        slave_checksum = calculate_checksum(self.slave_config)
        
        return master_checksum, slave_checksum
    
    def check_table_consistency(self, table_name):
        """检查表数据一致性"""
        master_checksum, slave_checksum = self.table_checksum(table_name)
        
        if master_checksum == slave_checksum:
            return True, "数据一致"
        else:
            return False, f"数据不一致: 主库={master_checksum}, 从库={slave_checksum}"
    
    def check_row_count(self, table_name):
        """检查表行数一致性"""
        def get_count(config):
            conn = mysql.connector.connect(**config)
            cursor = conn.cursor()
            
            cursor.execute(f"SELECT COUNT(*) FROM {table_name}")
            count = cursor.fetchone()[0]
            
            cursor.close()
            conn.close()
            return count
        
        master_count = get_count(self.master_config)
        slave_count = get_count(self.slave_config)
        
        if master_count == slave_count:
            return True, f"行数一致: {master_count}"
        else:
            return False, f"行数不一致: 主库={master_count}, 从库={slave_count}"
    
    def full_consistency_check(self, tables):
        """全面一致性检查"""
        results = []
        
        for table in tables:
            row_result = self.check_row_count(table)
            checksum_result = self.check_table_consistency(table)
            
            results.append({
                'table': table,
                'row_check': row_result,
                'checksum_check': checksum_result
            })
        
        return results
```

## 6. 高可用最佳实践

### 6.1 架构设计原则

1. **消除单点故障**：每个组件都应有冗余
2. **自动化管理**：减少人工干预，提高恢复速度
3. **多层防护**：从硬件到应用多层防护
4. **可扩展性**：支持水平和垂直扩展
5. **监控可视化**：全面监控系统状态

### 6.2 网络与安全

```bash
# 配置多路径网络
# 主服务器网络配置
auto eth0
iface eth0 inet static
    address 192.168.1.101
    netmask 255.255.255.0
    gateway 192.168.1.1

auto eth1
iface eth1 inet static
    address 192.168.2.101
    netmask 255.255.255.0

# 配置防火墙规则
iptables -A INPUT -p tcp --dport 3306 -m state --state NEW,ESTABLISHED -j ACCEPT
iptables -A INPUT -s 192.168.1.0/24 -p tcp --dport 3306 -j ACCEPT
iptables -A INPUT -s 192.168.2.0/24 -p tcp --dport 3306 -j ACCEPT
iptables -A INPUT -p tcp --dport 3306 -j DROP
```

### 6.3 数据保护

```sql
-- 启用二进制日志
SET GLOBAL log_bin = ON;
SET GLOBAL binlog_format = 'ROW';
SET GLOBAL sync_binlog = 1;
SET GLOBAL innodb_flush_log_at_trx_commit = 1;

-- 启用GTID
SET GLOBAL gtid_mode = ON;
SET GLOBAL enforce_gtid_consistency = 1;
SET GLOBAL log_slave_updates = 1;

-- 配置半同步复制
INSTALL PLUGIN rpl_semi_sync_master SONAME 'semisync_master.so';
SET GLOBAL rpl_semi_sync_master_enabled = 1;
SET GLOBAL rpl_semi_sync_master_timeout = 1000;

INSTALL PLUGIN rpl_semi_sync_slave SONAME 'semisync_slave.so';
SET GLOBAL rpl_semi_sync_slave_enabled = 1;
```

### 6.4 性能调优

```ini
[mysqld]
# 高可用相关的性能优化
innodb_flush_log_at_trx_commit = 1
innodb_buffer_pool_size = 8G
innodb_log_file_size = 512M
innodb_log_buffer_size = 64M
innodb_flush_method = O_DIRECT

# 复制优化
slave_parallel_workers = 8
slave_parallel_type = LOGICAL_CLOCK
slave_pending_jobs_size_max = 1073741824

# 连接池优化
max_connections = 2000
thread_cache_size = 100
table_open_cache = 4000
```

## 7. 故障排除

### 7.1 常见高可用问题

#### 复制中断

```bash
# 检查复制状态
mysql -e "SHOW SLAVE STATUS\G" | grep -E "(Slave_IO_Running|Slave_SQL_Running|Last_Error)"

# 常见错误解决
# 1. 主从数据不一致
SET GLOBAL sql_slave_skip_counter = 1;
START SLAVE;

# 2. 网络问题
CHANGE MASTER TO MASTER_HOST='new_master_host';

# 3. 凭证问题
CHANGE MASTER TO MASTER_PASSWORD='new_password';
```

#### 脑裂问题

```bash
# 检测脑裂
# 在所有节点执行，检查主服务器数量
mysql -e "SELECT @@global.server_id, @@global.read_only"

# 解决脑裂
# 1. 停止应用写入
# 2. 确定真正的主服务器
# 3. 在非主服务器上设置只读
SET GLOBAL read_only = 1;
# 4. 在从服务器上停止复制
STOP SLAVE;
```

#### 集群节点故障

```sql
-- 检查集群状态
SELECT * FROM performance_schema.replication_group_members;

-- 重新加入集群
STOP GROUP_REPLICATION;
START GROUP_REPLICATION;

-- 强制重新配置集群
SET GLOBAL group_replication_allow_local_disjoint_gtids_join=ON;
START GROUP_REPLICATION;
```

### 7.2 性能问题诊断

```sql
-- 检查慢查询
SELECT 
    start_time,
    query_time,
    sql_text
FROM mysql.slow_log
ORDER BY query_time DESC
LIMIT 10;

-- 检查锁等待
SELECT 
    r.trx_id waiting_trx_id,
    r.trx_mysql_thread_id waiting_thread,
    r.trx_query waiting_query,
    b.trx_id blocking_trx_id,
    b.trx_mysql_thread_id blocking_thread,
    b.trx_query blocking_query
FROM information_schema.innodb_lock_waits w
INNER JOIN information_schema.innodb_trx b ON b.trx_id = w.blocking_trx_id
INNER JOIN information_schema.innodb_trx r ON r.trx_id = w.requesting_trx_id;
```

## 8. 总结

本章详细介绍了MySQL高可用与负载均衡的各个方面，包括：

### 8.1 高可用性基础

- 理解了高可用性的概念和核心指标
- 掌握了不同的高可用架构模式
- 学会了故障类型和相应的应对策略

### 8.2 高可用解决方案

- 掌握了基于复制的高可用实现
- 理解了基于集群的高可用架构
- 学会了第三方高可用工具的使用

### 8.3 负载均衡技术

- 掌握了读写分离的实现方法
- 学会了分片负载均衡策略
- 理解了连接池管理的重要性

### 8.4 健康检查与监控

- 学会了数据库健康检查的方法
- 掌握了监控指标收集技术
- 理解了告警规则的配置

### 8.5 故障转移

- 掌握了自动和手动故障转移的实现
- 学会了切换后的数据一致性验证
- 理解了故障转移的最佳实践

通过本章的学习，您应该能够：

1. **设计高可用架构**，根据业务需求选择合适的高可用方案
2. **实现负载均衡**，通过读写分离和分片分散系统负载
3. **监控系统状态**，建立完善的监控和告警体系
4. **执行故障转移**，在系统故障时快速恢复服务
5. **保障数据一致性**，确保故障切换后的数据完整性
6. **处理常见问题**，诊断和解决高可用系统的常见故障

高可用与负载均衡是构建可靠数据库系统的关键。掌握这些知识和技术，将帮助您设计、构建和维护一个高可用、高性能的MySQL应用系统，确保业务的连续性和稳定性。