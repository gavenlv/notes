# 第9章 Doris实战应用

## 学习目标

- 掌握Doris在不同业务场景下的应用方法
- 学会设计Doris数据模型以适应特定业务需求
- 了解Doris与其他系统的集成方案
- 掌握Doris在大数据分析中的应用技巧
- 学会解决Doris在实际应用中的常见问题

## 1. 电商数据分析平台

### 1.1 业务场景

电商数据分析平台需要处理海量的用户行为数据、订单数据、商品数据等，支持多维度的实时分析和报表展示。

### 1.2 数据模型设计

#### 1.2.1 用户行为分析表

```sql
-- 用户行为表（使用Duplicate模型）
CREATE TABLE ecommerce.user_behavior (
    user_id BIGINT COMMENT '用户ID',
    session_id VARCHAR(64) COMMENT '会话ID',
    event_time DATETIME COMMENT '事件时间',
    event_type VARCHAR(20) COMMENT '事件类型',
    page_url VARCHAR(500) COMMENT '页面URL',
    referrer_url VARCHAR(500) COMMENT '来源URL',
    stay_time INT COMMENT '停留时间(秒)',
    device_type VARCHAR(20) COMMENT '设备类型',
    os_type VARCHAR(20) COMMENT '操作系统',
    browser VARCHAR(50) COMMENT '浏览器',
    ip_address VARCHAR(50) COMMENT 'IP地址',
    city VARCHAR(50) COMMENT '城市',
    province VARCHAR(50) COMMENT '省份',
    country VARCHAR(50) COMMENT '国家'
)
DUPLICATE KEY(user_id, event_time)
PARTITION BY RANGE(event_time) (
    PARTITION p20231101 VALUES LESS THAN ("2023-11-02"),
    PARTITION p20231102 VALUES LESS THAN ("2023-11-03"),
    PARTITION p20231103 VALUES LESS THAN ("2023-11-04"),
    PARTITION p20231104 VALUES LESS THAN ("2023-11-05"),
    PARTITION p20231105 VALUES LESS THAN ("2023-11-06")
)
DISTRIBUTED BY HASH(user_id) BUCKETS 32
PROPERTIES (
    "replication_num" = "3",
    "dynamic_partition.enable" = "true",
    "dynamic_partition.time_unit" = "DAY",
    "dynamic_partition.start" = "-30",
    "dynamic_partition.end" = "7",
    "dynamic_partition.prefix" = "p",
    "compression" = "LZ4",
    "bloom_filter_columns" = "user_id, session_id"
);
```

#### 1.2.2 订单表

```sql
-- 订单表（使用Aggregate模型）
CREATE TABLE ecommerce.orders (
    order_id BIGINT COMMENT '订单ID',
    user_id BIGINT COMMENT '用户ID',
    order_status VARCHAR(20) COMMENT '订单状态',
    order_amount DECIMAL(18, 2) COMMENT '订单金额',
    discount_amount DECIMAL(18, 2) COMMENT '折扣金额',
    payment_amount DECIMAL(18, 2) COMMENT '支付金额',
    payment_method VARCHAR(20) COMMENT '支付方式',
    order_time DATETIME COMMENT '下单时间',
    payment_time DATETIME COMMENT '支付时间',
    delivery_time DATETIME COMMENT '发货时间',
    finish_time DATETIME COMMENT '完成时间',
    cancel_time DATETIME COMMENT '取消时间',
    refund_amount DECIMAL(18, 2) SUM DEFAULT "0" COMMENT '退款金额',
    order_count INT SUM DEFAULT "1" COMMENT '订单数量'
)
AGGREGATE KEY(order_id, user_id, order_status, order_amount, discount_amount, 
              payment_amount, payment_method, order_time, payment_time, 
              delivery_time, finish_time, cancel_time)
PARTITION BY RANGE(order_time) (
    PARTITION p20231101 VALUES LESS THAN ("2023-11-02"),
    PARTITION p20231102 VALUES LESS THAN ("2023-11-03"),
    PARTITION p20231103 VALUES LESS THAN ("2023-11-04"),
    PARTITION p20231104 VALUES LESS THAN ("2023-11-05"),
    PARTITION p20231105 VALUES LESS THAN ("2023-11-06")
)
DISTRIBUTED BY HASH(user_id) BUCKETS 32
PROPERTIES (
    "replication_num" = "3",
    "dynamic_partition.enable" = "true",
    "dynamic_partition.time_unit" = "DAY",
    "dynamic_partition.start" = "-30",
    "dynamic_partition.end" = "7",
    "dynamic_partition.prefix" = "p",
    "compression" = "LZ4",
    "bloom_filter_columns" = "user_id, order_id"
);
```

#### 1.2.3 商品表

```sql
-- 商品表（使用Unique模型）
CREATE TABLE ecommerce.products (
    product_id BIGINT COMMENT '商品ID',
    product_name VARCHAR(200) COMMENT '商品名称',
    category_id BIGINT COMMENT '类目ID',
    category_name VARCHAR(100) COMMENT '类目名称',
    brand_id BIGINT COMMENT '品牌ID',
    brand_name VARCHAR(100) COMMENT '品牌名称',
    price DECIMAL(18, 2) COMMENT '价格',
    cost_price DECIMAL(18, 2) COMMENT '成本价',
    stock INT COMMENT '库存',
    status VARCHAR(20) COMMENT '状态',
    create_time DATETIME COMMENT '创建时间',
    update_time DATETIME COMMENT '更新时间'
)
UNIQUE KEY(product_id)
DISTRIBUTED BY HASH(product_id) BUCKETS 16
PROPERTIES (
    "replication_num" = "3",
    "compression" = "LZ4",
    "bloom_filter_columns" = "product_id, category_id, brand_id"
);
```

### 1.3 实时分析应用

#### 1.3.1 用户行为分析

```sql
-- 创建用户行为分析物化视图
CREATE MATERIALIZED VIEW ecommerce.user_behavior_daily_stats AS
SELECT 
    DATE(event_time) AS event_date,
    event_type,
    device_type,
    os_type,
    city,
    COUNT(DISTINCT user_id) AS unique_users,
    COUNT(DISTINCT session_id) AS unique_sessions,
    COUNT(*) AS event_count,
    SUM(stay_time) AS total_stay_time,
    AVG(stay_time) AS avg_stay_time
FROM ecommerce.user_behavior 
GROUP BY DATE(event_time), event_type, device_type, os_type, city;

-- 用户行为分析查询
SELECT 
    event_date,
    event_type,
    device_type,
    unique_users,
    unique_sessions,
    event_count,
    avg_stay_time
FROM ecommerce.user_behavior_daily_stats 
WHERE event_date >= '2023-11-01' AND event_date <= '2023-11-05'
ORDER BY event_date, event_count DESC;
```

#### 1.3.2 订单分析

```sql
-- 创建订单分析物化视图
CREATE MATERIALIZED VIEW ecommerce.order_daily_stats AS
SELECT 
    DATE(order_time) AS order_date,
    order_status,
    payment_method,
    COUNT(DISTINCT user_id) AS unique_users,
    COUNT(*) AS order_count,
    SUM(order_amount) AS total_order_amount,
    SUM(payment_amount) AS total_payment_amount,
    AVG(order_amount) AS avg_order_amount,
    SUM(refund_amount) AS total_refund_amount
FROM ecommerce.orders 
GROUP BY DATE(order_time), order_status, payment_method;

-- 订单分析查询
SELECT 
    order_date,
    order_status,
    order_count,
    total_order_amount,
    avg_order_amount,
    total_refund_amount
FROM ecommerce.order_daily_stats 
WHERE order_date >= '2023-11-01' AND order_date <= '2023-11-05'
  AND order_status IN ('pending', 'paid', 'shipped', 'completed', 'cancelled')
ORDER BY order_date, order_status;
```

#### 1.3.3 商品销售分析

```sql
-- 创建商品销售分析物化视图
CREATE MATERIALIZED VIEW ecommerce.product_sales_stats AS
SELECT 
    DATE(o.order_time) AS order_date,
    p.category_id,
    p.category_name,
    p.brand_id,
    p.brand_name,
    COUNT(DISTINCT o.user_id) AS unique_buyers,
    COUNT(*) AS order_count,
    SUM(o.payment_amount) AS total_sales_amount,
    AVG(o.payment_amount) AS avg_order_amount
FROM ecommerce.orders o
JOIN ecommerce.order_items oi ON o.order_id = oi.order_id
JOIN ecommerce.products p ON oi.product_id = p.product_id
WHERE o.order_status = 'completed'
GROUP BY DATE(o.order_time), p.category_id, p.category_name, p.brand_id, p.brand_name;

-- 商品销售分析查询
SELECT 
    order_date,
    category_name,
    brand_name,
    order_count,
    total_sales_amount,
    avg_order_amount
FROM ecommerce.product_sales_stats 
WHERE order_date >= '2023-11-01' AND order_date <= '2023-11-05'
ORDER BY order_date, total_sales_amount DESC;
```

### 1.4 实时数据导入

#### 1.4.1 Stream Load导入用户行为数据

```bash
# 使用Stream Load导入用户行为数据
curl --location-trusted -u root: \
    -H "column_separator:," \
    -H "columns:user_id,session_id,event_time,event_type,page_url,referrer_url,stay_time,device_type,os_type,browser,ip_address,city,province,country" \
    -T user_behavior.csv \
    http://fe_host:http_port/api/ecommerce/user_behavior/_stream_load
```

#### 1.4.2 Routine Load导入实时订单数据

```sql
-- 创建Kafka数据源
CREATE EXTERNAL TABLE ecommerce.kafka_orders (
    order_id BIGINT,
    user_id BIGINT,
    order_status VARCHAR(20),
    order_amount DECIMAL(18, 2),
    discount_amount DECIMAL(18, 2),
    payment_amount DECIMAL(18, 2),
    payment_method VARCHAR(20),
    order_time VARCHAR(50),
    payment_time VARCHAR(50),
    delivery_time VARCHAR(50),
    finish_time VARCHAR(50),
    cancel_time VARCHAR(50)
)
ENGINE=KAFKA
PROPERTIES (
    "kafka_broker_list" = "kafka_host1:9092,kafka_host2:9092,kafka_host3:9092",
    "kafka_topic" = "orders",
    "property.group.id" = "doris_orders_group",
    "property.client.id" = "doris_orders_client"
);

-- 创建Routine Load任务
CREATE ROUTINE LOAD ecommerce.orders_load ON ecommerce.orders
COLUMNS(user_id, order_id, order_status, order_amount, discount_amount, 
        payment_amount, payment_method, order_time, payment_time, 
        delivery_time, finish_time, cancel_time)
PROPERTIES (
    "desired_concurrent_number" = "3",
    "max_batch_interval" = "20",
    "max_batch_rows" = "300000",
    "max_batch_size" = "100000000"
)
FROM KAFKA (
    "kafka_broker_list" = "kafka_host1:9092,kafka_host2:9092,kafka_host3:9092",
    "kafka_topic" = "orders",
    "property.group.id" = "doris_orders_group",
    "property.client.id" = "doris_orders_client"
);
```

## 2. 用户画像系统

### 2.1 业务场景

用户画像系统需要整合用户的各种行为数据、交易数据、社交数据等，构建全面的用户标签体系，支持精准营销、个性化推荐等应用。

### 2.2 数据模型设计

#### 2.2.1 用户基础信息表

```sql
-- 用户基础信息表（使用Unique模型）
CREATE TABLE user_profile.user_basic_info (
    user_id BIGINT COMMENT '用户ID',
    user_name VARCHAR(50) COMMENT '用户名',
    gender VARCHAR(10) COMMENT '性别',
    birthday DATE COMMENT '生日',
    age INT COMMENT '年龄',
    city VARCHAR(50) COMMENT '城市',
    province VARCHAR(50) COMMENT '省份',
    country VARCHAR(50) COMMENT '国家',
    registration_channel VARCHAR(50) COMMENT '注册渠道',
    registration_time DATETIME COMMENT '注册时间',
    last_login_time DATETIME COMMENT '最后登录时间',
    user_level VARCHAR(20) COMMENT '用户等级',
    is_vip BOOLEAN COMMENT '是否VIP',
    update_time DATETIME COMMENT '更新时间'
)
UNIQUE KEY(user_id)
DISTRIBUTED BY HASH(user_id) BUCKETS 16
PROPERTIES (
    "replication_num" = "3",
    "compression" = "LZ4",
    "bloom_filter_columns" = "user_id"
);
```

#### 2.2.2 用户标签表

```sql
-- 用户标签表（使用Aggregate模型）
CREATE TABLE user_profile.user_tags (
    user_id BIGINT COMMENT '用户ID',
    tag_type VARCHAR(50) COMMENT '标签类型',
    tag_name VARCHAR(100) COMMENT '标签名称',
    tag_value VARCHAR(200) COMMENT '标签值',
    weight DECIMAL(5, 2) COMMENT '权重',
    confidence DECIMAL(5, 2) COMMENT '置信度',
    source VARCHAR(50) COMMENT '来源',
    create_time DATETIME COMMENT '创建时间',
    update_time DATETIME COMMENT '更新时间',
    tag_count INT SUM DEFAULT "1" COMMENT '标签计数'
)
AGGREGATE KEY(user_id, tag_type, tag_name, tag_value, weight, confidence, source, create_time, update_time)
DISTRIBUTED BY HASH(user_id) BUCKETS 16
PROPERTIES (
    "replication_num" = "3",
    "compression" = "LZ4",
    "bloom_filter_columns" = "user_id, tag_type"
);
```

#### 2.2.3 用户行为聚合表

```sql
-- 用户行为聚合表（使用Aggregate模型）
CREATE TABLE user_profile.user_behavior_agg (
    user_id BIGINT COMMENT '用户ID',
    stat_date DATE COMMENT '统计日期',
    active_days INT SUM DEFAULT "0" COMMENT '活跃天数',
    login_count INT SUM DEFAULT "0" COMMENT '登录次数',
    page_view_count INT SUM DEFAULT "0" COMMENT '页面浏览次数',
    stay_time INT SUM DEFAULT "0" COMMENT '停留时间(秒)',
    order_count INT SUM DEFAULT "0" COMMENT '订单数量',
    order_amount DECIMAL(18, 2) SUM DEFAULT "0" COMMENT '订单金额',
    last_active_time DATETIME REPLACE COMMENT '最后活跃时间'
)
AGGREGATE KEY(user_id, stat_date)
PARTITION BY RANGE(stat_date) (
    PARTITION p20231101 VALUES LESS THAN ("2023-11-02"),
    PARTITION p20231102 VALUES LESS THAN ("2023-11-03"),
    PARTITION p20231103 VALUES LESS THAN ("2023-11-04"),
    PARTITION p20231104 VALUES LESS THAN ("2023-11-05"),
    PARTITION p20231105 VALUES LESS THAN ("2023-11-06")
)
DISTRIBUTED BY HASH(user_id) BUCKETS 16
PROPERTIES (
    "replication_num" = "3",
    "dynamic_partition.enable" = "true",
    "dynamic_partition.time_unit" = "DAY",
    "dynamic_partition.start" = "-30",
    "dynamic_partition.end" = "7",
    "dynamic_partition.prefix" = "p",
    "compression" = "LZ4",
    "bloom_filter_columns" = "user_id"
);
```

### 2.3 标签计算与应用

#### 2.3.1 基础标签计算

```sql
-- 计算用户活跃度标签
INSERT INTO user_profile.user_tags
SELECT 
    user_id,
    'activity' AS tag_type,
    CASE 
        WHEN active_days >= 25 THEN 'high_activity'
        WHEN active_days >= 15 THEN 'medium_activity'
        WHEN active_days >= 5 THEN 'low_activity'
        ELSE 'inactive'
    END AS tag_name,
    active_days AS tag_value,
    CASE 
        WHEN active_days >= 25 THEN 0.9
        WHEN active_days >= 15 THEN 0.7
        WHEN active_days >= 5 THEN 0.5
        ELSE 0.2
    END AS weight,
    0.9 AS confidence,
    'behavior_analysis' AS source,
    NOW() AS create_time,
    NOW() AS update_time,
    1 AS tag_count
FROM user_profile.user_behavior_agg 
WHERE stat_date >= DATE_SUB(CURRENT_DATE(), INTERVAL 30 DAY)
GROUP BY user_id;

-- 计算用户消费能力标签
INSERT INTO user_profile.user_tags
SELECT 
    user_id,
    'consumption' AS tag_type,
    CASE 
        WHEN avg_order_amount >= 1000 THEN 'high_consumption'
        WHEN avg_order_amount >= 500 THEN 'medium_consumption'
        WHEN avg_order_amount >= 100 THEN 'low_consumption'
        ELSE 'minimal_consumption'
    END AS tag_name,
    avg_order_amount AS tag_value,
    CASE 
        WHEN avg_order_amount >= 1000 THEN 0.9
        WHEN avg_order_amount >= 500 THEN 0.7
        WHEN avg_order_amount >= 100 THEN 0.5
        ELSE 0.2
    END AS weight,
    0.9 AS confidence,
    'order_analysis' AS source,
    NOW() AS create_time,
    NOW() AS update_time,
    1 AS tag_count
FROM (
    SELECT 
        user_id,
        SUM(order_amount) / COUNT(DISTINCT order_id) AS avg_order_amount
    FROM ecommerce.orders 
    WHERE order_time >= DATE_SUB(CURRENT_DATE(), INTERVAL 90 DAY)
      AND order_status = 'completed'
    GROUP BY user_id
) t;
```

#### 2.3.2 兴趣标签计算

```sql
-- 计算用户兴趣标签
INSERT INTO user_profile.user_tags
SELECT 
    user_id,
    'interest' AS tag_type,
    category_name AS tag_name,
    COUNT(*) AS tag_value,
    COUNT(*) * 1.0 / (SELECT COUNT(*) FROM ecommerce.orders o2 WHERE o2.user_id = o1.user_id AND o2.order_status = 'completed') AS weight,
    0.8 AS confidence,
    'behavior_analysis' AS source,
    NOW() AS create_time,
    NOW() AS update_time,
    1 AS tag_count
FROM ecommerce.orders o1
JOIN ecommerce.order_items oi ON o1.order_id = oi.order_id
JOIN ecommerce.products p ON oi.product_id = p.product_id
WHERE o1.order_time >= DATE_SUB(CURRENT_DATE(), INTERVAL 90 DAY)
  AND o1.order_status = 'completed'
GROUP BY user_id, category_name;
```

#### 2.3.3 用户画像查询

```sql
-- 查询用户完整画像
SELECT 
    b.user_id,
    b.user_name,
    b.gender,
    b.age,
    b.city,
    b.user_level,
    b.is_vip,
    GROUP_CONCAT(CASE WHEN t.tag_type = 'activity' THEN t.tag_name END) AS activity_tags,
    GROUP_CONCAT(CASE WHEN t.tag_type = 'consumption' THEN t.tag_name END) AS consumption_tags,
    GROUP_CONCAT(CASE WHEN t.tag_type = 'interest' THEN t.tag_name END) AS interest_tags
FROM user_profile.user_basic_info b
LEFT JOIN user_profile.user_tags t ON b.user_id = t.user_id
WHERE b.user_id = 1001
GROUP BY b.user_id, b.user_name, b.gender, b.age, b.city, b.user_level, b.is_vip;

-- 查询特定标签的用户群体
SELECT 
    b.user_id,
    b.user_name,
    b.city,
    b.user_level,
    t.tag_name,
    t.weight
FROM user_profile.user_basic_info b
JOIN user_profile.user_tags t ON b.user_id = t.user_id
WHERE t.tag_type = 'activity' AND t.tag_name = 'high_activity'
  AND b.city = 'Beijing'
ORDER BY t.weight DESC;
```

## 3. 实时监控大屏

### 3.1 业务场景

实时监控大屏需要展示实时业务指标，如实时销售额、实时订单量、实时用户数等，支持决策者实时了解业务状况。

### 3.2 数据模型设计

#### 3.2.1 实时指标表

```sql
-- 实时指标表（使用Aggregate模型）
CREATE TABLE realtime_dashboard.realtime_metrics (
    metric_time DATETIME COMMENT '指标时间',
    metric_name VARCHAR(100) COMMENT '指标名称',
    metric_dimension VARCHAR(100) COMMENT '指标维度',
    metric_value BIGINT SUM DEFAULT "0" COMMENT '指标值',
    metric_dimension2 VARCHAR(100) COMMENT '指标维度2',
    update_time DATETIME REPLACE COMMENT '更新时间'
)
AGGREGATE KEY(metric_time, metric_name, metric_dimension, metric_dimension2)
DISTRIBUTED BY HASH(metric_name) BUCKETS 16
PROPERTIES (
    "replication_num" = "3",
    "compression" = "LZ4",
    "bloom_filter_columns" = "metric_name"
);
```

#### 3.2.2 实时排名表

```sql
-- 实时排名表（使用Duplicate模型）
CREATE TABLE realtime_dashboard.realtime_ranking (
    rank_time DATETIME COMMENT '排名时间',
    rank_type VARCHAR(50) COMMENT '排名类型',
    item_id BIGINT COMMENT '项目ID',
    item_name VARCHAR(200) COMMENT '项目名称',
    rank_value BIGINT COMMENT '排名值',
    rank_position INT COMMENT '排名位置',
    extra_info VARCHAR(500) COMMENT '额外信息'
)
DUPLICATE KEY(rank_time, rank_type, item_id)
DISTRIBUTED BY HASH(rank_type) BUCKETS 16
PROPERTIES (
    "replication_num" = "3",
    "compression" = "LZ4",
    "bloom_filter_columns" = "rank_type"
);
```

### 3.3 实时指标计算

#### 3.3.1 实时销售额计算

```sql
-- 创建实时销售额物化视图
CREATE MATERIALIZED VIEW realtime_dashboard.realtime_sales AS
SELECT 
    DATE_FORMAT(order_time, '%Y-%m-%d %H:%i:00') AS metric_time,
    'sales_amount' AS metric_name,
    'total' AS metric_dimension,
    SUM(payment_amount) AS metric_value,
    '' AS metric_dimension2,
    NOW() AS update_time
FROM ecommerce.orders 
WHERE order_status = 'completed'
GROUP BY DATE_FORMAT(order_time, '%Y-%m-%d %H:%i:00');

-- 插入实时销售额数据
INSERT INTO realtime_dashboard.realtime_metrics
SELECT 
    DATE_FORMAT(order_time, '%Y-%m-%d %H:%i:00') AS metric_time,
    'sales_amount' AS metric_name,
    'total' AS metric_dimension,
    SUM(payment_amount) AS metric_value,
    '' AS metric_dimension2,
    NOW() AS update_time
FROM ecommerce.orders 
WHERE order_status = 'completed'
  AND order_time >= DATE_SUB(NOW(), INTERVAL 1 HOUR)
GROUP BY DATE_FORMAT(order_time, '%Y-%m-%d %H:%i:00');
```

#### 3.3.2 实时订单量计算

```sql
-- 创建实时订单量物化视图
CREATE MATERIALIZED VIEW realtime_dashboard.realtime_orders AS
SELECT 
    DATE_FORMAT(order_time, '%Y-%m-%d %H:%i:00') AS metric_time,
    'order_count' AS metric_name,
    'total' AS metric_dimension,
    COUNT(*) AS metric_value,
    '' AS metric_dimension2,
    NOW() AS update_time
FROM ecommerce.orders 
GROUP BY DATE_FORMAT(order_time, '%Y-%m-%d %H:%i:00');

-- 插入实时订单量数据
INSERT INTO realtime_dashboard.realtime_metrics
SELECT 
    DATE_FORMAT(order_time, '%Y-%m-%d %H:%i:00') AS metric_time,
    'order_count' AS metric_name,
    'total' AS metric_dimension,
    COUNT(*) AS metric_value,
    '' AS metric_dimension2,
    NOW() AS update_time
FROM ecommerce.orders 
WHERE order_time >= DATE_SUB(NOW(), INTERVAL 1 HOUR)
GROUP BY DATE_FORMAT(order_time, '%Y-%m-%d %H:%i:00');
```

#### 3.3.3 实时用户数计算

```sql
-- 插入实时用户数数据
INSERT INTO realtime_dashboard.realtime_metrics
SELECT 
    DATE_FORMAT(event_time, '%Y-%m-%d %H:%i:00') AS metric_time,
    'active_users' AS metric_name,
    'total' AS metric_dimension,
    COUNT(DISTINCT user_id) AS metric_value,
    '' AS metric_dimension2,
    NOW() AS update_time
FROM ecommerce.user_behavior 
WHERE event_time >= DATE_SUB(NOW(), INTERVAL 1 HOUR)
GROUP BY DATE_FORMAT(event_time, '%Y-%m-%d %H:%i:00');
```

### 3.4 实时排名计算

#### 3.4.1 热销商品排名

```sql
-- 计算热销商品排名
INSERT INTO realtime_dashboard.realtime_ranking
WITH hot_products AS (
    SELECT 
        oi.product_id,
        p.product_name,
        SUM(oi.quantity) AS total_quantity,
        SUM(oi.quantity * oi.price) AS total_amount
    FROM ecommerce.orders o
    JOIN ecommerce.order_items oi ON o.order_id = oi.order_id
    JOIN ecommerce.products p ON oi.product_id = p.product_id
    WHERE o.order_status = 'completed'
      AND o.order_time >= DATE_SUB(NOW(), INTERVAL 1 DAY)
    GROUP BY oi.product_id, p.product_name
    ORDER BY total_amount DESC
    LIMIT 100
),
ranked_products AS (
    SELECT 
        product_id,
        product_name,
        total_amount,
        ROW_NUMBER() OVER (ORDER BY total_amount DESC) AS rank_position
    FROM hot_products
)
SELECT 
    NOW() AS rank_time,
    'hot_products' AS rank_type,
    rp.product_id AS item_id,
    rp.product_name AS item_name,
    hp.total_amount AS rank_value,
    rp.rank_position,
    CONCAT('quantity: ', hp.total_quantity) AS extra_info
FROM ranked_products rp
JOIN hot_products hp ON rp.product_id = hp.product_id;
```

#### 3.4.2 活跃用户排名

```sql
-- 计算活跃用户排名
INSERT INTO realtime_dashboard.realtime_ranking
WITH active_users AS (
    SELECT 
        user_id,
        COUNT(DISTINCT DATE(event_time)) AS active_days,
        COUNT(*) AS event_count,
        SUM(stay_time) AS total_stay_time
    FROM ecommerce.user_behavior 
    WHERE event_time >= DATE_SUB(NOW(), INTERVAL 7 DAY)
    GROUP BY user_id
    ORDER BY active_days DESC, event_count DESC
    LIMIT 100
),
ranked_users AS (
    SELECT 
        user_id,
        active_days,
        event_count,
        total_stay_time,
        ROW_NUMBER() OVER (ORDER BY active_days DESC, event_count DESC) AS rank_position
    FROM active_users
)
SELECT 
    NOW() AS rank_time,
    'active_users' AS rank_type,
    ru.user_id AS item_id,
    CONCAT('user_id: ', ru.user_id) AS item_name,
    ru.active_days AS rank_value,
    ru.rank_position,
    CONCAT('events: ', au.event_count, ', stay_time: ', au.total_stay_time) AS extra_info
FROM ranked_users ru
JOIN active_users au ON ru.user_id = au.user_id;
```

### 3.5 实时监控查询

#### 3.5.1 实时指标查询

```sql
-- 查询实时销售额
SELECT 
    metric_time,
    metric_value AS sales_amount
FROM realtime_dashboard.realtime_metrics 
WHERE metric_name = 'sales_amount' 
  AND metric_dimension = 'total'
  AND metric_time >= DATE_SUB(NOW(), INTERVAL 1 HOUR)
ORDER BY metric_time DESC;

-- 查询实时订单量
SELECT 
    metric_time,
    metric_value AS order_count
FROM realtime_dashboard.realtime_metrics 
WHERE metric_name = 'order_count' 
  AND metric_dimension = 'total'
  AND metric_time >= DATE_SUB(NOW(), INTERVAL 1 HOUR)
ORDER BY metric_time DESC;

-- 查询实时用户数
SELECT 
    metric_time,
    metric_value AS active_users
FROM realtime_dashboard.realtime_metrics 
WHERE metric_name = 'active_users' 
  AND metric_dimension = 'total'
  AND metric_time >= DATE_SUB(NOW(), INTERVAL 1 HOUR)
ORDER BY metric_time DESC;
```

#### 3.5.2 实时排名查询

```sql
-- 查询热销商品排名
SELECT 
    item_name,
    rank_value,
    rank_position,
    extra_info
FROM realtime_dashboard.realtime_ranking 
WHERE rank_type = 'hot_products'
  AND rank_time >= DATE_SUB(NOW(), INTERVAL 1 HOUR)
ORDER BY rank_position;

-- 查询活跃用户排名
SELECT 
    item_name,
    rank_value,
    rank_position,
    extra_info
FROM realtime_dashboard.realtime_ranking 
WHERE rank_type = 'active_users'
  AND rank_time >= DATE_SUB(NOW(), INTERVAL 1 HOUR)
ORDER BY rank_position;
```

## 4. 日志分析系统

### 4.1 业务场景

日志分析系统需要处理海量的应用日志、服务器日志、用户行为日志等，支持实时监控、异常检测、故障诊断等功能。

### 4.2 数据模型设计

#### 4.2.1 应用日志表

```sql
-- 应用日志表（使用Duplicate模型）
CREATE TABLE log_analysis.app_logs (
    log_time DATETIME COMMENT '日志时间',
    app_name VARCHAR(50) COMMENT '应用名称',
    instance_id VARCHAR(50) COMMENT '实例ID',
    level VARCHAR(20) COMMENT '日志级别',
    logger VARCHAR(100) COMMENT '记录器名称',
    message TEXT COMMENT '日志消息',
    exception TEXT COMMENT '异常信息',
    thread_name VARCHAR(100) COMMENT '线程名称',
    class_name VARCHAR(200) COMMENT '类名',
    method_name VARCHAR(100) COMMENT '方法名',
    line_number INT COMMENT '行号',
    user_id BIGINT COMMENT '用户ID',
    session_id VARCHAR(64) COMMENT '会话ID',
    request_id VARCHAR(64) COMMENT '请求ID',
    ip_address VARCHAR(50) COMMENT 'IP地址',
    user_agent TEXT COMMENT '用户代理'
)
DUPLICATE KEY(log_time, app_name, instance_id, level)
PARTITION BY RANGE(log_time) (
    PARTITION p20231120 VALUES LESS THAN ("2023-11-21"),
    PARTITION p20231121 VALUES LESS THAN ("2023-11-22"),
    PARTITION p20231122 VALUES LESS THAN ("2023-11-23"),
    PARTITION p20231123 VALUES LESS THAN ("2023-11-24"),
    PARTITION p20231124 VALUES LESS THAN ("2023-11-25")
)
DISTRIBUTED BY HASH(app_name) BUCKETS 32
PROPERTIES (
    "replication_num" = "3",
    "dynamic_partition.enable" = "true",
    "dynamic_partition.time_unit" = "DAY",
    "dynamic_partition.start" = "-7",
    "dynamic_partition.end" = "3",
    "dynamic_partition.prefix" = "p",
    "compression" = "LZ4",
    "bloom_filter_columns" = "app_name, level, user_id"
);
```

#### 4.2.2 服务器日志表

```sql
-- 服务器日志表（使用Duplicate模型）
CREATE TABLE log_analysis.server_logs (
    log_time DATETIME COMMENT '日志时间',
    server_name VARCHAR(50) COMMENT '服务器名称',
    log_type VARCHAR(20) COMMENT '日志类型',
    component VARCHAR(50) COMMENT '组件',
    message TEXT COMMENT '日志消息',
    severity VARCHAR(20) COMMENT '严重程度',
    source VARCHAR(100) COMMENT '来源',
    process_id INT COMMENT '进程ID',
    thread_id INT COMMENT '线程ID',
    user_id VARCHAR(50) COMMENT '用户ID',
    command TEXT COMMENT '命令',
    status_code INT COMMENT '状态码',
    response_time INT COMMENT '响应时间(毫秒)',
    request_size INT COMMENT '请求大小(字节)',
    response_size INT COMMENT '响应大小(字节)'
)
DUPLICATE KEY(log_time, server_name, log_type, component)
PARTITION BY RANGE(log_time) (
    PARTITION p20231120 VALUES LESS THAN ("2023-11-21"),
    PARTITION p20231121 VALUES LESS THAN ("2023-11-22"),
    PARTITION p20231122 VALUES LESS THAN ("2023-11-23"),
    PARTITION p20231123 VALUES LESS THAN ("2023-11-24"),
    PARTITION p20231124 VALUES LESS THAN ("2023-11-25")
)
DISTRIBUTED BY HASH(server_name) BUCKETS 32
PROPERTIES (
    "replication_num" = "3",
    "dynamic_partition.enable" = "true",
    "dynamic_partition.time_unit" = "DAY",
    "dynamic_partition.start" = "-7",
    "dynamic_partition.end" = "3",
    "dynamic_partition.prefix" = "p",
    "compression" = "LZ4",
    "bloom_filter_columns" = "server_name, log_type, component"
);
```

### 4.3 日志分析应用

#### 4.3.1 错误日志分析

```sql
-- 创建错误日志统计物化视图
CREATE MATERIALIZED VIEW log_analysis.error_log_stats AS
SELECT 
    DATE(log_time) AS log_date,
    app_name,
    level,
    class_name,
    method_name,
    COUNT(*) AS error_count,
    COUNT(DISTINCT instance_id) AS affected_instances,
    COUNT(DISTINCT user_id) AS affected_users
FROM log_analysis.app_logs 
WHERE level IN ('ERROR', 'FATAL')
GROUP BY DATE(log_time), app_name, level, class_name, method_name;

-- 查询错误日志统计
SELECT 
    log_date,
    app_name,
    level,
    class_name,
    method_name,
    error_count,
    affected_instances,
    affected_users
FROM log_analysis.error_log_stats 
WHERE log_date >= '2023-11-20' AND log_date <= '2023-11-24'
ORDER BY error_count DESC;
```

#### 4.3.2 性能分析

```sql
-- 创建性能统计物化视图
CREATE MATERIALIZED VIEW log_analysis.performance_stats AS
SELECT 
    DATE(log_time) AS log_date,
    server_name,
    component,
    AVG(response_time) AS avg_response_time,
    MAX(response_time) AS max_response_time,
    MIN(response_time) AS min_response_time,
    COUNT(*) AS request_count,
    SUM(request_size) AS total_request_size,
    SUM(response_size) AS total_response_size
FROM log_analysis.server_logs 
WHERE log_type = 'access' AND status_code < 500
GROUP BY DATE(log_time), server_name, component;

-- 查询性能统计
SELECT 
    log_date,
    server_name,
    component,
    avg_response_time,
    max_response_time,
    request_count
FROM log_analysis.performance_stats 
WHERE log_date >= '2023-11-20' AND log_date <= '2023-11-24'
ORDER BY avg_response_time DESC;
```

#### 4.3.3 异常检测

```sql
-- 检测异常请求
SELECT 
    log_time,
    server_name,
    component,
    status_code,
    response_time,
    request_size,
    response_size
FROM log_analysis.server_logs 
WHERE log_type = 'access' 
  AND (
    status_code >= 500 OR 
    response_time > 5000 OR 
    request_size > 10000000 OR 
    response_size > 100000000
  )
  AND log_time >= DATE_SUB(NOW(), INTERVAL 1 HOUR)
ORDER BY log_time DESC;

-- 检测异常错误模式
SELECT 
    log_time,
    app_name,
    instance_id,
    level,
    class_name,
    method_name,
    message,
    exception
FROM log_analysis.app_logs 
WHERE level IN ('ERROR', 'FATAL')
  AND log_time >= DATE_SUB(NOW(), INTERVAL 1 HOUR)
ORDER BY log_time DESC;
```

### 4.4 实时日志导入

#### 4.4.1 使用Flume导入应用日志

```bash
# Flume配置文件 flume.conf
agent.sources = logSource
agent.channels = memoryChannel
agent.sinks = dorisSink

# Source配置
agent.sources.logSource.type = exec
agent.sources.logSource.command = tail -F /var/log/app/app.log
agent.sources.logSource.channels = memoryChannel

# Channel配置
agent.channels.memoryChannel.type = memory
agent.channels.memoryChannel.capacity = 10000
agent.channels.memoryChannel.transactionCapacity = 1000

# Sink配置
agent.sinks.dorisSink.type = http
agent.sinks.dorisSink.channel = memoryChannel
agent.sinks.dorisSink.http.endpoint = http://fe_host:http_port/api/_stream_load
agent.sinks.dorisSink.http.method = POST
agent.sinks.dorisSink.http.headers = Content-Type: application/json
agent.sinks.dorisSink.http.headers = Authorization: Basic cm9vdDp
agent.sinks.dorisSink.http.request.body = {"data": "${body}"}
```

#### 4.4.2 使用Logstash导入服务器日志

```bash
# Logstash配置文件 logstash.conf
input {
  file {
    path => "/var/log/nginx/access.log"
    start_position => "beginning"
    type => "nginx_access"
  }
  file {
    path => "/var/log/nginx/error.log"
    start_position => "beginning"
    type => "nginx_error"
  }
}

filter {
  if [type] == "nginx_access" {
    grok {
      match => { "message" => "%{COMBINEDAPACHELOG}" }
    }
    date {
      match => [ "timestamp", "dd/MMM/yyyy:HH:mm:ss Z" ]
    }
  } else if [type] == "nginx_error" {
    grok {
      match => { "message" => "%{NGINXERROR}" }
    }
    date {
      match => [ "timestamp", "yyyy/MM/dd HH:mm:ss" ]
    }
  }
}

output {
  http {
    url => "http://fe_host:http_port/api/log_analysis/server_logs/_stream_load"
    http_method => "post"
    headers => {
      "Content-Type" => "application/json"
      "Authorization" => "Basic cm9vdDp"
      "columns" => "log_time, server_name, log_type, component, message, severity, source, process_id, thread_id, user_id, command, status_code, response_time, request_size, response_size"
    }
    format => "json"
  }
}
```

## 5. 本章小结

本章详细介绍了Doris在四个典型业务场景中的实战应用：电商数据分析平台、用户画像系统、实时监控大屏和日志分析系统。每个场景都包含了业务背景、数据模型设计、实际应用实现和实时数据导入方案。

### 5.1 关键要点

1. **数据模型设计**：根据业务特点选择合适的数据模型
2. **实时分析**：使用物化视图和实时导入实现实时分析
3. **性能优化**：通过分区、分桶、索引等手段优化查询性能
4. **数据导入**：选择合适的导入方式满足实时性要求

### 5.2 最佳实践

1. 根据业务特点设计合理的数据模型
2. 使用物化视图预计算常用指标
3. 合理设计分区和分桶策略
4. 为高频查询创建合适的索引
5. 选择合适的数据导入方式
6. 建立完善的监控和告警体系

## 6. 延伸阅读

- [Doris官方文档-应用案例](https://doris.apache.org/docs/dev/user-guide/)
- [Doris官方文档-最佳实践](https://doris.apache.org/docs/dev/best-practice/)
- [Doris官方文档-集成方案](https://doris.apache.org/docs/dev/ecosystem/)

## 7. 实践练习

1. 设计一个电商数据分析平台的数据模型
2. 实现一个用户画像系统的标签计算
3. 构建一个实时监控大屏的数据模型和查询
4. 设计一个日志分析系统的数据导入和分析方案
5. 实现一个实时数据导入的Pipeline
6. 优化一个复杂查询的性能
7. 设计一个完整的Doris应用架构