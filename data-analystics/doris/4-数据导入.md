# 第4章 Doris数据导入

## 学习目标

- 掌握Doris多种数据导入方式及其适用场景
- 学会使用Stream Load、Broker Load、Routine Load等导入方法
- 了解数据导入的最佳实践和性能优化技巧
- 掌握数据导入过程中的错误处理和监控方法
- 学会设计高效的数据导入流程

## 1. 数据导入概述

Apache Doris提供了多种数据导入方式，以满足不同业务场景的需求。根据数据量、实时性要求、数据源类型等因素，可以选择合适的导入方式。

### 1.1 导入方式分类

Doris的数据导入方式主要分为以下几类：

1. **同步导入**：
   - Stream Load：通过HTTP接口导入数据
   - Broker Load：通过Broker服务导入数据
   - Insert Into：通过SQL语句导入数据

2. **异步导入**：
   - Routine Load：持续导入Kafka等消息队列数据
   - Spark Load：通过Spark导入大数据量数据

3. **其他导入方式**：
   - MySQL Load：通过MySQL协议导入数据
   - S3 Load：直接从S3导入数据

### 1.2 导入方式对比

| 导入方式 | 数据量 | 实时性 | 适用场景 | 优点 | 缺点 |
|---------|--------|--------|---------|------|------|
| Stream Load | 小到中等 | 高 | 实时导入 | 简单易用，实时性好 | 不适合大数据量 |
| Broker Load | 大到超大 | 低 | 批量导入 | 适合大数据量，支持多种数据源 | 配置复杂，实时性差 |
| Routine Load | 中到大 | 高 | 流式导入 | 持续导入，实时性好 | 依赖消息队列 |
| Insert Into | 小到中等 | 中 | SQL导入 | 简单方便，支持复杂查询 | 性能一般 |
| Spark Load | 超大 | 低 | 大数据量导入 | 性能高，适合TB级数据 | 依赖Spark集群 |

## 2. Stream Load

### 2.1 基本概念

Stream Load是Doris提供的一种同步导入方式，通过HTTP接口将数据导入到Doris表中。它适合小到中等规模的数据导入，具有实时性好、使用简单的特点。

### 2.2 语法结构

```bash
curl --location-trusted -u user:password \
    -H "label:label_name" \
    -H "column_separator:separator" \
    -H "columns:col1,col2,=col3" \
    -T data_file \
    http://fe_host:http_port/api/{db}/{table}/_stream_load
```

### 2.3 参数说明

| 参数 | 说明 | 示例 |
|------|------|------|
| user:password | 用户名和密码 | root: |
| label | 导入任务的唯一标识 | label_20231120_001 |
| column_separator | 列分隔符 | , |
| line_delimiter | 行分隔符 | \n |
| columns | 列映射关系 | col1,col2,=col3 |
| format | 数据格式 | json, csv |
| max_filter_ratio | 最大容错率 | 0.1 |
| strict_mode | 严格模式 | true |
| timezone | 时区 | Asia/Shanghai |

### 2.4 使用示例

#### 2.4.1 CSV格式数据导入

```bash
# 准备数据文件
cat > user_behavior.csv << EOF
1001,2023-11-20 10:30:00,view,/home,,192.168.1.1,Mozilla/5.0...,30
1002,2023-11-20 10:31:00,click,/product/123,/home,192.168.1.2,Mozilla/5.0...,5
1003,2023-11-20 10:32:00,view,/home,,192.168.1.3,Mozilla/5.0...,15
1004,2023-11-20 10:33:00,click,/product/456,/home,192.168.1.4,Mozilla/5.0...,8
EOF

# 导入数据
curl --location-trusted -u root: \
    -H "label:user_behavior_20231120_001" \
    -H "column_separator:," \
    -H "columns:user_id,event_time,event_type,page_url,referrer_url,client_ip,user_agent,stay_time" \
    -T user_behavior.csv \
    http://127.0.0.1:8030/api/demo/user_behavior/_stream_load
```

#### 2.4.2 JSON格式数据导入

```bash
# 准备数据文件
cat > user_behavior.json << EOF
{"user_id": 1001, "event_time": "2023-11-20 10:30:00", "event_type": "view", "page_url": "/home", "referrer_url": "", "client_ip": "192.168.1.1", "user_agent": "Mozilla/5.0...", "stay_time": 30}
{"user_id": 1002, "event_time": "2023-11-20 10:31:00", "event_type": "click", "page_url": "/product/123", "referrer_url": "/home", "client_ip": "192.168.1.2", "user_agent": "Mozilla/5.0...", "stay_time": 5}
{"user_id": 1003, "event_time": "2023-11-20 10:32:00", "event_type": "view", "page_url": "/home", "referrer_url": "", "client_ip": "192.168.1.3", "user_agent": "Mozilla/5.0...", "stay_time": 15}
{"user_id": 1004, "event_time": "2023-11-20 10:33:00", "event_type": "click", "page_url": "/product/456", "referrer_url": "/home", "client_ip": "192.168.1.4", "user_agent": "Mozilla/5.0...", "stay_time": 8}
EOF

# 导入数据
curl --location-trusted -u root: \
    -H "label:user_behavior_20231120_002" \
    -H "format:json" \
    -H "columns:user_id,event_time,event_type,page_url,referrer_url,client_ip,user_agent,stay_time" \
    -T user_behavior.json \
    http://127.0.0.1:8030/api/demo/user_behavior/_stream_load
```

#### 2.4.3 数据转换导入

```bash
# 准备数据文件（日期格式为yyyyMMdd）
cat > user_behavior_transform.csv << EOF
1001,20231120,103000,view,/home,,192.168.1.1,Mozilla/5.0...,30
1002,20231120,103100,click,/product/123,/home,192.168.1.2,Mozilla/5.0...,5
1003,20231120,103200,view,/home,,192.168.1.3,Mozilla/5.0...,15
1004,20231120,103300,click,/product/456,/home,192.168.1.4,Mozilla/5.0...,8
EOF

# 导入数据（进行数据转换）
curl --location-trusted -u root: \
    -H "label:user_behavior_20231120_003" \
    -H "column_separator:," \
    -H "columns:user_id,date,time,event_type,page_url,referrer_url,client_ip,user_agent,stay_time, event_time=concat(date, ' ', substr(time, 1, 2), ':', substr(time, 3, 2), ':', substr(time, 5, 2))" \
    -T user_behavior_transform.csv \
    http://127.0.0.1:8030/api/demo/user_behavior/_stream_load
```

### 2.5 返回结果

Stream Load的返回结果是一个JSON对象，包含以下字段：

```json
{
    "TxnId": 1001,
    "Label": "user_behavior_20231120_001",
    "Status": "Success",
    "ExistingJobStatus": "N/A",
    "Message": "OK",
    "NumberTotalRows": 4,
    "NumberLoadedRows": 4,
    "NumberFilteredRows": 0,
    "NumberUnselectedRows": 0,
    "LoadBytes": 1024,
    "LoadTimeMs": 100,
    "BeginTxnTimeMs": 10,
    "StreamLoadPlanTimeMs": 20,
    "ReadDataTimeMs": 30,
    "WriteDataTimeMs": 40,
    "CommitAndPublishTimeMs": 50
}
```

### 2.6 适用场景

- 实时数据导入
- 小到中等规模数据导入
- 需要数据转换的场景
- 客户端应用程序集成

## 3. Broker Load

### 3.1 基本概念

Broker Load是一种异步导入方式，通过Broker服务访问外部数据源（如HDFS、S3、BOS等），将数据导入到Doris表中。它适合大数据量的批量导入。

### 3.2 语法结构

```sql
LOAD LABEL [database.]label_name
(
    data_desc1[, data_desc2, ...]
)
WITH BROKER broker_name
[broker_properties]
[PROPERTIES (load_properties)]
```

### 3.3 参数说明

#### 3.3.1 data_desc

```sql
data_desc:
    DATA INFILE ('file_path', ...)
    [NEGATIVE]
    INTO TABLE tbl_name
    [PARTITION (p1, p2, ...)]
    [COLUMNS (col1, col2, ...)]
    [COLUMNS FROM PATH AS (col1, col2, ...)]
    [SET (col1 = expr1, col2 = expr2, ...)]
    [WHERE predicate]
    [DELETE ON condition]
    [MERGE ON condition]
    [ORDER BY source_col1[, source_col2, ...]]
    [PROPERTIES (source_properties)]
```

#### 3.3.2 broker_properties

```sql
broker_properties:
    ("key"="value", ...)
```

#### 3.3.3 load_properties

```sql
load_properties:
    ("key"="value", ...)
```

### 3.4 使用示例

#### 3.4.1 从HDFS导入数据

```sql
-- 创建HDFS Broker
CREATE BROKER hdfs_broker (
    "username"="hdfs_user",
    "password"="hdfs_password"
);

-- 导入数据
LOAD LABEL demo.user_behavior_hdfs_20231120
(
    DATA INFILE("hdfs://namenode:8020/data/user_behavior/2023/11/20/*")
    INTO TABLE user_behavior
    COLUMNS TERMINATED BY ","
    (user_id, event_time, event_type, page_url, referrer_url, client_ip, user_agent, stay_time)
)
WITH BROKER hdfs_broker
PROPERTIES
(
    "timeout"="3600",
    "max_filter_ratio"="0.1"
);
```

#### 3.4.2 从S3导入数据

```sql
-- 创建S3 Broker
CREATE BROKER s3_broker (
    "AWS_ACCESS_KEY"="your_access_key",
    "AWS_SECRET_KEY"="your_secret_key",
    "AWS_ENDPOINT"="s3.amazonaws.com",
    "AWS_REGION"="us-east-1"
);

-- 导入数据
LOAD LABEL demo.user_behavior_s3_20231120
(
    DATA INFILE("s3://your-bucket/data/user_behavior/2023/11/20/*")
    INTO TABLE user_behavior
    COLUMNS TERMINATED BY ","
    FORMAT AS "csv"
    (user_id, event_time, event_type, page_url, referrer_url, client_ip, user_agent, stay_time)
)
WITH BROKER s3_broker
PROPERTIES
(
    "timeout"="3600",
    "max_filter_ratio"="0.1"
);
```

#### 3.4.3 多文件导入

```sql
-- 导入多个文件
LOAD LABEL demo.multi_file_import_20231120
(
    DATA INFILE("hdfs://namenode:8020/data/user_behavior/2023/11/20/00/*")
    INTO TABLE user_behavior
    COLUMNS TERMINATED BY ","
    (user_id, event_time, event_type, page_url, referrer_url, client_ip, user_agent, stay_time),
    
    DATA INFILE("hdfs://namenode:8020/data/user_behavior/2023/11/20/01/*")
    INTO TABLE user_behavior
    COLUMNS TERMINATED BY ","
    (user_id, event_time, event_type, page_url, referrer_url, client_ip, user_agent, stay_time)
)
WITH BROKER hdfs_broker
PROPERTIES
(
    "timeout"="3600",
    "max_filter_ratio"="0.1"
);
```

#### 3.4.4 数据转换导入

```sql
-- 导入数据并进行转换
LOAD LABEL demo.user_behavior_transform_20231120
(
    DATA INFILE("hdfs://namenode:8020/data/user_behavior_raw/2023/11/20/*")
    INTO TABLE user_behavior
    COLUMNS TERMINATED BY ","
    (user_id, date, time, event_type, page_url, referrer_url, client_ip, user_agent, stay_time)
    SET (
        event_time = concat(date, ' ', substr(time, 1, 2), ':', substr(time, 3, 2), ':', substr(time, 5, 2))
    )
)
WITH BROKER hdfs_broker
PROPERTIES
(
    "timeout"="3600",
    "max_filter_ratio"="0.1"
);
```

### 3.5 查看导入状态

```sql
-- 查看导入任务状态
SHOW LOAD;

-- 查看特定导入任务状态
SHOW LOAD WHERE LABEL = 'user_behavior_hdfs_20231120';

-- 取消导入任务
CANCEL LOAD FROM demo WHERE LABEL = 'user_behavior_hdfs_20231120';
```

### 3.6 适用场景

- 大数据量批量导入
- 从HDFS导入数据
- 从云存储（S3、BOS等）导入数据
- 需要复杂数据转换的场景

## 4. Routine Load

### 4.1 基本概念

Routine Load是一种持续导入方式，可以从Kafka等消息队列中持续消费数据并导入到Doris表中。它适合实时数据流处理场景。

### 4.2 语法结构

```sql
CREATE ROUTINE LOAD [database.]job_name ON tbl_name
[load_properties]
FROM KAFKA
[kafka_properties]
```

### 4.3 参数说明

#### 4.3.1 load_properties

```sql
load_properties:
    (
        [PROPERTIES
        (
            "desired_concurrent_number"="num",
            "max_batch_interval"="time",
            "max_batch_rows"="num",
            "max_batch_size"="size",
            "format"="json|csv",
            "jsonpaths"="json_path",
            "strip_outer_array"="true|false",
            "json_root"="json_root",
            "field_separator"="separator",
            "line_delimiter"="delimiter",
            "columns"="col1, col2, =col3",
            "where"="condition",
            "partitions"="p1, p2, ...",
            "strict_mode"="true|false",
            "timezone"="timezone",
            "max_filter_ratio"="ratio"
        )]
    )
```

#### 4.3.2 kafka_properties

```sql
kafka_properties:
    (
        "kafka_broker_list"="broker1:9092,broker2:9092,...",
        "kafka_topic"="topic_name",
        "kafka_partitions"="p1,p2,...",
        "kafka_offsets"="o1,o2,...",
        "kafka_group_id"="group_id",
        "property.kafka_default_offsets"="OFFSET_BEGINNING|OFFSET_END",
        "property.auto.offset.reset"="latest|earliest"
    )
```

### 4.4 使用示例

#### 4.4.1 从Kafka导入CSV数据

```sql
-- 创建Routine Load任务
CREATE ROUTINE LOAD demo.user_behavior_kafka_job ON user_behavior
COLUMNS TERMINATED BY ","
COLUMNS(user_id, event_time, event_type, page_url, referrer_url, client_ip, user_agent, stay_time)
PROPERTIES
(
    "desired_concurrent_number"="3",
    "max_batch_interval"="20",
    "max_batch_rows"="300000",
    "max_batch_size"="209715200"
)
FROM KAFKA
(
    "kafka_broker_list"="kafka1:9092,kafka2:9092,kafka3:9092",
    "kafka_topic"="user_behavior",
    "property.group.id"="doris_user_behavior_group",
    "property.auto.offset.reset"="latest"
);
```

#### 4.4.2 从Kafka导入JSON数据

```sql
-- 创建Routine Load任务（JSON格式）
CREATE ROUTINE LOAD demo.user_behavior_json_job ON user_behavior
PROPERTIES
(
    "format"="json",
    "jsonpaths"="$.user_id, $.event_time, $.event_type, $.page_url, $.referrer_url, $.client_ip, $.user_agent, $.stay_time",
    "desired_concurrent_number"="3",
    "max_batch_interval"="20",
    "max_batch_rows"="300000",
    "max_batch_size"="209715200"
)
FROM KAFKA
(
    "kafka_broker_list"="kafka1:9092,kafka2:9092,kafka3:9092",
    "kafka_topic"="user_behavior_json",
    "property.group.id"="doris_user_behavior_json_group",
    "property.auto.offset.reset"="latest"
);
```

#### 4.4.3 指定分区和偏移量

```sql
-- 创建Routine Load任务（指定分区和偏移量）
CREATE ROUTINE LOAD demo.user_behavior_partition_job ON user_behavior
COLUMNS TERMINATED BY ","
COLUMNS(user_id, event_time, event_type, page_url, referrer_url, client_ip, user_agent, stay_time)
PROPERTIES
(
    "desired_concurrent_number"="3",
    "max_batch_interval"="20",
    "max_batch_rows"="300000",
    "max_batch_size"="209715200"
)
FROM KAFKA
(
    "kafka_broker_list"="kafka1:9092,kafka2:9092,kafka3:9092",
    "kafka_topic"="user_behavior",
    "kafka_partitions"="0,1,2",
    "kafka_offsets"="100,200,300",
    "property.group.id"="doris_user_behavior_partition_group"
);
```

### 4.5 管理Routine Load

```sql
-- 查看Routine Load任务
SHOW ROUTINE LOAD;

-- 查看特定Routine Load任务
SHOW ROUTINE LOAD FOR demo.user_behavior_kafka_job;

-- 暂停Routine Load任务
PAUSE ROUTINE LOAD FOR demo.user_behavior_kafka_job;

-- 恢复Routine Load任务
RESUME ROUTINE LOAD FOR demo.user_behavior_kafka_job;

-- 停止Routine Load任务
STOP ROUTINE LOAD FOR demo.user_behavior_kafka_job;
```

### 4.6 适用场景

- 实时数据流处理
- 从Kafka导入数据
- 持续数据同步
- 实时数据分析

## 5. Insert Into

### 5.1 基本概念

Insert Into是通过SQL语句将数据导入到Doris表中的方式，支持从其他表查询数据导入，也支持直接插入常量数据。

### 5.2 语法结构

```sql
INSERT INTO table_name [PARTITION (p1, p2, ...)] [column_list]
[VALUES | select_stmt]
```

### 5.3 使用示例

#### 5.3.1 直接插入数据

```sql
-- 插入单行数据
INSERT INTO demo.user_behavior (user_id, event_time, event_type, page_url, referrer_url, client_ip, user_agent, stay_time)
VALUES (1001, '2023-11-20 10:30:00', 'view', '/home', '', '192.168.1.1', 'Mozilla/5.0...', 30);

-- 插入多行数据
INSERT INTO demo.user_behavior (user_id, event_time, event_type, page_url, referrer_url, client_ip, user_agent, stay_time)
VALUES 
    (1002, '2023-11-20 10:31:00', 'click', '/product/123', '/home', '192.168.1.2', 'Mozilla/5.0...', 5),
    (1003, '2023-11-20 10:32:00', 'view', '/home', '', '192.168.1.3', 'Mozilla/5.0...', 15),
    (1004, '2023-11-20 10:33:00', 'click', '/product/456', '/home', '192.168.1.4', 'Mozilla/5.0...', 8);
```

#### 5.3.2 从查询结果插入数据

```sql
-- 从查询结果插入数据
INSERT INTO demo.user_daily_stats (user_id, stat_date, page_views, total_stay_time)
SELECT 
    user_id,
    DATE(event_time) AS stat_date,
    COUNT(*) AS page_views,
    SUM(stay_time) AS total_stay_time
FROM demo.user_behavior
WHERE event_time >= '2023-11-20 00:00:00' AND event_time < '2023-11-21 00:00:00'
GROUP BY user_id, DATE(event_time);
```

#### 5.3.3 指定分区插入数据

```sql
-- 指定分区插入数据
INSERT INTO demo.monthly_sales PARTITION (p202311) (sale_date, region, product_id, product_name, sales_amount, sales_count)
VALUES ('2023-11-20', 'North', 1001, 'Product A', 1200.50, 25);
```

### 5.4 性能优化

```sql
-- 使用批量插入
INSERT INTO demo.user_behavior (user_id, event_time, event_type, page_url, referrer_url, client_ip, user_agent, stay_time)
VALUES 
    (1001, '2023-11-20 10:30:00', 'view', '/home', '', '192.168.1.1', 'Mozilla/5.0...', 30),
    (1002, '2023-11-20 10:31:00', 'click', '/product/123', '/home', '192.168.1.2', 'Mozilla/5.0...', 5),
    (1003, '2023-11-20 10:32:00', 'view', '/home', '', '192.168.1.3', 'Mozilla/5.0...', 15),
    -- 更多数据...
;

-- 使用INSERT INTO SELECT批量导入
INSERT INTO demo.user_behavior_archive
SELECT * FROM demo.user_behavior
WHERE event_time < '2023-01-01 00:00:00';
```

### 5.5 适用场景

- 小数据量导入
- 数据转换后导入
- 从其他表导入数据
- 开发测试环境

## 6. Spark Load

### 6.1 基本概念

Spark Load是通过Spark集群进行大数据量导入的方式，适合TB级数据导入。它利用Spark的分布式计算能力，可以高效处理大规模数据。

### 6.2 语法结构

```sql
LOAD LABEL [database.]label_name
(
    data_desc1[, data_desc2, ...]
)
WITH RESOURCE [resource_name]
[PROPERTIES (load_properties)]
```

### 6.3 使用示例

```sql
-- 创建Spark资源
CREATE EXTERNAL RESOURCE "spark_resource"
PROPERTIES
(
    "type"="spark",
    "spark.master"="yarn",
    "spark.submit.deployMode"="cluster",
    "spark.jars"="path/to/spark-doris-connector.jar",
    "spark.executor.memory"="2g",
    "spark.executor.cores"="2",
    "spark.executor.instances"="10"
);

-- 导入数据
LOAD LABEL demo.spark_load_user_behavior_20231120
(
    DATA INFILE("hdfs://namenode:8020/data/user_behavior/2023/11/*")
    INTO TABLE user_behavior
    COLUMNS TERMINATED BY ","
    (user_id, event_time, event_type, page_url, referrer_url, client_ip, user_agent, stay_time)
)
WITH RESOURCE "spark_resource"
PROPERTIES
(
    "timeout"="86400",
    "max_filter_ratio"="0.1"
);
```

### 6.4 适用场景

- TB级数据导入
- 需要复杂数据处理的场景
- 从HDFS导入大数据量
- 需要高性能导入的场景

## 7. 数据导入最佳实践

### 7.1 导入方式选择

根据业务场景选择合适的导入方式：

1. **实时小数据量导入**：选择Stream Load
2. **批量大数据量导入**：选择Broker Load
3. **持续流式导入**：选择Routine Load
4. **TB级数据导入**：选择Spark Load
5. **SQL方式导入**：选择Insert Into

### 7.2 性能优化

1. **批量导入**：
   - 避免频繁小批量导入
   - 合理设置批次大小
   - 使用并发导入

2. **数据预处理**：
   - 在导入前进行数据清洗
   - 统一数据格式
   - 减少数据转换开销

3. **资源配置**：
   - 合理设置导入并发数
   - 优化内存配置
   - 调整超时时间

4. **表设计优化**：
   - 合理设计分区分桶
   - 选择合适的数据模型
   - 优化列类型

### 7.3 错误处理

1. **数据格式错误**：
   - 检查数据格式是否符合要求
   - 验证列分隔符和行分隔符
   - 确认数据类型匹配

2. **导入失败处理**：
   - 查看错误日志
   - 分析失败原因
   - 重新导入失败数据

3. **数据一致性**：
   - 使用事务保证数据一致性
   - 设置合理的容错率
   - 定期检查数据质量

### 7.4 监控与维护

1. **导入任务监控**：
   - 监控导入任务状态
   - 跟踪导入进度
   - 记录导入性能指标

2. **资源监控**：
   - 监控集群资源使用情况
   - 跟踪导入任务对集群的影响
   - 及时调整资源配置

3. **数据质量监控**：
   - 定期检查导入数据质量
   - 验证数据完整性
   - 监控数据一致性

## 8. 实战案例

### 8.1 电商实时数据分析系统

#### 8.1.1 场景描述

构建一个电商实时数据分析系统，需要从多个数据源导入数据，包括用户行为日志、订单数据、商品信息等。

#### 8.1.2 数据导入方案

```sql
-- 1. 用户行为日志导入（Routine Load）
CREATE ROUTINE LOAD ecommerce.user_behavior_kafka_job ON user_behavior
COLUMNS TERMINATED BY ","
COLUMNS(user_id, session_id, event_time, event_type, page_url, referrer_url, client_ip, user_agent, stay_time)
PROPERTIES
(
    "desired_concurrent_number"="5",
    "max_batch_interval"="10",
    "max_batch_rows"="500000",
    "max_batch_size"="209715200"
)
FROM KAFKA
(
    "kafka_broker_list"="kafka1:9092,kafka2:9092,kafka3:9092",
    "kafka_topic"="user_behavior",
    "property.group.id"="ecommerce_user_behavior_group",
    "property.auto.offset.reset"="latest"
);

-- 2. 订单数据导入（Stream Load）
-- 每天凌晨导入前一天订单数据
curl --location-trusted -u root: \
    -H "label:orders_$(date +%Y%m%d)" \
    -H "column_separator:," \
    -H "columns:order_id,user_id,product_id,quantity,price,total_amount,order_status,order_time,payment_time,shipping_time,receive_time" \
    -T orders_$(date +%Y%m%d).csv \
    http://127.0.0.1:8030/api/ecommerce/orders/_stream_load

-- 3. 商品信息导入（Broker Load）
LOAD LABEL ecommerce.product_info_20231120
(
    DATA INFILE("hdfs://namenode:8020/ecommerce/product_info/2023/11/20/*")
    INTO TABLE product_info
    COLUMNS TERMINATED BY ","
    (product_id, product_name, product_category, brand, price, stock, status, create_time, update_time)
)
WITH BROKER hdfs_broker
PROPERTIES
(
    "timeout"="3600",
    "max_filter_ratio"="0.1"
);

-- 4. 用户画像更新（Insert Into）
INSERT INTO ecommerce.user_profile (user_id, age_group, gender, city_level, purchase_power, last_update_time)
SELECT 
    u.user_id,
    CASE 
        WHEN u.age < 18 THEN 'Under 18'
        WHEN u.age BETWEEN 18 AND 24 THEN '18-24'
        WHEN u.age BETWEEN 25 AND 34 THEN '25-34'
        WHEN u.age BETWEEN 35 AND 44 THEN '35-44'
        WHEN u.age BETWEEN 45 AND 54 THEN '45-54'
        ELSE '55+'
    END AS age_group,
    u.gender,
    CASE 
        WHEN u.city IN ('Beijing', 'Shanghai', 'Guangzhou', 'Shenzhen') THEN 'Tier 1'
        WHEN u.city IN ('Hangzhou', 'Nanjing', 'Chengdu', 'Wuhan', 'Suzhou', 'Xi'an', 'Tianjin', 'Chongqing') THEN 'Tier 2'
        ELSE 'Tier 3'
    END AS city_level,
    CASE 
        WHEN SUM(o.total_amount) < 1000 THEN 'Low'
        WHEN SUM(o.total_amount) BETWEEN 1000 AND 5000 THEN 'Medium'
        WHEN SUM(o.total_amount) BETWEEN 5000 AND 10000 THEN 'High'
        ELSE 'Very High'
    END AS purchase_power,
    NOW() AS last_update_time
FROM ecommerce.user_info u
LEFT JOIN ecommerce.orders o ON u.user_id = o.user_id
WHERE o.order_time >= DATE_SUB(NOW(), INTERVAL 30 DAY)
GROUP BY u.user_id, u.age, u.gender, u.city
ON DUPLICATE KEY UPDATE age_group=VALUES(age_group), gender=VALUES(gender), city_level=VALUES(city_level), purchase_power=VALUES(purchase_power), last_update_time=VALUES(last_update_time);
```

#### 8.1.3 数据导入监控

```sql
-- 创建导入监控表
CREATE TABLE IF NOT EXISTS ecommerce.import_monitor (
    job_name VARCHAR(100),
    job_type VARCHAR(50),
    table_name VARCHAR(100),
    import_time DATETIME,
    status VARCHAR(20),
    total_rows BIGINT,
    success_rows BIGINT,
    failed_rows BIGINT,
    import_duration_ms BIGINT,
    error_message VARCHAR(1024),
    update_time DATETIME
) DUPLICATE KEY(job_name, import_time)
DISTRIBUTED BY HASH(job_name) BUCKETS 8;

-- 创建Routine Load监控存储过程
CREATE PROCEDURE ecommerce.monitor_routine_load()
BEGIN
    DECLARE done INT DEFAULT FALSE;
    DECLARE job_name VARCHAR(100);
    DECLARE table_name VARCHAR(100);
    DECLARE status VARCHAR(20);
    DECLARE error_rows BIGINT;
    DECLARE total_rows BIGINT;
    DECLARE error_message VARCHAR(1024);
    
    DECLARE cur CURSOR FOR 
        SELECT 
            JobName,
            TableName,
            State,
            ErrorLogRows,
            ReceivedDataBytes / 1000 AS TotalRows,
            ErrorMsg
        FROM information_schema.routine_loads;
    
    DECLARE CONTINUE HANDLER FOR NOT FOUND SET done = TRUE;
    
    OPEN cur;
    
    read_loop: LOOP
        FETCH cur INTO job_name, table_name, status, error_rows, total_rows, error_message;
        IF done THEN
            LEAVE read_loop;
        END IF;
        
        INSERT INTO ecommerce.import_monitor (
            job_name, job_type, table_name, import_time, status, 
            total_rows, success_rows, failed_rows, import_duration_ms, error_message, update_time
        ) VALUES (
            job_name, 'Routine Load', table_name, NOW(), status, 
            total_rows, total_rows - error_rows, error_rows, 0, error_message, NOW()
        )
        ON DUPLICATE KEY UPDATE 
            status=VALUES(status), 
            total_rows=VALUES(total_rows), 
            success_rows=VALUES(success_rows), 
            failed_rows=VALUES(failed_rows), 
            error_message=VALUES(error_message), 
            update_time=VALUES(update_time);
    END LOOP;
    
    CLOSE cur;
END;

-- 定时执行监控（需要外部调度系统，如Crontab）
-- 每5分钟执行一次
*/5 * * * * mysql -h127.0.0.1 -P9030 -uroot -e "CALL ecommerce.monitor_routine_load();"
```

### 8.2 日志分析系统

#### 8.2.1 场景描述

构建一个日志分析系统，需要从多个服务器收集日志数据，并进行实时分析。

#### 8.2.2 数据导入方案

```sql
-- 1. 创建日志表
CREATE TABLE IF NOT EXISTS logs.web_access_log (
    server_ip VARCHAR(50),
    access_time DATETIME,
    method VARCHAR(10),
    url VARCHAR(2048),
    protocol VARCHAR(20),
    status_code INT,
    response_size BIGINT,
    referer VARCHAR(2048),
    user_agent VARCHAR(512),
    client_ip VARCHAR(50),
    request_time INT,
    upstream_response_time INT,
    country VARCHAR(50),
    province VARCHAR(50),
    city VARCHAR(50)
) DUPLICATE KEY(server_ip, access_time)
DISTRIBUTED BY HASH(server_ip) BUCKETS 32
PROPERTIES (
    "compression"="LZ4",
    "dynamic_partition.enable"="true",
    "dynamic_partition.time_unit"="DAY",
    "dynamic_partition.start"="-30",
    "dynamic_partition.end"="7",
    "dynamic_partition.prefix"="p",
    "dynamic_partition.buckets"="32"
);

-- 2. 创建Routine Load任务
CREATE ROUTINE LOAD logs.web_access_log_kafka_job ON web_access_log
COLUMNS TERMINATED BY " "
COLUMNS(server_ip, access_time, method, url, protocol, status_code, response_size, referer, user_agent, client_ip, request_time, upstream_response_time)
SET (
    country = get_country_from_ip(client_ip),
    province = get_province_from_ip(client_ip),
    city = get_city_from_ip(client_ip)
)
PROPERTIES
(
    "desired_concurrent_number"="10",
    "max_batch_interval"="5",
    "max_batch_rows"="1000000",
    "max_batch_size"="209715200"
)
FROM KAFKA
(
    "kafka_broker_list"="kafka1:9092,kafka2:9092,kafka3:9092",
    "kafka_topic"="web_access_log",
    "property.group.id"="logs_web_access_log_group",
    "property.auto.offset.reset"="latest"
);

-- 3. 创建日志统计表
CREATE TABLE IF NOT EXISTS logs.web_access_stats (
    stat_date DATE,
    stat_hour INT,
    server_ip VARCHAR(50),
    status_code INT,
    request_count BIGINT SUM DEFAULT '0',
    total_response_size BIGINT SUM DEFAULT '0',
    avg_response_time DECIMAL(10,2) REPLACE DEFAULT '0.00',
    error_rate DECIMAL(5,2) REPLACE DEFAULT '0.00',
    unique_visitors BIGINT REPLACE DEFAULT '0'
) AGGREGATE KEY(stat_date, stat_hour, server_ip, status_code)
DISTRIBUTED BY HASH(stat_date, stat_hour, server_ip) BUCKETS 16
PROPERTIES (
    "compression"="LZ4",
    "enable_unique_key_merge_on_write"="true"
);

-- 4. 定时统计数据（需要外部调度系统）
-- 每小时执行一次
0 * * * * mysql -h127.0.0.1 -P9030 -uroot -e "
INSERT INTO logs.web_access_stats (stat_date, stat_hour, server_ip, status_code, request_count, total_response_size, avg_response_time, error_rate, unique_visitors)
SELECT 
    DATE(access_time) AS stat_date,
    HOUR(access_time) AS stat_hour,
    server_ip,
    status_code,
    COUNT(*) AS request_count,
    SUM(response_size) AS total_response_size,
    AVG(request_time) AS avg_response_time,
    SUM(CASE WHEN status_code >= 400 THEN 1 ELSE 0 END) * 100.0 / COUNT(*) AS error_rate,
    COUNT(DISTINCT client_ip) AS unique_visitors
FROM logs.web_access_log
WHERE access_time >= DATE_SUB(NOW(), INTERVAL 2 HOUR) AND access_time < DATE_SUB(NOW(), INTERVAL 1 HOUR)
GROUP BY DATE(access_time), HOUR(access_time), server_ip, status_code
ON DUPLICATE KEY UPDATE 
    request_count=VALUES(request_count), 
    total_response_size=VALUES(total_response_size), 
    avg_response_time=VALUES(avg_response_time), 
    error_rate=VALUES(error_rate), 
    unique_visitors=VALUES(unique_visitors);
"
```

## 9. 常见问题与解决方案

### 9.1 导入性能问题

**问题**：数据导入速度慢

**解决方案**：
1. 增加导入并发数
2. 调整批次大小
3. 优化表设计（分区分桶）
4. 检查集群资源使用情况
5. 优化数据预处理流程

### 9.2 数据格式问题

**问题**：数据格式不匹配导致导入失败

**解决方案**：
1. 检查数据格式是否符合要求
2. 验证列分隔符和行分隔符
3. 确认数据类型匹配
4. 使用数据转换功能
5. 设置合理的容错率

### 9.3 内存不足问题

**问题**：导入过程中出现内存不足错误

**解决方案**：
1. 减少批次大小
2. 增加BE节点内存
3. 优化查询语句
4. 使用流式导入
5. 分批导入大数据量

### 9.4 数据一致性问题

**问题**：导入数据不一致或丢失

**解决方案**：
1. 使用事务保证数据一致性
2. 设置合适的导入标签
3. 检查导入结果
4. 实现数据校验机制
5. 建立数据备份机制

## 10. 本章小结

本章详细介绍了Doris的多种数据导入方式，包括Stream Load、Broker Load、Routine Load、Insert Into和Spark Load，以及它们的使用场景、语法结构和最佳实践。

### 10.1 关键要点

1. **Stream Load**：适合小到中等规模数据的实时导入
2. **Broker Load**：适合大数据量的批量导入
3. **Routine Load**：适合从消息队列持续导入数据
4. **Insert Into**：适合SQL方式导入数据
5. **Spark Load**：适合TB级大数据量导入

### 10.2 最佳实践

1. 根据业务场景选择合适的导入方式
2. 优化导入性能，提高导入效率
3. 处理导入过程中的错误和异常
4. 监控导入任务，确保数据质量
5. 建立完善的数据导入流程

## 11. 延伸阅读

- [Doris官方文档-数据导入](https://doris.apache.org/docs/data-operate/import/)
- [Doris数据导入最佳实践](https://doris.apache.org/docs/data-operate/import-best-practice/)
- [Doris数据导入性能调优](https://doris.apache.org/docs/data-operate/import-performance-tuning/)

## 12. 实践练习

1. 使用Stream Load导入CSV格式数据
2. 使用Stream Load导入JSON格式数据并进行数据转换
3. 使用Broker Load从HDFS导入数据
4. 使用Routine Load从Kafka导入数据
5. 使用Insert Into从查询结果导入数据
6. 设计一个电商系统的数据导入方案
7. 设计一个日志分析系统的数据导入方案
8. 实现数据导入监控和报警机制