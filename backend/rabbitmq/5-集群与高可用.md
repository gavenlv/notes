# 第5章：集群与高可用

## 1. 集群基础

### 1.1 集群概述

#### 什么是集群

RabbitMQ集群是将多个RabbitMQ服务器节点连接在一起，形成一个逻辑上的单一消息代理。集群提供以下优势：

- **高可用性**：当某个节点故障时，其他节点可以继续提供服务
- **扩展性**：可以通过添加节点来扩展系统容量
- **负载均衡**：可以将客户端连接分散到多个节点
- **数据冗余**：重要数据可以在多个节点间复制

#### 集群架构

RabbitMQ集群采用对等架构（Peer-to-Peer），所有节点都是平等的：

```
+-------------+      +-------------+      +-------------+
|  RabbitMQ   | <--> |  RabbitMQ   | <--> |  RabbitMQ   |
|  Node 1     |      |  Node 2     |      |  Node 3     |
+-------------+      +-------------+      +-------------+
```

#### 集群中的数据

集群中的数据分为两类：

1. **元数据**：包括交换机、队列、绑定等定义，所有节点都有完整的副本
2. **消息数据**：默认情况下只存储在声明队列的节点上（除非使用镜像队列）

### 1.2 集群类型

#### 普通集群

默认的集群模式，只有元数据复制，消息不复制：

- **优点**：性能高，存储开销小
- **缺点**：队列所在节点故障时，队列不可用

#### 镜像队列集群

使用HaMode配置，队列和消息在多个节点间复制：

- **优点**：高可用，节点故障时自动故障转移
- **缺点**：性能较低，存储开销大

#### 仲裁队列集群

使用Quorum Queues（基于Raft协议）：

- **优点**：强一致性，数据安全性高
- **缺点**：延迟较高，资源消耗大

## 2. 集群搭建

### 2.1 集群规划

#### 节点规划

- **最小集群**：3个节点（提供故障容忍能力）
- **推荐配置**：奇数节点（避免脑裂）
- **节点角色**：所有节点都可以接受连接和消息

#### 网络规划

- **内部通信**：节点间需要稳定的网络连接
- **客户端访问**：可以使用负载均衡器或DNS轮询
- **防火墙**：开放必要端口（5672、15672、25672等）

#### 存储规划

- **数据目录**：每个节点需要独立的存储
- **磁盘空间**：考虑消息大小和保留时间
- **I/O性能**：高I/O性能的存储提高队列性能

### 2.2 手动搭建集群

#### 步骤1：安装多个RabbitMQ节点

```bash
# 在三台服务器上安装RabbitMQ
sudo apt-get install rabbitmq-server

# 启动RabbitMQ服务
sudo systemctl start rabbitmq-server
sudo systemctl enable rabbitmq-server
```

#### 步骤2：配置Erlang Cookie

```bash
# 停止RabbitMQ服务
sudo systemctl stop rabbitmq-server

# 确保所有节点的Erlang Cookie相同
# 将node1的cookie复制到其他节点
sudo scp /var/lib/rabbitmq/.erlang.cookie user@node2:/var/lib/rabbitmq/.erlang.cookie
sudo scp /var/lib/rabbitmq/.erlang.cookie user@node3:/var/lib/rabbitmq/.erlang.cookie

# 设置正确的权限
sudo chmod 400 /var/lib/rabbitmq/.erlang.cookie
sudo chown rabbitmq:rabbitmq /var/lib/rabbitmq/.erlang.cookie

# 重启RabbitMQ服务
sudo systemctl start rabbitmq-server
```

#### 步骤3：组成集群

```bash
# 在node2上加入集群
sudo rabbitmqctl stop_app
sudo rabbitmqctl join_cluster rabbit@node1
sudo rabbitmqctl start_app

# 在node3上加入集群
sudo rabbitmqctl stop_app
sudo rabbitmqctl join_cluster rabbit@node1
sudo rabbitmqctl start_app
```

#### 步骤4：验证集群状态

```bash
# 查看集群状态
sudo rabbitmqctl cluster_status

# 输出示例：
# Cluster status of node 'rabbit@node1' ...
# [{nodes,[{disc,['rabbit@node1','rabbit@node2','rabbit@node3']}]},
#  {running_nodes,['rabbit@node3','rabbit@node2','rabbit@node1']},
#  {cluster_name,<<"rabbit@node1">>},
#  {partitions,[]},
#  {alarms,[{'rabbit@node1',[]},
#           {'rabbit@node2',[]},
#           {'rabbit@node3',[]}]}]
```

### 2.3 自动化集群搭建

#### 使用Docker Compose

```yaml
# docker-compose.yml
version: '3.8'
services:
  rabbitmq1:
    image: rabbitmq:3.9-management
    hostname: rabbitmq1
    environment:
      - RABBITMQ_ERLANG_COOKIE=rabbitmq_cookie_secret
      - RABBITMQ_DEFAULT_USER=admin
      - RABBITMQ_DEFAULT_PASS=password
    ports:
      - "5672:5672"
      - "15672:15672"
    volumes:
      - rabbitmq1_data:/var/lib/rabbitmq
    command: >
      bash -c "
      rabbitmq-server -detached;
      rabbitmqctl wait /var/run/rabbitmq/server.pid;
      rabbitmqctl stop_app;
      rabbitmqctl reset;
      rabbitmqctl start_app;
      tail -f /dev/null
      "

  rabbitmq2:
    image: rabbitmq:3.9-management
    hostname: rabbitmq2
    environment:
      - RABBITMQ_ERLANG_COOKIE=rabbitmq_cookie_secret
      - RABBITMQ_DEFAULT_USER=admin
      - RABBITMQ_DEFAULT_PASS=password
    ports:
      - "5673:5672"
      - "15673:15672"
    volumes:
      - rabbitmq2_data:/var/lib/rabbitmq
    depends_on:
      - rabbitmq1
    command: >
      bash -c "
      rabbitmq-server -detached;
      rabbitmqctl wait /var/run/rabbitmq/server.pid;
      rabbitmqctl stop_app;
      rabbitmqctl reset;
      rabbitmqctl join_cluster rabbitmq1;
      rabbitmqctl start_app;
      tail -f /dev/null
      "

  rabbitmq3:
    image: rabbitmq:3.9-management
    hostname: rabbitmq3
    environment:
      - RABBITMQ_ERLANG_COOKIE=rabbitmq_cookie_secret
      - RABBITMQ_DEFAULT_USER=admin
      - RABBITMQ_DEFAULT_PASS=password
    ports:
      - "5674:5672"
      - "15674:15672"
    volumes:
      - rabbitmq3_data:/var/lib/rabbitmq
    depends_on:
      - rabbitmq1
    command: >
      bash -c "
      rabbitmq-server -detached;
      rabbitmqctl wait /var/run/rabbitmq/server.pid;
      rabbitmqctl stop_app;
      rabbitmqctl reset;
      rabbitmqctl join_cluster rabbitmq1;
      rabbitmqctl start_app;
      tail -f /dev/null
      "

volumes:
  rabbitmq1_data:
  rabbitmq2_data:
  rabbitmq3_data:
```

#### 使用Kubernetes

```yaml
# rabbitmq-statefulset.yaml
apiVersion: v1
kind: Service
metadata:
  name: rabbitmq
  labels:
    app: rabbitmq
spec:
  ports:
  - port: 5672
    name: amqp
  - port: 15672
    name: http
  clusterIP: None
  selector:
    app: rabbitmq
---
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: rabbitmq
spec:
  serviceName: "rabbitmq"
  replicas: 3
  selector:
    matchLabels:
      app: rabbitmq
  template:
    metadata:
      labels:
        app: rabbitmq
    spec:
      containers:
      - name: rabbitmq
        image: rabbitmq:3.9-management
        ports:
        - containerPort: 5672
          name: amqp
        - containerPort: 15672
          name: http
        env:
        - name: RABBITMQ_ERLANG_COOKIE
          value: "rabbitmq_cookie_secret"
        - name: RABBITMQ_DEFAULT_USER
          value: "admin"
        - name: RABBITMQ_DEFAULT_PASS
          value: "password"
        volumeMounts:
        - name: rabbitmq-storage
          mountPath: /var/lib/rabbitmq
  volumeClaimTemplates:
  - metadata:
      name: rabbitmq-storage
    spec:
      accessModes: ["ReadWriteOnce"]
      resources:
        requests:
          storage: 10Gi
```

## 3. 镜像队列与高可用

### 3.1 镜像队列

#### 什么是镜像队列

镜像队列是一种高可用队列，将队列的内容复制到多个节点上。当主节点故障时，其中一个副本会自动成为新的主节点。

#### 镜像队列工作原理

```
生产者请求      主节点         副本1         副本2
     |           |            |            |
     +---------->|            |            |
                 |----------->|            |
                 |            |----------->|
                 |            |            |
                 |<-----------|            |
                 |<------------------------|
     |<----------|            |            |
```

#### 镜像队列配置

##### 使用策略配置

```bash
# 配置所有队列为镜像队列，在所有节点复制
sudo rabbitmqctl set_policy ha-all "^" '{"ha-mode":"all"}'

# 配置以ha开头的队列为镜像队列，在两个节点复制
sudo rabbitmqctl set_policy ha-two "^ha\." '{"ha-mode":"exactly","ha-params":2}'

# 配置特定节点上的镜像队列
sudo rabbitmqctl set_policy ha-nodes "^critical\." '{"ha-mode":"nodes","ha-params":["rabbit@node1","rabbit@node2"]}'
```

##### 使用参数声明队列

```python
# Python示例：声明镜像队列
import pika

connection = pika.BlockingConnection(pika.ConnectionParameters('localhost'))
channel = connection.channel()

# 声明镜像队列
channel.queue_declare(
    queue='ha_queue',
    durable=True,
    arguments={
        "x-ha-policy": "all",  # 在所有节点复制
    }
)

# 或者指定特定节点
channel.queue_declare(
    queue='ha_queue_specific',
    durable=True,
    arguments={
        "x-ha-policy": "nodes",
        "x-ha-nodes": ["rabbit@node1", "rabbit@node2"]  # 在特定节点复制
    }
)

connection.close()
```

### 3.2 仲裁队列

#### 什么是仲裁队列

仲裁队列（Quorum Queues）是基于Raft共识算法实现的高可用队列，提供强一致性和容错能力。

#### 仲裁队列特点

- **强一致性**：所有副本上的消息顺序一致
- **容错能力**：可以容忍(n-1)/2个节点故障
- **数据安全**：消息在写入磁盘后才确认
- **选举机制**：主节点故障时自动选举新主节点

#### 仲裁队列配置

```python
# Python示例：声明仲裁队列
import pika

connection = pika.BlockingConnection(pika.ConnectionParameters('localhost'))
channel = connection.channel()

# 声明仲裁队列
channel.queue_declare(
    queue='quorum_queue',
    durable=True,
    arguments={
        "x-queue-type": "quorum",  # 声明为仲裁队列
        "x-delivery-limit": 10,    # 投递限制
    }
)

connection.close()
```

#### 仲裁队列高级配置

```python
# 声明高级仲裁队列
channel.queue_declare(
    queue='advanced_quorum_queue',
    durable=True,
    arguments={
        "x-queue-type": "quorum",
        "x-delivery-limit": 10,           # 投递限制
        "x-max-length": 100000,           # 最大消息数
        "x-max-length-bytes": 1000000000, # 最大字节数
        "x-overflow": "drop-head",        # 溢出策略
        "x-quorum-initial-group-size": 3, # 初始副本数
        "x-dead-letter-exchange": "dlx",  # 死信交换机
        "x-dead-letter-routing-key": "dlq" # 死信路由键
    }
)
```

### 3.3 故障处理与恢复

#### 节点故障场景

##### 场景1：镜像队列主节点故障

```
故障前：
主节点    副本1    副本2
  |        |        |
消息A     消息A     消息A
消息B     消息B     消息B

故障后：
         副本1(新主) 副本2
           |        |
         消息A     消息A
         消息B     消息B
```

##### 场景2：仲裁队列主节点故障

```
故障前：
主节点    副本1    副本2
  |        |        |
消息A     消息A     消息A
消息B     消息B     消息B

故障后（选举过程）：
         副本1     副本2
           |        |
        选举中     选举中

恢复后：
         副本1(新主) 副本2
           |        |
         消息A     消息A
         消息B     消息B
```

#### 故障处理最佳实践

##### 1. 监控集群状态

```python
# Python示例：监控集群状态
import pika
import json
import time

class ClusterMonitor:
    def __init__(self, host='localhost', username='admin', password='password'):
        credentials = pika.PlainCredentials(username, password)
        self.connection = pika.BlockingConnection(
            pika.ConnectionParameters(host, credentials=credentials)
        )
        self.channel = self.connection.channel()
    
    def check_cluster_status(self):
        # 使用RabbitMQ API获取集群状态
        # 实际项目中可以使用requests库调用HTTP API
        print("[*] Checking cluster status...")
        
        # 这里只是示例，实际应该调用API获取状态
        status = {
            "nodes": ["rabbit@node1", "rabbit@node2", "rabbit@node3"],
            "running_nodes": ["rabbit@node1", "rabbit@node2", "rabbit@node3"],
            "partitions": []
        }
        
        print(f"  Nodes: {', '.join(status['nodes'])}")
        print(f"  Running nodes: {', '.join(status['running_nodes'])}")
        
        if len(status['partitions']) > 0:
            print(f"  [!] Partitions detected: {', '.join(status['partitions'])}")
        else:
            print("  [✓] No partitions")
            
        # 检查所有节点是否在线
        offline_nodes = set(status['nodes']) - set(status['running_nodes'])
        if offline_nodes:
            print(f"  [!] Offline nodes: {', '.join(offline_nodes)}")
        else:
            print("  [✓] All nodes online")
        
        return status
    
    def check_queue_status(self, queue_name):
        # 获取队列状态
        print(f"[*] Checking queue status for '{queue_name}'...")
        
        # 检查队列是否存在
        try:
            self.channel.queue_declare(queue=queue_name, passive=True)
            print(f"  [✓] Queue exists")
        except pika.exceptions.ChannelClosedByBroker:
            print(f"  [!] Queue not found")
            return
        
        # 实际应该调用API获取队列详细状态
        queue_status = {
            "state": "running",
            "messages": 100,
            "consumers": 2,
            "backing_queue_status": "ok"
        }
        
        print(f"  State: {queue_status['state']}")
        print(f"  Messages: {queue_status['messages']}")
        print(f"  Consumers: {queue_status['consumers']}")
        print(f"  Backing queue: {queue_status['backing_queue_status']}")
        
        return queue_status
    
    def monitor_loop(self, interval=10):
        """监控循环"""
        try:
            while True:
                print("\n" + "=" * 50)
                print(f"Cluster Monitor - {time.ctime()}")
                print("=" * 50)
                
                # 检查集群状态
                cluster_status = self.check_cluster_status()
                
                # 检查重要队列状态
                important_queues = ["ha_queue", "quorum_queue", "critical_queue"]
                for queue in important_queues:
                    try:
                        self.check_queue_status(queue)
                    except Exception as e:
                        print(f"  [!] Error checking queue '{queue}': {e}")
                
                print("\nMonitoring... (Ctrl+C to stop)")
                time.sleep(interval)
                
        except KeyboardInterrupt:
            print("\n[*] Stopping monitor...")
            self.connection.close()
    
    def close(self):
        self.connection.close()

# 运行集群监控
if __name__ == "__main__":
    monitor = ClusterMonitor()
    try:
        monitor.monitor_loop(interval=10)
    finally:
        monitor.close()
```

##### 2. 自动故障转移

```python
# Python示例：自动故障转移
import pika
import time
import random
import threading

class FailoverHandler:
    def __init__(self, hosts, username='admin', password='password'):
        self.hosts = hosts
        self.username = username
        self.password = password
        self.current_host_index = 0
        self.connection = None
        self.channel = None
        self.is_connected = False
        self.reconnect_thread = None
        self.stop_flag = False
        
        # 初始连接
        self._connect()
    
    def _connect(self):
        """连接到RabbitMQ集群"""
        while not self.stop_flag:
            try:
                if self.connection and self.connection.is_open:
                    self.connection.close()
                
                host = self.hosts[self.current_host_index]
                print(f"[*] Connecting to RabbitMQ at {host}...")
                
                credentials = pika.PlainCredentials(self.username, self.password)
                self.connection = pika.BlockingConnection(
                    pika.ConnectionParameters(
                        host,
                        credentials=credentials,
                        heartbeat=600,  # 心跳间隔
                        blocked_connection_timeout=300,  # 阻塞连接超时
                    )
                )
                self.channel = self.connection.channel()
                
                self.is_connected = True
                print(f"[✓] Connected to RabbitMQ at {host}")
                return True
                
            except Exception as e:
                print(f"[!] Failed to connect to {host}: {e}")
                
                # 尝试下一个节点
                self.current_host_index = (self.current_host_index + 1) % len(self.hosts)
                time.sleep(2)
        
        return False
    
    def _ensure_connected(self):
        """确保连接有效"""
        if not self.is_connected or not self.connection.is_open:
            self.is_connected = False
            self._connect()
        return self.is_connected
    
    def _monitor_connection(self):
        """监控连接状态并自动重连"""
        while not self.stop_flag:
            try:
                if self.is_connected and self.connection:
                    try:
                        # 测试连接
                        self.connection.process_data_events(time_limit=1)
                    except pika.exceptions.ConnectionClosed:
                        print("[!] Connection lost, attempting to reconnect...")
                        self.is_connected = False
                    except pika.exceptions.AMQPConnectionError:
                        print("[!] Connection error, attempting to reconnect...")
                        self.is_connected = False
                
                if not self.is_connected:
                    self._connect()
                
                time.sleep(5)
                
            except Exception as e:
                print(f"[!] Error in connection monitor: {e}")
                time.sleep(10)
    
    def start_monitoring(self):
        """启动连接监控"""
        if self.reconnect_thread is None or not self.reconnect_thread.is_alive():
            self.stop_flag = False
            self.reconnect_thread = threading.Thread(target=self._monitor_connection)
            self.reconnect_thread.daemon = True
            self.reconnect_thread.start()
            print("[*] Connection monitoring started")
    
    def stop_monitoring(self):
        """停止连接监控"""
        self.stop_flag = True
        if self.reconnect_thread and self.reconnect_thread.is_alive():
            self.reconnect_thread.join(timeout=5)
        print("[*] Connection monitoring stopped")
    
    def publish_message(self, exchange, routing_key, message):
        """发布消息（带有自动重连）"""
        if not self._ensure_connected():
            return False
        
        try:
            self.channel.basic_publish(
                exchange=exchange,
                routing_key=routing_key,
                body=message.encode('utf-8'),
                properties=pika.BasicProperties(
                    delivery_mode=2,  # 持久化
                )
            )
            return True
        except (pika.exceptions.ConnectionClosed, pika.exceptions.AMQPConnectionError):
            print("[!] Connection lost during publish, marking as disconnected")
            self.is_connected = False
            return False
    
    def consume_messages(self, queue_name, callback):
        """消费消息（带有自动重连）"""
        if not self._ensure_connected():
            return False
        
        try:
            self.channel.basic_consume(
                queue=queue_name,
                on_message_callback=callback,
                auto_ack=False
            )
            return True
        except (pika.exceptions.ConnectionClosed, pika.exceptions.AMQPConnectionError):
            print("[!] Connection lost during consume setup, marking as disconnected")
            self.is_connected = False
            return False
    
    def start_consuming(self):
        """开始消费消息"""
        if self._ensure_connected():
            try:
                self.channel.start_consuming()
            except (pika.exceptions.ConnectionClosed, pika.exceptions.AMQPConnectionError):
                print("[!] Connection lost during consume, marking as disconnected")
                self.is_connected = False
    
    def close(self):
        """关闭连接"""
        self.stop_monitoring()
        
        if self.connection and self.connection.is_open:
            try:
                self.connection.close()
            except Exception:
                pass
        
        print("[*] Connection closed")

# 使用故障转移处理器
if __name__ == "__main__":
    # 定义集群节点
    cluster_hosts = ["node1", "node2", "node3"]
    
    # 创建故障转移处理器
    failover_handler = FailoverHandler(cluster_hosts)
    
    # 启动连接监控
    failover_handler.start_monitoring()
    
    def message_callback(ch, method, properties, body):
        """消息回调"""
        print(f" [x] Received: {body.decode()}")
        ch.basic_ack(delivery_tag=method.delivery_tag)
    
    try:
        # 设置队列
        failover_handler.channel.queue_declare(queue="failover_test", durable=True)
        
        # 设置消费者
        failover_handler.consume_messages("failover_test", message_callback)
        
        # 开始消费
        print(" [*] Waiting for messages. To exit press CTRL+C")
        failover_handler.start_consuming()
        
    except KeyboardInterrupt:
        print(" [*] Shutting down")
    finally:
        failover_handler.close()
```

##### 3. 数据备份与恢复

```bash
# 导出定义（交换机、队列、绑定等）
sudo rabbitmqctl export_definitions /path/to/definitions.json

# 导入定义
sudo rabbitmqctl import_definitions /path/to/definitions.json

# 导出队列消息（需要插件）
sudo rabbitmq-plugins enable rabbitmq_shovel_management

# 使用federation或shovel备份数据到另一个集群
```

## 4. 负载均衡与分片

### 4.1 客户端负载均衡

#### 多主机连接

```python
# Python示例：客户端负载均衡
import pika
import random
import threading
import time

class ClientLoadBalancer:
    def __init__(self, hosts, username='admin', password='password'):
        self.hosts = hosts
        self.username = username
        self.password = password
        self.connections = {}
        self.channels = {}
        self.round_robin_index = 0
        
        # 初始化所有连接
        for host in hosts:
            self._create_connection(host)
    
    def _create_connection(self, host):
        """创建连接"""
        try:
            credentials = pika.PlainCredentials(self.username, self.password)
            connection = pika.BlockingConnection(
                pika.ConnectionParameters(
                    host,
                    credentials=credentials
                )
            )
            
            self.connections[host] = connection
            self.channels[host] = connection.channel()
            print(f"[✓] Connected to {host}")
            return True
        except Exception as e:
            print(f"[!] Failed to connect to {host}: {e}")
            return False
    
    def get_random_connection(self):
        """获取随机连接"""
        available_hosts = list(self.connections.keys())
        if not available_hosts:
            return None, None
        
        host = random.choice(available_hosts)
        return host, self.connections[host]
    
    def get_round_robin_connection(self):
        """获取轮询连接"""
        available_hosts = list(self.connections.keys())
        if not available_hosts:
            return None, None
        
        host = available_hosts[self.round_robin_index % len(available_hosts)]
        self.round_robin_index += 1
        return host, self.connections[host]
    
    def publish_with_load_balance(self, exchange, routing_key, message, strategy="random"):
        """使用负载均衡发布消息"""
        if strategy == "random":
            host, connection = self.get_random_connection()
        elif strategy == "round_robin":
            host, connection = self.get_round_robin_connection()
        else:
            host, connection = self.get_random_connection()
        
        if not connection:
            print("[!] No available connections")
            return False
        
        try:
            channel = self.channels[host]
            channel.basic_publish(
                exchange=exchange,
                routing_key=routing_key,
                body=message.encode('utf-2')
            )
            print(f"[x] Sent to {host}: {message}")
            return True
        except Exception as e:
            print(f"[!] Failed to publish to {host}: {e}")
            # 尝试重新连接
            self._create_connection(host)
            return False
    
    def close_all(self):
        """关闭所有连接"""
        for host, connection in self.connections.items():
            try:
                connection.close()
                print(f"[✓] Closed connection to {host}")
            except Exception as e:
                print(f"[!] Error closing connection to {host}: {e}")

# 使用客户端负载均衡
if __name__ == "__main__":
    # 定义集群节点
    cluster_hosts = ["localhost", "localhost", "localhost"]  # 在实际中应该是不同的主机
    
    # 创建负载均衡器
    load_balancer = ClientLoadBalancer(cluster_hosts)
    
    try:
        # 发布100条消息，使用随机策略
        print(" [*] Publishing messages with random strategy")
        for i in range(10):
            message = f"Message {i}"
            load_balancer.publish_with_load_balance("", "load_balance_test", message, "random")
            time.sleep(0.1)
        
        # 发布100条消息，使用轮询策略
        print("\n [*] Publishing messages with round-robin strategy")
        for i in range(10):
            message = f"Message {i}"
            load_balancer.publish_with_load_balance("", "load_balance_test", message, "round_robin")
            time.sleep(0.1)
        
    except KeyboardInterrupt:
        print("\n [*] Shutting down")
    finally:
        load_balancer.close_all()
```

### 4.2 集群负载均衡

#### 使用HAProxy

```
# haproxy.cfg
global
    daemon
    maxconn 4096

defaults
    mode tcp
    timeout connect 5000ms
    timeout client 50000ms
    timeout server 50000ms

listen rabbitmq
    bind *:5672
    mode tcp
    balance roundrobin
    server rabbit1 node1:5672 check
    server rabbit2 node2:5672 check
    server rabbit3 node3:5672 check

listen rabbitmq_mgmt
    bind *:15672
    mode http
    balance roundrobin
    server rabbit1 node1:15672 check
    server rabbit2 node2:15672 check
    server rabbit3 node3:15672 check
```

#### 使用Nginx

```
# nginx.conf
stream {
    upstream rabbitmq {
        server node1:5672;
        server node2:5672;
        server node3:5672;
    }
    
    server {
        listen 5672;
        proxy_pass rabbitmq;
        proxy_timeout 1s;
        proxy_responses 1;
        error_log /var/log/nginx/rabbitmq.log;
    }
}

http {
    upstream rabbitmq_mgmt {
        server node1:15672;
        server node2:15672;
        server node3:15672;
    }
    
    server {
        listen 15672;
        location / {
            proxy_pass http://rabbitmq_mgmt;
        }
    }
}
```

### 4.3 队列分片

#### 基于哈希的分片

```python
# Python示例：基于哈希的队列分片
import pika
import hashlib

class QueueSharding:
    def __init__(self, host='localhost', shard_count=3, username='admin', password='password'):
        credentials = pika.PlainCredentials(username, password)
        self.connection = pika.BlockingConnection(
            pika.ConnectionParameters(host, credentials=credentials)
        )
        self.channel = self.connection.channel()
        self.shard_count = shard_count
        
        # 创建分片队列
        self._create_sharded_queues()
    
    def _create_sharded_queues(self):
        """创建分片队列"""
        for i in range(self.shard_count):
            queue_name = f"sharded_queue_{i}"
            self.channel.queue_declare(queue=queue_name, durable=True)
            print(f"[✓] Created shard queue: {queue_name}")
    
    def _get_shard_number(self, key):
        """根据键获取分片编号"""
        # 使用哈希算法确定分片
        hash_obj = hashlib.md5(key.encode())
        return int(hash_obj.hexdigest(), 16) % self.shard_count
    
    def publish_to_shard(self, message, key=None):
        """发布消息到特定分片"""
        if key is None:
            # 如果没有键，随机选择分片
            shard_num = random.randint(0, self.shard_count - 1)
        else:
            # 根据键确定分片
            shard_num = self._get_shard_number(key)
        
        queue_name = f"sharded_queue_{shard_num}"
        
        self.channel.basic_publish(
            exchange='',
            routing_key=queue_name,
            body=message.encode('utf-8'),
            properties=pika.BasicProperties(
                delivery_mode=2,  # 持久化
            )
        )
        
        print(f"[x] Sent to shard {shard_num} ({queue_name}): {message}")
        return shard_num
    
    def create_shard_consumer(self, shard_num, callback):
        """创建特定分片的消费者"""
        queue_name = f"sharded_queue_{shard_num}"
        
        self.channel.basic_consume(
            queue=queue_name,
            on_message_callback=callback,
            auto_ack=False
        )
        
        print(f"[*] Consumer for shard {shard_num} started")
    
    def consume_all_shards(self, callback):
        """消费所有分片的消息"""
        for i in range(self.shard_count):
            self.create_shard_consumer(i, callback)
        
        print(f"[*] Consuming from all {self.shard_count} shards")
        self.channel.start_consuming()
    
    def close(self):
        """关闭连接"""
        self.connection.close()

# 使用队列分片
if __name__ == "__main__":
    # 创建分片队列管理器
    sharding = QueueSharding(shard_count=3)
    
    def message_callback(ch, method, properties, body):
        """消息回调"""
        queue_name = method.routing_key
        shard_num = queue_name.split('_')[-1]
        print(f" [x] Received from shard {shard_num}: {body.decode()}")
        ch.basic_ack(delivery_tag=method.delivery_tag)
    
    try:
        # 发布消息到不同分片
        users = ["user1", "user2", "user3", "user4", "user5"]
        for user in users:
            message = f"Order data for {user}"
            sharding.publish_to_shard(message, key=user)
        
        # 消费所有分片的消息
        print("\n [*] Waiting for messages from all shards. To exit press CTRL+C")
        sharding.consume_all_shards(message_callback)
        
    except KeyboardInterrupt:
        print("\n [*] Shutting down")
    finally:
        sharding.close()
```

## 5. 集群监控与运维

### 5.1 集群监控

#### 关键指标

1. **节点状态**：节点是否在线，是否同步
2. **队列状态**：消息数量，消费者数量，镜像状态
3. **连接状态**：客户端连接数量，通道数量
4. **消息吞吐量**：发布速率，投递速率，确认速率
5. **资源使用**：内存使用，磁盘使用，CPU使用
6. **网络状态**：节点间网络延迟，分区状态

#### 监控工具

##### 1. RabbitMQ Management API

```python
# Python示例：使用RabbitMQ API监控集群
import requests
import json
import time
from datetime import datetime

class RabbitMQMonitor:
    def __init__(self, host='localhost', port=15672, username='admin', password='password'):
        self.base_url = f"http://{host}:{port}/api/"
        self.auth = (username, password)
        self.session = requests.Session()
        self.session.auth = self.auth
    
    def get_overview(self):
        """获取集群概览信息"""
        response = self.session.get(self.base_url + "overview")
        response.raise_for_status()
        return response.json()
    
    def get_nodes(self):
        """获取节点信息"""
        response = self.session.get(self.base_url + "nodes")
        response.raise_for_status()
        return response.json()
    
    def get_queues(self):
        """获取队列信息"""
        response = self.session.get(self.base_url + "queues")
        response.raise_for_status()
        return response.json()
    
    def get_connections(self):
        """获取连接信息"""
        response = self.session.get(self.base_url + "connections")
        response.raise_for_status()
        return response.json()
    
    def check_cluster_health(self):
        """检查集群健康状态"""
        try:
            overview = self.get_overview()
            nodes = self.get_nodes()
            queues = self.get_queues()
            
            # 检查节点状态
            online_nodes = [node for node in nodes if node["running"]]
            if len(online_nodes) < len(nodes):
                print(f"[!] Not all nodes are online: {len(online_nodes)}/{len(nodes)}")
            else:
                print(f"[✓] All {len(nodes)} nodes are online")
            
            # 检查分区
            if overview["partitions"]:
                print(f"[!] Partitions detected: {overview['partitions']}")
            else:
                print("[✓] No partitions")
            
            # 检查内存使用
            total_mem_used = sum(node["mem_used"] for node in nodes)
            total_mem_limit = sum(node["mem_limit"] for node in nodes)
            mem_usage_percent = (total_mem_used / total_mem_limit) * 100
            
            print(f"[*] Memory usage: {mem_usage_percent:.2f}% ({total_mem_used/1024/1024:.2f}MB/{total_mem_limit/1024/1024:.2f}MB)")
            
            if mem_usage_percent > 80:
                print("[!] High memory usage detected")
            
            # 检查队列状态
            total_messages = sum(queue.get("messages", 0) for queue in queues)
            total_unacked = sum(queue.get("messages_unacknowledged", 0) for queue in queues)
            
            print(f"[*] Total messages: {total_messages}")
            print(f"[*] Unacknowledged messages: {total_unacked}")
            
            # 检查大队列
            large_queues = [queue for queue in queues if queue.get("messages", 0) > 1000]
            if large_queues:
                print(f"[!] Large queues detected: {len(large_queues)}")
                for queue in large_queues[:5]:  # 只显示前5个
                    print(f"    - {queue['name']}: {queue['messages']} messages")
            
            return {
                "timestamp": datetime.now().isoformat(),
                "nodes": len(nodes),
                "online_nodes": len(online_nodes),
                "mem_usage_percent": mem_usage_percent,
                "total_messages": total_messages,
                "total_unacked": total_unacked,
                "partitions": len(overview["partitions"]),
                "large_queues": len(large_queues)
            }
            
        except Exception as e:
            print(f"[!] Error checking cluster health: {e}")
            return None
    
    def monitor_loop(self, interval=30):
        """监控循环"""
        history = []
        
        try:
            while True:
                print("\n" + "=" * 60)
                print(f"RabbitMQ Cluster Monitor - {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
                print("=" * 60)
                
                health = self.check_cluster_health()
                if health:
                    history.append(health)
                    
                    # 保持最近24小时的数据（假设每30秒一次，最多2880条）
                    if len(history) > 2880:
                        history = history[-2880:]
                
                print("\nMonitoring... (Ctrl+C to stop)")
                time.sleep(interval)
                
        except KeyboardInterrupt:
            print("\n[*] Stopping monitor...")
            return history
    
    def export_health_data(self, filename, history):
        """导出健康数据到文件"""
        try:
            with open(filename, 'w') as f:
                json.dump(history, f, indent=2)
            print(f"[*] Health data exported to {filename}")
        except Exception as e:
            print(f"[!] Error exporting data: {e}")

# 使用RabbitMQ监控器
if __name__ == "__main__":
    monitor = RabbitMQMonitor()
    
    try:
        # 运行监控循环
        history = monitor.monitor_loop(interval=30)
        
        # 导出历史数据
        if history:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filename = f"rabbitmq_health_{timestamp}.json"
            monitor.export_health_data(filename, history)
            
    except KeyboardInterrupt:
        print("\n[*] Shutting down")
```

##### 2. Prometheus + Grafana

```yaml
# prometheus.yml
global:
  scrape_interval: 15s

scrape_configs:
  - job_name: 'rabbitmq'
    static_configs:
      - targets: ['localhost:15692']
    metrics_path: /metrics
```

```yaml
# rabbitmq-exporter.yaml (Kubernetes)
apiVersion: apps/v1
kind: Deployment
metadata:
  name: rabbitmq-exporter
spec:
  replicas: 1
  selector:
    matchLabels:
      app: rabbitmq-exporter
  template:
    metadata:
      labels:
        app: rabbitmq-exporter
    spec:
      containers:
      - name: rabbitmq-exporter
        image: kbudde/rabbitmq-exporter:latest
        env:
        - name: RABBIT_URL
          value: "http://rabbitmq:15672"
        - name: RABBIT_USER
          value: "admin"
        - name: RABBIT_PASSWORD
          value: "password"
        ports:
        - containerPort: 9090
```

### 5.2 集群运维

#### 节点管理

##### 1. 添加新节点

```bash
# 在新节点上安装RabbitMQ
sudo apt-get install rabbitmq-server

# 停止服务并设置相同的Erlang Cookie
sudo systemctl stop rabbitmq-server
sudo bash -c "echo 'shared_cookie_value' > /var/lib/rabbitmq/.erlang.cookie"
sudo chmod 400 /var/lib/rabbitmq/.erlang.cookie
sudo chown rabbitmq:rabbitmq /var/lib/rabbitmq/.erlang.cookie

# 启动服务
sudo systemctl start rabbitmq-server

# 加入集群
sudo rabbitmqctl stop_app
sudo rabbitmqctl join_cluster rabbit@existing_node
sudo rabbitmqctl start_app

# 验证集群状态
sudo rabbitmqctl cluster_status
```

##### 2. 移除节点

```bash
# 在要移除的节点上
sudo rabbitmqctl stop_app
sudo rabbitmqctl reset
sudo rabbitmqctl start_app

# 或者在其他节点上强制移除
sudo rabbitmqctl forget_cluster_node rabbit@node_to_remove
```

##### 3. 重启节点

```bash
# 正常重启
sudo systemctl restart rabbitmq-server

# 或使用rabbitmqctl
sudo rabbitmqctl stop_app
sudo rabbitmqctl start_app
```

#### 集群升级

##### 滚动升级步骤

1. **检查集群状态**：确保所有节点健康
2. **停止一个节点**：选择一个节点进行升级
3. **备份配置和数据**：防止升级失败
4. **升级软件**：安装新版本的RabbitMQ
5. **启动节点**：验证节点正常加入集群
6. **重复步骤**：对其他节点重复上述步骤
7. **完成升级**：所有节点升级后验证功能

```bash
#!/bin/bash
# 滚动升级脚本示例

NODES=("rabbit@node1" "rabbit@node2" "rabbit@node3")
USER="admin"
PASSWORD="password"

for node in "${NODES[@]}"; do
    echo "Upgrading node: $node"
    
    # 1. 检查节点状态
    rabbitmqctl cluster_status | grep "$node"
    
    # 2. 停止节点
    ssh $node "sudo rabbitmqctl stop_app"
    
    # 3. 备份数据
    ssh $node "sudo cp -r /var/lib/rabbitmq /var/lib/rabbitmq.backup.$(date +%Y%m%d)"
    
    # 4. 升级软件
    ssh $node "sudo apt-get update && sudo apt-get install -y rabbitmq-server"
    
    # 5. 启动节点
    ssh $node "sudo rabbitmqctl start_app"
    
    # 6. 验证节点状态
    rabbitmqctl cluster_status | grep "$node"
    
    # 7. 等待节点同步
    sleep 60
    
    echo "Node $node upgraded successfully"
    echo "============================"
done

echo "Rolling upgrade completed"
```

#### 数据迁移

##### 1. 使用Federation

```python
# Python示例：设置Federation上游
import pika

def setup_federation_upstream():
    """设置Federation上游"""
    # 连接到目标集群
    credentials = pika.PlainCredentials('admin', 'password')
    connection = pika.BlockingConnection(
        pika.ConnectionParameters('target-cluster', credentials=credentials)
    )
    channel = connection.channel()
    
    # 声明上游
    channel.exchange_declare(
        exchange='federation_upstream',
        exchange_type='x-federation-upstream',
        arguments={
            'uri': 'amqp://admin:password@source-cluster',
            'exchange': 'source_exchange'
        }
    )
    
    # 声明联邦交换机
    channel.exchange_declare(
        exchange='federated_exchange',
        exchange_type='topic',
        arguments={
            'federation-upstream-set': 'federation_upstream'
        }
    )
    
    connection.close()
    print("[*] Federation upstream configured")
```

##### 2. 使用Shovel

```python
# Python示例：设置Shovel
import pika

def setup_shovel():
    """设置Shovel"""
    # 连接到源集群
    credentials = pika.PlainCredentials('admin', 'password')
    connection = pika.BlockingConnection(
        pika.ConnectionParameters('source-cluster', credentials=credentials)
    )
    channel = connection.channel()
    
    # 配置Shovel
    shovel_config = {
        'src-uri': 'amqp://admin:password@source-cluster',
        'src-queue': 'source_queue',
        'dest-uri': 'amqp://admin:password@target-cluster',
        'dest-queue': 'target_queue',
        'reconnect-delay': 5
    }
    
    # 声明Shovel
    channel.queue_declare(
        queue='migration_shovel',
        arguments={'x-dead-letter-exchange': ''}
    )
    
    # 设置Shovel策略
    connection.close()
    print("[*] Shovel configured")
```

## 6. 最佳实践

### 6.1 集群规划

1. **节点数量**：使用奇数节点（3、5、7）避免脑裂
2. **节点分布**：将节点分布在不同机架或可用区
3. **资源配置**：确保节点有足够的内存和磁盘空间
4. **网络隔离**：使用专用网络进行节点间通信

### 6.2 高可用策略

1. **关键队列镜像**：对关键业务队列使用镜像队列或仲裁队列
2. **客户端重连**：实现客户端自动重连和故障转移
3. **数据备份**：定期备份交换机、队列和绑定定义
4. **监控告警**：设置全面的监控和告警机制

### 6.3 性能优化

1. **队列分片**：对高吞吐量队列使用分片策略
2. **连接池**：使用连接池管理客户端连接
3. **预取设置**：调整消费者预取数量优化处理速度
4. **持久化权衡**：根据业务需求选择合适的持久化级别

### 6.4 运维管理

1. **版本升级**：使用滚动升级策略避免服务中断
2. **容量规划**：定期评估和扩展集群容量
3. **故障演练**：定期进行故障转移演练
4. **文档维护**：保持架构和配置文档的更新

## 总结

RabbitMQ集群与高可用是构建可靠消息系统的核心组件。通过本章的学习，您应该能够：

1. **设计合理的集群架构**：根据业务需求规划集群拓扑和节点配置
2. **实现高可用队列**：使用镜像队列和仲裁队列保障消息可靠性
3. **处理集群故障**：实现自动故障转移和快速恢复机制
4. **优化集群性能**：通过负载均衡和队列分片提高处理能力
5. **监控集群状态**：建立全面的监控和告警体系
6. **进行运维操作**：完成节点管理、升级和数据迁移等运维任务

集群与高可用是RabbitMQ高级应用中的关键领域，掌握这些知识将帮助您构建企业级可靠的消息系统，为业务连续性和数据安全性提供坚实保障。