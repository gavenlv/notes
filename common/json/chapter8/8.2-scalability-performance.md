# 8.2 JSON处理的可扩展性与性能优化

## 大规模JSON处理挑战

在企业级应用中，JSON数据通常具有以下特点：
- 数据量大：从几KB到GB级别
- 处理频率高：每秒成千上万的请求
- 结构复杂：多层嵌套和动态字段
- 延迟要求低：毫秒级响应时间

本节将探讨如何在各种场景下优化JSON处理的可扩展性和性能。

## 高效解析技术

### 流式解析

#### 处理大型JSON文件

```javascript
// 使用Node.js流式解析大型JSON文件
const JSONStream = require('JSONStream');
const fs = require('fs');
const { Transform } = require('stream');

// 创建自定义转换流处理数据
const processData = new Transform({
  objectMode: true,
  transform(data, encoding, callback) {
    // 处理每个数据对象
    const processed = {
      id: data.id,
      name: data.name,
      processedAt: new Date().toISOString(),
      // 执行复杂转换
      metrics: calculateMetrics(data.values)
    };
    
    this.push(processed);
    callback();
  }
});

// 设置流处理管道
console.time('Processing time');

fs.createReadStream('large-dataset.jsonl')
  .pipe(JSONStream.parse('*'))  // 解析JSON数组中的每个对象
  .pipe(processData)
  .pipe(JSONStream.stringify())
  .pipe(fs.createWriteStream('processed-dataset.json'))
  .on('finish', () => {
    console.timeEnd('Processing time');
    console.log('Processing completed successfully');
  })
  .on('error', (error) => {
    console.error('Processing failed:', error);
  });

// 复杂指标计算函数
function calculateMetrics(values) {
  return {
    average: values.reduce((sum, val) => sum + val, 0) / values.length,
    max: Math.max(...values),
    min: Math.min(...values),
    count: values.length
  };
}
```

#### Python中的流式JSON处理

```python
import ijson
import time
from typing import Iterator, Dict, Any

def process_large_json(file_path: str) -> Iterator[Dict[str, Any]]:
    """使用ijson库流式处理大型JSON文件"""
    start_time = time.time()
    
    with open(file_path, 'rb') as file:
        # 解析JSON数组中的每个对象
        parser = ijson.items(file, 'item')
        
        for i, item in enumerate(parser, 1):
            # 处理每个数据项
            processed_item = {
                'id': item.get('id'),
                'name': item.get('name'),
                'processed_at': time.time(),
                'metrics': calculate_metrics(item.get('values', []))
            }
            
            yield processed_item
            
            # 每处理1000项记录进度
            if i % 1000 == 0:
                elapsed = time.time() - start_time
                print(f"Processed {i} items in {elapsed:.2f} seconds")

def calculate_metrics(values: list) -> Dict[str, Any]:
    """计算数据指标的辅助函数"""
    if not values:
        return {'average': 0, 'max': 0, 'min': 0, 'count': 0}
    
    return {
        'average': sum(values) / len(values),
        'max': max(values),
        'min': min(values),
        'count': len(values)
    }

# 使用示例
output_file = open('processed-data.json', 'w')
output_file.write('[\n')  # 开始JSON数组

first_item = True
for processed_item in process_large_json('large-dataset.json'):
    if not first_item:
        output_file.write(',\n')
    else:
        first_item = False
    
    import json
    output_file.write(json.dumps(processed_item, indent=2))

output_file.write('\n]')  # 结束JSON数组
output_file.close()
```

### 增量解析技术

#### Java中基于SAX的JSON解析

```java
import com.fasterxml.jackson.core.JsonFactory;
import com.fasterxml.jackson.core.JsonParser;
import com.fasterxml.jackson.core.JsonToken;
import com.fasterxml.jackson.databind.JsonNode;
import com.fasterxml.jackson.databind.ObjectMapper;
import java.io.InputStream;
import java.util.HashMap;
import java.util.Map;

public class IncrementalJsonProcessor {
    
    private final ObjectMapper mapper = new ObjectMapper();
    
    /**
     * 增量解析JSON流，提取特定字段
     */
    public Map<String, Object> extractFields(InputStream inputStream, String... fieldsToExtract) throws Exception {
        Map<String, Object> result = new HashMap<>();
        JsonFactory factory = new JsonFactory();
        
        try (JsonParser parser = factory.createParser(inputStream)) {
            while (parser.nextToken() != null) {
                if (parser.currentToken() == JsonToken.FIELD_NAME) {
                    String fieldName = parser.getCurrentName();
                    
                    // 检查是否是需要提取的字段
                    if (contains(fieldsToExtract, fieldName)) {
                        parser.nextToken(); // 移动到值
                        JsonNode value = mapper.readTree(parser);
                        result.put(fieldName, value);
                    }
                }
            }
        }
        
        return result;
    }
    
    /**
     * 流式处理大型JSON数组
     */
    public void processArrayElements(InputStream inputStream, JsonElementProcessor processor) throws Exception {
        JsonFactory factory = new JsonFactory();
        
        try (JsonParser parser = factory.createParser(inputStream)) {
            // 查找数组开始
            while (parser.nextToken() != JsonToken.START_ARRAY) {
                // 跳过直到找到数组
            }
            
            // 处理数组中的每个元素
            while (parser.nextToken() != JsonToken.END_ARRAY) {
                JsonNode element = mapper.readTree(parser);
                processor.process(element);
            }
        }
    }
    
    @FunctionalInterface
    public interface JsonElementProcessor {
        void process(JsonNode element) throws Exception;
    }
    
    private boolean contains(String[] array, String value) {
        for (String item : array) {
            if (item.equals(value)) {
                return true;
            }
        }
        return false;
    }
}
```

## 内存优化技术

### 内存映射文件

#### 使用内存映射处理大型JSON

```python
import mmap
import json
import os
from typing import Dict, List, Any

class MappedJsonReader:
    """使用内存映射技术高效读取大型JSON文件"""
    
    def __init__(self, file_path: str):
        self.file_path = file_path
        self.file_size = os.path.getsize(file_path)
        self._file = None
        self._mapped_file = None
    
    def __enter__(self):
        self._file = open(self.file_path, 'r+b')
        self._mapped_file = mmap.mmap(self._file.fileno(), 0, access=mmap.ACCESS_READ)
        return self
    
    def __exit__(self, exc_type, exc_val, exc_tb):
        if self._mapped_file:
            self._mapped_file.close()
        if self._file:
            self._file.close()
    
    def extract_objects(self, start_byte: int, length: int) -> List[Dict[str, Any]]:
        """从文件的特定位置提取JSON对象"""
        if start_byte + length > self.file_size:
            raise ValueError("Requested range exceeds file size")
        
        # 读取指定范围的字节
        chunk = self._mapped_file[start_byte:start_byte + length].decode('utf-8')
        
        # 尝试解析JSON对象
        try:
            # 对于可能不完整的JSON，需要处理边界情况
            if chunk.strip().startswith('{') and chunk.strip().endswith('}'):
                return [json.loads(chunk)]
            else:
                # 查找完整的JSON对象边界
                return self._extract_complete_objects(chunk)
        except json.JSONDecodeError as e:
            print(f"JSON解析错误: {e}")
            return []
    
    def _extract_complete_objects(self, text: str) -> List[Dict[str, Any]]:
        """从文本中提取完整的JSON对象"""
        objects = []
        stack = []
        start_idx = 0
        
        for i, char in enumerate(text):
            if char == '{':
                if not stack:
                    start_idx = i
                stack.append(char)
            elif char == '}':
                if stack:
                    stack.pop()
                    if not stack:
                        # 找到完整对象
                        try:
                            obj_str = text[start_idx:i+1]
                            obj = json.loads(obj_str)
                            objects.append(obj)
                        except json.JSONDecodeError:
                            # 忽略无效的JSON对象
                            pass
        
        return objects

# 使用示例
def process_large_json_in_chunks(file_path: str, chunk_size: int = 1024 * 1024):
    """分块处理大型JSON文件"""
    file_size = os.path.getsize(file_path)
    
    with MappedJsonReader(file_path) as reader:
        for start in range(0, file_size, chunk_size):
            end = min(start + chunk_size, file_size)
            objects = reader.extract_objects(start, end - start)
            
            for obj in objects:
                process_json_object(obj)

def process_json_object(obj: Dict[str, Any]):
    """处理单个JSON对象"""
    # 实现你的对象处理逻辑
    print(f"Processing object with ID: {obj.get('id')}")

# 执行处理
process_large_json_in_chunks('very-large-dataset.json', chunk_size=2*1024*1024)  # 2MB chunks
```

#### Go中的内存映射JSON处理

```go
package main

import (
	"encoding/json"
	"fmt"
	"io/ioutil"
	"log"
	"os"
	"syscall"
	"unsafe"
)

type JSONObject map[string]interface{}

func main() {
	filePath := "large-dataset.json"
	chunkSize := 1024 * 1024 // 1MB chunks

	// 获取文件大小
	fileInfo, err := os.Stat(filePath)
	if err != nil {
		log.Fatal("无法获取文件信息:", err)
	}
	fileSize := fileInfo.Size()

	// 打开文件
	file, err := os.Open(filePath)
	if err != nil {
		log.Fatal("无法打开文件:", err)
	}
	defer file.Close()

	// 创建内存映射
	data, err := syscall.Mmap(int(file.Fd()), 0, int(fileSize), syscall.PROT_READ, syscall.MAP_SHARED)
	if err != nil {
		log.Fatal("内存映射失败:", err)
	}
	defer syscall.Munmap(data)

	// 分块处理
	for i := int64(0); i < fileSize; i += int64(chunkSize) {
		end := i + int64(chunkSize)
		if end > fileSize {
			end = fileSize
		}

		chunk := data[i:end]
		processChunk(chunk)
	}
}

func processChunk(chunk []byte) {
	// 转换为字符串以便处理
	chunkStr := string(chunk)

	// 查找完整的JSON对象
	objects := findCompleteJSONObjects(chunkStr)

	// 处理每个对象
	for _, objStr := range objects {
		var obj JSONObject
		if err := json.Unmarshal([]byte(objStr), &obj); err == nil {
			processJSONObject(obj)
		}
	}
}

func findCompleteJSONObjects(text string) []string {
	var objects []string
	stack := make([]rune, 0)
	startIdx := 0

	for i, char := range text {
		if char == '{' {
			if len(stack) == 0 {
				startIdx = i
			}
			stack = append(stack, char)
		} else if char == '}' {
			if len(stack) > 0 {
				stack = stack[:len(stack)-1]
				if len(stack) == 0 {
					// 找到完整对象
					objStr := text[startIdx : i+1]
					objects = append(objects, objStr)
				}
			}
		}
	}

	return objects
}

func processJSONObject(obj JSONObject) {
	// 实现你的对象处理逻辑
	if id, ok := obj["id"].(string); ok {
		fmt.Printf("Processing object with ID: %s\n", id)
	}
}
```

## 并行处理技术

### 多线程JSON解析

#### Java并行JSON处理

```java
import com.fasterxml.jackson.databind.JsonNode;
import com.fasterxml.jackson.databind.ObjectMapper;
import java.io.IOException;
import java.nio.file.Files;
import java.nio.file.Path;
import java.nio.file.Paths;
import java.util.List;
import java.util.concurrent.*;
import java.util.stream.Collectors;
import java.util.stream.IntStream;

public class ParallelJsonProcessor {
    
    private final ObjectMapper mapper = new ObjectMapper();
    private final ExecutorService executor;
    
    public ParallelJsonProcessor(int threadPoolSize) {
        this.executor = Executors.newFixedThreadPool(threadPoolSize);
    }
    
    /**
     * 并行处理JSON数组中的元素
     */
    public List<ProcessedData> processJsonArray(String filePath) throws IOException, InterruptedException {
        // 读取整个JSON文件
        String jsonContent = Files.readString(Paths.get(filePath));
        
        // 解析为JSON数组
        JsonNode rootNode = mapper.readTree(jsonContent);
        
        if (!rootNode.isArray()) {
            throw new IllegalArgumentException("Expected JSON array but got: " + rootNode.getNodeType());
        }
        
        int totalElements = rootNode.size();
        System.out.println("Processing " + totalElements + " elements");
        
        // 计算每个线程处理的元素数量
        int numThreads = Runtime.getRuntime().availableProcessors();
        int batchSize = totalElements / numThreads;
        if (batchSize == 0) batchSize = 1;
        
        // 创建任务列表
        List<Future<List<ProcessedData>>> futures = IntStream.range(0, numThreads)
            .mapToObj(i -> {
                int startIdx = i * batchSize;
                int endIdx = (i == numThreads - 1) ? totalElements : (i + 1) * batchSize;
                
                return executor.submit(() -> processBatch(rootNode, startIdx, endIdx));
            })
            .collect(Collectors.toList());
        
        // 等待所有任务完成并收集结果
        List<ProcessedData> allResults = new CopyOnWriteArrayList<>();
        
        for (Future<List<ProcessedData>> future : futures) {
            try {
                allResults.addAll(future.get());
            } catch (ExecutionException e) {
                System.err.println("Error processing batch: " + e.getCause().getMessage());
            }
        }
        
        return allResults;
    }
    
    /**
     * 处理一批JSON元素
     */
    private List<ProcessedData> processBatch(JsonNode rootNode, int startIdx, int endIdx) {
        List<ProcessedData> results = new ArrayList<>();
        
        for (int i = startIdx; i < endIdx; i++) {
            JsonNode element = rootNode.get(i);
            try {
                ProcessedData processed = processElement(element);
                results.add(processed);
            } catch (Exception e) {
                System.err.println("Error processing element at index " + i + ": " + e.getMessage());
            }
        }
        
        return results;
    }
    
    /**
     * 处理单个JSON元素
     */
    private ProcessedData processElement(JsonNode element) {
        // 提取数据
        String id = element.get("id").asText();
        String name = element.get("name").asText();
        
        // 计算指标（假设有一个values数组）
        JsonNode valuesNode = element.get("values");
        double[] values = new double[valuesNode.size()];
        for (int i = 0; i < valuesNode.size(); i++) {
            values[i] = valuesNode.get(i).asDouble();
        }
        
        Metrics metrics = calculateMetrics(values);
        
        return new ProcessedData(id, name, metrics, System.currentTimeMillis());
    }
    
    /**
     * 计算指标
     */
    private Metrics calculateMetrics(double[] values) {
        if (values.length == 0) {
            return new Metrics(0, 0, 0, 0);
        }
        
        double sum = 0;
        double max = values[0];
        double min = values[0];
        
        for (double value : values) {
            sum += value;
            if (value > max) max = value;
            if (value < min) min = value;
        }
        
        return new Metrics(sum / values.length, max, min, values.length);
    }
    
    public void shutdown() {
        executor.shutdown();
        try {
            if (!executor.awaitTermination(60, TimeUnit.SECONDS)) {
                executor.shutdownNow();
            }
        } catch (InterruptedException e) {
            executor.shutdownNow();
            Thread.currentThread().interrupt();
        }
    }
    
    // 数据类
    public static class ProcessedData {
        private final String id;
        private final String name;
        private final Metrics metrics;
        private final long processedAt;
        
        public ProcessedData(String id, String name, Metrics metrics, long processedAt) {
            this.id = id;
            this.name = name;
            this.metrics = metrics;
            this.processedAt = processedAt;
        }
        
        // Getters and toString...
    }
    
    public static class Metrics {
        private final double average;
        private final double max;
        private final double min;
        private final int count;
        
        public Metrics(double average, double max, double min, int count) {
            this.average = average;
            this.max = max;
            this.min = min;
            this.count = count;
        }
        
        // Getters and toString...
    }
}
```

#### Python中的并行处理

```python
import json
import multiprocessing
from typing import Dict, List, Any, Callable
import time

class ParallelJsonProcessor:
    """并行处理JSON数据"""
    
    def __init__(self, num_workers: int = None):
        self.num_workers = num_workers or multiprocessing.cpu_count()
        self.pool = multiprocessing.Pool(self.num_workers)
    
    def process_json_file(self, file_path: str, process_func: Callable[[Dict], Any]) -> List[Any]:
        """并行处理JSON文件中的所有对象"""
        with open(file_path, 'r') as file:
            data = json.load(file)
        
        if not isinstance(data, list):
            raise ValueError("Expected JSON array but got: " + type(data).__name__)
        
        print(f"Processing {len(data)} items with {self.num_workers} workers")
        start_time = time.time()
        
        # 并行处理
        results = self.pool.map(process_func, data)
        
        elapsed = time.time() - start_time
        print(f"Completed processing in {elapsed:.2f} seconds")
        
        return results
    
    def process_jsonl_file(self, file_path: str, process_func: Callable[[Dict], Any]) -> List[Any]:
        """并行处理JSONL文件（每行一个JSON对象）"""
        # 首先读取所有行
        with open(file_path, 'r') as file:
            lines = file.readlines()
        
        print(f"Processing {len(lines)} lines with {self.num_workers} workers")
        start_time = time.time()
        
        # 解析JSON并并行处理
        def parse_and_process(line):
            try:
                data = json.loads(line.strip())
                return process_func(data)
            except json.JSONDecodeError as e:
                print(f"JSON解析错误: {e}")
                return None
        
        results = self.pool.map(parse_and_process, lines)
        
        # 过滤掉解析失败的项
        results = [result for result in results if result is not None]
        
        elapsed = time.time() - start_time
        print(f"Completed processing in {elapsed:.2f} seconds")
        
        return results
    
    def close(self):
        """关闭处理池"""
        self.pool.close()
        self.pool.join()

# 示例处理函数
def example_process_function(item: Dict) -> Dict:
    """示例处理函数"""
    # 提取数据
    item_id = item.get('id')
    name = item.get('name')
    values = item.get('values', [])
    
    # 计算指标
    if values:
        average = sum(values) / len(values)
        max_val = max(values)
        min_val = min(values)
        count = len(values)
    else:
        average = max_val = min_val = 0
        count = 0
    
    # 返回处理后的结果
    return {
        'id': item_id,
        'name': name,
        'metrics': {
            'average': average,
            'max': max_val,
            'min': min_val,
            'count': count
        },
        'processed_at': time.time()
    }

# 使用示例
if __name__ == '__main__':
    processor = ParallelJsonProcessor()
    
    try:
        # 处理JSON数组文件
        results = processor.process_json_file('large-dataset.json', example_process_function)
        print(f"Processed {len(results)} items")
        
        # 处理JSONL文件
        # results = processor.process_jsonl_file('large-dataset.jsonl', example_process_function)
        
    finally:
        processor.close()
```

## 性能优化技巧

### 数据结构优化

#### 精简JSON结构

```json
// 优化前：冗余的重复字段
[
  {
    "id": "user_1",
    "type": "user",
    "timestamp": "2023-04-22T14:30:45Z",
    "source": "mobile_app",
    "attributes": {
      "name": "John Doe",
      "email": "john.doe@example.com",
      "age": 30,
      "city": "New York",
      "country": "USA",
      "membership": "premium",
      "registration_date": "2022-01-15T08:30:00Z",
      "last_login": "2023-04-22T10:15:30Z",
      "profile_image_url": "https://example.com/images/user_1.jpg",
      "settings": {
        "notifications": true,
        "privacy": "public",
        "theme": "dark"
      }
    },
    "metadata": {
      "version": "1.2.0",
      "updated_by": "admin",
      "created_at": "2022-01-15T08:30:00Z",
      "updated_at": "2023-04-22T14:30:45Z"
    }
  },
  {
    "id": "user_2",
    "type": "user",
    "timestamp": "2023-04-22T14:31:10Z",
    "source": "web_app",
    "attributes": {
      "name": "Jane Smith",
      "email": "jane.smith@example.com",
      "age": 28,
      "city": "San Francisco",
      "country": "USA",
      "membership": "basic",
      "registration_date": "2022-03-10T12:45:00Z",
      "last_login": "2023-04-21T18:20:15Z",
      "profile_image_url": "https://example.com/images/user_2.jpg",
      "settings": {
        "notifications": false,
        "privacy": "private",
        "theme": "light"
      }
    },
    "metadata": {
      "version": "1.2.0",
      "updated_by": "admin",
      "created_at": "2022-03-10T12:45:00Z",
      "updated_at": "2023-04-22T14:31:10Z"
    }
  }
]

// 优化后：减少冗余，使用引用
{
  "meta": {
    "type": "user_export",
    "timestamp": "2023-04-22T14:30:00Z",
    "version": "1.2.0",
    "schema": {
      "membership_levels": ["basic", "premium"],
      "countries": ["USA", "Canada", "UK"],
      "settings": {
        "themes": ["light", "dark"],
        "privacy_levels": ["public", "private", "friends"]
      }
    }
  },
  "data": [
    ["user_1", "John Doe", "john.doe@example.com", 30, "New York", 0, "premium", 1642245400, 1682153730, 1, "mobile_app"],
    ["user_2", "Jane Smith", "jane.smith@example.com", 28, "San Francisco", 0, "basic", 1646916300, 1682153730, 0, "web_app"]
  ],
  "fields": [
    "id", "name", "email", "age", "city", "country", "membership", 
    "registration_date", "last_login", "notification_enabled", "source"
  ],
  "lookups": {
    "countries": {0: "USA"},
    "membership_levels": {"basic": 0, "premium": 1},
    "themes": {"light": 0, "dark": 1},
    "privacy_levels": {"public": 0, "private": 1}
  }
}
```

#### 列式存储优化

```javascript
// 列式JSON格式 - 适合分析查询
{
  "metadata": {
    "columns": 8,
    "rows": 1000,
    "compression": "gzip"
  },
  "columns": {
    "id": {
      "type": "string",
      "encoding": "dictionary",
      "data": ["user_1", "user_2", "user_3"],
      "dictionary": {"user_1": 0, "user_2": 1, "user_3": 2}
    },
    "timestamp": {
      "type": "datetime",
      "encoding": "delta",
      "base": "2023-04-22T00:00:00Z",
      "data": [52045, 52070, 52115]
    },
    "values": {
      "type": "numeric",
      "encoding": "bit-packing",
      "bits": 16,
      "data": [1234, 1567, 1890]
    }
  }
}
```

### 缓存策略

#### 智能JSON缓存系统

```python
import json
import hashlib
import time
from typing import Any, Dict, Optional, Callable
import pickle
import os
from functools import wraps

class JsonCache:
    """JSON缓存系统，支持LRU和TTL策略"""
    
    def __init__(self, cache_dir: str = "json_cache", max_size: int = 1000):
        self.cache_dir = cache_dir
        self.max_size = max_size
        self.memory_cache: Dict[str, Dict[str, Any]] = {}
        self.access_times: Dict[str, float] = {}
        
        # 确保缓存目录存在
        os.makedirs(cache_dir, exist_ok=True)
    
    def _get_cache_key(self, json_data: Any) -> str:
        """生成JSON数据的缓存键"""
        json_str = json.dumps(json_data, sort_keys=True)
        return hashlib.md5(json_str.encode()).hexdigest()
    
    def _get_file_path(self, cache_key: str) -> str:
        """获取缓存文件路径"""
        return os.path.join(self.cache_dir, f"{cache_key}.cache")
    
    def get(self, json_data: Any, processor: Callable[[Any], Any], ttl: int = 3600) -> Any:
        """获取处理后的JSON数据，支持缓存"""
        cache_key = self._get_cache_key(json_data)
        current_time = time.time()
        
        # 检查内存缓存
        if cache_key in self.memory_cache:
            cache_entry = self.memory_cache[cache_key]
            if current_time - cache_entry["timestamp"] < ttl:
                self.access_times[cache_key] = current_time
                return cache_entry["result"]
        
        # 检查磁盘缓存
        file_path = self._get_file_path(cache_key)
        if os.path.exists(file_path):
            try:
                with open(file_path, 'rb') as f:
                    cache_entry = pickle.load(f)
                
                if current_time - cache_entry["timestamp"] < ttl:
                    result = cache_entry["result"]
                    # 更新内存缓存
                    self._update_memory_cache(cache_key, result, current_time)
                    return result
            except Exception as e:
                print(f"加载缓存失败: {e}")
        
        # 处理数据
        result = processor(json_data)
        
        # 更新缓存
        cache_entry = {
            "result": result,
            "timestamp": current_time
        }
        
        # 更新内存缓存
        self._update_memory_cache(cache_key, result, current_time)
        
        # 更新磁盘缓存
        try:
            with open(file_path, 'wb') as f:
                pickle.dump(cache_entry, f)
        except Exception as e:
            print(f"保存缓存失败: {e}")
        
        return result
    
    def _update_memory_cache(self, cache_key: str, result: Any, current_time: float):
        """更新内存缓存，实现LRU策略"""
        # 如果缓存已满，移除最久未使用的项
        if len(self.memory_cache) >= self.max_size:
            oldest_key = min(self.access_times.keys(), key=lambda k: self.access_times[k])
            del self.memory_cache[oldest_key]
            del self.access_times[oldest_key]
        
        # 添加新项
        self.memory_cache[cache_key] = {
            "result": result,
            "timestamp": current_time
        }
        self.access_times[cache_key] = current_time
    
    def clear(self):
        """清空缓存"""
        self.memory_cache.clear()
        self.access_times.clear()
        
        # 清空磁盘缓存
        try:
            for file_name in os.listdir(self.cache_dir):
                if file_name.endswith('.cache'):
                    os.remove(os.path.join(self.cache_dir, file_name))
        except Exception as e:
            print(f"清空磁盘缓存失败: {e}")
    
    def get_stats(self) -> Dict[str, Any]:
        """获取缓存统计信息"""
        disk_files = 0
        disk_size = 0
        
        try:
            for file_name in os.listdir(self.cache_dir):
                if file_name.endswith('.cache'):
                    disk_files += 1
                    file_path = os.path.join(self.cache_dir, file_name)
                    disk_size += os.path.getsize(file_path)
        except Exception:
            pass
        
        return {
            "memory_entries": len(self.memory_cache),
            "disk_files": disk_files,
            "disk_size_mb": disk_size / (1024 * 1024)
        }

def cached_json_processing(ttl: int = 3600):
    """装饰器，用于缓存JSON处理函数"""
    def decorator(func):
        cache = JsonCache()
        
        @wraps(func)
        def wrapper(json_data, *args, **kwargs):
            # 创建包含所有参数的缓存键
            cache_data = {
                "json": json_data,
                "args": args,
                "kwargs": kwargs
            }
            
            def processor(data):
                return func(data["json"], *data["args"], **data["kwargs"])
            
            return cache.get(cache_data, processor, ttl)
        
        # 添加缓存管理方法
        wrapper.clear_cache = cache.clear
        wrapper.cache_stats = cache.get_stats
        
        return wrapper
    return decorator

# 使用示例
@cached_json_processing(ttl=1800)  # 30分钟缓存
def complex_json_processing(json_data):
    """复杂的JSON处理函数"""
    # 模拟耗时处理
    time.sleep(2)
    
    # 执行复杂计算
    if isinstance(json_data, list):
        processed = []
        for item in json_data:
            # 复杂转换逻辑
            processed_item = {
                "id": item.get("id"),
                "processed_value": item.get("value", 0) * 2.5,
                "metadata": f"processed_{int(time.time())}"
            }
            processed.append(processed_item)
        return processed
    else:
        return {"error": "Expected list of objects"}

# 使用示例
if __name__ == "__main__":
    test_data = [
        {"id": "item_1", "value": 10},
        {"id": "item_2", "value": 20},
        {"id": "item_3", "value": 30}
    ]
    
    print("第一次处理 (应该较慢):")
    start_time = time.time()
    result1 = complex_json_processing(test_data)
    elapsed1 = time.time() - start_time
    print(f"处理完成，耗时: {elapsed1:.2f}秒")
    
    print("\n第二次处理 (应该较快，来自缓存):")
    start_time = time.time()
    result2 = complex_json_processing(test_data)
    elapsed2 = time.time() - start_time
    print(f"处理完成，耗时: {elapsed2:.2f}秒")
    
    print(f"\n加速效果: {elapsed1/elapsed2:.1f}x")
    print(f"缓存统计: {complex_json_processing.cache_stats()}")
```

## 性能监控与调优

### JSON处理性能监控

```python
import time
import psutil
import threading
from typing import Dict, List, Any, Callable
from dataclasses import dataclass
import json

@dataclass
class PerformanceMetrics:
    """性能指标数据类"""
    processing_time: float
    memory_usage_mb: float
    cpu_usage_percent: float
    input_size_bytes: int
    output_size_bytes: int
    throughput_mb_per_sec: float

class JsonPerformanceMonitor:
    """JSON处理性能监控器"""
    
    def __init__(self):
        self.metrics: List[PerformanceMetrics] = []
        self.monitoring = False
        self.monitor_thread = None
        self.current_cpu_usage = 0
        self.current_memory_usage = 0
    
    def start_monitoring(self):
        """开始性能监控"""
        self.monitoring = True
        self.monitor_thread = threading.Thread(target=self._monitor_resources)
        self.monitor_thread.daemon = True
        self.monitor_thread.start()
    
    def stop_monitoring(self):
        """停止性能监控"""
        self.monitoring = False
        if self.monitor_thread:
            self.monitor_thread.join()
    
    def _monitor_resources(self):
        """监控资源使用情况"""
        process = psutil.Process()
        
        while self.monitoring:
            self.current_cpu_usage = process.cpu_percent()
            self.current_memory_usage = process.memory_info().rss / (1024 * 1024)  # MB
            time.sleep(0.1)  # 每100ms采样一次
    
    def measure_processing(self, json_data: Any, processor: Callable[[Any], Any]) -> Dict[str, Any]:
        """测量JSON处理性能"""
        # 准备输入数据
        if isinstance(json_data, str):
            input_json = json.loads(json_data)
        else:
            input_json = json_data
        
        input_data = json.dumps(input_json)
        input_size = len(input_data.encode('utf-8'))
        
        # 开始监控和处理
        self.start_monitoring()
        start_time = time.time()
        
        result = processor(input_json)
        
        processing_time = time.time() - start_time
        self.stop_monitoring()
        
        # 准备输出数据
        if isinstance(result, str):
            output_data = result
        else:
            output_data = json.dumps(result)
        
        output_size = len(output_data.encode('utf-8'))
        
        # 计算吞吐量
        throughput = (input_size + output_size) / (1024 * 1024) / processing_time  # MB/s
        
        # 记录性能指标
        metrics = PerformanceMetrics(
            processing_time=processing_time,
            memory_usage_mb=self.current_memory_usage,
            cpu_usage_percent=self.current_cpu_usage,
            input_size_bytes=input_size,
            output_size_bytes=output_size,
            throughput_mb_per_sec=throughput
        )
        self.metrics.append(metrics)
        
        return {
            "result": result,
            "metrics": metrics
        }
    
    def get_average_metrics(self) -> PerformanceMetrics:
        """获取平均性能指标"""
        if not self.metrics:
            return PerformanceMetrics(0, 0, 0, 0, 0, 0)
        
        return PerformanceMetrics(
            processing_time=sum(m.processing_time for m in self.metrics) / len(self.metrics),
            memory_usage_mb=sum(m.memory_usage_mb for m in self.metrics) / len(self.metrics),
            cpu_usage_percent=sum(m.cpu_usage_percent for m in self.metrics) / len(self.metrics),
            input_size_bytes=sum(m.input_size_bytes for m in self.metrics) / len(self.metrics),
            output_size_bytes=sum(m.output_size_bytes for m in self.metrics) / len(self.metrics),
            throughput_mb_per_sec=sum(m.throughput_mb_per_sec for m in self.metrics) / len(self.metrics)
        )
    
    def get_performance_report(self) -> str:
        """生成性能报告"""
        if not self.metrics:
            return "没有性能数据"
        
        avg_metrics = self.get_average_metrics()
        max_throughput = max(m.throughput_mb_per_sec for m in self.metrics)
        min_processing_time = min(m.processing_time for m in self.metrics)
        max_processing_time = max(m.processing_time for m in self.metrics)
        
        report = f"""
=== JSON处理性能报告 ===
总处理次数: {len(self.metrics)}
平均处理时间: {avg_metrics.processing_time:.3f}秒
最快处理时间: {min_processing_time:.3f}秒
最慢处理时间: {max_processing_time:.3f}秒

平均内存使用: {avg_metrics.memory_usage_mb:.2f} MB
平均CPU使用率: {avg_metrics.cpu_usage_percent:.1f}%

平均吞吐量: {avg_metrics.throughput_mb_per_sec:.2f} MB/s
最大吞吐量: {max_throughput:.2f} MB/s

平均输入大小: {avg_metrics.input_size_bytes / 1024:.2f} KB
平均输出大小: {avg_metrics.output_size_bytes / 1024:.2f} KB
        """
        
        return report

# 使用示例
def example_json_processor(json_data):
    """示例JSON处理函数"""
    # 模拟复杂的JSON处理
    if isinstance(json_data, list):
        processed = []
        for item in json_data:
            # 复杂的数据转换
            if "values" in item and isinstance(item["values"], list):
                values = item["values"]
                processed_item = {
                    "id": item.get("id"),
                    "metrics": {
                        "sum": sum(values),
                        "avg": sum(values) / len(values) if values else 0,
                        "max": max(values) if values else 0,
                        "min": min(values) if values else 0
                    },
                    "processed_at": int(time.time())
                }
                processed.append(processed_item)
        return processed
    return []

# 性能测试
if __name__ == "__main__":
    # 创建测试数据
    test_data = []
    for i in range(100):
        test_data.append({
            "id": f"item_{i}",
            "values": [i + j for j in range(10)],
            "metadata": {
                "source": f"source_{i % 5}",
                "timestamp": int(time.time()) - i * 60
            }
        })
    
    monitor = JsonPerformanceMonitor()
    
    # 运行多次测试
    for i in range(5):
        print(f"\n运行测试 #{i+1}")
        result = monitor.measure_processing(test_data, example_json_processor)
        metrics = result["metrics"]
        print(f"处理时间: {metrics.processing_time:.3f}秒")
        print(f"内存使用: {metrics.memory_usage_mb:.2f}MB")
        print(f"吞吐量: {metrics.throughput_mb_per_sec:.2f}MB/s")
    
    # 生成性能报告
    print(monitor.get_performance_report())
```

## 总结

JSON处理的可扩展性和性能优化是企业级应用中的关键考虑因素。通过应用本章讨论的技术，您可以显著提高JSON处理的效率和可靠性：

### 关键优化策略

1. **流式处理技术**
   - 使用流式解析器处理大型JSON文件
   - 实现增量解析减少内存使用
   - 采用内存映射技术提高I/O效率

2. **并行处理**
   - 利用多核CPU并行解析和处理JSON
   - 实现任务分批处理提高吞吐量
   - 采用异步编程模型提高响应性

3. **内存优化**
   - 使用高效的数据结构减少内存占用
   - 实现智能缓存策略减少重复计算
   - 采用压缩技术减少存储和传输开销

4. **性能监控**
   - 实施全面的性能监控和分析
   - 建立基准测试和回归测试
   - 持续优化和调优处理流程

### 性能指标

通过应用这些技术，企业可以实现：
- 处理速度提升5-10倍
- 内存使用减少30-50%
- 支持更大规模的数据处理
- 降低系统响应延迟

选择合适的优化策略取决于您的具体应用场景、数据特性和性能要求。始终建议进行基准测试以验证优化效果。