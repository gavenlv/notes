#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ç¬¬11ç« ï¼šRabbitMQå®æ—¶æ•°æ®å¤„ç†é›†æˆ - æ•°æ®æµå¤„ç†ç®¡é“ç¤ºä¾‹

æœ¬æ¨¡å—æ¼”ç¤ºå¦‚ä½•æ„å»ºé«˜æ€§èƒ½çš„å®æ—¶æ•°æ®æµå¤„ç†ç®¡é“ï¼Œ
åŒ…æ‹¬æ•°æ®æ¥å…¥ã€å¤„ç†ã€èšåˆå’Œè¾“å‡ºçš„å®Œæ•´æµç¨‹ã€‚
"""

import asyncio
import json
import time
import threading
from datetime import datetime, timedelta
from typing import Dict, List, Optional, Any, Callable
from dataclasses import dataclass, asdict
from enum import Enum
from collections import defaultdict, deque
import hashlib
import uuid

try:
    import pika
except ImportError:
    print("è¯·å®‰è£…pika: pip install pika")
    pika = None

try:
    import redis
except ImportError:
    print("è¯·å®‰è£…redis: pip install redis")
    redis = None

# =============================================================================
# 1. æ•°æ®ç»“æ„å’Œæšä¸¾ç±»å®šä¹‰
# =============================================================================

class DataType(Enum):
    """æ•°æ®ç±»å‹æšä¸¾"""
    SENSOR_DATA = "sensor"
    USER_EVENT = "user_event"
    SYSTEM_METRIC = "system_metric"
    BUSINESS_EVENT = "business_event"
    AUDIT_LOG = "audit_log"

class ProcessingStage(Enum):
    """å¤„ç†é˜¶æ®µæšä¸¾"""
    INGESTION = "ingestion"
    VALIDATION = "validation"
    TRANSFORMATION = "transformation"
    AGGREGATION = "aggregation"
    ENRICHMENT = "enrichment"
    OUTPUT = "output"

class QualityLevel(Enum):
    """æ•°æ®è´¨é‡ç­‰çº§"""
    EXCELLENT = 1.0
    GOOD = 0.8
    FAIR = 0.6
    POOR = 0.4
    INVALID = 0.0

@dataclass
class DataMessage:
    """æ•°æ®æ¶ˆæ¯ç»“æ„"""
    message_id: str
    data_type: DataType
    timestamp: datetime
    source_id: str
    payload: Dict[str, Any]
    metadata: Optional[Dict[str, Any]] = None
    quality_score: QualityLevel = QualityLevel.GOOD
    processed_stages: List[ProcessingStage] = None
    
    def __post_init__(self):
        if self.processed_stages is None:
            self.processed_stages = []
        if isinstance(self.timestamp, str):
            self.timestamp = datetime.fromisoformat(self.timestamp)

@dataclass
class ProcessingMetrics:
    """å¤„ç†æŒ‡æ ‡"""
    stage: ProcessingStage
    start_time: datetime
    end_time: Optional[datetime] = None
    message_count: int = 0
    error_count: int = 0
    avg_processing_time: float = 0.0
    throughput: float = 0.0

# =============================================================================
# 2. æ¶ˆæ¯é˜Ÿåˆ—è¿æ¥å™¨
# =============================================================================

class RabbitMQConnector:
    """RabbitMQè¿æ¥å™¨"""
    
    def __init__(self, host='localhost', port=5672, username='admin', password='admin'):
        if pika is None:
            raise ImportError("pikaåº“æœªå®‰è£…ï¼Œè¯·è¿è¡Œ: pip install pika")
            
        self.config = {
            'host': host,
            'port': port,
            'username': username,
            'password': password,
            'virtual_host': '/realtime_data'
        }
        self.connection = None
        self.channel = None
        self.exchanges = {}
        self.queues = {}
        
    def connect(self):
        """å»ºç«‹è¿æ¥"""
        credentials = pika.PlainCredentials(self.config['username'], self.config['password'])
        parameters = pika.ConnectionParameters(
            host=self.config['host'],
            port=self.config['port'],
            credentials=credentials,
            virtual_host=self.config['virtual_host']
        )
        
        self.connection = pika.BlockingConnection(parameters)
        self.channel = self.connection.channel()
        
        # å£°æ˜åŸºç¡€äº¤æ¢å™¨
        self.declare_exchanges()
        
    def disconnect(self):
        """æ–­å¼€è¿æ¥"""
        if self.connection:
            self.connection.close()
            
    def declare_exchanges(self):
        """å£°æ˜äº¤æ¢å™¨"""
        exchanges = {
            'data_ingestion': {'type': 'topic', 'durable': True},
            'processing_pipeline': {'type': 'direct', 'durable': True},
            'aggregated_data': {'type': 'fanout', 'durable': True},
            'dead_letters': {'type': 'topic', 'durable': True}
        }
        
        for name, props in exchanges.items():
            self.channel.exchange_declare(
                exchange=name,
                exchange_type=props['type'],
                durable=props['durable']
            )
            self.exchanges[name] = props
            
    def declare_queue(self, queue_name: str, durable=True, arguments=None):
        """å£°æ˜é˜Ÿåˆ—"""
        if arguments is None:
            arguments = {
                'x-message-ttl': 3600000,  # 1å°æ—¶
                'x-dead-letter-exchange': 'dead_letters'
            }
            
        result = self.channel.queue_declare(
            queue=queue_name,
            durable=durable,
            arguments=arguments
        )
        self.queues[queue_name] = result.method.queue
        return result.method.queue
        
    def publish(self, exchange: str, routing_key: str, message: DataMessage, properties=None):
        """å‘å¸ƒæ¶ˆæ¯"""
        if properties is None:
            properties = pika.BasicProperties(
                message_id=message.message_id,
                timestamp=int(message.timestamp.timestamp()),
                delivery_mode=2,  # æŒä¹…åŒ–
                headers={
                    'data_type': message.data_type.value,
                    'source_id': message.source_id,
                    'quality_score': message.quality_score.value
                }
            )
            
        body = json.dumps(asdict(message), default=str, ensure_ascii=False)
        
        self.channel.basic_publish(
            exchange=exchange,
            routing_key=routing_key,
            body=body,
            properties=properties
        )
        
    def consume(self, queue: str, callback: Callable, auto_ack=False):
        """æ¶ˆè´¹æ¶ˆæ¯"""
        self.channel.basic_consume(
            queue=queue,
            on_message_callback=callback,
            auto_ack=auto_ack
        )

class RedisCache:
    """Redisç¼“å­˜ç®¡ç†å™¨"""
    
    def __init__(self, host='localhost', port=6379, db=0):
        if redis is None:
            raise ImportError("redisåº“æœªå®‰è£…ï¼Œè¯·è¿è¡Œ: pip install redis")
            
        self.client = redis.Redis(host=host, port=port, db=db, decode_responses=True)
        
    def get(self, key: str) -> Optional[str]:
        """è·å–ç¼“å­˜å€¼"""
        try:
            return self.client.get(key)
        except Exception:
            return None
            
    def set(self, key: str, value: str, ttl: int = 3600):
        """è®¾ç½®ç¼“å­˜å€¼"""
        try:
            return self.client.setex(key, ttl, value)
        except Exception:
            return False
            
    def delete(self, key: str) -> bool:
        """åˆ é™¤ç¼“å­˜å€¼"""
        try:
            return bool(self.client.delete(key))
        except Exception:
            return False

# =============================================================================
# 3. æ•°æ®éªŒè¯å™¨
# =============================================================================

class DataValidator:
    """æ•°æ®éªŒè¯å™¨åŸºç±»"""
    
    def validate(self, message: DataMessage) -> bool:
        """éªŒè¯æ¶ˆæ¯æ•°æ®"""
        try:
            # åŸºç¡€éªŒè¯
            if not self._validate_basic_fields(message):
                return False
                
            # ç±»å‹ç‰¹å®šéªŒè¯
            if not self._validate_type_specific(message):
                return False
                
            # æ•°æ®è´¨é‡æ£€æŸ¥
            quality_score = self._calculate_quality_score(message)
            message.quality_score = quality_score
            
            return quality_score.value > 0.0
            
        except Exception as e:
            print(f"éªŒè¯é”™è¯¯: {e}")
            return False
            
    def _validate_basic_fields(self, message: DataMessage) -> bool:
        """åŸºç¡€å­—æ®µéªŒè¯"""
        required_fields = ['message_id', 'data_type', 'timestamp', 'source_id', 'payload']
        
        for field in required_fields:
            if not hasattr(message, field) or getattr(message, field) is None:
                return False
                
        # éªŒè¯æ¶ˆæ¯IDæ ¼å¼
        if not message.message_id or len(message.message_id) < 10:
            return False
            
        # éªŒè¯æ—¶é—´æˆ³
        if isinstance(message.timestamp, datetime):
            now = datetime.now()
            if abs((message.timestamp - now).total_seconds()) > 86400:  # 24å°æ—¶
                return False
                
        return True
        
    def _validate_type_specific(self, message: DataMessage) -> bool:
        """ç±»å‹ç‰¹å®šéªŒè¯"""
        if message.data_type == DataType.SENSOR_DATA:
            return self._validate_sensor_data(message)
        elif message.data_type == DataType.USER_EVENT:
            return self._validate_user_event(message)
        elif message.data_type == DataType.SYSTEM_METRIC:
            return self._validate_system_metric(message)
        else:
            return True
            
    def _validate_sensor_data(self, message: DataMessage) -> bool:
        """ä¼ æ„Ÿå™¨æ•°æ®éªŒè¯"""
        payload = message.payload
        
        # å¿…éœ€å­—æ®µæ£€æŸ¥
        required_sensor_fields = ['device_id', 'sensor_type', 'value', 'unit']
        for field in required_sensor_fields:
            if field not in payload:
                return False
                
        # æ•°å€¼èŒƒå›´æ£€æŸ¥
        if 'value' in payload:
            try:
                value = float(payload['value'])
                if not (-1000 <= value <= 1000):  # åˆç†çš„ä¼ æ„Ÿå™¨å€¼èŒƒå›´
                    return False
            except (ValueError, TypeError):
                return False
                
        return True
        
    def _validate_user_event(self, message: DataMessage) -> bool:
        """ç”¨æˆ·äº‹ä»¶éªŒè¯"""
        payload = message.payload
        
        # æ£€æŸ¥ç”¨æˆ·ID
        if 'user_id' not in payload:
            return False
            
        # æ£€æŸ¥äº‹ä»¶ç±»å‹
        if 'event_type' not in payload:
            return False
            
        return True
        
    def _validate_system_metric(self, message: DataMessage) -> bool:
        """ç³»ç»ŸæŒ‡æ ‡éªŒè¯"""
        payload = message.payload
        
        # æ£€æŸ¥æŒ‡æ ‡åç§°
        if 'metric_name' not in payload:
            return False
            
        # æ£€æŸ¥æŒ‡æ ‡å€¼
        if 'metric_value' not in payload:
            return False
            
        return True
        
    def _calculate_quality_score(self, message: DataMessage) -> QualityLevel:
        """è®¡ç®—æ•°æ®è´¨é‡åˆ†æ•°"""
        score = 1.0
        
        # åŸºç¡€å®Œæ•´æ€§æ£€æŸ¥
        if not hasattr(message, 'metadata') or message.metadata is None:
            score -= 0.2
            
        # æ•°æ®æ–°é²œåº¦æ£€æŸ¥
        if hasattr(message, 'timestamp'):
            age = (datetime.now() - message.timestamp).total_seconds()
            if age > 3600:  # 1å°æ—¶
                score -= 0.3
            elif age > 600:  # 10åˆ†é’Ÿ
                score -= 0.1
                
        # è´Ÿè½½å®Œæ•´æ€§æ£€æŸ¥
        if isinstance(message.payload, dict):
            expected_fields = self._get_expected_fields(message.data_type)
            missing_fields = set(expected_fields) - set(message.payload.keys())
            score -= len(missing_fields) * 0.1
            
        # æ˜ å°„åˆ°è´¨é‡ç­‰çº§
        if score >= 0.9:
            return QualityLevel.EXCELLENT
        elif score >= 0.7:
            return QualityLevel.GOOD
        elif score >= 0.5:
            return QualityLevel.FAIR
        elif score >= 0.3:
            return QualityLevel.POOR
        else:
            return QualityLevel.INVALID
            
    def _get_expected_fields(self, data_type: DataType) -> List[str]:
        """è·å–é¢„æœŸå­—æ®µåˆ—è¡¨"""
        field_mappings = {
            DataType.SENSOR_DATA: ['device_id', 'sensor_type', 'value', 'unit'],
            DataType.USER_EVENT: ['user_id', 'event_type', 'timestamp'],
            DataType.SYSTEM_METRIC: ['metric_name', 'metric_value', 'source'],
            DataType.BUSINESS_EVENT: ['event_name', 'event_data', 'timestamp'],
            DataType.AUDIT_LOG: ['action', 'user_id', 'resource', 'timestamp']
        }
        
        return field_mappings.get(data_type, [])

# =============================================================================
# 4. æ•°æ®è½¬æ¢å™¨
# =============================================================================

class DataTransformer:
    """æ•°æ®è½¬æ¢å™¨"""
    
    def __init__(self):
        self.transformation_rules = {}
        self.setup_default_rules()
        
    def setup_default_rules(self):
        """è®¾ç½®é»˜è®¤è½¬æ¢è§„åˆ™"""
        self.transformation_rules = {
            DataType.SENSOR_DATA: self._transform_sensor_data,
            DataType.USER_EVENT: self._transform_user_event,
            DataType.SYSTEM_METRIC: self._transform_system_metric
        }
        
    def transform(self, message: DataMessage) -> DataMessage:
        """è½¬æ¢æ¶ˆæ¯æ•°æ®"""
        try:
            # åº”ç”¨é€šç”¨è½¬æ¢
            transformed_message = self._apply_common_transformations(message)
            
            # åº”ç”¨ç±»å‹ç‰¹å®šè½¬æ¢
            if message.data_type in self.transformation_rules:
                transformed_message = self.transformation_rules[message.data_type](transformed_message)
                
            # æ·»åŠ è½¬æ¢é˜¶æ®µè®°å½•
            if ProcessingStage.TRANSFORMATION not in transformed_message.processed_stages:
                transformed_message.processed_stages.append(ProcessingStage.TRANSFORMATION)
                
            return transformed_message
            
        except Exception as e:
            print(f"æ•°æ®è½¬æ¢é”™è¯¯: {e}")
            return message
            
    def _apply_common_transformations(self, message: DataMessage) -> DataMessage:
        """åº”ç”¨é€šç”¨è½¬æ¢"""
        # æ—¶é—´æˆ³æ ‡å‡†åŒ–
        if isinstance(message.timestamp, str):
            message.timestamp = datetime.fromisoformat(message.timestamp)
            
        # è´Ÿè½½æ•°æ®æ¸…ç†
        if isinstance(message.payload, dict):
            message.payload = self._clean_payload(message.payload)
            
        # å…ƒæ•°æ®ä¸°å¯Œ
        if message.metadata is None:
            message.metadata = {}
            
        message.metadata.update({
            'transformed_at': datetime.now().isoformat(),
            'transformer_version': '1.0',
            'message_hash': self._calculate_message_hash(message)
        })
        
        return message
        
    def _transform_sensor_data(self, message: DataMessage) -> DataMessage:
        """ä¼ æ„Ÿå™¨æ•°æ®è½¬æ¢"""
        payload = message.payload
        
        # å•ä½æ ‡å‡†åŒ–
        if 'unit' in payload:
            payload['unit'] = self._normalize_unit(payload['unit'])
            
        # æ•°å€¼ç±»å‹è½¬æ¢
        if 'value' in payload:
            try:
                payload['value'] = float(payload['value'])
                payload['value_rounded'] = round(payload['value'], 2)
            except (ValueError, TypeError):
                payload['value'] = 0.0
                
        # æ·»åŠ ä¼ æ„Ÿå™¨å…ƒæ•°æ®
        if 'device_id' in payload:
            payload['device_category'] = self._categorize_device(payload['device_id'])
            
        return message
        
    def _transform_user_event(self, message: DataMessage) -> DataMessage:
        """ç”¨æˆ·äº‹ä»¶è½¬æ¢"""
        payload = message.payload
        
        # äº‹ä»¶æ—¶é—´æ ‡å‡†åŒ–
        if 'event_timestamp' in payload:
            try:
                payload['event_timestamp'] = datetime.fromisoformat(payload['event_timestamp'])
            except:
                payload['event_timestamp'] = message.timestamp
                
        # ç”¨æˆ·IDæ ‡å‡†åŒ–
        if 'user_id' in payload:
            payload['user_id'] = str(payload['user_id']).strip().lower()
            
        # æ·»åŠ äº‹ä»¶åˆ†ç±»
        if 'event_type' in payload:
            payload['event_category'] = self._categorize_event_type(payload['event_type'])
            
        return message
        
    def _transform_system_metric(self, message: DataMessage) -> DataMessage:
        """ç³»ç»ŸæŒ‡æ ‡è½¬æ¢"""
        payload = message.payload
        
        # æŒ‡æ ‡å€¼ç±»å‹è½¬æ¢
        if 'metric_value' in payload:
            try:
                payload['metric_value'] = float(payload['metric_value'])
            except (ValueError, TypeError):
                payload['metric_value'] = 0.0
                
        # æ·»åŠ æŒ‡æ ‡å…ƒæ•°æ®
        if 'metric_name' in payload:
            payload['metric_type'] = self._classify_metric_type(payload['metric_name'])
            payload['metric_unit'] = self._get_metric_unit(payload['metric_name'])
            
        return message
        
    def _clean_payload(self, payload: Dict[str, Any]) -> Dict[str, Any]:
        """æ¸…ç†è´Ÿè½½æ•°æ®"""
        cleaned = {}
        
        for key, value in payload.items():
            # è·³è¿‡Noneå€¼
            if value is None:
                continue
                
            # å­—ç¬¦ä¸²æ¸…ç†
            if isinstance(value, str):
                value = value.strip()
                if value:  # éç©ºå­—ç¬¦ä¸²
                    cleaned[key] = value
            else:
                cleaned[key] = value
                
        return cleaned
        
    def _normalize_unit(self, unit: str) -> str:
        """æ ‡å‡†åŒ–å•ä½"""
        unit_mapping = {
            'celsius': 'Â°C',
            'fahrenheit': 'Â°F',
            'kelvin': 'K',
            'percent': '%',
            'milliseconds': 'ms',
            'seconds': 's',
            'bytes': 'B',
            'kilobytes': 'KB',
            'megabytes': 'MB'
        }
        
        unit_lower = unit.lower()
        return unit_mapping.get(unit_lower, unit)
        
    def _categorize_device(self, device_id: str) -> str:
        """è®¾å¤‡åˆ†ç±»"""
        if device_id.startswith('temp_'):
            return 'temperature_sensor'
        elif device_id.startswith('humid_'):
            return 'humidity_sensor'
        elif device_id.startswith('press_'):
            return 'pressure_sensor'
        else:
            return 'unknown_device'
            
    def _categorize_event_type(self, event_type: str) -> str:
        """äº‹ä»¶ç±»å‹åˆ†ç±»"""
        if event_type in ['login', 'logout', 'session_start', 'session_end']:
            return 'session_event'
        elif event_type in ['click', 'view', 'purchase', 'search']:
            return 'user_interaction'
        elif event_type in ['api_call', 'service_request', 'database_query']:
            return 'system_event'
        else:
            return 'general_event'
            
    def _classify_metric_type(self, metric_name: str) -> str:
        """æŒ‡æ ‡ç±»å‹åˆ†ç±»"""
        if 'cpu' in metric_name.lower() or 'processor' in metric_name.lower():
            return 'cpu_metric'
        elif 'memory' in metric_name.lower() or 'ram' in metric_name.lower():
            return 'memory_metric'
        elif 'disk' in metric_name.lower() or 'storage' in metric_name.lower():
            return 'disk_metric'
        elif 'network' in metric_name.lower() or 'bandwidth' in metric_name.lower():
            return 'network_metric'
        else:
            return 'general_metric'
            
    def _get_metric_unit(self, metric_name: str) -> str:
        """è·å–æŒ‡æ ‡å•ä½"""
        unit_mapping = {
            'cpu_usage': '%',
            'memory_usage': '%',
            'disk_usage': '%',
            'network_latency': 'ms',
            'response_time': 'ms',
            'throughput': 'req/s',
            'error_rate': '%',
            'temperature': 'Â°C',
            'humidity': '%'
        }
        
        for key, unit in unit_mapping.items():
            if key in metric_name.lower():
                return unit
                
        return 'unit'
        
    def _calculate_message_hash(self, message: DataMessage) -> str:
        """è®¡ç®—æ¶ˆæ¯å“ˆå¸Œ"""
        content = f"{message.message_id}{message.source_id}{message.timestamp.isoformat()}"
        return hashlib.md5(content.encode()).hexdigest()

# =============================================================================
# 5. æ»‘åŠ¨çª—å£èšåˆå™¨
# =============================================================================

class SlidingWindowAggregator:
    """æ»‘åŠ¨çª—å£èšåˆå™¨"""
    
    def __init__(self, window_size_minutes=5, slide_interval_seconds=60):
        self.window_size = timedelta(minutes=window_size_minutes)
        self.slide_interval = timedelta(seconds=slide_interval_seconds)
        self.data_buffer = defaultdict(lambda: defaultdict(list))
        self.last_slide_time = datetime.now()
        
        # èšåˆå‡½æ•°æ˜ å°„
        self.aggregation_functions = {
            'count': len,
            'sum': lambda values: sum(values),
            'avg': lambda values: sum(values) / len(values) if values else 0,
            'min': lambda values: min(values) if values else 0,
            'max': lambda values: max(values) if values else 0,
            'median': self._calculate_median,
            'stddev': self._calculate_stddev
        }
        
    def add_data(self, message: DataMessage):
        """æ·»åŠ æ•°æ®åˆ°æ»‘åŠ¨çª—å£"""
        window_key = self._get_window_key(message.timestamp)
        data_key = self._get_data_key(message)
        
        # æ·»åŠ æ•°æ®åˆ°ç¼“å†²åŒº
        self.data_buffer[window_key][data_key].append({
            'timestamp': message.timestamp,
            'value': self._extract_numeric_value(message),
            'message': message
        })
        
        # æ¸…ç†è¿‡æœŸæ•°æ®
        self._cleanup_expired_data()
        
    def _get_window_key(self, timestamp: datetime) -> str:
        """è·å–çª—å£é”®"""
        window_start = timestamp.replace(
            minute=(timestamp.minute // 5) * 5,  # 5åˆ†é’Ÿçª—å£
            second=0,
            microsecond=0
        )
        return window_start.strftime('%Y%m%d%H%M')
        
    def _get_data_key(self, message: DataMessage) -> str:
        """è·å–æ•°æ®é”®"""
        if message.data_type == DataType.SENSOR_DATA:
            return f"{message.payload.get('device_id', 'unknown')}_{message.payload.get('sensor_type', 'unknown')}"
        elif message.data_type == DataType.USER_EVENT:
            return f"{message.payload.get('user_id', 'unknown')}_{message.payload.get('event_type', 'unknown')}"
        else:
            return f"{message.source_id}_{message.data_type.value}"
            
    def _extract_numeric_value(self, message: DataMessage) -> Optional[float]:
        """æå–æ•°å€¼"""
        if message.data_type == DataType.SENSOR_DATA:
            return message.payload.get('value')
        elif message.data_type == DataType.SYSTEM_METRIC:
            return message.payload.get('metric_value')
        else:
            return None
            
    def _cleanup_expired_data(self):
        """æ¸…ç†è¿‡æœŸæ•°æ®"""
        current_time = datetime.now()
        cutoff_time = current_time - (self.window_size * 2)  # ä¿ç•™2å€çª—å£å¤§å°çš„æ•°æ®
        
        expired_windows = []
        for window_key in self.data_buffer:
            window_time = datetime.strptime(window_key, '%Y%m%d%H%M')
            if window_time < cutoff_time:
                expired_windows.append(window_key)
                
        for window_key in expired_windows:
            del self.data_buffer[window_key]
            
    def should_slide(self) -> bool:
        """æ£€æŸ¥æ˜¯å¦åº”è¯¥æ»‘åŠ¨çª—å£"""
        return datetime.now() - self.last_slide_time >= self.slide_interval
        
    def slide_window(self) -> Dict[str, Any]:
        """æ‰§è¡Œçª—å£æ»‘åŠ¨"""
        if not self.should_slide():
            return {}
            
        # è·å–å½“å‰çª—å£æ•°æ®
        current_window_key = self._get_window_key(datetime.now())
        window_data = self.data_buffer.get(current_window_key, {})
        
        # è®¡ç®—èšåˆç»“æœ
        aggregated_results = {}
        for data_key, data_points in window_data.items():
            aggregated_results[data_key] = self._aggregate_data_points(data_points)
            
        # æ·»åŠ çª—å£ä¿¡æ¯
        result = {
            'window_start': datetime.strptime(current_window_key, '%Y%m%d%H%M'),
            'window_end': datetime.strptime(current_window_key, '%Y%m%d%H%M') + self.window_size,
            'aggregated_data': aggregated_results,
            'data_point_count': sum(len(points) for points in window_data.values())
        }
        
        self.last_slide_time = datetime.now()
        return result
        
    def _aggregate_data_points(self, data_points: List[Dict]) -> Dict[str, Any]:
        """èšåˆæ•°æ®ç‚¹"""
        if not data_points:
            return {}
            
        # æå–æ•°å€¼
        values = [point['value'] for point in data_points if point['value'] is not None]
        
        if not values:
            return {}
            
        # è®¡ç®—å„ç§èšåˆå€¼
        result = {}
        for agg_name, agg_func in self.aggregation_functions.items():
            try:
                result[agg_name] = agg_func(values)
            except Exception as e:
                result[agg_name] = 0.0
                
        # æ·»åŠ æ—¶é—´èŒƒå›´ä¿¡æ¯
        timestamps = [point['timestamp'] for point in data_points]
        result['time_range'] = {
            'start': min(timestamps).isoformat(),
            'end': max(timestamps).isoformat()
        }
        
        return result
        
    def _calculate_median(self, values: List[float]) -> float:
        """è®¡ç®—ä¸­ä½æ•°"""
        if not values:
            return 0.0
            
        sorted_values = sorted(values)
        n = len(sorted_values)
        
        if n % 2 == 0:
            return (sorted_values[n//2 - 1] + sorted_values[n//2]) / 2
        else:
            return sorted_values[n//2]
            
    def _calculate_stddev(self, values: List[float]) -> float:
        """è®¡ç®—æ ‡å‡†å·®"""
        if not values:
            return 0.0
            
        import statistics
        try:
            return statistics.stdev(values)
        except statistics.StatisticsError:
            return 0.0

# =============================================================================
# 6. æ•°æ®ä¸°å¯ŒæœåŠ¡
# =============================================================================

class DataEnrichmentService:
    """æ•°æ®ä¸°å¯ŒæœåŠ¡"""
    
    def __init__(self, redis_cache: RedisCache):
        self.cache = redis_cache
        self.enrichment_sources = {
            'geolocation': GeoLocationService(),
            'user_profile': UserProfileService(),
            'device_info': DeviceInfoService(),
            'business_context': BusinessContextService()
        }
        
    def enrich_message(self, message: DataMessage) -> DataMessage:
        """ä¸°å¯Œæ¶ˆæ¯æ•°æ®"""
        try:
            enriched_metadata = {}
            
            # æ ¹æ®æ•°æ®ç±»å‹è¿›è¡Œç›¸åº”çš„ä¸°å¯Œ
            if message.data_type == DataType.SENSOR_DATA:
                enriched_metadata.update(self._enrich_sensor_data(message))
            elif message.data_type == DataType.USER_EVENT:
                enriched_metadata.update(self._enrich_user_event(message))
            elif message.data_type == DataType.SYSTEM_METRIC:
                enriched_metadata.update(self._enrich_system_metric(message))
                
            # æ›´æ–°æ¶ˆæ¯å…ƒæ•°æ®
            if message.metadata is None:
                message.metadata = {}
                
            message.metadata.update(enriched_metadata)
            message.metadata['enriched_at'] = datetime.now().isoformat()
            
            # è®°å½•ä¸°å¯Œé˜¶æ®µ
            if ProcessingStage.ENRICHMENT not in message.processed_stages:
                message.processed_stages.append(ProcessingStage.ENRICHMENT)
                
            return message
            
        except Exception as e:
            print(f"æ•°æ®ä¸°å¯Œé”™è¯¯: {e}")
            return message
            
    def _enrich_sensor_data(self, message: DataMessage) -> Dict[str, Any]:
        """ä¸°å¯Œä¼ æ„Ÿå™¨æ•°æ®"""
        enrichment = {}
        
        device_id = message.payload.get('device_id')
        if device_id:
            # è·å–è®¾å¤‡ä¿¡æ¯
            device_info = self.enrichment_sources['device_info'].get_device_info(device_id)
            enrichment['device_info'] = device_info
            
            # è·å–è®¾å¤‡åœ°ç†ä½ç½®
            if device_info and 'location' in device_info:
                enrichment['geolocation'] = device_info['location']
                
        return enrichment
        
    def _enrich_user_event(self, message: DataMessage) -> Dict[str, Any]:
        """ä¸°å¯Œç”¨æˆ·äº‹ä»¶"""
        enrichment = {}
        
        user_id = message.payload.get('user_id')
        if user_id:
            # è·å–ç”¨æˆ·ç”»åƒ
            user_profile = self.enrichment_sources['user_profile'].get_user_profile(user_id)
            enrichment['user_profile'] = user_profile
            
            # è·å–åœ°ç†ä½ç½®ï¼ˆå¦‚æœæœ‰IPåœ°å€ï¼‰
            if 'ip_address' in message.payload:
                geo_info = self.enrichment_sources['geolocation'].lookup(message.payload['ip_address'])
                enrichment['geolocation'] = geo_info
                
        return enrichment
        
    def _enrich_system_metric(self, message: DataMessage) -> Dict[str, Any]:
        """ä¸°å¯Œç³»ç»ŸæŒ‡æ ‡"""
        enrichment = {}
        
        source = message.payload.get('source', message.source_id)
        
        # æ·»åŠ ä¸šåŠ¡ä¸Šä¸‹æ–‡
        business_context = self.enrichment_sources['business_context'].get_context(source)
        enrichment['business_context'] = business_context
        
        return enrichment

# =============================================================================
# 7. æ•°æ®ä¸°å¯ŒæºæœåŠ¡ï¼ˆæ¨¡æ‹Ÿï¼‰
# =============================================================================

class GeoLocationService:
    """åœ°ç†ä½ç½®æœåŠ¡"""
    
    def __init__(self):
        self.geo_cache = {}
        
    def lookup(self, ip_address: str) -> Dict[str, Any]:
        """æŸ¥æ‰¾IPåœ°å€åœ°ç†ä½ç½®"""
        # æ¨¡æ‹Ÿåœ°ç†ä½ç½®æ•°æ®
        return {
            'country': 'CN',
            'region': 'Beijing',
            'city': 'Beijing',
            'latitude': 39.9042,
            'longitude': 116.4074,
            'timezone': 'Asia/Shanghai'
        }

class UserProfileService:
    """ç”¨æˆ·ç”»åƒæœåŠ¡"""
    
    def __init__(self):
        self.profile_cache = {}
        
    def get_user_profile(self, user_id: str) -> Dict[str, Any]:
        """è·å–ç”¨æˆ·ç”»åƒ"""
        # æ¨¡æ‹Ÿç”¨æˆ·ç”»åƒæ•°æ®
        return {
            'age_group': '25-35',
            'gender': 'unknown',
            'interests': ['technology', 'data_analysis'],
            'subscription_level': 'premium',
            'last_active': datetime.now().isoformat(),
            'preferences': {
                'language': 'zh-CN',
                'notifications': True
            }
        }

class DeviceInfoService:
    """è®¾å¤‡ä¿¡æ¯æœåŠ¡"""
    
    def __init__(self):
        self.device_cache = {}
        
    def get_device_info(self, device_id: str) -> Dict[str, Any]:
        """è·å–è®¾å¤‡ä¿¡æ¯"""
        # æ¨¡æ‹Ÿè®¾å¤‡ä¿¡æ¯æ•°æ®
        if device_id.startswith('temp_'):
            return {
                'device_type': 'temperature_sensor',
                'model': 'DHT22',
                'location': {
                    'building': 'Building A',
                    'floor': 3,
                    'room': 'Server Room'
                },
                'manufacturer': 'Adafruit',
                'specifications': {
                    'accuracy': 'Â±0.5Â°C',
                    'range': '-40Â°C to 80Â°C'
                }
            }
        else:
            return {
                'device_type': 'unknown',
                'location': {
                    'building': 'Unknown',
                    'floor': 1,
                    'room': 'Unknown'
                }
            }

class BusinessContextService:
    """ä¸šåŠ¡ä¸Šä¸‹æ–‡æœåŠ¡"""
    
    def __init__(self):
        self.context_cache = {}
        
    def get_context(self, source: str) -> Dict[str, Any]:
        """è·å–ä¸šåŠ¡ä¸Šä¸‹æ–‡"""
        # æ¨¡æ‹Ÿä¸šåŠ¡ä¸Šä¸‹æ–‡æ•°æ®
        return {
            'service_name': source,
            'environment': 'production',
            'business_critical': source in ['payment_service', 'user_service'],
            'owner_team': 'backend_team',
            'sla_level': 'high' if source in ['payment_service', 'user_service'] else 'medium'
        }

# =============================================================================
# 8. æ•°æ®æµå¤„ç†ç¼–æ’å™¨
# =============================================================================

class DataFlowOrchestrator:
    """æ•°æ®æµå¤„ç†ç¼–æ’å™¨"""
    
    def __init__(self, rabbitmq_config=None, redis_config=None):
        # åˆå§‹åŒ–ç»„ä»¶
        self.rabbitmq = RabbitMQConnector(**(rabbitmq_config or {}))
        self.cache = RedisCache(**(redis_config or {}))
        self.validator = DataValidator()
        self.transformer = DataTransformer()
        self.aggregator = SlidingWindowAggregator()
        self.enricher = DataEnrichmentService(self.cache)
        
        # æ€§èƒ½æŒ‡æ ‡
        self.metrics = defaultdict(ProcessingMetrics)
        self.processing_stats = {
            'total_messages': 0,
            'successful_messages': 0,
            'failed_messages': 0,
            'avg_processing_time': 0.0
        }
        
        # çº¿ç¨‹æ§åˆ¶
        self.is_running = False
        self.processing_threads = []
        
    def start(self):
        """å¯åŠ¨æ•°æ®æµå¤„ç†"""
        print("ğŸš€ å¯åŠ¨å®æ—¶æ•°æ®æµå¤„ç†ç³»ç»Ÿ...")
        
        try:
            # å»ºç«‹è¿æ¥
            self.rabbitmq.connect()
            
            # å£°æ˜é˜Ÿåˆ—
            self.setup_queues()
            
            # è®¾ç½®æ¶ˆè´¹è€…
            self.setup_consumers()
            
            # å¯åŠ¨å¤„ç†çº¿ç¨‹
            self.start_processing_threads()
            
            self.is_running = True
            print("âœ… æ•°æ®æµå¤„ç†ç³»ç»Ÿå¯åŠ¨æˆåŠŸ")
            
        except Exception as e:
            print(f"âŒ å¯åŠ¨å¤±è´¥: {e}")
            raise
            
    def stop(self):
        """åœæ­¢æ•°æ®æµå¤„ç†"""
        print("ğŸ›‘ åœæ­¢æ•°æ®æµå¤„ç†ç³»ç»Ÿ...")
        
        self.is_running = False
        
        # åœæ­¢æ‰€æœ‰çº¿ç¨‹
        for thread in self.processing_threads:
            if thread.is_alive():
                thread.join(timeout=5)
                
        # æ–­å¼€è¿æ¥
        self.rabbitmq.disconnect()
        
        print("âœ… æ•°æ®æµå¤„ç†ç³»ç»Ÿå·²åœæ­¢")
        
    def setup_queues(self):
        """è®¾ç½®é˜Ÿåˆ—"""
        queues = [
            ('raw_data_queue', 'data_ingestion', '*.raw.*'),
            ('validated_data_queue', 'processing_pipeline', '*.validated.*'),
            ('transformed_data_queue', 'processing_pipeline', '*.transformed.*'),
            ('aggregated_data_queue', 'aggregated_data', ''),
            ('output_queue', 'data_ingestion', '*.output.*')
        ]
        
        for queue_name, exchange, routing_key in queues:
            queue = self.rabbitmq.declare_queue(queue_name)
            self.rabbitmq.channel.queue_bind(
                exchange=exchange,
                queue=queue,
                routing_key=routing_key
            )
            
    def setup_consumers(self):
        """è®¾ç½®æ¶ˆè´¹è€…"""
        # åŸå§‹æ•°æ®æ¶ˆè´¹è€…
        self.rabbitmq.channel.basic_qos(prefetch_count=10)
        
    def start_processing_threads(self):
        """å¯åŠ¨å¤„ç†çº¿ç¨‹"""
        # æ•°æ®éªŒè¯çº¿ç¨‹
        validation_thread = threading.Thread(target=self.validation_worker, daemon=True)
        validation_thread.start()
        self.processing_threads.append(validation_thread)
        
        # æ•°æ®è½¬æ¢çº¿ç¨‹
        transformation_thread = threading.Thread(target=self.transformation_worker, daemon=True)
        transformation_thread.start()
        self.processing_threads.append(transformation_thread)
        
        # èšåˆçº¿ç¨‹
        aggregation_thread = threading.Thread(target=self.aggregation_worker, daemon=True)
        aggregation_thread.start()
        self.processing_threads.append(aggregation_thread)
        
        # ä¸°å¯Œçº¿ç¨‹
        enrichment_thread = threading.Thread(target=self.enrichment_worker, daemon=True)
        enrichment_thread.start()
        self.processing_threads.append(enrichment_thread)
        
    def validation_worker(self):
        """æ•°æ®éªŒè¯å·¥ä½œçº¿ç¨‹"""
        while self.is_running:
            try:
                # æ¨¡æ‹Ÿä»é˜Ÿåˆ—è·å–æ•°æ®
                raw_messages = self.simulate_get_messages('raw_data_queue', batch_size=5)
                
                for message_data in raw_messages:
                    message = self.parse_message(message_data)
                    if message and self.validator.validate(message):
                        # å‘é€åˆ°éªŒè¯åé˜Ÿåˆ—
                        self.rabbitmq.publish(
                            'processing_pipeline',
                            f'validated.{message.source_id}',
                            message
                        )
                        self.update_metrics(ProcessingStage.VALIDATION, success=True)
                    else:
                        self.update_metrics(ProcessingStage.VALIDATION, success=False)
                        
            except Exception as e:
                print(f"éªŒè¯çº¿ç¨‹é”™è¯¯: {e}")
                
            time.sleep(0.1)
            
    def transformation_worker(self):
        """æ•°æ®è½¬æ¢å·¥ä½œçº¿ç¨‹"""
        while self.is_running:
            try:
                validated_messages = self.simulate_get_messages('validated_data_queue', batch_size=5)
                
                for message_data in validated_messages:
                    message = self.parse_message(message_data)
                    if message:
                        transformed_message = self.transformer.transform(message)
                        # å‘é€åˆ°è½¬æ¢åé˜Ÿåˆ—
                        self.rabbitmq.publish(
                            'processing_pipeline',
                            f'transformed.{message.source_id}',
                            transformed_message
                        )
                        self.update_metrics(ProcessingStage.TRANSFORMATION, success=True)
                        
            except Exception as e:
                print(f"è½¬æ¢çº¿ç¨‹é”™è¯¯: {e}")
                
            time.sleep(0.1)
            
    def aggregation_worker(self):
        """èšåˆå·¥ä½œçº¿ç¨‹"""
        while self.is_running:
            try:
                transformed_messages = self.simulate_get_messages('transformed_data_queue', batch_size=10)
                
                for message_data in transformed_messages:
                    message = self.parse_message(message_data)
                    if message:
                        # æ·»åŠ åˆ°èšåˆå™¨
                        self.aggregator.add_data(message)
                        
                # æ£€æŸ¥æ˜¯å¦éœ€è¦æ»‘åŠ¨çª—å£
                if self.aggregator.should_slide():
                    window_result = self.aggregator.slide_window()
                    if window_result:
                        # å‘å¸ƒèšåˆç»“æœ
                        self.publish_aggregated_result(window_result)
                        
                self.update_metrics(ProcessingStage.AGGREGATION, success=True)
                
            except Exception as e:
                print(f"èšåˆçº¿ç¨‹é”™è¯¯: {e}")
                
            time.sleep(1)
            
    def enrichment_worker(self):
        """æ•°æ®ä¸°å¯Œå·¥ä½œçº¿ç¨‹"""
        while self.is_running:
            try:
                # è·å–éœ€è¦ä¸°å¯Œçš„æ•°æ®
                enrichment_messages = self.simulate_get_messages('aggregated_data_queue', batch_size=3)
                
                for message_data in enrichment_messages:
                    message = self.parse_message(message_data)
                    if message:
                        enriched_message = self.enricher.enrich_message(message)
                        # å‘é€åˆ°è¾“å‡ºé˜Ÿåˆ—
                        self.rabbitmq.publish(
                            'data_ingestion',
                            f'output.{message.source_id}',
                            enriched_message
                        )
                        self.update_metrics(ProcessingStage.ENRICHMENT, success=True)
                        
            except Exception as e:
                print(f"ä¸°å¯Œçº¿ç¨‹é”™è¯¯: {e}")
                
            time.sleep(0.5)
            
    def simulate_get_messages(self, queue_name: str, batch_size: int = 1) -> List[str]:
        """æ¨¡æ‹Ÿä»é˜Ÿåˆ—è·å–æ¶ˆæ¯ï¼ˆå®é™…åº”ç”¨ä¸­æ›¿æ¢ä¸ºçœŸå®çš„RabbitMQæ¶ˆè´¹ï¼‰"""
        # è¿™é‡Œæ¨¡æ‹Ÿç”Ÿæˆæµ‹è¯•æ•°æ®
        messages = []
        for i in range(batch_size):
            if self.should_generate_test_data():
                message = self.generate_test_message()
                messages.append(json.dumps(message))
        return messages
        
    def should_generate_test_data(self) -> bool:
        """åˆ¤æ–­æ˜¯å¦åº”è¯¥ç”Ÿæˆæµ‹è¯•æ•°æ®"""
        return datetime.now().second % 3 == 0  # æ¯3ç§’ç”Ÿæˆä¸€æ‰¹æ•°æ®
        
    def generate_test_message(self) -> Dict[str, Any]:
        """ç”Ÿæˆæµ‹è¯•æ¶ˆæ¯"""
        message_id = str(uuid.uuid4())
        data_types = list(DataType)
        data_type = data_types[datetime.now().second % len(data_types)]
        
        base_message = {
            'message_id': message_id,
            'data_type': data_type.value,
            'timestamp': datetime.now().isoformat(),
            'source_id': f'source_{datetime.now().second % 10}',
            'payload': {},
            'metadata': {'generated_at': datetime.now().isoformat()}
        }
        
        if data_type == DataType.SENSOR_DATA:
            base_message['payload'] = {
                'device_id': f'temp_sensor_{datetime.now().second % 5}',
                'sensor_type': 'temperature',
                'value': round(20 + (datetime.now().second % 10) * 0.5, 1),
                'unit': 'Â°C'
            }
        elif data_type == DataType.USER_EVENT:
            base_message['payload'] = {
                'user_id': f'user_{datetime.now().second % 100}',
                'event_type': ['login', 'click', 'view', 'purchase'][datetime.now().second % 4],
                'ip_address': f'192.168.1.{datetime.now().second % 255}'
            }
        elif data_type == DataType.SYSTEM_METRIC:
            base_message['payload'] = {
                'metric_name': 'cpu_usage',
                'metric_value': round(50 + (datetime.now().second % 50), 1),
                'source': 'server_01'
            }
            
        return base_message
        
    def parse_message(self, message_data: str) -> Optional[DataMessage]:
        """è§£ææ¶ˆæ¯"""
        try:
            data = json.loads(message_data)
            return DataMessage(**data)
        except Exception as e:
            print(f"æ¶ˆæ¯è§£æé”™è¯¯: {e}")
            return None
            
    def publish_aggregated_result(self, window_result: Dict[str, Any]):
        """å‘å¸ƒèšåˆç»“æœ"""
        aggregation_message = DataMessage(
            message_id=str(uuid.uuid4()),
            data_type=DataType.BUSINESS_EVENT,
            timestamp=datetime.now(),
            source_id='aggregation_engine',
            payload=window_result,
            metadata={
                'aggregation_type': 'sliding_window',
                'window_size_minutes': 5,
                'generated_at': datetime.now().isoformat()
            }
        )
        
        self.rabbitmq.publish(
            'aggregated_data',
            'aggregated.results',
            aggregation_message
        )
        
    def update_metrics(self, stage: ProcessingStage, success: bool = True, processing_time: float = 0.0):
        """æ›´æ–°å¤„ç†æŒ‡æ ‡"""
        if stage not in self.metrics:
            self.metrics[stage] = ProcessingMetrics(stage=stage, start_time=datetime.now())
            
        self.metrics[stage].message_count += 1
        if not success:
            self.metrics[stage].error_count += 1
            
        # æ›´æ–°å¤„ç†æ—¶é—´ç»Ÿè®¡
        if processing_time > 0:
            current_avg = self.metrics[stage].avg_processing_time
            count = self.metrics[stage].message_count
            self.metrics[stage].avg_processing_time = (current_avg * (count - 1) + processing_time) / count
            
    def get_processing_stats(self) -> Dict[str, Any]:
        """è·å–å¤„ç†ç»Ÿè®¡ä¿¡æ¯"""
        stats = self.processing_stats.copy()
        
        # æ·»åŠ å„é˜¶æ®µè¯¦ç»†æŒ‡æ ‡
        stats['stage_metrics'] = {}
        for stage, metrics in self.metrics.items():
            stats['stage_metrics'][stage.value] = {
                'message_count': metrics.message_count,
                'error_count': metrics.error_count,
                'success_rate': (metrics.message_count - metrics.error_count) / max(metrics.message_count, 1),
                'avg_processing_time': metrics.avg_processing_time
            }
            
        return stats
        
    def print_dashboard(self):
        """æ‰“å°å¤„ç†ä»ªè¡¨æ¿"""
        stats = self.get_processing_stats()
        
        print("\n" + "="*60)
        print("ğŸ“Š å®æ—¶æ•°æ®æµå¤„ç†ä»ªè¡¨æ¿")
        print("="*60)
        print(f"ğŸ“ˆ æ€»å¤„ç†æ¶ˆæ¯æ•°: {stats['total_messages']:,}")
        print(f"âœ… æˆåŠŸæ¶ˆæ¯æ•°: {stats['successful_messages']:,}")
        print(f"âŒ å¤±è´¥æ¶ˆæ¯æ•°: {stats['failed_messages']:,}")
        print(f"â±ï¸  å¹³å‡å¤„ç†æ—¶é—´: {stats['avg_processing_time']:.3f}s")
        
        print("\nğŸ“‹ å„é˜¶æ®µå¤„ç†æƒ…å†µ:")
        for stage_name, stage_stats in stats['stage_metrics'].items():
            print(f"  {stage_name}: {stage_stats['message_count']:,} æ¶ˆæ¯, "
                  f"{stage_stats['success_rate']:.1%} æˆåŠŸç‡, "
                  f"{stage_stats['avg_processing_time']:.3f}s å¹³å‡è€—æ—¶")
                  
        print("="*60)

# =============================================================================
# 9. æ¼”ç¤ºç¨‹åº
# =============================================================================

def main():
    """ä¸»æ¼”ç¤ºç¨‹åº"""
    print("ğŸ¯ RabbitMQå®æ—¶æ•°æ®æµå¤„ç†ç®¡é“æ¼”ç¤º")
    print("="*50)
    
    # é…ç½®
    rabbitmq_config = {
        'host': 'localhost',
        'port': 5672,
        'username': 'admin',
        'password': 'admin'
    }
    
    redis_config = {
        'host': 'localhost',
        'port': 6379,
        'db': 0
    }
    
    # åˆ›å»ºç¼–æ’å™¨
    orchestrator = DataFlowOrchestrator(rabbitmq_config, redis_config)
    
    try:
        # å¯åŠ¨å¤„ç†ç³»ç»Ÿ
        orchestrator.start()
        
        # è¿è¡Œæ¼”ç¤º
        print("ğŸ”„ å¼€å§‹å¤„ç†å®æ—¶æ•°æ®æµ...")
        print("â° æ¼”ç¤ºå°†è¿è¡Œ30ç§’ï¼Œæ˜¾ç¤ºå®æ—¶å¤„ç†ç»Ÿè®¡")
        
        start_time = time.time()
        dashboard_interval = 5  # æ¯5ç§’æ˜¾ç¤ºä¸€æ¬¡ä»ªè¡¨æ¿
        
        while time.time() - start_time < 30:
            time.sleep(dashboard_interval)
            
            # æ›´æ–°ç»Ÿè®¡æ•°æ®ï¼ˆæ¨¡æ‹Ÿï¼‰
            orchestrator.processing_stats['total_messages'] += 50
            orchestrator.processing_stats['successful_messages'] += 48
            orchestrator.processing_stats['failed_messages'] += 2
            
            # æ˜¾ç¤ºä»ªè¡¨æ¿
            orchestrator.print_dashboard()
            
        print("\nğŸ‰ æ¼”ç¤ºå®Œæˆï¼")
        
    except KeyboardInterrupt:
        print("\nğŸ›‘ ç”¨æˆ·ä¸­æ–­æ¼”ç¤º")
    except Exception as e:
        print(f"\nâŒ æ¼”ç¤ºé”™è¯¯: {e}")
    finally:
        # åœæ­¢ç³»ç»Ÿ
        orchestrator.stop()

if __name__ == "__main__":
    main()