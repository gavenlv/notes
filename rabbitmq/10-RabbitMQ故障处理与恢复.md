# ç¬¬10ç« ï¼šRabbitMQæ•…éšœå¤„ç†ä¸æ¢å¤

## ğŸ“– æ¦‚è¿°

åœ¨ç”Ÿäº§ç¯å¢ƒä¸­ï¼ŒRabbitMQé›†ç¾¤å¯èƒ½é¢ä¸´å„ç§æ•…éšœæƒ…å†µï¼ŒåŒ…æ‹¬ç½‘ç»œåˆ†åŒºã€èŠ‚ç‚¹å®•æœºã€ç£ç›˜ç©ºé—´ä¸è¶³ã€å†…å­˜å‹åŠ›ã€æ¶ˆæ¯ç§¯å‹ç­‰ã€‚æœ¬ç« å°†è¯¦ç»†ä»‹ç»RabbitMQæ•…éšœæ£€æµ‹ã€è¯Šæ–­ã€å¤„ç†å’Œæ¢å¤çš„æœ€ä½³å®è·µï¼Œå¸®åŠ©æ‚¨æ„å»ºé«˜å¯ç”¨çš„æ¶ˆæ¯ç³»ç»Ÿã€‚

## ğŸ¯ å­¦ä¹ ç›®æ ‡

- æŒæ¡RabbitMQå¸¸è§æ•…éšœç±»å‹å’Œè¯†åˆ«æ–¹æ³•
- å­¦ä¼šé…ç½®æ•…éšœæ£€æµ‹å’Œè‡ªåŠ¨æ¢å¤æœºåˆ¶
- ç†è§£æ•°æ®å¤‡ä»½å’Œç¾éš¾æ¢å¤ç­–ç•¥
- æŒæ¡æ•…éšœè¯Šæ–­å’Œæ€§èƒ½åˆ†ææ–¹æ³•
- å­¦ä¼šæ„å»ºæ•…éšœå¤„ç†è‡ªåŠ¨åŒ–å·¥å…·
- äº†è§£å®¹é‡è§„åˆ’å’Œé¢„é˜²æ€§ç»´æŠ¤ç­–ç•¥

## ğŸ” æ•…éšœç±»å‹åˆ†æ

### 1. ç¡¬ä»¶å±‚é¢æ•…éšœ

#### èŠ‚ç‚¹å®•æœº
- **ç—‡çŠ¶**: èŠ‚ç‚¹æ— å“åº”ã€å¿ƒè·³ä¸¢å¤±ã€ç®¡ç†APIä¸å¯è®¿é—®
- **å½±å“**: è¯¥èŠ‚ç‚¹ä¸Šçš„é˜Ÿåˆ—å’Œäº¤æ¢æœºä¸å¯ç”¨
- **æ£€æµ‹**: é€šè¿‡é›†ç¾¤çŠ¶æ€æ£€æŸ¥ã€å¿ƒè·³ç›‘æ§

#### ç£ç›˜æ•…éšœ
- **ç—‡çŠ¶**: ç£ç›˜å†™å…¥å¤±è´¥ã€ç£ç›˜ç©ºé—´ä¸è¶³
- **å½±å“**: æŒä¹…åŒ–æ¶ˆæ¯æ— æ³•å†™å…¥ã€é˜Ÿåˆ—é˜»å¡
- **æ£€æµ‹**: ç£ç›˜ç›‘æ§å·¥å…·ã€æ—¥å¿—åˆ†æ

#### ç½‘ç»œæ•…éšœ
- **ç—‡çŠ¶**: èŠ‚ç‚¹é—´é€šä¿¡ä¸­æ–­ã€ç½‘ç»œåˆ†åŒº
- **å½±å“**: é›†ç¾¤åˆ†è£‚ã€æ•°æ®ä¸ä¸€è‡´
- **æ£€æµ‹**: ç½‘ç»œç›‘æ§ã€é›†ç¾¤çŠ¶æ€æ£€æŸ¥

### 2. è½¯ä»¶å±‚é¢æ•…éšœ

#### RabbitMQæœåŠ¡å´©æºƒ
- **ç—‡çŠ¶**: è¿›ç¨‹å¼‚å¸¸é€€å‡ºã€æ ¸å¿ƒè½¬å‚¨
- **å½±å“**: æ•´ä¸ªèŠ‚ç‚¹æœåŠ¡ä¸­æ–­
- **æ£€æµ‹**: æœåŠ¡ç›‘æ§ã€å¥åº·æ£€æŸ¥

#### å†…å­˜å‹åŠ›
- **ç—‡çŠ¶**: å†…å­˜ä½¿ç”¨ç‡è¿‡é«˜ã€é¢‘ç¹GC
- **å½±å“**: æ€§èƒ½ä¸‹é™ã€è¿æ¥æ‹’ç»
- **æ£€æµ‹**: å†…å­˜ç›‘æ§ã€å †è½¬å‚¨åˆ†æ

#### é…ç½®é”™è¯¯
- **ç—‡çŠ¶**: å¯åŠ¨å¤±è´¥ã€é…ç½®æ— æ•ˆ
- **å½±å“**: æœåŠ¡æ— æ³•æ­£å¸¸å¯åŠ¨
- **æ£€æµ‹**: å¯åŠ¨æ—¥å¿—ã€é…ç½®éªŒè¯

### 3. åº”ç”¨å±‚é¢æ•…éšœ

#### æ¶ˆæ¯ç§¯å‹
- **ç—‡çŠ¶**: é˜Ÿåˆ—æ¶ˆæ¯æ•°é‡å¼‚å¸¸å¢é•¿
- **å½±å“**: å†…å­˜ä½¿ç”¨å¢åŠ ã€å¤„ç†å»¶è¿Ÿ
- **æ£€æµ‹**: é˜Ÿåˆ—ç›‘æ§ã€æ¶ˆæ¯è®¡æ•°

#### æ¶ˆè´¹è€…å¼‚å¸¸
- **ç—‡çŠ¶**: æ¶ˆè´¹è€…è¿æ¥æ–­å¼€ã€æ¶ˆæ¯å¤„ç†å¤±è´¥
- **å½±å“**: æ¶ˆæ¯æ— æ³•æ­£å¸¸æ¶ˆè´¹ã€é˜Ÿåˆ—é˜»å¡
- **æ£€æµ‹**: è¿æ¥ç›‘æ§ã€æ¶ˆè´¹æˆåŠŸç‡

## ğŸ› ï¸ æ•…éšœæ£€æµ‹æœºåˆ¶

### 1. å¥åº·æ£€æŸ¥é…ç½®

```bash
# åŸºç¡€å¥åº·æ£€æŸ¥
rabbitmq-diagnostics ping
rabbitmq-diagnostics check_running
rabbitmq-diagnostics check_port_connectivity

# é›†ç¾¤å¥åº·æ£€æŸ¥
rabbitmq-diagnostics cluster_status
rabbitmq-diagnostics check_cluster_health
```

### 2. è‡ªå®šä¹‰å¥åº·æ£€æŸ¥è„šæœ¬

```bash
#!/bin/bash
# health_check.sh - RabbitMQå¥åº·æ£€æŸ¥è„šæœ¬

RABBITMQ_HOST=${RABBITMQ_HOST:-"localhost"}
RABBITMQ_PORT=${RABBITMQ_PORT:-"15672"}
RABBITMQ_USER=${RABBITMQ_USER:-"admin"}
RABBITMQ_PASS=${RABBITMQ_PASS:-"password"}

# æ£€æŸ¥RabbitMQ APIå“åº”
check_api_health() {
    response=$(curl -s -o /dev/null -w "%{http_code}" \
        -u ${RABBITMQ_USER}:${RABBITMQ_PASS} \
        http://${RABBITMQ_HOST}:${RABBITMQ_PORT}/api/health/checks/alarms)
    
    if [ "$response" = "200" ]; then
        echo "APIå¥åº·æ£€æŸ¥é€šè¿‡"
        return 0
    else
        echo "APIå¥åº·æ£€æŸ¥å¤±è´¥ (HTTP: $response)"
        return 1
    fi
}

# æ£€æŸ¥èŠ‚ç‚¹çŠ¶æ€
check_node_status() {
    nodes=$(curl -s -u ${RABBITMQ_USER}:${RABBITMQ_PASS} \
        http://${RABBITMQ_HOST}:${RABBITMQ_PORT}/api/nodes)
    
    running_nodes=$(echo "$nodes" | jq '[.[] | select(.running == true)] | length')
    total_nodes=$(echo "$nodes" | jq 'length')
    
    echo "è¿è¡ŒèŠ‚ç‚¹æ•°: $running_nodes/$total_nodes"
    
    if [ "$running_nodes" -gt 0 ] && [ "$running_nodes" -eq "$total_nodes" ]; then
        echo "æ‰€æœ‰èŠ‚ç‚¹æ­£å¸¸è¿è¡Œ"
        return 0
    else
        echo "å­˜åœ¨å¼‚å¸¸èŠ‚ç‚¹"
        return 1
    fi
}

# æ£€æŸ¥ç£ç›˜ç©ºé—´
check_disk_space() {
    disk_usage=$(df -h /var/lib/rabbitmq | awk 'NR==2 {print $5}' | sed 's/%//')
    
    if [ "$disk_usage" -lt 80 ]; then
        echo "ç£ç›˜ç©ºé—´å……è¶³ (${disk_usage}%)"
        return 0
    else
        echo "ç£ç›˜ç©ºé—´ä¸è¶³ (${disk_usage}%)"
        return 1
    fi
}

# æ£€æŸ¥å†…å­˜ä½¿ç”¨
check_memory() {
    memory_usage=$(free | awk 'FNR==2{printf "%.1f", $3/($3+$4)*100}')
    
    if (( $(echo "$memory_usage < 80" | bc -l) )); then
        echo "å†…å­˜ä½¿ç”¨æ­£å¸¸ (${memory_usage}%)"
        return 0
    else
        echo "å†…å­˜ä½¿ç”¨è¿‡é«˜ (${memory_usage}%)"
        return 1
    fi
}

# æ‰§è¡Œæ‰€æœ‰æ£€æŸ¥
main() {
    echo "=== RabbitMQå¥åº·æ£€æŸ¥ ==="
    
    api_healthy=true
    node_healthy=true
    disk_healthy=true
    memory_healthy=true
    
    check_api_health || api_healthy=false
    check_node_status || node_healthy=false
    check_disk_space || disk_healthy=false
    check_memory || memory_healthy=false
    
    if $api_healthy && $node_healthy && $disk_healthy && $memory_healthy; then
        echo "=== æ•´ä½“å¥åº·çŠ¶æ€: å¥åº· ==="
        exit 0
    else
        echo "=== æ•´ä½“å¥åº·çŠ¶æ€: ä¸å¥åº· ==="
        exit 1
    fi
}

main "$@"
```

### 3. ç›‘æ§å‘Šè­¦é…ç½®

```yaml
# prometheus_alerts.yml - Prometheuså‘Šè­¦è§„åˆ™
groups:
- name: rabbitmq.rules
  rules:
  - alert: RabbitMQNodeDown
    expr: rabbitmq_up == 0
    for: 1m
    labels:
      severity: critical
    annotations:
      summary: "RabbitMQèŠ‚ç‚¹ {{ $labels.instance }} å®•æœº"
      description: "RabbitMQèŠ‚ç‚¹å·²å®•æœºè¶…è¿‡1åˆ†é’Ÿ"

  - alert: RabbitMQQueueMessagesHigh
    expr: rabbitmq_queue_messages > 10000
    for: 5m
    labels:
      severity: warning
    annotations:
      summary: "é˜Ÿåˆ— {{ $labels.queue }} æ¶ˆæ¯ç§¯å‹"
      description: "é˜Ÿåˆ— {{ $labels.queue }} æ¶ˆæ¯æ•°é‡è¾¾åˆ° {{ $value }}"

  - alert: RabbitMQMemoryHigh
    expr: rabbitmq_process_resident_memory_bytes / 1024 / 1024 > 2048
    for: 5m
    labels:
      severity: warning
    annotations:
      summary: "RabbitMQå†…å­˜ä½¿ç”¨è¿‡é«˜"
      description: "èŠ‚ç‚¹ {{ $labels.instance }} å†…å­˜ä½¿ç”¨è¶…è¿‡2GB"

  - alert: RabbitMQDiskSpaceLow
    expr: rabbitmq_disk_free_bytes / 1024 / 1024 / 1024 < 5
    for: 2m
    labels:
      severity: critical
    annotations:
      summary: "RabbitMQç£ç›˜ç©ºé—´ä¸è¶³"
      description: "èŠ‚ç‚¹ {{ $labels.instance }} ç£ç›˜å‰©ä½™ç©ºé—´ä¸è¶³5GB"
```

## ğŸ”§ æ•…éšœå¤„ç†ç­–ç•¥

### 1. èŠ‚ç‚¹æ•…éšœå¤„ç†

#### å•èŠ‚ç‚¹æ•…éšœæ¢å¤

```bash
# 1. æ£€æŸ¥èŠ‚ç‚¹çŠ¶æ€
rabbitmq-diagnostics cluster_status

# 2. åœæ­¢å¹¶é‡æ–°å¯åŠ¨èŠ‚ç‚¹
rabbitmqctl stop
systemctl start rabbitmq-server

# 3. æ£€æŸ¥èŠ‚ç‚¹æ¢å¤çŠ¶æ€
rabbitmq-diagnostics ping
rabbitmq-diagnostics check_running
```

#### é›†ç¾¤èŠ‚ç‚¹æ¢å¤

```bash
# 1. æ£€æŸ¥é›†ç¾¤çŠ¶æ€
rabbitmqctl cluster_status

# 2. ç¡®è®¤ç£ç›˜èŠ‚ç‚¹åŒæ­¥
rabbitmqctl sync_offline_queue

# 3. é‡æ–°åŠ å…¥é›†ç¾¤ï¼ˆå¦‚æœèŠ‚ç‚¹è¢«ç§»é™¤ï¼‰
rabbitmqctl stop_app
rabbitmqctl reset
rabbitmqctl join_cluster rabbit@master
rabbitmqctl start_app

# 4. æ£€æŸ¥é•œåƒé˜Ÿåˆ—çŠ¶æ€
rabbitmqctl list_policies
```

### 2. å†…å­˜æ•…éšœå¤„ç†

#### å†…å­˜æ³„æ¼æ£€æµ‹

```bash
# æ£€æŸ¥å†…å­˜ä½¿ç”¨æƒ…å†µ
rabbitmq-diagnostics memory_breakdown

# è·å–å †è½¬å‚¨
rabbitmqctl eval 'erlang:halt(1).'
# æˆ–è€…ä½¿ç”¨è¯Šæ–­å·¥å…·
rabbitmq-diagnostics dump_os_processes
```

#### å†…å­˜ä¼˜åŒ–é…ç½®

```ini
# rabbitmq.conf - å†…å­˜ä¼˜åŒ–é…ç½®
# è®¾ç½®å†…å­˜é˜ˆå€¼
vm_memory_high_watermark.relative = 0.6
vm_memory_high_watermark_paging_ratio = 0.5

# é˜Ÿåˆ—å†…å­˜é™åˆ¶
queue_memory_soft_limit = 0.5

# è¿æ¥å†…å­˜æ± 
connection_channel_max = 2048
connection_max_buffer_size = 33554432
```

#### ç´§æ€¥å†…å­˜æ¸…ç†

```bash
# å¼ºåˆ¶åƒåœ¾å›æ”¶
rabbitmqctl eval 'garbage_collect(all).'

# æ¸…ç†é˜Ÿåˆ—ä¸­æœªç¡®è®¤çš„æ¶ˆæ¯
rabbitmqctl purge_queue queue_name

# å…³é—­é—²ç½®è¿æ¥
rabbitmqctl close_connection connection_id "Memory pressure"
```

### 3. ç£ç›˜æ•…éšœå¤„ç†

#### ç£ç›˜ç©ºé—´åˆ†æ

```bash
# æ£€æŸ¥ç£ç›˜ä½¿ç”¨æƒ…å†µ
rabbitmq-diagnostics disk_free

# åˆ†ææ—¥å¿—æ–‡ä»¶å¤§å°
du -sh /var/log/rabbitmq/

# æ¸…ç†æ—§æ—¥å¿—
find /var/log/rabbitmq/ -name "*.log.*" -mtime +7 -delete

# åˆ†æé˜Ÿåˆ—æ•°æ®æ–‡ä»¶
du -sh /var/lib/rabbitmq/mnesia/
```

#### ç£ç›˜ç©ºé—´é‡Šæ”¾

```bash
# æ¸…ç†Mnesiaæ•°æ®ç›®å½•
rabbitmqctl stop_app
rm -rf /var/lib/rabbitmq/mnesia/*
rabbitmqctl start_app

# æ¸…ç†æŒä¹…åŒ–æ¶ˆæ¯ï¼ˆè°¨æ…æ“ä½œï¼‰
rabbitmqctl clear_policy all <<policy_name>>

# å‹ç¼©ç£ç›˜ç©ºé—´
find /var/lib/rabbitmq/mnesia/ -name "*.dets" -exec sudo dets_compact {} \;
```

### 4. ç½‘ç»œåˆ†åŒºå¤„ç†

#### åˆ†åŒºæ£€æµ‹

```bash
# æ£€æŸ¥é›†ç¾¤çŠ¶æ€
rabbitmq-diagnostics cluster_status

# æ£€æŸ¥ç½‘ç»œè¿é€šæ€§
rabbitmq-diagnostics check_port_connectivity

# åˆ†æåˆ†åŒºåŸå› 
rabbitmqctl environment
```

#### åˆ†åŒºè§£å†³ç­–ç•¥

```bash
# æ‰‹åŠ¨åœæ­¢å¹¶é‡æ–°å¯åŠ¨åˆ†åŒºä¸­çš„èŠ‚ç‚¹
rabbitmqctl stop_app

# é€‰æ‹©ä¸»è¦åˆ†åŒºå¹¶é‡æ–°å¯åŠ¨
rabbitmqctl start_app

# å¯¹äºéä¸»è¦åˆ†åŒºï¼Œé‡ç½®å¹¶é‡æ–°åŠ å…¥
rabbitmqctl reset
rabbitmqctl join_cluster rabbit@primary
```

#### è‡ªåŠ¨åŒ–åˆ†åŒºæ¢å¤

```json
{
  "policy": {
    "recovery": "automatic",
    "stop_consumers_on_failure": false,
    "resume_publishing": true
  },
  "partitions": {
    "detection": "enabled",
    "resolution": "auto",
    "timeout": 300
  }
}
```

## ğŸ’¾ æ•°æ®å¤‡ä»½ä¸æ¢å¤

### 1. é…ç½®å¤‡ä»½

#### è‡ªåŠ¨é…ç½®å¤‡ä»½è„šæœ¬

```bash
#!/bin/bash
# backup_config.sh - RabbitMQé…ç½®å¤‡ä»½è„šæœ¬

BACKUP_DIR="/backup/rabbitmq/$(date +%Y%m%d_%H%M%S)"
RABBITMQ_HOST="localhost"
RABBITMQ_PORT="15672"
RABBITMQ_USER="admin"
RABBITMQ_PASS="password"

mkdir -p "$BACKUP_DIR"

# å¤‡ä»½ç”¨æˆ·å’Œæƒé™
curl -u ${RABBITMQ_USER}:${RABBITMQ_PASS} \
    http://${RABBITMQ_HOST}:${RABBITMQ_PORT}/api/users \
    > "$BACKUP_DIR/users.json"

# å¤‡ä»½è™šæ‹Ÿä¸»æœº
curl -u ${RABBITMQ_USER}:${RABBITMQ_PASS} \
    http://${RABBITMQ_HOST}:${RABBITMQ_PORT}/api/vhosts \
    > "$BACKUP_DIR/vhosts.json"

# å¤‡ä»½ç­–ç•¥
curl -u ${RABBITMQ_USER}:${RABBITMQ_PASS} \
    http://${RABBITMQ_HOST}:${RABBITMQ_PORT}/api/policies \
    > "$BACKUP_DIR/policies.json"

# å¤‡ä»½é›†ç¾¤é…ç½®
rabbitmqctl environment > "$BACKUP_DIR/environment.txt"
rabbitmqctl cluster_status > "$BACKUP_DIR/cluster_status.txt"

# å‹ç¼©å¤‡ä»½æ–‡ä»¶
tar -czf "$BACKUP_DIR.tar.gz" -C "$(dirname "$BACKUP_DIR")" "$(basename "$BACKUP_DIR")"
rm -rf "$BACKUP_DIR"

echo "é…ç½®å¤‡ä»½å®Œæˆ: $BACKUP_DIR.tar.gz"
```

#### é…ç½®æ¢å¤è„šæœ¬

```bash
#!/bin/bash
# restore_config.sh - RabbitMQé…ç½®æ¢å¤è„šæœ¬

BACKUP_FILE="$1"
TEMP_DIR="/tmp/rabbitmq_restore"

if [ -z "$BACKUP_FILE" ]; then
    echo "Usage: $0 <backup_file.tar.gz>"
    exit 1
fi

# è§£å‹å¤‡ä»½æ–‡ä»¶
tar -xzf "$BACKUP_FILE" -C "$(dirname "$TEMP_DIR")"

# æ¢å¤ç”¨æˆ·å’Œæƒé™
curl -X PUT \
    -u admin:password \
    -H "Content-Type: application/json" \
    -d "$(cat "$TEMP_DIR/users.json")" \
    http://localhost:15672/api/users/bulk/update

# æ¢å¤è™šæ‹Ÿä¸»æœº
curl -X PUT \
    -u admin:password \
    -H "Content-Type: application/json" \
    -d "$(cat "$TEMP_DIR/vhosts.json")" \
    http://localhost:15672/api/vhosts/bulk/update

# æ¸…ç†ä¸´æ—¶æ–‡ä»¶
rm -rf "$TEMP_DIR"

echo "é…ç½®æ¢å¤å®Œæˆ"
```

### 2. æ¶ˆæ¯æ•°æ®å¤‡ä»½

#### æ¶ˆæ¯å¯¼å‡ºè„šæœ¬

```python
#!/usr/bin/env python3
# message_backup.py - å¯¼å‡ºé˜Ÿåˆ—æ¶ˆæ¯

import pika
import json
import argparse
from datetime import datetime

def backup_queue_messages(rabbitmq_url, queue_name, output_file):
    """å¤‡ä»½æŒ‡å®šé˜Ÿåˆ—çš„æ‰€æœ‰æ¶ˆæ¯"""
    connection = pika.BlockingConnection(pika.URLParameters(rabbitmq_url))
    channel = connection.channel()
    
    # è·å–é˜Ÿåˆ—æ¶ˆæ¯æ•°é‡
    queue_state = channel.queue_declare(queue=queue_name, passive=True)
    message_count = queue_state.method.message_count
    
    if message_count == 0:
        print(f"é˜Ÿåˆ— {queue_name} ä¸ºç©º")
        return
    
    messages = []
    message_count = min(message_count, 1000)  # é™åˆ¶æ¶ˆæ¯æ•°é‡é¿å…å†…å­˜æº¢å‡º
    
    print(f"æ­£åœ¨å¤‡ä»½é˜Ÿåˆ— {queue_name} çš„ {message_count} æ¡æ¶ˆæ¯...")
    
    for i in range(message_count):
        try:
            method_frame, header_frame, body = channel.basic_get(
                queue=queue_name, 
                auto_ack=False
            )
            
            if method_frame:
                message_data = {
                    'delivery_tag': method_frame.delivery_tag,
                    'redelivered': method_frame.redelivered,
                    'exchange': method_frame.exchange,
                    'routing_key': method_frame.routing_key,
                    'properties': {
                        'message_id': header_frame.message_id,
                        'correlation_id': header_frame.correlation_id,
                        'timestamp': header_frame.timestamp,
                        'delivery_mode': header_frame.delivery_mode,
                        'priority': header_frame.priority,
                        'reply_to': header_frame.reply_to,
                        'type': header_frame.type,
                        'user_id': header_frame.user_id,
                        'app_id': header_frame.app_id
                    },
                    'body': body.decode('utf-8') if isinstance(body, bytes) else body
                }
                messages.append(message_data)
                
                # ä¸ç¡®è®¤æ¶ˆæ¯ï¼Œåªè·å–æ•°æ®
                channel.basic_nack(delivery_tag=method_frame.delivery_tag, requeue=True)
            else:
                break
                
        except Exception as e:
            print(f"è·å–æ¶ˆæ¯æ—¶å‡ºé”™: {e}")
            break
    
    # ä¿å­˜å¤‡ä»½æ•°æ®
    backup_data = {
        'queue_name': queue_name,
        'backup_time': datetime.now().isoformat(),
        'message_count': len(messages),
        'messages': messages
    }
    
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(backup_data, f, indent=2, ensure_ascii=False)
    
    connection.close()
    print(f"æ¶ˆæ¯å¤‡ä»½å®Œæˆï¼Œå…± {len(messages)} æ¡æ¶ˆæ¯ï¼Œä¿å­˜åˆ°: {output_file}")

def main():
    parser = argparse.ArgumentParser(description='RabbitMQé˜Ÿåˆ—æ¶ˆæ¯å¤‡ä»½å·¥å…·')
    parser.add_argument('--rabbitmq-url', default='amqp://guest:guest@localhost:5672',
                        help='RabbitMQè¿æ¥URL')
    parser.add_argument('--queue', required=True, help='è¦å¤‡ä»½çš„é˜Ÿåˆ—åç§°')
    parser.add_argument('--output', required=True, help='è¾“å‡ºæ–‡ä»¶è·¯å¾„')
    
    args = parser.parse_args()
    
    backup_queue_messages(args.rabbitmq_url, args.queue, args.output)

if __name__ == '__main__':
    main()
```

#### æ¶ˆæ¯æ¢å¤è„šæœ¬

```python
#!/usr/bin/env python3
# message_restore.py - æ¢å¤é˜Ÿåˆ—æ¶ˆæ¯

import pika
import json
import argparse
import time

def restore_queue_messages(rabbitmq_url, queue_name, backup_file, requeue=False):
    """ä»å¤‡ä»½æ–‡ä»¶æ¢å¤æ¶ˆæ¯åˆ°é˜Ÿåˆ—"""
    connection = pika.BlockingConnection(pika.URLParameters(rabbitmq_url))
    channel = connection.channel()
    
    # å£°æ˜é˜Ÿåˆ—
    channel.queue_declare(queue=queue_name, durable=True)
    
    # è¯»å–å¤‡ä»½æ•°æ®
    with open(backup_file, 'r', encoding='utf-8') as f:
        backup_data = json.load(f)
    
    messages = backup_data['messages']
    print(f"æ­£åœ¨æ¢å¤é˜Ÿåˆ— {queue_name} çš„ {len(messages)} æ¡æ¶ˆæ¯...")
    
    for i, message_data in enumerate(messages):
        try:
            # è®¾ç½®æ¶ˆæ¯å±æ€§
            properties = pika.BasicProperties(
                message_id=message_data['properties'].get('message_id'),
                correlation_id=message_data['properties'].get('correlation_id'),
                timestamp=message_data['properties'].get('timestamp'),
                delivery_mode=message_data['properties'].get('delivery_mode', 2),  # æŒä¹…åŒ–
                priority=message_data['properties'].get('priority'),
                reply_to=message_data['properties'].get('reply_to'),
                type=message_data['properties'].get('type'),
                user_id=message_data['properties'].get('user_id'),
                app_id=message_data['properties'].get('app_id')
            )
            
            # å‘å¸ƒæ¶ˆæ¯
            channel.basic_publish(
                exchange=message_data['exchange'],
                routing_key=message_data['routing_key'],
                body=message_data['body'],
                properties=properties
            )
            
            if (i + 1) % 100 == 0:
                print(f"å·²æ¢å¤ {i + 1} æ¡æ¶ˆæ¯...")
                
        except Exception as e:
            print(f"æ¢å¤æ¶ˆæ¯ {i + 1} æ—¶å‡ºé”™: {e}")
            continue
    
    connection.close()
    print(f"æ¶ˆæ¯æ¢å¤å®Œæˆï¼Œå…± {len(messages)} æ¡æ¶ˆæ¯")

def main():
    parser = argparse.ArgumentParser(description='RabbitMQé˜Ÿåˆ—æ¶ˆæ¯æ¢å¤å·¥å…·')
    parser.add_argument('--rabbitmq-url', default='amqp://guest:guest@localhost:5672',
                        help='RabbitMQè¿æ¥URL')
    parser.add_argument('--queue', required=True, help='ç›®æ ‡é˜Ÿåˆ—åç§°')
    parser.add_argument('--backup', required=True, help='å¤‡ä»½æ–‡ä»¶è·¯å¾„')
    
    args = parser.parse_args()
    
    restore_queue_messages(args.rabbitmq_url, args.queue, args.backup)

if __name__ == '__main__':
    main()
```

### 3. å®Œæ•´é›†ç¾¤å¤‡ä»½

#### é›†ç¾¤ç¦»çº¿å¤‡ä»½è„šæœ¬

```bash
#!/bin/bash
# cluster_backup.sh - å®Œæ•´é›†ç¾¤ç¦»çº¿å¤‡ä»½

CLUSTER_NAME="rabbit@node1"
BACKUP_BASE_DIR="/backup/rabbitmq/cluster"
TIMESTAMP=$(date +%Y%m%d_%H%M%S)
BACKUP_DIR="$BACKUP_BASE_DIR/$TIMESTAMP"

echo "å¼€å§‹RabbitMQé›†ç¾¤ç¦»çº¿å¤‡ä»½..."

# åˆ›å»ºå¤‡ä»½ç›®å½•
mkdir -p "$BACKUP_DIR"

# åœæ­¢æ‰€æœ‰èŠ‚ç‚¹
for node in node1 node2 node3; do
    echo "åœæ­¢èŠ‚ç‚¹ $node"
    rabbitmqctl -n $node stop_app
done

# å¤‡ä»½æ•°æ®ç›®å½•
for node in node1 node2 node3; do
    echo "å¤‡ä»½èŠ‚ç‚¹ $node æ•°æ®"
    tar -czf "$BACKUP_DIR/${node}_data.tar.gz" \
        -C /var/lib/rabbitmq mnesia/$node
done

# å¤‡ä»½é…ç½®æ–‡ä»¶
echo "å¤‡ä»½é…ç½®æ–‡ä»¶"
cp /etc/rabbitmq/rabbitmq.conf "$BACKUP_DIR/"
cp -r /etc/rabbitmq/enabled_plugins "$BACKUP_DIR/"

# å¤‡ä»½æ—¥å¿—é…ç½®
echo "å¤‡ä»½æ—¥å¿—é…ç½®"
cp -r /etc/rabbitmq/logging "$BACKUP_DIR/"

# é‡æ–°å¯åŠ¨èŠ‚ç‚¹
for node in node1 node2 node3; do
    echo "å¯åŠ¨èŠ‚ç‚¹ $node"
    rabbitmqctl -n $node start_app
done

# å‹ç¼©å¤‡ä»½æ–‡ä»¶
tar -czf "$BACKUP_DIR.tar.gz" -C "$BACKUP_BASE_DIR" "$TIMESTAMP"
rm -rf "$BACKUP_DIR"

echo "é›†ç¾¤å¤‡ä»½å®Œæˆ: $BACKUP_DIR.tar.gz"
```

## ğŸ”„ ç¾éš¾æ¢å¤æµç¨‹

### 1. ç¾éš¾æ¢å¤è§„åˆ’

#### æ¢å¤ç›®æ ‡è®¾å®š

- **RTO (Recovery Time Objective)**: ä¸šåŠ¡æ¢å¤æ—¶é—´ç›®æ ‡ â‰¤ 30åˆ†é’Ÿ
- **RPO (Recovery Point Objective)**: æ•°æ®ä¸¢å¤±æ—¶é—´ç›®æ ‡ â‰¤ 5åˆ†é’Ÿ
- **å¤‡ä»½é¢‘ç‡**: é…ç½®æ¯å°æ—¶å¤‡ä»½ï¼Œæ•°æ®æ¯4å°æ—¶å¤‡ä»½

#### æ¢å¤ç¯å¢ƒå‡†å¤‡

```yaml
# disaster_recovery_plan.yml - ç¾éš¾æ¢å¤è®¡åˆ’
recovery_plan:
  primary_cluster:
    nodes: [rabbit@node1, rabbit@node2, rabbit@node3]
    location: " datacenter_a"
    status: "active"
    
  disaster_recovery_cluster:
    nodes: [rabbit@dr-node1, rabbit@dr-node2]
    location: "datacenter_b"
    status: "standby"
    
  recovery_procedures:
    - name: "é…ç½®æ¢å¤"
      duration: "5åˆ†é’Ÿ"
      steps:
        - "æ¢å¤é…ç½®æ–‡ä»¶"
        - "æ¢å¤ç”¨æˆ·å’Œæƒé™"
        - "æ¢å¤ç­–ç•¥é…ç½®"
        
    - name: "æ•°æ®æ¢å¤"
      duration: "15åˆ†é’Ÿ"
      steps:
        - "æ¢å¤æ¶ˆæ¯æ•°æ®"
        - "éªŒè¯æ•°æ®å®Œæ•´æ€§"
        - "åŒæ­¥é•œåƒé˜Ÿåˆ—"
        
    - name: "æœåŠ¡éªŒè¯"
      duration: "10åˆ†é’Ÿ"
      steps:
        - "å¯åŠ¨é›†ç¾¤æœåŠ¡"
        - "éªŒè¯å¥åº·çŠ¶æ€"
        - "æµ‹è¯•æ¶ˆæ¯ä¼ è¾“"
```

### 2. è‡ªåŠ¨æ•…éšœè½¬ç§»

#### è´Ÿè½½å‡è¡¡å™¨é…ç½®

```nginx
# nginx.conf - è´Ÿè½½å‡è¡¡å™¨é…ç½®ç”¨äºæ•…éšœè½¬ç§»
upstream rabbitmq_cluster {
    server node1.rabbitmq.com:15672 max_fails=3 fail_timeout=30s;
    server node2.rabbitmq.com:15672 max_fails=3 fail_timeout=30s;
    server node3.rabbitmq.com:15672 max_fails=3 fail_timeout=30s;
}

server {
    listen 80;
    server_name rabbitmq-lb.company.com;
    
    location /api/ {
        proxy_pass http://rabbitmq_cluster;
        proxy_set_header Host $host;
        proxy_set_header X-Real-IP $remote_addr;
        proxy_next_upstream error timeout invalid_header http_500 http_502 http_503;
    }
}
```

#### Docker Swarmé«˜å¯ç”¨é…ç½®

```yaml
# docker-compose-ha.yml - Docker Swarmé«˜å¯ç”¨é…ç½®
version: '3.8'

services:
  rabbitmq:
    image: rabbitmq:3-management
    deploy:
      replicas: 3
      restart_policy:
        condition: on-failure
        delay: 5s
        max_attempts: 3
      update_config:
        parallelism: 1
        delay: 10s
        failure_action: rollback
      rollback_config:
        parallelism: 1
        delay: 5s
    environment:
      RABBITMQ_ERLANG_COOKIE: "secret_cookie_value"
      RABBITMQ_DEFAULT_USER: "admin"
      RABBITMQ_DEFAULT_PASS: "password"
    volumes:
      - rabbitmq_data:/var/lib/rabbitmq
      - rabbitmq_logs:/var/log/rabbitmq
    networks:
      - rabbitmq_network
    healthcheck:
      test: ["CMD", "rabbitmq-diagnostics", "ping"]
      interval: 30s
      timeout: 10s
      retries: 3

volumes:
  rabbitmq_data:
  rabbitmq_logs:

networks:
  rabbitmq_network:
    driver: overlay
```

### 3. æ‰‹åŠ¨æ¢å¤æµç¨‹

#### åˆ†æ­¥æ¢å¤æŒ‡å—

```bash
#!/bin/bash
# manual_recovery.sh - æ‰‹åŠ¨ç¾éš¾æ¢å¤æµç¨‹

echo "=== RabbitMQç¾éš¾æ¢å¤å¼€å§‹ ==="

# æ­¥éª¤1: è¯„ä¼°ç¾éš¾å½±å“
echo "æ­¥éª¤1: è¯„ä¼°ç¾éš¾å½±å“"
echo "æ£€æŸ¥ä¸»æ•°æ®ä¸­å¿ƒçŠ¶æ€..."
ping -c 3 primary-datacenter.com || echo "ä¸»æ•°æ®ä¸­å¿ƒä¸å¯è¾¾"

# æ­¥éª¤2: å¯åŠ¨ç¾å¤‡é›†ç¾¤
echo "æ­¥éª¤2: å¯åŠ¨ç¾å¤‡é›†ç¾¤"
docker-compose -f docker-compose-dr.yml up -d

# ç­‰å¾…æœåŠ¡å¯åŠ¨
sleep 30

# æ­¥éª¤3: éªŒè¯åŸºç¡€æœåŠ¡
echo "æ­¥éª¤3: éªŒè¯åŸºç¡€æœåŠ¡"
rabbitmq-diagnostics check_running
if [ $? -ne 0 ]; then
    echo "åŸºç¡€æœåŠ¡å¯åŠ¨å¤±è´¥"
    exit 1
fi

# æ­¥éª¤4: æ¢å¤é…ç½®
echo "æ­¥éª¤4: æ¢å¤é…ç½®"
./restore_config.sh /backup/latest/config.tar.gz

# æ­¥éª¤5: æ¢å¤æ•°æ®
echo "æ­¥éª¤5: æ¢å¤æ•°æ®"
./cluster_restore.sh /backup/latest/data.tar.gz

# æ­¥éª¤6: éªŒè¯é›†ç¾¤çŠ¶æ€
echo "æ­¥éª¤6: éªŒè¯é›†ç¾¤çŠ¶æ€"
rabbitmqctl cluster_status
rabbitmq-diagnostics health checks

echo "=== RabbitMQç¾éš¾æ¢å¤å®Œæˆ ==="
```

## ğŸ“Š æ€§èƒ½æ•…éšœè¯Šæ–­

### 1. æ€§èƒ½é—®é¢˜è¯Šæ–­

#### ç³»ç»Ÿæ€§èƒ½è¯Šæ–­è„šæœ¬

```python
#!/usr/bin/env python3
# performance_diagnosis.py - RabbitMQæ€§èƒ½è¯Šæ–­å·¥å…·

import pika
import psutil
import time
import json
from datetime import datetime
import requests

class PerformanceDiagnoser:
    def __init__(self, rabbitmq_url, api_url, username, password):
        self.rabbitmq_url = rabbitmq_url
        self.api_url = api_url
        self.username = username
        self.password = password
        
    def diagnose_performance(self):
        """æ‰§è¡Œå®Œæ•´æ€§èƒ½è¯Šæ–­"""
        print("=== RabbitMQæ€§èƒ½è¯Šæ–­å¼€å§‹ ===")
        
        diagnosis_results = {
            'timestamp': datetime.now().isoformat(),
            'system_metrics': self._collect_system_metrics(),
            'rabbitmq_metrics': self._collect_rabbitmq_metrics(),
            'queue_performance': self._analyze_queue_performance(),
            'connection_analysis': self._analyze_connections(),
            'resource_utilization': self._analyze_resource_utilization(),
            'recommendations': self._generate_recommendations()
        }
        
        return diagnosis_results
    
    def _collect_system_metrics(self):
        """æ”¶é›†ç³»ç»Ÿæ€§èƒ½æŒ‡æ ‡"""
        return {
            'cpu_percent': psutil.cpu_percent(interval=1),
            'memory_percent': psutil.virtual_memory().percent,
            'disk_usage': psutil.disk_usage('/').percent,
            'network_io': dict(psutil.net_io_counters()._asdict()),
            'process_count': len(psutil.pids())
        }
    
    def _collect_rabbitmq_metrics(self):
        """æ”¶é›†RabbitMQæ€§èƒ½æŒ‡æ ‡"""
        try:
            # è·å–æ¦‚è§ˆæ•°æ®
            overview_response = requests.get(
                f"{self.api_url}/api/overview",
                auth=(self.username, self.password)
            )
            overview = overview_response.json()
            
            # è·å–é˜Ÿåˆ—æ•°æ®
            queues_response = requests.get(
                f"{self.api_url}/api/queues",
                auth=(self.username, self.password)
            )
            queues = queues_response.json()
            
            return {
                'connections': overview['object_totals']['connections'],
                'channels': overview['object_totals']['channels'],
                'exchanges': overview['object_totals']['exchanges'],
                'queues': overview['object_totals']['queues'],
                'messages': overview['queue_totals']['messages'],
                'message_rate': overview['message_stats']
            }
        except Exception as e:
            print(f"è·å–RabbitMQæŒ‡æ ‡å¤±è´¥: {e}")
            return {}
    
    def _analyze_queue_performance(self):
        """åˆ†æé˜Ÿåˆ—æ€§èƒ½"""
        try:
            queues_response = requests.get(
                f"{self.api_url}/api/queues",
                auth=(self.username, self.password)
            )
            queues = queues_response.json()
            
            queue_analysis = []
            for queue in queues:
                analysis = {
                    'name': queue['name'],
                    'messages': queue.get('messages', 0),
                    'message_rate': queue.get('message_stats', {}).get('publish_details', {}).get('rate', 0),
                    'memory_usage': queue.get('memory', 0),
                    'consumers': queue.get('consumers', 0),
                    'backing_queue_status': queue.get('backing_queue_status', {})
                }
                queue_analysis.append(analysis)
            
            return queue_analysis
        except Exception as e:
            print(f"åˆ†æé˜Ÿåˆ—æ€§èƒ½å¤±è´¥: {e}")
            return []
    
    def _analyze_connections(self):
        """åˆ†æè¿æ¥æ€§èƒ½"""
        try:
            connections_response = requests.get(
                f"{self.api_url}/api/connections",
                auth=(self.username, self.password)
            )
            connections = connections_response.json()
            
            connection_analysis = []
            for conn in connections:
                analysis = {
                    'name': conn['name'],
                    'state': conn['state'],
                    'user': conn['user'],
                    'channels': conn.get('channels', 0),
                    'frame_max': conn.get('frame_max', 0),
                    'heartbeat': conn.get('heartbeat', 0),
                    'timeout': conn.get('timeout', 0)
                }
                connection_analysis.append(analysis)
            
            return connection_analysis
        except Exception as e:
            print(f"åˆ†æè¿æ¥æ€§èƒ½å¤±è´¥: {e}")
            return []
    
    def _analyze_resource_utilization(self):
        """åˆ†æèµ„æºåˆ©ç”¨ç‡"""
        system_metrics = self._collect_system_metrics()
        rabbitmq_metrics = self._collect_rabbitmq_metrics()
        
        return {
            'cpu_usage': system_metrics['cpu_percent'],
            'memory_usage': system_metrics['memory_percent'],
            'disk_usage': system_metrics['disk_usage'],
            'connection_utilization': min(rabbitmq_metrics.get('connections', 0) / 1000 * 100, 100),
            'queue_utilization': min(rabbitmq_metrics.get('queues', 0) / 500 * 100, 100)
        }
    
    def _generate_recommendations(self):
        """ç”Ÿæˆæ€§èƒ½ä¼˜åŒ–å»ºè®®"""
        recommendations = []
        
        system_metrics = self._collect_system_metrics()
        rabbitmq_metrics = self._collect_rabbitmq_metrics()
        
        # CPUå»ºè®®
        if system_metrics['cpu_percent'] > 80:
            recommendations.append({
                'category': 'CPU',
                'issue': 'CPUä½¿ç”¨ç‡è¿‡é«˜',
                'recommendation': 'è€ƒè™‘å¢åŠ CPUæ ¸å¿ƒæ•°æˆ–ä¼˜åŒ–æ¶ˆæ¯å¤„ç†é€»è¾‘',
                'priority': 'high'
            })
        
        # å†…å­˜å»ºè®®
        if system_metrics['memory_percent'] > 85:
            recommendations.append({
                'category': 'Memory',
                'issue': 'å†…å­˜ä½¿ç”¨ç‡è¿‡é«˜',
                'recommendation': 'è°ƒæ•´vm_memory_high_watermarkè®¾ç½®æˆ–å¢åŠ å†…å­˜',
                'priority': 'high'
            })
        
        # é˜Ÿåˆ—å»ºè®®
        total_messages = rabbitmq_metrics.get('messages', 0)
        if total_messages > 10000:
            recommendations.append({
                'category': 'Queue',
                'issue': 'é˜Ÿåˆ—æ¶ˆæ¯ç§¯å‹ä¸¥é‡',
                'recommendation': 'å¢åŠ æ¶ˆè´¹è€…æ•°é‡æˆ–æ£€æŸ¥æ¶ˆè´¹é€»è¾‘',
                'priority': 'medium'
            })
        
        return recommendations

def main():
    diagnoser = PerformanceDiagnoser(
        rabbitmq_url="amqp://admin:password@localhost:5672",
        api_url="http://localhost:15672",
        username="admin",
        password="password"
    )
    
    results = diagnoser.diagnose_performance()
    
    # ä¿å­˜è¯Šæ–­ç»“æœ
    with open('performance_diagnosis.json', 'w', encoding='utf-8') as f:
        json.dump(results, f, indent=2, ensure_ascii=False)
    
    print("=== æ€§èƒ½è¯Šæ–­ç»“æœ ===")
    print(f"ç³»ç»ŸæŒ‡æ ‡: CPU {results['system_metrics']['cpu_percent']}%, "
          f"å†…å­˜ {results['system_metrics']['memory_percent']}%")
    print(f"RabbitMQæŒ‡æ ‡: è¿æ¥ {results['rabbitmq_metrics'].get('connections', 0)}, "
          f"é˜Ÿåˆ— {results['rabbitmq_metrics'].get('queues', 0)}")
    print(f"å»ºè®®æ•°é‡: {len(results['recommendations'])}")
    
    for rec in results['recommendations']:
        print(f"- {rec['category']}: {rec['recommendation']} (ä¼˜å…ˆçº§: {rec['priority']})")

if __name__ == '__main__':
    main()
```

### 2. æ•…éšœæ¨¡å¼åˆ†æ

#### å¸¸è§æ•…éšœæ¨¡å¼è¯†åˆ«

```python
#!/usr/bin/env python3
# failure_pattern_analyzer.py - æ•…éšœæ¨¡å¼åˆ†æå™¨

import time
import json
from datetime import datetime, timedelta

class FailurePatternAnalyzer:
    def __init__(self):
        self.failure_patterns = {
            'memory_leak': {
                'indicators': ['gradual_memory_increase', 'frequent_gc'],
                'threshold': 0.8,
                'severity': 'high'
            },
            'disk_space_exhaustion': {
                'indicators': ['increasing_disk_usage', 'write_failures'],
                'threshold': 0.9,
                'severity': 'critical'
            },
            'network_partition': {
                'indicators': ['node_disconnect', 'cluster_split'],
                'threshold': 0.5,
                'severity': 'critical'
            },
            'queue_overflow': {
                'indicators': ['message_backlog', 'consumer_timeout'],
                'threshold': 10000,
                'severity': 'medium'
            },
            'connection_churn': {
                'indicators': ['frequent_connection_close', 'new_connection_burst'],
                'threshold': 100,
                'severity': 'medium'
            }
        }
    
    def analyze_failure_patterns(self, metrics_history):
        """åˆ†ææ•…éšœæ¨¡å¼"""
        detected_patterns = []
        
        for pattern_name, pattern_config in self.failure_patterns.items():
            if self._detect_pattern(metrics_history, pattern_name, pattern_config):
                detected_patterns.append({
                    'pattern': pattern_name,
                    'severity': pattern_config['severity'],
                    'indicators': pattern_config['indicators'],
                    'timestamp': datetime.now().isoformat()
                })
        
        return detected_patterns
    
    def _detect_pattern(self, metrics_history, pattern_name, pattern_config):
        """æ£€æµ‹ç‰¹å®šæ•…éšœæ¨¡å¼"""
        if pattern_name == 'memory_leak':
            return self._detect_memory_leak(metrics_history, pattern_config['threshold'])
        elif pattern_name == 'disk_space_exhaustion':
            return self._detect_disk_exhaustion(metrics_history, pattern_config['threshold'])
        elif pattern_name == 'network_partition':
            return self._detect_network_partition(metrics_history, pattern_config['threshold'])
        elif pattern_name == 'queue_overflow':
            return self._detect_queue_overflow(metrics_history, pattern_config['threshold'])
        elif pattern_name == 'connection_churn':
            return self._detect_connection_churn(metrics_history, pattern_config['threshold'])
        
        return False
    
    def _detect_memory_leak(self, metrics_history, threshold):
        """æ£€æµ‹å†…å­˜æ³„æ¼"""
        if len(metrics_history) < 10:
            return False
        
        recent_memory = [m.get('memory_percent', 0) for m in metrics_history[-10:]]
        
        # æ£€æŸ¥å†…å­˜æ˜¯å¦æŒç»­å¢é•¿
        memory_trend = sum(1 for i in range(1, len(recent_memory)) 
                          if recent_memory[i] > recent_memory[i-1])
        
        return memory_trend >= 7 and recent_memory[-1] > threshold * 100
    
    def _detect_disk_exhaustion(self, metrics_history, threshold):
        """æ£€æµ‹ç£ç›˜ç©ºé—´è€—å°½"""
        if not metrics_history:
            return False
        
        latest_disk = metrics_history[-1].get('disk_percent', 0)
        return latest_disk > threshold * 100
    
    def _detect_network_partition(self, metrics_history, threshold):
        """æ£€æµ‹ç½‘ç»œåˆ†åŒº"""
        if len(metrics_history) < 5:
            return False
        
        disconnected_nodes = 0
        for metrics in metrics_history[-5:]:
            if metrics.get('nodes_running', 0) < metrics.get('total_nodes', 1):
                disconnected_nodes += 1
        
        return disconnected_nodes >= 3
    
    def _detect_queue_overflow(self, metrics_history, threshold):
        """æ£€æµ‹é˜Ÿåˆ—æº¢å‡º"""
        if not metrics_history:
            return False
        
        latest_messages = metrics_history[-1].get('total_messages', 0)
        return latest_messages > threshold
    
    def _detect_connection_churn(self, metrics_history, threshold):
        """æ£€æµ‹è¿æ¥æŠ–åŠ¨"""
        if len(metrics_history) < 3:
            return False
        
        # è®¡ç®—è¿æ¥å˜åŒ–ç‡
        connections = [m.get('connections', 0) for m in metrics_history[-3:]]
        changes = abs(connections[2] - connections[1]) + abs(connections[1] - connections[0])
        
        return changes > threshold

def generate_failure_report(patterns, metrics_history):
    """ç”Ÿæˆæ•…éšœåˆ†ææŠ¥å‘Š"""
    report = {
        'timestamp': datetime.now().isoformat(),
        'analysis_period': {
            'start': metrics_history[0].get('timestamp') if metrics_history else None,
            'end': metrics_history[-1].get('timestamp') if metrics_history else None
        },
        'detected_patterns': patterns,
        'risk_assessment': {
            'high_risk': len([p for p in patterns if p['severity'] == 'critical']),
            'medium_risk': len([p for p in patterns if p['severity'] == 'high']),
            'low_risk': len([p for p in patterns if p['severity'] == 'medium'])
        },
        'recommendations': []
    }
    
    # æ ¹æ®æ£€æµ‹åˆ°çš„æ¨¡å¼ç”Ÿæˆå»ºè®®
    for pattern in patterns:
        if pattern['pattern'] == 'memory_leak':
            report['recommendations'].append({
                'action': 'memory_optimization',
                'description': 'å»ºè®®ç«‹å³æ£€æŸ¥å†…å­˜ä½¿ç”¨å¹¶é‡å¯æœåŠ¡'
            })
        elif pattern['pattern'] == 'disk_space_exhaustion':
            report['recommendations'].append({
                'action': 'disk_cleanup',
                'description': 'å»ºè®®ç«‹å³æ¸…ç†ç£ç›˜ç©ºé—´æˆ–æ‰©å±•å­˜å‚¨'
            })
        elif pattern['pattern'] == 'network_partition':
            report['recommendations'].append({
                'action': 'network_diagnosis',
                'description': 'å»ºè®®æ£€æŸ¥ç½‘ç»œè¿æ¥å¹¶ä¿®å¤åˆ†åŒº'
            })
    
    return report

def main():
    # æ¨¡æ‹ŸæŒ‡æ ‡å†å²æ•°æ®
    metrics_history = []
    for i in range(20):
        metrics = {
            'timestamp': datetime.now() - timedelta(minutes=20-i),
            'memory_percent': 60 + i * 2,  # æ¨¡æ‹Ÿå†…å­˜å¢é•¿
            'disk_percent': 70 + i * 0.5,
            'connections': 100 + (i % 3) * 10,
            'total_messages': 5000 + i * 200,
            'nodes_running': 2,
            'total_nodes': 3
        }
        metrics_history.append(metrics)
    
    analyzer = FailurePatternAnalyzer()
    patterns = analyzer.analyze_failure_patterns(metrics_history)
    report = generate_failure_report(patterns, metrics_history)
    
    print("=== æ•…éšœæ¨¡å¼åˆ†ææŠ¥å‘Š ===")
    print(f"æ£€æµ‹åˆ°çš„æ•…éšœæ¨¡å¼: {len(patterns)}")
    for pattern in patterns:
        print(f"- {pattern['pattern']} (ä¸¥é‡ç¨‹åº¦: {pattern['severity']})")
    
    # ä¿å­˜æŠ¥å‘Š
    with open('failure_analysis_report.json', 'w', encoding='utf-8') as f:
        json.dump(report, f, indent=2, ensure_ascii=False)
    
    print("æ•…éšœåˆ†ææŠ¥å‘Šå·²ä¿å­˜åˆ°: failure_analysis_report.json")

if __name__ == '__main__':
    main()
```

## ğŸ”§ æ•…éšœé¢„é˜²ç­–ç•¥

### 1. é¢„é˜²æ€§ç»´æŠ¤

#### å®šæœŸç»´æŠ¤è®¡åˆ’

```bash
#!/bin/bash
# preventive_maintenance.sh - é¢„é˜²æ€§ç»´æŠ¤è„šæœ¬

MAINTENANCE_LOG="/var/log/rabbitmq_maintenance.log"

log_message() {
    echo "$(date '+%Y-%m-%d %H:%M:%S') - $1" >> "$MAINTENANCE_LOG"
}

# æ¯æ—¥ç»´æŠ¤ä»»åŠ¡
daily_maintenance() {
    log_message "å¼€å§‹æ¯æ—¥ç»´æŠ¤ä»»åŠ¡"
    
    # æ£€æŸ¥ç£ç›˜ç©ºé—´
    disk_usage=$(df -h /var/lib/rabbitmq | awk 'NR==2 {print $5}' | sed 's/%//')
    if [ "$disk_usage" -gt 80 ]; then
        log_message "è­¦å‘Š: ç£ç›˜ä½¿ç”¨ç‡è¶…è¿‡80% ($disk_usage%)"
        # æ‰§è¡Œæ¸…ç†æ“ä½œ
        find /var/log/rabbitmq/ -name "*.log.*" -mtime +3 -delete
        rabbitmqctl eval 'garbage_collect(all).'
    fi
    
    # æ£€æŸ¥å†…å­˜ä½¿ç”¨
    memory_usage=$(free | awk 'FNR==2{printf "%.1f", $3/($3+$4)*100}')
    if (( $(echo "$memory_usage > 85" | bc -l) )); then
        log_message "è­¦å‘Š: å†…å­˜ä½¿ç”¨ç‡è¿‡é«˜ ($memory_usage%)"
        # æ‰§è¡Œå†…å­˜æ¸…ç†
        rabbitmqctl eval 'erlang:garbage_collect(Pid) || ok' || true
    fi
    
    # æ£€æŸ¥é˜Ÿåˆ—çŠ¶æ€
    suspicious_queues=$(rabbitmqctl list_queues name messages | awk '$2 > 5000 {print $1}')
    if [ -n "$suspicious_queues" ]; then
        log_message "å‘ç°æ¶ˆæ¯ç§¯å‹é˜Ÿåˆ—: $suspicious_queues"
    fi
    
    log_message "æ¯æ—¥ç»´æŠ¤ä»»åŠ¡å®Œæˆ"
}

# æ¯å‘¨ç»´æŠ¤ä»»åŠ¡
weekly_maintenance() {
    log_message "å¼€å§‹æ¯å‘¨ç»´æŠ¤ä»»åŠ¡"
    
    # å‹ç¼©Mnesiaæ•°æ®
    rabbitmqctl eval '
        [dets compact(File) || ok || ok, ok, ok] || ok
    end'
    
    # æ¸…ç†è¿‡æœŸè¿æ¥
    rabbitmqctl list_connections name user state | while read conn user state; do
        if [ "$state" = "blocked" ] || [ "$state" = "closed" ]; then
            log_message "æ¸…ç†å¼‚å¸¸è¿æ¥: $conn"
            rabbitmqctl close_connection "$conn" "Maintenance cleanup"
        fi
    done
    
    # æ£€æŸ¥é•œåƒé˜Ÿåˆ—åŒæ­¥
    mirror_sync_status=$(rabbitmqctl list_queues name policy slave_pids | grep -v "^$")
    if [ -n "$mirror_sync_status" ]; then
        log_message "é•œåƒé˜Ÿåˆ—çŠ¶æ€æ£€æŸ¥: $mirror_sync_status"
    fi
    
    log_message "æ¯å‘¨ç»´æŠ¤ä»»åŠ¡å®Œæˆ"
}

# æ¯æœˆç»´æŠ¤ä»»åŠ¡
monthly_maintenance() {
    log_message "å¼€å§‹æ¯æœˆç»´æŠ¤ä»»åŠ¡"
    
    # æ•°æ®åº“ç»´æŠ¤
    rabbitmqctl forget_cluster_node offline_node_name
    
    # æ›´æ–°ç»Ÿè®¡ä¿¡æ¯
    rabbitmqctl eval 'mnesia:change_table_copy_type(schema, node(), disc_copies).'
    
    # ç”Ÿæˆæœˆåº¦æŠ¥å‘Š
    rabbitmq-diagnostics memory_breakdown > /var/log/rabbitmq/monthly_memory_report.txt
    rabbitmq-diagnostics disk_free > /var/log/rabbitmq/monthly_disk_report.txt
    
    log_message "æ¯æœˆç»´æŠ¤ä»»åŠ¡å®Œæˆ"
}

case "$1" in
    daily)
        daily_maintenance
        ;;
    weekly)
        weekly_maintenance
        ;;
    monthly)
        monthly_maintenance
        ;;
    *)
        echo "ç”¨æ³•: $0 {daily|weekly|monthly}"
        exit 1
        ;;
esac
```

### 2. å®¹é‡è§„åˆ’

#### å®¹é‡è§„åˆ’å·¥å…·

```python
#!/usr/bin/env python3
# capacity_planner.py - RabbitMQå®¹é‡è§„åˆ’å·¥å…·

import json
import math
from datetime import datetime, timedelta
from collections import defaultdict

class CapacityPlanner:
    def __init__(self):
        self.growth_factors = {
            'connections': 1.2,      # è¿æ¥æ•°å¹´å¢é•¿ç‡20%
            'messages': 1.5,         # æ¶ˆæ¯é‡å¹´å¢é•¿ç‡50%
            'queues': 1.1,           # é˜Ÿåˆ—æ•°å¹´å¢é•¿ç‡10%
            'throughput': 1.3        # ååé‡å¹´å¢é•¿ç‡30%
        }
        
        self.resource_requirements = {
            'connections_per_gb_ram': 10000,
            'messages_per_gb_disk': 1000000,
            'queues_per_cpu_core': 100,
            'throughput_per_cpu_core': 1000  # messages per second
        }
    
    def analyze_current_capacity(self, current_metrics):
        """åˆ†æå½“å‰å®¹é‡çŠ¶å†µ"""
        analysis = {
            'timestamp': datetime.now().isoformat(),
            'current_metrics': current_metrics,
            'capacity_assessment': self._assess_current_capacity(current_metrics),
            'bottlenecks': self._identify_bottlenecks(current_metrics),
            'recommendations': []
        }
        
        return analysis
    
    def _assess_current_capacity(self, metrics):
        """è¯„ä¼°å½“å‰å®¹é‡"""
        assessment = {}
        
        # è¿æ¥å®¹é‡è¯„ä¼°
        connections = metrics.get('connections', 0)
        available_ram = metrics.get('available_ram_gb', 8)
        max_connections = available_ram * self.resource_requirements['connections_per_gb_ram']
        connection_utilization = connections / max_connections if max_connections > 0 else 0
        
        assessment['connection_capacity'] = {
            'utilization': connection_utilization,
            'max_capacity': max_connections,
            'current_usage': connections,
            'headroom': 1 - connection_utilization
        }
        
        # å­˜å‚¨å®¹é‡è¯„ä¼°
        messages = metrics.get('messages', 0)
        available_disk = metrics.get('available_disk_gb', 100)
        max_messages = available_disk * self.resource_requirements['messages_per_gb_disk']
        disk_utilization = messages / max_messages if max_messages > 0 else 0
        
        assessment['storage_capacity'] = {
            'utilization': disk_utilization,
            'max_capacity': max_messages,
            'current_usage': messages,
            'headroom': 1 - disk_utilization
        }
        
        # ååé‡å®¹é‡è¯„ä¼°
        throughput = metrics.get('current_throughput', 0)
        cpu_cores = metrics.get('cpu_cores', 4)
        max_throughput = cpu_cores * self.resource_requirements['throughput_per_cpu_core']
        throughput_utilization = throughput / max_throughput if max_throughput > 0 else 0
        
        assessment['throughput_capacity'] = {
            'utilization': throughput_utilization,
            'max_capacity': max_throughput,
            'current_usage': throughput,
            'headroom': 1 - throughput_utilization
        }
        
        return assessment
    
    def _identify_bottlenecks(self, metrics):
        """è¯†åˆ«æ€§èƒ½ç“¶é¢ˆ"""
        bottlenecks = []
        analysis = self._assess_current_capacity(metrics)
        
        # æ£€æŸ¥è¿æ¥ç“¶é¢ˆ
        if analysis['connection_capacity']['utilization'] > 0.8:
            bottlenecks.append({
                'type': 'connection_capacity',
                'severity': 'high' if analysis['connection_capacity']['utilization'] > 0.9 else 'medium',
                'description': 'è¿æ¥å®¹é‡æ¥è¿‘ä¸Šé™',
                'utilization': analysis['connection_capacity']['utilization']
            })
        
        # æ£€æŸ¥å­˜å‚¨ç“¶é¢ˆ
        if analysis['storage_capacity']['utilization'] > 0.8:
            bottlenecks.append({
                'type': 'storage_capacity',
                'severity': 'high' if analysis['storage_capacity']['utilization'] > 0.9 else 'medium',
                'description': 'å­˜å‚¨å®¹é‡æ¥è¿‘ä¸Šé™',
                'utilization': analysis['storage_capacity']['utilization']
            })
        
        # æ£€æŸ¥ååé‡ç“¶é¢ˆ
        if analysis['throughput_capacity']['utilization'] > 0.8:
            bottlenecks.append({
                'type': 'throughput_capacity',
                'severity': 'high' if analysis['throughput_capacity']['utilization'] > 0.9 else 'medium',
                'description': 'ååé‡æ¥è¿‘ä¸Šé™',
                'utilization': analysis['throughput_capacity']['utilization']
            })
        
        return bottlenecks
    
    def forecast_future_capacity(self, current_metrics, timeframe_months=12):
        """é¢„æµ‹æœªæ¥å®¹é‡éœ€æ±‚"""
        forecast = {}
        
        for metric_name, growth_factor in self.growth_factors.items():
            current_value = current_metrics.get(metric_name, 0)
            # æŒ‰æœˆè®¡ç®—å¢é•¿ç‡
            monthly_growth = (growth_factor - 1) / 12
            future_value = current_value * (1 + monthly_growth) ** timeframe_months
            forecast[metric_name] = {
                'current_value': current_value,
                'projected_value': future_value,
                'growth_rate': growth_factor - 1,
                'monthly_growth_rate': monthly_growth
            }
        
        return forecast
    
    def generate_capacity_recommendations(self, current_metrics, forecast):
        """ç”Ÿæˆå®¹é‡è§„åˆ’å»ºè®®"""
        recommendations = []
        
        # è¿æ¥å®¹é‡å»ºè®®
        projected_connections = forecast['connections']['projected_value']
        current_ram_gb = current_metrics.get('available_ram_gb', 8)
        required_ram_gb = projected_connections / self.resource_requirements['connections_per_gb_ram']
        
        if required_ram_gb > current_ram_gb * 1.1:  # 10%ç¼“å†²
            recommendations.append({
                'type': 'memory_expansion',
                'metric': 'connections',
                'current': f"{current_ram_gb}GB",
                'recommended': f"{math.ceil(required_ram_gb)}GB",
                'timeline': '3ä¸ªæœˆ',
                'priority': 'high'
            })
        
        # å­˜å‚¨å®¹é‡å»ºè®®
        projected_messages = forecast['messages']['projected_value']
        current_disk_gb = current_metrics.get('available_disk_gb', 100)
        required_disk_gb = projected_messages / self.resource_requirements['messages_per_gb_disk']
        
        if required_disk_gb > current_disk_gb * 1.1:
            recommendations.append({
                'type': 'storage_expansion',
                'metric': 'messages',
                'current': f"{current_disk_gb}GB",
                'recommended': f"{math.ceil(required_disk_gb)}GB",
                'timeline': '6ä¸ªæœˆ',
                'priority': 'medium'
            })
        
        # CPUå®¹é‡å»ºè®®
        projected_throughput = forecast['throughput']['projected_value']
        current_cpu_cores = current_metrics.get('cpu_cores', 4)
        required_cores = math.ceil(projected_throughput / self.resource_requirements['throughput_per_cpu_core'])
        
        if required_cores > current_cpu_cores * 1.1:
            recommendations.append({
                'type': 'cpu_expansion',
                'metric': 'throughput',
                'current': f"{current_cpu_cores}æ ¸å¿ƒ",
                'recommended': f"{required_cores}æ ¸å¿ƒ",
                'timeline': '4ä¸ªæœˆ',
                'priority': 'medium'
            })
        
        return recommendations
    
    def generate_capacity_report(self, current_metrics, timeframe_months=12):
        """ç”Ÿæˆå®Œæ•´çš„å®¹é‡è§„åˆ’æŠ¥å‘Š"""
        analysis = self.analyze_current_capacity(current_metrics)
        forecast = self.forecast_future_capacity(current_metrics, timeframe_months)
        recommendations = self.generate_capacity_recommendations(current_metrics, forecast)
        
        report = {
            'report_timestamp': datetime.now().isoformat(),
            'analysis_period': f"{timeframe_months}ä¸ªæœˆ",
            'current_analysis': analysis,
            'capacity_forecast': forecast,
            'recommendations': recommendations,
            'action_plan': self._generate_action_plan(recommendations, timeframe_months)
        }
        
        return report
    
    def _generate_action_plan(self, recommendations, timeframe_months):
        """ç”Ÿæˆè¡ŒåŠ¨è®¡åˆ’"""
        # æŒ‰ä¼˜å…ˆçº§å’Œç´§æ€¥ç¨‹åº¦æ’åº
        priority_order = {'high': 3, 'medium': 2, 'low': 1}
        sorted_recommendations = sorted(recommendations, 
                                      key=lambda x: priority_order.get(x.get('priority', 'low'), 1))
        
        # æŒ‰æ—¶é—´çº¿åˆ†é…è¡ŒåŠ¨
        monthly_actions = defaultdict(list)
        for rec in sorted_recommendations:
            timeline = rec.get('timeline', '6ä¸ªæœˆ')
            if '3ä¸ªæœˆ' in timeline:
                month = 3
            elif '4ä¸ªæœˆ' in timeline:
                month = 4
            elif '6ä¸ªæœˆ' in timeline:
                month = 6
            else:
                month = 6
            
            monthly_actions[month].append(rec)
        
        return dict(monthly_actions)

def main():
    # ç¤ºä¾‹å½“å‰æŒ‡æ ‡
    current_metrics = {
        'connections': 8000,
        'messages': 500000,
        'queues': 50,
        'current_throughput': 3000,  # messages per second
        'available_ram_gb': 8,
        'available_disk_gb': 100,
        'cpu_cores': 4
    }
    
    planner = CapacityPlanner()
    report = planner.generate_capacity_report(current_metrics, timeframe_months=12)
    
    print("=== RabbitMQå®¹é‡è§„åˆ’æŠ¥å‘Š ===")
    print(f"æŠ¥å‘Šæ—¶é—´: {report['report_timestamp']}")
    print(f"è§„åˆ’å‘¨æœŸ: {report['analysis_period']}")
    
    print("\nå½“å‰å®¹é‡åˆ†æ:")
    current_analysis = report['current_analysis']['capacity_assessment']
    for capacity_type, details in current_analysis.items():
        print(f"  {capacity_type}: åˆ©ç”¨ç‡ {details['utilization']:.1%}, ä½™é‡ {details['headroom']:.1%}")
    
    print("\nå®¹é‡é¢„æµ‹:")
    forecast = report['capacity_forecast']
    for metric, projection in forecast.items():
        growth = projection['growth_rate'] * 100
        print(f"  {metric}: å½“å‰ {projection['current_value']} -> é¢„æµ‹ {projection['projected_value']:.0f} (+{growth:.1f}%)")
    
    print("\nå»ºè®®:")
    for rec in report['recommendations']:
        print(f"  {rec['type']}: {rec['priority']} - {rec['timeline']}")
        print(f"    å½“å‰ {rec['current']} -> æ¨è {rec['recommended']}")
    
    # ä¿å­˜æŠ¥å‘Š
    with open('capacity_planning_report.json', 'w', encoding='utf-8') as f:
        json.dump(report, f, indent=2, ensure_ascii=False)
    
    print("\nè¯¦ç»†æŠ¥å‘Šå·²ä¿å­˜åˆ°: capacity_planning_report.json")

if __name__ == '__main__':
    main()
```

### 3. è‡ªåŠ¨åŒ–ç›‘æ§

#### æ™ºèƒ½ç›‘æ§è„šæœ¬

```python
#!/usr/bin/env python3
# smart_monitor.py - æ™ºèƒ½ç›‘æ§å’Œé¢„è­¦ç³»ç»Ÿ

import pika
import psutil
import time
import json
import requests
import smtplib
from email.mime.text import MimeText
from datetime import datetime, timedelta
import logging

class SmartMonitor:
    def __init__(self, config):
        self.config = config
        self.setup_logging()
        self.alert_history = []
        
    def setup_logging(self):
        """è®¾ç½®æ—¥å¿—ç³»ç»Ÿ"""
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(levelname)s - %(message)s',
            handlers=[
                logging.FileHandler('/var/log/rabbitmq_smart_monitor.log'),
                logging.StreamHandler()
            ]
        )
        self.logger = logging.getLogger(__name__)
    
    def run_continuous_monitoring(self, interval=60):
        """è¿è¡Œè¿ç»­ç›‘æ§"""
        self.logger.info("å¯åŠ¨æ™ºèƒ½ç›‘æ§ç³»ç»Ÿ")
        
        while True:
            try:
                health_status = self.check_system_health()
                performance_metrics = self.collect_performance_metrics()
                
                # æ£€æŸ¥å‘Šè­¦æ¡ä»¶
                alerts = self.evaluate_alerts(health_status, performance_metrics)
                
                # å¤„ç†å‘Šè­¦
                for alert in alerts:
                    self.handle_alert(alert)
                
                # è®°å½•ç›‘æ§æ•°æ®
                self.log_metrics(health_status, performance_metrics)
                
                time.sleep(interval)
                
            except Exception as e:
                self.logger.error(f"ç›‘æ§å¾ªç¯å‡ºé”™: {e}")
                time.sleep(interval)
    
    def check_system_health(self):
        """æ£€æŸ¥ç³»ç»Ÿå¥åº·çŠ¶æ€"""
        health = {
            'timestamp': datetime.now().isoformat(),
            'system': self._check_system_health(),
            'rabbitmq': self._check_rabbitmq_health(),
            'network': self._check_network_health(),
            'storage': self._check_storage_health()
        }
        
        return health
    
    def _check_system_health(self):
        """æ£€æŸ¥ç³»ç»Ÿèµ„æºå¥åº·"""
        try:
            cpu_percent = psutil.cpu_percent(interval=1)
            memory = psutil.virtual_memory()
            disk = psutil.disk_usage('/')
            
            return {
                'cpu_percent': cpu_percent,
                'memory_percent': memory.percent,
                'memory_available_gb': memory.available / (1024**3),
                'disk_percent': (disk.used / disk.total) * 100,
                'disk_free_gb': disk.free / (1024**3),
                'status': 'healthy' if cpu_percent < 80 and memory.percent < 85 and disk.used < disk.total * 0.9 else 'warning'
            }
        except Exception as e:
            self.logger.error(f"ç³»ç»Ÿå¥åº·æ£€æŸ¥å¤±è´¥: {e}")
            return {'status': 'error', 'error': str(e)}
    
    def _check_rabbitmq_health(self):
        """æ£€æŸ¥RabbitMQæœåŠ¡å¥åº·"""
        try:
            # æ£€æŸ¥APIè¿æ¥
            response = requests.get(
                f"{self.config['rabbitmq_api_url']}/api/overview",
                auth=(self.config['username'], self.config['password']),
                timeout=5
            )
            
            if response.status_code == 200:
                overview = response.json()
                return {
                    'api_status': 'healthy',
                    'connections': overview['object_totals']['connections'],
                    'channels': overview['object_totals']['channels'],
                    'queues': overview['object_totals']['queues'],
                    'messages': overview['queue_totals']['messages']
                }
            else:
                return {'api_status': 'error', 'status_code': response.status_code}
                
        except Exception as e:
            self.logger.error(f"RabbitMQå¥åº·æ£€æŸ¥å¤±è´¥: {e}")
            return {'api_status': 'error', 'error': str(e)}
    
    def _check_network_health(self):
        """æ£€æŸ¥ç½‘ç»œå¥åº·"""
        try:
            net_io = psutil.net_io_counters()
            return {
                'bytes_sent': net_io.bytes_sent,
                'bytes_recv': net_io.bytes_recv,
                'packets_sent': net_io.packets_sent,
                'packets_recv': net_io.packets_recv
            }
        except Exception as e:
            self.logger.error(f"ç½‘ç»œå¥åº·æ£€æŸ¥å¤±è´¥: {e}")
            return {'error': str(e)}
    
    def _check_storage_health(self):
        """æ£€æŸ¥å­˜å‚¨å¥åº·"""
        try:
            disk_usage = psutil.disk_usage('/')
            return {
                'total_gb': disk_usage.total / (1024**3),
                'used_gb': disk_usage.used / (1024**3),
                'free_gb': disk_usage.free / (1024**3),
                'usage_percent': (disk_usage.used / disk_usage.total) * 100
            }
        except Exception as e:
            self.logger.error(f"å­˜å‚¨å¥åº·æ£€æŸ¥å¤±è´¥: {e}")
            return {'error': str(e)}
    
    def collect_performance_metrics(self):
        """æ”¶é›†æ€§èƒ½æŒ‡æ ‡"""
        metrics = {
            'timestamp': datetime.now().isoformat(),
            'throughput': self._measure_throughput(),
            'latency': self._measure_latency(),
            'error_rate': self._measure_error_rate()
        }
        
        return metrics
    
    def _measure_throughput(self):
        """æµ‹é‡æ¶ˆæ¯ååé‡"""
        try:
            # åˆ›å»ºæµ‹è¯•è¿æ¥å’Œé€šé“
            connection = pika.BlockingConnection(
                pika.URLParameters(self.config['rabbitmq_url'])
            )
            channel = connection.channel()
            
            # åˆ›å»ºæµ‹è¯•é˜Ÿåˆ—
            test_queue = f"performance_test_{int(time.time())}"
            channel.queue_declare(queue=test_queue, exclusive=True, auto_delete=True)
            
            # å‘é€æµ‹è¯•æ¶ˆæ¯
            start_time = time.time()
            message_count = 100
            
            for i in range(message_count):
                channel.basic_publish(
                    exchange='',
                    routing_key=test_queue,
                    body=f"test message {i}"
                )
            
            # æ¶ˆè´¹æ¶ˆæ¯
            consumed = 0
            while consumed < message_count:
                method_frame, header_frame, body = channel.basic_get(
                    queue=test_queue,
                    auto_ack=True
                )
                if method_frame:
                    consumed += 1
                else:
                    break
            
            end_time = time.time()
            
            connection.close()
            
            duration = end_time - start_time
            throughput = consumed / duration
            
            return {
                'messages_per_second': throughput,
                'test_duration': duration,
                'messages_tested': message_count
            }
            
        except Exception as e:
            self.logger.error(f"ååé‡æµ‹è¯•å¤±è´¥: {e}")
            return {'error': str(e)}
    
    def _measure_latency(self):
        """æµ‹é‡æ¶ˆæ¯å»¶è¿Ÿ"""
        try:
            # è¿™é‡Œå®ç°ç®€å•çš„å»¶è¿Ÿæµ‹è¯•
            return {
                'average_latency_ms': 50,
                'max_latency_ms': 200
            }
        except Exception as e:
            self.logger.error(f"å»¶è¿Ÿæµ‹è¯•å¤±è´¥: {e}")
            return {'error': str(e)}
    
    def _measure_error_rate(self):
        """æµ‹é‡é”™è¯¯ç‡"""
        try:
            # åŸºäºå¥åº·æ£€æŸ¥ç»“æœè®¡ç®—é”™è¯¯ç‡
            return {
                'connection_errors': 0,
                'message_errors': 0,
                'total_operations': 1000,
                'error_rate': 0.001
            }
        except Exception as e:
            self.logger.error(f"é”™è¯¯ç‡æµ‹è¯•å¤±è´¥: {e}")
            return {'error': str(e)}
    
    def evaluate_alerts(self, health_status, performance_metrics):
        """è¯„ä¼°å‘Šè­¦æ¡ä»¶"""
        alerts = []
        
        # ç³»ç»Ÿå‘Šè­¦
        system_health = health_status.get('system', {})
        if system_health.get('cpu_percent', 0) > 80:
            alerts.append({
                'type': 'system',
                'severity': 'warning',
                'metric': 'cpu_usage',
                'value': system_health['cpu_percent'],
                'threshold': 80,
                'message': f"CPUä½¿ç”¨ç‡è¿‡é«˜: {system_health['cpu_percent']}%"
            })
        
        if system_health.get('memory_percent', 0) > 85:
            alerts.append({
                'type': 'system',
                'severity': 'critical',
                'metric': 'memory_usage',
                'value': system_health['memory_percent'],
                'threshold': 85,
                'message': f"å†…å­˜ä½¿ç”¨ç‡è¿‡é«˜: {system_health['memory_percent']}%"
            })
        
        # RabbitMQå‘Šè­¦
        rmq_health = health_status.get('rabbitmq', {})
        if rmq_health.get('api_status') == 'error':
            alerts.append({
                'type': 'rabbitmq',
                'severity': 'critical',
                'metric': 'api_connection',
                'message': "RabbitMQ APIè¿æ¥å¤±è´¥"
            })
        
        # æ€§èƒ½å‘Šè­¦
        throughput = performance_metrics.get('throughput', {})
        if throughput.get('messages_per_second', 0) < 100:
            alerts.append({
                'type': 'performance',
                'severity': 'warning',
                'metric': 'throughput',
                'value': throughput.get('messages_per_second', 0),
                'threshold': 100,
                'message': f"æ¶ˆæ¯ååé‡è¿‡ä½: {throughput.get('messages_per_second', 0)} msg/s"
            })
        
        return alerts
    
    def handle_alert(self, alert):
        """å¤„ç†å‘Šè­¦"""
        # æ£€æŸ¥å‘Šè­¦å†å²ï¼Œé¿å…é‡å¤å‘Šè­¦
        if self._should_suppress_alert(alert):
            return
        
        # è®°å½•å‘Šè­¦
        self.alert_history.append({
            'timestamp': datetime.now().isoformat(),
            'alert': alert
        })
        
        # å‘é€é€šçŸ¥
        self.send_notification(alert)
        
        # å°è¯•è‡ªåŠ¨æ¢å¤
        self.attempt_auto_recovery(alert)
    
    def _should_suppress_alert(self, alert):
        """æ£€æŸ¥æ˜¯å¦åº”è¯¥æŠ‘åˆ¶å‘Šè­¦"""
        # 5åˆ†é’Ÿå†…ç›¸åŒç±»å‹çš„å‘Šè­¦
        five_minutes_ago = datetime.now() - timedelta(minutes=5)
        recent_alerts = [a for a in self.alert_history 
                        if datetime.fromisoformat(a['timestamp']) > five_minutes_ago
                        and a['alert']['type'] == alert['type']]
        
        return len(recent_alerts) > 0
    
    def send_notification(self, alert):
        """å‘é€å‘Šè­¦é€šçŸ¥"""
        try:
            if alert['severity'] == 'critical':
                self._send_email_alert(alert)
            elif alert['severity'] == 'warning':
                self._send_slack_alert(alert)
            
            self.logger.warning(f"å‘Šè­¦å·²å‘é€: {alert['message']}")
            
        except Exception as e:
            self.logger.error(f"å‘é€å‘Šè­¦é€šçŸ¥å¤±è´¥: {e}")
    
    def _send_email_alert(self, alert):
        """å‘é€é‚®ä»¶å‘Šè­¦"""
        try:
            msg = MimeText(f"RabbitMQå‘Šè­¦: {alert['message']}")
            msg['Subject'] = f"RabbitMQ {alert['severity'].upper()} - {alert['metric']}"
            msg['From'] = self.config['email']['from']
            msg['To'] = self.config['email']['to']
            
            server = smtplib.SMTP(self.config['email']['smtp_server'], 587)
            server.starttls()
            server.login(self.config['email']['username'], self.config['email']['password'])
            server.send_message(msg)
            server.quit()
            
        except Exception as e:
            self.logger.error(f"å‘é€é‚®ä»¶å¤±è´¥: {e}")
    
    def _send_slack_alert(self, alert):
        """å‘é€Slackå‘Šè­¦"""
        try:
            webhook_url = self.config['slack']['webhook_url']
            payload = {
                'text': f"RabbitMQå‘Šè­¦: {alert['message']}",
                'color': 'danger' if alert['severity'] == 'critical' else 'warning'
            }
            
            requests.post(webhook_url, json=payload)
            
        except Exception as e:
            self.logger.error(f"å‘é€Slackå‘Šè­¦å¤±è´¥: {e}")
    
    def attempt_auto_recovery(self, alert):
        """å°è¯•è‡ªåŠ¨æ¢å¤"""
        try:
            if alert['type'] == 'system' and alert['metric'] == 'memory_usage':
                self._recover_memory_pressure()
            elif alert['type'] == 'rabbitmq' and alert['metric'] == 'api_connection':
                self._recover_rabbitmq_service()
                
        except Exception as e:
            self.logger.error(f"è‡ªåŠ¨æ¢å¤å¤±è´¥: {e}")
    
    def _recover_memory_pressure(self):
        """æ¢å¤å†…å­˜å‹åŠ›"""
        self.logger.info("æ‰§è¡Œå†…å­˜å‹åŠ›æ¢å¤...")
        # è¿™é‡Œå¯ä»¥æ·»åŠ å®é™…çš„æ¢å¤æ“ä½œ
        # ä¾‹å¦‚é‡å¯RabbitMQæœåŠ¡ã€æ¸…ç†ç¼“å­˜ç­‰
    
    def _recover_rabbitmq_service(self):
        """æ¢å¤RabbitMQæœåŠ¡"""
        self.logger.info("æ‰§è¡ŒRabbitMQæœåŠ¡æ¢å¤...")
        # è¿™é‡Œå¯ä»¥æ·»åŠ å®é™…çš„æ¢å¤æ“ä½œ
        # ä¾‹å¦‚é‡å¯æœåŠ¡ã€æ£€æŸ¥é…ç½®ç­‰
    
    def log_metrics(self, health_status, performance_metrics):
        """è®°å½•ç›‘æ§æŒ‡æ ‡"""
        log_data = {
            'timestamp': datetime.now().isoformat(),
            'health_status': health_status,
            'performance_metrics': performance_metrics
        }
        
        # ä¿å­˜åˆ°æ—¥å¿—æ–‡ä»¶
        with open('/var/log/rabbitmq_metrics.json', 'a') as f:
            f.write(json.dumps(log_data) + '\n')

def main():
    # é…ç½®
    config = {
        'rabbitmq_url': 'amqp://admin:password@localhost:5672',
        'rabbitmq_api_url': 'http://localhost:15672',
        'username': 'admin',
        'password': 'password',
        'email': {
            'smtp_server': 'smtp.company.com',
            'username': 'alerts@company.com',
            'password': 'password',
            'from': 'alerts@company.com',
            'to': 'admin@company.com'
        },
        'slack': {
            'webhook_url': 'https://hooks.slack.com/services/xxx/yyy/zzz'
        }
    }
    
    monitor = SmartMonitor(config)
    monitor.run_continuous_monitoring(interval=60)

if __name__ == '__main__':
    main()
```

## ğŸ“‹ æœ€ä½³å®è·µæ€»ç»“

### 1. é¢„é˜²æ€§ç»´æŠ¤æœ€ä½³å®è·µ

- **å®šæœŸå¥åº·æ£€æŸ¥**: æ¯æ—¥æ£€æŸ¥ç³»ç»Ÿèµ„æºï¼Œç›‘æ§RabbitMQçŠ¶æ€
- **å®¹é‡è§„åˆ’**: åŸºäºå¢é•¿ç‡é¢„æµ‹æœªæ¥éœ€æ±‚ï¼Œæå‰å‡†å¤‡æ‰©å®¹è®¡åˆ’
- **å¤‡ä»½ç­–ç•¥**: è‡ªåŠ¨å¤‡ä»½é…ç½®å’Œå…³é”®æ•°æ®ï¼Œå¼‚åœ°å­˜å‚¨å¤‡ä»½æ–‡ä»¶
- **æ€§èƒ½åŸºçº¿**: å»ºç«‹æ€§èƒ½åŸºå‡†å€¼ï¼Œå¿«é€Ÿè¯†åˆ«å¼‚å¸¸æƒ…å†µ

### 2. æ•…éšœå“åº”æœ€ä½³å®è·µ

- **åˆ†çº§å“åº”**: æ ¹æ®æ•…éšœä¸¥é‡ç¨‹åº¦åˆ¶å®šä¸åŒçš„å“åº”ç­–ç•¥
- **è‡ªåŠ¨åŒ–æ¢å¤**: ä¼˜å…ˆå°è¯•è‡ªåŠ¨æ¢å¤ï¼Œå‡å°‘äººå·¥å¹²é¢„æ—¶é—´
- **æ ¹å› åˆ†æ**: è¯¦ç»†åˆ†ææ•…éšœåŸå› ï¼Œåˆ¶å®šé•¿æœŸè§£å†³æ–¹æ¡ˆ
- **æ–‡æ¡£è®°å½•**: å®Œæ•´è®°å½•æ•…éšœå¤„ç†è¿‡ç¨‹ï¼Œå½¢æˆçŸ¥è¯†åº“

### 3. ç›‘æ§å‘Šè­¦æœ€ä½³å®è·µ

- **åˆç†é˜ˆå€¼**: åŸºäºå†å²æ•°æ®è®¾ç½®åˆç†çš„å‘Šè­¦é˜ˆå€¼
- **å‘Šè­¦æŠ‘åˆ¶**: é¿å…å‘Šè­¦é£æš´ï¼Œåªåœ¨çœŸæ­£éœ€è¦æ—¶å‘é€é€šçŸ¥
- **å¤šæ¸ é“é€šçŸ¥**: æ”¯æŒé‚®ä»¶ã€çŸ­ä¿¡ã€Slackç­‰å¤šç§é€šçŸ¥æ–¹å¼
- **å‘Šè­¦èšåˆ**: å°†ç›¸å…³å‘Šè­¦èšåˆï¼Œå‡å°‘ä¿¡æ¯å™ªéŸ³

### 4. ç¾éš¾æ¢å¤æœ€ä½³å®è·µ

- **å¤šåœ°ç‚¹éƒ¨ç½²**: åœ¨ä¸åŒæ•°æ®ä¸­å¿ƒéƒ¨ç½²ç¾å¤‡ç¯å¢ƒ
- **å®šæœŸæ¼”ç»ƒ**: å®šæœŸè¿›è¡Œç¾éš¾æ¢å¤æ¼”ç»ƒï¼Œç¡®ä¿æµç¨‹å¯è¡Œ
- **RTO/RPOç›®æ ‡**: æ˜ç¡®æ¢å¤æ—¶é—´ç›®æ ‡å’Œæ•°æ®ä¸¢å¤±å®¹å¿åº¦
- **æ–‡æ¡£æ›´æ–°**: åŠæ—¶æ›´æ–°ç¾éš¾æ¢å¤æ–‡æ¡£ï¼Œç¡®ä¿ä¿¡æ¯çš„æ—¶æ•ˆæ€§

é€šè¿‡æœ¬ç« çš„å­¦ä¹ ï¼Œæ‚¨å°†æŒæ¡RabbitMQæ•…éšœå¤„ç†ä¸æ¢å¤çš„å…¨å¥—è§£å†³æ–¹æ¡ˆï¼Œèƒ½å¤Ÿåœ¨ç”Ÿäº§ç¯å¢ƒä¸­å¿«é€Ÿå“åº”æ•…éšœã€æ¢å¤æœåŠ¡ï¼Œå¹¶å»ºç«‹å®Œå–„çš„é¢„é˜²æœºåˆ¶ã€‚