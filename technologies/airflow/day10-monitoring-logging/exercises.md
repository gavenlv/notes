# Day 10: ç›‘æ§ä¸æ—¥å¿— - å®è·µç»ƒä¹ 

## ğŸ¯ ç»ƒä¹ ç›®æ ‡

é€šè¿‡å®è·µç»ƒä¹ ï¼ŒæŒæ¡Airflowç›‘æ§ä¸æ—¥å¿—ç®¡ç†çš„æ ¸å¿ƒæŠ€èƒ½ï¼š
1. é…ç½®å’Œç®¡ç†Airflowæ—¥å¿—ç³»ç»Ÿ
2. æ”¶é›†å’Œåˆ†æç³»ç»Ÿç›‘æ§æŒ‡æ ‡
3. å®ç°è‡ªå®šä¹‰å‘Šè­¦æœºåˆ¶
4. è¿›è¡Œæ•…éšœæ’æŸ¥å’Œæ€§èƒ½ä¼˜åŒ–

## ğŸ‹ï¸â€â™‚ï¸ åŸºç¡€ç»ƒä¹ 

### ç»ƒä¹ 1: é…ç½®æ—¥å¿—ç³»ç»Ÿ

**ç›®æ ‡**: é…ç½®Airflowä»¥å°†æ—¥å¿—å­˜å‚¨åˆ°è¿œç¨‹å­˜å‚¨ï¼ˆå¦‚S3ï¼‰

**æ­¥éª¤**:
1. åˆ›å»ºä¸€ä¸ªS3å­˜å‚¨æ¡¶ç”¨äºå­˜å‚¨æ—¥å¿—
2. é…ç½®Airflowè¿æ¥ä»¥è®¿é—®S3
3. ä¿®æ”¹`airflow.cfg`æ–‡ä»¶å¯ç”¨è¿œç¨‹æ—¥å¿—å­˜å‚¨
4. éªŒè¯æ—¥å¿—æ˜¯å¦æ­£ç¡®å†™å…¥S3

**é…ç½®ç¤ºä¾‹**:
```ini
[logging]
remote_logging = True
remote_base_log_folder = s3://your-bucket/airflow-logs
remote_log_conn_id = aws_s3_logs
delete_local_logs = False
```

**éªŒè¯æ–¹æ³•**:
- è¿è¡Œä¸€ä¸ªç®€å•çš„DAG
- æ£€æŸ¥S3å­˜å‚¨æ¡¶ä¸­æ˜¯å¦ç”Ÿæˆäº†æ—¥å¿—æ–‡ä»¶
- åœ¨Airflow Web UIä¸­æŸ¥çœ‹ä»»åŠ¡æ—¥å¿—

### ç»ƒä¹ 2: åˆ†æä»»åŠ¡æ—¥å¿—

**ç›®æ ‡**: ç¼–å†™è„šæœ¬åˆ†æä»»åŠ¡æ—¥å¿—ï¼Œè¯†åˆ«å¸¸è§é”™è¯¯æ¨¡å¼

**æ­¥éª¤**:
1. åˆ›å»ºä¸€ä¸ªPythonè„šæœ¬ç”¨äºåˆ†æAirflowæ—¥å¿—
2. å®ç°é”™è¯¯æ¨¡å¼è¯†åˆ«åŠŸèƒ½
3. ç»Ÿè®¡ä¸åŒç±»å‹é”™è¯¯çš„å‘ç”Ÿé¢‘ç‡
4. ç”Ÿæˆé”™è¯¯åˆ†ææŠ¥å‘Š

**ä»£ç ç¤ºä¾‹**:
```python
import os
import re
from collections import defaultdict
from datetime import datetime, timedelta

class LogAnalyzer:
    def __init__(self, log_directory):
        self.log_directory = log_directory
        self.error_patterns = {
            'connection_error': r'(Connection|connection).*error',
            'timeout_error': r'(Timeout|timeout)',
            'memory_error': r'(Memory|memory).*error',
            'syntax_error': r'SyntaxError',
            'import_error': r'ImportError|ModuleNotFoundError'
        }
    
    def analyze_recent_logs(self, days=7):
        """åˆ†ææœ€è¿‘å‡ å¤©çš„æ—¥å¿—"""
        cutoff_date = datetime.now() - timedelta(days=days)
        error_stats = defaultdict(int)
        
        for root, dirs, files in os.walk(self.log_directory):
            for file in files:
                if file.endswith('.log'):
                    file_path = os.path.join(root, file)
                    file_mtime = datetime.fromtimestamp(os.path.getmtime(file_path))
                    
                    if file_mtime >= cutoff_date:
                        self._analyze_file(file_path, error_stats)
        
        return dict(error_stats)
    
    def _analyze_file(self, file_path, error_stats):
        """åˆ†æå•ä¸ªæ—¥å¿—æ–‡ä»¶"""
        try:
            with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:
                for line in f:
                    for error_type, pattern in self.error_patterns.items():
                        if re.search(pattern, line, re.IGNORECASE):
                            error_stats[error_type] += 1
        except Exception as e:
            print(f"Error reading {file_path}: {e}")

# ä½¿ç”¨ç¤ºä¾‹
analyzer = LogAnalyzer('/path/to/airflow/logs')
stats = analyzer.analyze_recent_logs(days=7)
print("Error statistics:", stats)
```

## ğŸ‹ï¸â€â™‚ï¸ è¿›é˜¶ç»ƒä¹ 

### ç»ƒä¹ 3: é›†æˆPrometheusç›‘æ§

**ç›®æ ‡**: é…ç½®Airflowä»¥é€šè¿‡Prometheusç«¯ç‚¹æš´éœ²æŒ‡æ ‡

**æ­¥éª¤**:
1. å®‰è£…å’Œé…ç½®PrometheusæœåŠ¡å™¨
2. é…ç½®Airflowå¯ç”¨PrometheusæŒ‡æ ‡
3. åˆ›å»ºGrafanaä»ªè¡¨æ¿å±•ç¤ºå…³é”®æŒ‡æ ‡
4. è®¾ç½®å‘Šè­¦è§„åˆ™

**é…ç½®ç¤ºä¾‹**:
```ini
[metrics]
metrics_allow_list = 
metrics_block_list = 
statsd_on = False
statsd_host = localhost
statsd_port = 8125
statsd_prefix = airflow

# å¯ç”¨Flask exporter
flask_monitoringdashboard_host = 0.0.0.0
flask_monitoringdashboard_port = 5005
```

**Prometheusé…ç½®**:
```yaml
scrape_configs:
  - job_name: 'airflow'
    static_configs:
      - targets: ['airflow-webserver:8080']
```

### ç»ƒä¹ 4: å®ç°è‡ªå®šä¹‰å‘Šè­¦ç³»ç»Ÿ

**ç›®æ ‡**: å¼€å‘ä¸€ä¸ªè‡ªå®šä¹‰å‘Šè­¦ç³»ç»Ÿï¼ŒåŸºäºç‰¹å®šæ¡ä»¶å‘é€å‘Šè­¦

**æ­¥éª¤**:
1. åˆ›å»ºä¸€ä¸ªPythonç±»ç”¨äºç›‘æ§å…³é”®æŒ‡æ ‡
2. å®ç°å¤šç§å‘Šè­¦æ¡ä»¶ï¼ˆå¦‚å¤±è´¥ç‡ã€å»¶è¿Ÿç­‰ï¼‰
3. é›†æˆå¤šç§é€šçŸ¥æ¸ é“ï¼ˆé‚®ä»¶ã€Slackã€Webhookç­‰ï¼‰
4. é…ç½®å‘Šè­¦æŠ‘åˆ¶å’Œå»é‡æœºåˆ¶

**ä»£ç ç¤ºä¾‹**:
```python
from datetime import datetime, timedelta
from airflow.models import DagRun, TaskInstance
from airflow.utils.state import State
import requests
import smtplib
from email.mime.text import MIMEText

class CustomAlertingSystem:
    def __init__(self, config):
        self.config = config
        self.notification_channels = {
            'email': self._send_email,
            'slack': self._send_slack,
            'webhook': self._send_webhook
        }
    
    def check_dag_health(self, dag_id, time_window_hours=24):
        """æ£€æŸ¥DAGå¥åº·çŠ¶å†µ"""
        cutoff_time = datetime.now() - timedelta(hours=time_window_hours)
        
        # è·å–DAGè¿è¡Œç»Ÿè®¡
        runs = DagRun.query.filter(
            DagRun.dag_id == dag_id,
            DagRun.start_date >= cutoff_time
        ).all()
        
        total_runs = len(runs)
        if total_runs == 0:
            return
        
        failed_runs = sum(1 for run in runs if run.state == State.FAILED)
        success_rate = (total_runs - failed_runs) / total_runs if total_runs > 0 else 0
        
        # æ£€æŸ¥å¤±è´¥ç‡æ˜¯å¦è¶…è¿‡é˜ˆå€¼
        if success_rate < self.config.get('success_rate_threshold', 0.95):
            self._trigger_alert(
                'DAG_HEALTH',
                f"DAG {dag_id} success rate dropped to {success_rate:.2%}",
                {'dag_id': dag_id, 'success_rate': success_rate, 'total_runs': total_runs}
            )
    
    def check_task_delays(self, dag_id, delay_threshold_minutes=30):
        """æ£€æŸ¥ä»»åŠ¡å»¶è¿Ÿ"""
        cutoff_time = datetime.now() - timedelta(hours=24)
        
        delayed_tasks = TaskInstance.query.filter(
            TaskInstance.dag_id == dag_id,
            TaskInstance.start_date >= cutoff_time,
            TaskInstance.start_date > TaskInstance.execution_date + timedelta(minutes=delay_threshold_minutes)
        ).all()
        
        if delayed_tasks:
            self._trigger_alert(
                'TASK_DELAY',
                f"Found {len(delayed_tasks)} delayed tasks in DAG {dag_id}",
                {'dag_id': dag_id, 'delayed_count': len(delayed_tasks)}
            )
    
    def _trigger_alert(self, alert_type, message, details):
        """è§¦å‘å‘Šè­¦"""
        alert_data = {
            'type': alert_type,
            'message': message,
            'details': details,
            'timestamp': datetime.now().isoformat()
        }
        
        # é€šè¿‡æ‰€æœ‰é…ç½®çš„æ¸ é“å‘é€å‘Šè­¦
        for channel in self.config.get('notification_channels', ['email']):
            if channel in self.notification_channels:
                try:
                    self.notification_channels[channel](alert_data)
                except Exception as e:
                    print(f"Failed to send alert via {channel}: {e}")
    
    def _send_email(self, alert_data):
        """å‘é€é‚®ä»¶å‘Šè­¦"""
        msg = MIMEText(f"Alert: {alert_data['message']}\nDetails: {alert_data['details']}")
        msg['Subject'] = f"Airflow Alert - {alert_data['type']}"
        msg['From'] = self.config.get('email_from', 'airflow@example.com')
        msg['To'] = ', '.join(self.config.get('email_to', []))
        
        smtp = smtplib.SMTP(self.config.get('smtp_host', 'localhost'))
        smtp.send_message(msg)
        smtp.quit()
    
    def _send_slack(self, alert_data):
        """å‘é€Slackå‘Šè­¦"""
        webhook_url = self.config.get('slack_webhook_url')
        if not webhook_url:
            return
        
        payload = {
            'text': f"Airflow Alert: {alert_data['message']}",
            'attachments': [{
                'color': 'danger',
                'fields': [
                    {'title': 'Type', 'value': alert_data['type'], 'short': True},
                    {'title': 'Time', 'value': alert_data['timestamp'], 'short': True},
                    {'title': 'Details', 'value': str(alert_data['details'])}
                ]
            }]
        }
        
        requests.post(webhook_url, json=payload)
    
    def _send_webhook(self, alert_data):
        """å‘é€Webhookå‘Šè­¦"""
        webhook_url = self.config.get('webhook_url')
        if not webhook_url:
            return
        
        requests.post(webhook_url, json=alert_data)

# ä½¿ç”¨ç¤ºä¾‹
config = {
    'success_rate_threshold': 0.95,
    'notification_channels': ['email', 'slack'],
    'email_from': 'airflow@example.com',
    'email_to': ['admin@example.com'],
    'smtp_host': 'smtp.example.com',
    'slack_webhook_url': 'https://hooks.slack.com/services/YOUR/SLACK/WEBHOOK'
}

alerting_system = CustomAlertingSystem(config)
alerting_system.check_dag_health('example_dag')
alerting_system.check_task_delays('example_dag')
```

## ğŸ‹ï¸â€â™‚ï¸ æŒ‘æˆ˜ç»ƒä¹ 

### ç»ƒä¹ 5: æ„å»ºå®æ—¶ç›‘æ§ä»ªè¡¨æ¿

**ç›®æ ‡**: ä½¿ç”¨Flaskå’ŒWebSocketæ„å»ºä¸€ä¸ªå®æ—¶ç›‘æ§Airflowçš„Webä»ªè¡¨æ¿

**æ­¥éª¤**:
1. åˆ›å»ºFlaskåº”ç”¨æä¾›å®æ—¶ç›‘æ§ç•Œé¢
2. å®ç°WebSocketè¿æ¥æ¨é€å®æ—¶æ•°æ®
3. å±•ç¤ºå…³é”®æŒ‡æ ‡ï¼ˆè¿è¡Œä¸­çš„ä»»åŠ¡ã€é˜Ÿåˆ—é•¿åº¦ç­‰ï¼‰
4. æ·»åŠ äº¤äº’å¼å›¾è¡¨å’Œå‘Šè­¦é¢æ¿

**ä»£ç ç¤ºä¾‹**:
```python
from flask import Flask, render_template
from flask_socketio import SocketIO, emit
from airflow.models import DagRun, TaskInstance
from airflow.utils.state import State
import threading
import time

app = Flask(__name__)
app.config['SECRET_KEY'] = 'your-secret-key'
socketio = SocketIO(app, cors_allowed_origins="*")

def background_monitor():
    """åå°ç›‘æ§çº¿ç¨‹"""
    while True:
        # è·å–å®æ—¶æ•°æ®
        active_runs = DagRun.query.filter(DagRun.state == State.RUNNING).count()
        queued_tasks = TaskInstance.query.filter(TaskInstance.state == State.QUEUED).count()
        running_tasks = TaskInstance.query.filter(TaskInstance.state == State.RUNNING).count()
        
        # å‘é€æ•°æ®åˆ°å‰ç«¯
        socketio.emit('metrics_update', {
            'active_runs': active_runs,
            'queued_tasks': queued_tasks,
            'running_tasks': running_tasks,
            'timestamp': time.time()
        })
        
        time.sleep(5)  # æ¯5ç§’æ›´æ–°ä¸€æ¬¡

@app.route('/')
def index():
    return render_template('dashboard.html')

@socketio.on('connect')
def handle_connect():
    print('Client connected')

if __name__ == '__main__':
    # å¯åŠ¨åå°ç›‘æ§çº¿ç¨‹
    monitor_thread = threading.Thread(target=background_monitor)
    monitor_thread.daemon = True
    monitor_thread.start()
    
    # å¯åŠ¨Flaskåº”ç”¨
    socketio.run(app, host='0.0.0.0', port=5000, debug=True)
```

### ç»ƒä¹ 6: æ€§èƒ½ä¼˜åŒ–åˆ†æ

**ç›®æ ‡**: åˆ†æAirflowæ€§èƒ½ç“¶é¢ˆå¹¶æå‡ºä¼˜åŒ–æ–¹æ¡ˆ

**æ­¥éª¤**:
1. ä½¿ç”¨æ€§èƒ½åˆ†æå·¥å…·ï¼ˆå¦‚py-spyï¼‰åˆ†æAirflowè¿›ç¨‹
2. è¯†åˆ«æ€§èƒ½ç“¶é¢ˆï¼ˆæ•°æ®åº“æŸ¥è¯¢ã€ç½‘ç»œå»¶è¿Ÿç­‰ï¼‰
3. å®æ–½ä¼˜åŒ–æªæ–½ï¼ˆç´¢å¼•ä¼˜åŒ–ã€ç¼“å­˜ç­‰ï¼‰
4. éªŒè¯ä¼˜åŒ–æ•ˆæœ

**åˆ†æè„šæœ¬**:
```python
import time
import psutil
from airflow.models import DagRun, TaskInstance
from airflow.utils.session import provide_session
from sqlalchemy import text

class PerformanceAnalyzer:
    def __init__(self):
        self.metrics = {}
    
    def measure_database_performance(self):
        """æµ‹é‡æ•°æ®åº“æ€§èƒ½"""
        start_time = time.time()
        
        # æµ‹é‡å¤æ‚æŸ¥è¯¢æ€§èƒ½
        with provide_session() as session:
            # æ¨¡æ‹Ÿå¤æ‚æŸ¥è¯¢
            result = session.execute(text("""
                SELECT dr.dag_id, COUNT(ti.task_id) as task_count
                FROM dag_run dr
                JOIN task_instance ti ON dr.dag_id = ti.dag_id
                WHERE dr.execution_date > NOW() - INTERVAL 7 DAY
                GROUP BY dr.dag_id
                ORDER BY task_count DESC
                LIMIT 10
            """))
            rows = result.fetchall()
        
        end_time = time.time()
        self.metrics['database_query_time'] = end_time - start_time
        self.metrics['result_rows'] = len(rows)
        
        return self.metrics
    
    def measure_system_resources(self):
        """æµ‹é‡ç³»ç»Ÿèµ„æºä½¿ç”¨æƒ…å†µ"""
        process = psutil.Process()
        
        self.metrics['cpu_percent'] = process.cpu_percent()
        self.metrics['memory_mb'] = process.memory_info().rss / 1024 / 1024
        self.metrics['threads'] = process.num_threads()
        
        return self.metrics
    
    def generate_performance_report(self):
        """ç”Ÿæˆæ€§èƒ½æŠ¥å‘Š"""
        self.measure_database_performance()
        self.measure_system_resources()
        
        report = f"""
        Airflow Performance Report
        ==========================
        
        Database Performance:
        - Query Time: {self.metrics.get('database_query_time', 0):.4f} seconds
        - Result Rows: {self.metrics.get('result_rows', 0)}
        
        System Resources:
        - CPU Usage: {self.metrics.get('cpu_percent', 0):.2f}%
        - Memory Usage: {self.metrics.get('memory_mb', 0):.2f} MB
        - Thread Count: {self.metrics.get('threads', 0)}
        
        Recommendations:
        """
        
        # åŸºäºæµ‹é‡ç»“æœæä¾›ä¼˜åŒ–å»ºè®®
        if self.metrics.get('database_query_time', 0) > 1.0:
            report += "- Consider adding database indexes for frequently queried columns\n"
        
        if self.metrics.get('memory_mb', 0) > 500:
            report += "- Monitor memory usage and consider optimizing data processing\n"
        
        return report

# ä½¿ç”¨ç¤ºä¾‹
analyzer = PerformanceAnalyzer()
report = analyzer.generate_performance_report()
print(report)
```

## ğŸ“ æ€»ç»“

å®Œæˆè¿™äº›ç»ƒä¹ åï¼Œä½ åº”è¯¥èƒ½å¤Ÿï¼š
- é…ç½®å’Œç®¡ç†Airflowæ—¥å¿—ç³»ç»Ÿ
- æ”¶é›†å’Œåˆ†æç³»ç»Ÿç›‘æ§æŒ‡æ ‡
- å®ç°è‡ªå®šä¹‰å‘Šè­¦æœºåˆ¶
- è¿›è¡Œæ•…éšœæ’æŸ¥å’Œæ€§èƒ½ä¼˜åŒ–
- æ„å»ºå®æ—¶ç›‘æ§ä»ªè¡¨æ¿

è®°å¾—åœ¨æ¯ä¸ªç»ƒä¹ åæ›´æ–°ä½ çš„å­¦ä¹ æ€»ç»“ï¼